<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 19]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Jailbreaking LLMs via Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge](https://arxiv.org/abs/2510.01223)
*Hui Dou,Ning Xu,Yiwen Zhang,Kaibin Wang*

Main category: cs.CR

TL;DR: 本文提出了RTS-Attack攻击框架，通过构建与查询高度语义相关的嵌套场景并整合针对性有害知识，成功绕过LLMs的对齐防御，且生成的越狱提示具有良好的隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有嵌套场景方法虽然潜力巨大，但由于恶意意图明显容易被检测。作者发现LLMs的对齐防御对语义相关且包含针对性有害知识的嵌套场景不敏感，这是一个重要但未被充分探索的方向。

Method: 提出RTS-Attack框架，构建与查询高度语义相关的嵌套场景，并整合针对性有害知识，生成不含有害查询的越狱提示。

Result: 在GPT-4o、Llama3-70b和Gemini-pro等多种先进LLMs上的实验表明，RTS-Attack在效率和通用性方面均优于基线方法。

Conclusion: RTS-Attack能够有效绕过LLMs的对齐防御，揭示了当前对齐机制在语义相关嵌套场景下的脆弱性，为LLMs安全研究提供了重要洞见。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various tasks. However, they remain exposed to jailbreak attacks, eliciting
harmful responses. The nested scenario strategy has been increasingly adopted
across various methods, demonstrating immense potential. Nevertheless, these
methods are easily detectable due to their prominent malicious intentions. In
this work, we are the first to find and systematically verify that LLMs'
alignment defenses are not sensitive to nested scenarios, where these scenarios
are highly semantically relevant to the queries and incorporate targeted toxic
knowledge. This is a crucial yet insufficiently explored direction. Based on
this, we propose RTS-Attack (Semantically Relevant Nested Scenarios with
Targeted Toxic Knowledge), an adaptive and automated framework to examine LLMs'
alignment. By building scenarios highly relevant to the queries and integrating
targeted toxic knowledge, RTS-Attack bypasses the alignment defenses of LLMs.
Moreover, the jailbreak prompts generated by RTS-Attack are free from harmful
queries, leading to outstanding concealment. Extensive experiments demonstrate
that RTS-Attack exhibits superior performance in both efficiency and
universality compared to the baselines across diverse advanced LLMs, including
GPT-4o, Llama3-70b, and Gemini-pro. Our complete code is available in the
supplementary material. WARNING: THIS PAPER CONTAINS POTENTIALLY HARMFUL
CONTENT.

</details>


### [2] [Fine-Tuning Jailbreaks under Highly Constrained Black-Box Settings: A Three-Pronged Approach](https://arxiv.org/abs/2510.01342)
*Xiangfang Li,Yu Wang,Bo Li*

Main category: cs.CR

TL;DR: 提出了一种三管齐下的越狱攻击方法，通过在仅数据黑盒微调场景下结合安全风格包装、良性词汇编码和后门机制，成功绕过提供商的多阶段防御。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，确保其安全使用变得至关重要。现有的越狱攻击研究多关注过于简化的攻击场景，缺乏对现实世界防御环境的实际相关性。

Method: 采用三管齐下的攻击策略：安全风格的前缀/后缀包装、敏感词的良性词汇编码（下划线）以及后门机制，使模型在单个数据点看似无害的情况下学习有害行为。

Result: 在真实部署中，该方法成功越狱了OpenAI平台上的GPT-4.1和GPT-4o模型，两种模型的攻击成功率均超过97%。

Conclusion: 该研究表明当前基于数据过滤、防御性微调和安全审计的多阶段防御机制存在严重漏洞，需要更强大的安全措施来应对此类复杂攻击。

Abstract: With the rapid advancement of large language models (LLMs), ensuring their
safe use becomes increasingly critical. Fine-tuning is a widely used method for
adapting models to downstream tasks, yet it is vulnerable to jailbreak attacks.
However, most existing studies focus on overly simplified attack scenarios,
limiting their practical relevance to real-world defense settings. To make this
risk concrete, we present a three-pronged jailbreak attack and evaluate it
against provider defenses under a dataset-only black-box fine-tuning interface.
In this setting, the attacker can only submit fine-tuning data to the provider,
while the provider may deploy defenses across stages: (1) pre-upload data
filtering, (2) training-time defensive fine-tuning, and (3) post-training
safety audit. Our attack combines safety-styled prefix/suffix wrappers, benign
lexical encodings (underscoring) of sensitive tokens, and a backdoor mechanism,
enabling the model to learn harmful behaviors while individual datapoints
appear innocuous. Extensive experiments demonstrate the effectiveness of our
approach. In real-world deployment, our method successfully jailbreaks GPT-4.1
and GPT-4o on the OpenAI platform with attack success rates above 97% for both
models. Our code is available at
https://github.com/lxf728/tri-pronged-ft-attack.

</details>


### [3] [Integrated Security Mechanisms for Weight Protection in Memristive Crossbar Arrays](https://arxiv.org/abs/2510.01350)
*Muhammad Faheemur Rahman,Wayne Burleson*

Main category: cs.CR

TL;DR: 提出了两种安全机制来保护忆阻器交叉阵列中的权重数据：键控置换器和水印保护列，在保持高性能的同时提供强大的安全保护


<details>
  <summary>Details</summary>
Motivation: 忆阻器交叉阵列虽然适合内存计算，但面临安全威胁，特别是当硬件被攻破时存储的权重可能被窃取。这些权重代表宝贵的知识产权，需要保护

Method: 使用键控置换器对权重进行随机重新排列，以及添加水印保护列来建立可验证的所有权。这两种机制都能与现有忆阻器架构高效集成

Result: 在45nm、22nm和7nm CMOS节点上的仿真显示，两种机制在面积、延迟和功耗方面的开销均低于10%，同时提供强大的安全保护

Conclusion: 该方案证明了在忆阻内存计算系统中实现安全保护的可行性，且性能损失最小，为保护机器学习模型知识产权提供了有效解决方案

Abstract: Memristive crossbar arrays enable in-memory computing by performing parallel
analog computations directly within memory, making them well-suited for machine
learning, neural networks, and neuromorphic systems. However, despite their
advantages, non-volatile memristors are vulnerable to security threats (such as
adversarial extraction of stored weights when the hardware is compromised.
Protecting these weights is essential since they represent valuable
intellectual property resulting from lengthy and costly training processes
using large, often proprietary, datasets. As a solution we propose two security
mechanisms: Keyed Permutor and Watermark Protection Columns; where both
safeguard critical weights and establish verifiable ownership (even in cases of
data leakage). Our approach integrates efficiently with existing memristive
crossbar architectures without significant design modifications. Simulations
across 45nm, 22nm, and 7nm CMOS nodes, using a realistic interconnect model and
a large RF dataset, show that both mechanisms offer robust protection with
under 10% overhead in area, delay and power. We also present initial
experiments employing the widely known MNIST dataset; further highlighting the
feasibility of securing memristive in-memory computing systems with minimal
performance trade-offs.

</details>


### [4] [WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents](https://arxiv.org/abs/2510.01354)
*Yinuo Liu,Ruohan Xu,Xilong Wang,Yuqi Jia,Neil Zhenqiang Gong*

Main category: cs.CR

TL;DR: 首次针对网络代理的提示注入攻击检测进行全面基准研究，提出了细粒度攻击分类并构建了包含恶意和良性样本的数据集，评估了文本和图像检测方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未系统评估针对网络代理的提示注入攻击检测，需要填补这一研究空白。

Method: 提出细粒度攻击分类，构建包含恶意/良性文本和图像样本的数据集，系统化文本和图像检测方法并进行多场景评估。

Result: 部分检测器能识别依赖显式文本指令或可见图像扰动的攻击，但对省略显式指令或使用不可察觉扰动的攻击基本失效。

Conclusion: 现有检测方法在复杂攻击场景下存在局限性，需要开发更鲁棒的检测技术。

Abstract: Multiple prompt injection attacks have been proposed against web agents. At
the same time, various methods have been developed to detect general prompt
injection attacks, but none have been systematically evaluated for web agents.
In this work, we bridge this gap by presenting the first comprehensive
benchmark study on detecting prompt injection attacks targeting web agents. We
begin by introducing a fine-grained categorization of such attacks based on the
threat model. We then construct datasets containing both malicious and benign
samples: malicious text segments generated by different attacks, benign text
segments from four categories, malicious images produced by attacks, and benign
images from two categories. Next, we systematize both text-based and
image-based detection methods. Finally, we evaluate their performance across
multiple scenarios. Our key findings show that while some detectors can
identify attacks that rely on explicit textual instructions or visible image
perturbations with moderate to high accuracy, they largely fail against attacks
that omit explicit instructions or employ imperceptible perturbations. Our
datasets and code are released at:
https://github.com/Norrrrrrr-lyn/WAInjectBench.

</details>


### [5] [Breaking the Code: Security Assessment of AI Code Agents Through Systematic Jailbreaking Attacks](https://arxiv.org/abs/2510.01359)
*Shoumik Saha,Jifan Chen,Sam Mayers,Sanjay Krishna Gouda,Zijian Wang,Varun Kumar*

Main category: cs.CR

TL;DR: JAWS-BENCH是一个评估代码能力LLM代理安全性的基准测试，涵盖三个攻击场景，发现代码代理在单文件和多文件环境中攻击成功率分别达到71%和75%，32%的攻击代码可直接部署。


<details>
  <summary>Details</summary>
Motivation: 随着代码能力LLM代理被集成到软件工程工作流中，传统的文本拒绝检测已不足以评估真实风险，需要测试代理是否会实际编译和运行恶意程序。

Method: 构建JAWS-BENCH基准，包含三个递增的工作空间场景（空、单文件、多文件），并设计分层可执行感知的评估框架，测试合规性、攻击成功率、语法正确性和运行时可执行性。

Result: 在JAWS-0中，代码代理平均接受61%的攻击，27%能端到端运行；在JAWS-1中攻击成功率约71%；在JAWS-M中达75%，32%攻击代码可立即部署。将LLM包装为代理使漏洞增加1.6倍。

Conclusion: 需要执行感知的防御措施、代码上下文安全过滤器，以及在整个代理多步推理和工具使用过程中保持拒绝决策的机制。

Abstract: Code-capable large language model (LLM) agents are increasingly embedded into
software engineering workflows where they can read, write, and execute code,
raising the stakes of safety-bypass ("jailbreak") attacks beyond text-only
settings. Prior evaluations emphasize refusal or harmful-text detection,
leaving open whether agents actually compile and run malicious programs. We
present JAWS-BENCH (Jailbreaks Across WorkSpaces), a benchmark spanning three
escalating workspace regimes that mirror attacker capability: empty (JAWS-0),
single-file (JAWS-1), and multi-file (JAWS-M). We pair this with a
hierarchical, executable-aware Judge Framework that tests (i) compliance, (ii)
attack success, (iii) syntactic correctness, and (iv) runtime executability,
moving beyond refusal to measure deployable harm. Using seven LLMs from five
families as backends, we find that under prompt-only conditions in JAWS-0, code
agents accept 61% of attacks on average; 58% are harmful, 52% parse, and 27%
run end-to-end. Moving to single-file regime in JAWS-1 drives compliance to ~
100% for capable models and yields a mean ASR (Attack Success Rate) ~ 71%; the
multi-file regime (JAWS-M) raises mean ASR to ~ 75%, with 32% instantly
deployable attack code. Across models, wrapping an LLM in an agent
substantially increases vulnerability -- ASR raises by 1.6x -- because initial
refusals are frequently overturned during later planning/tool-use steps.
Category-level analyses identify which attack classes are most vulnerable and
most readily deployable, while others exhibit large execution gaps. These
findings motivate execution-aware defenses, code-contextual safety filters, and
mechanisms that preserve refusal decisions throughout the agent's multi-step
reasoning and tool use.

</details>


### [6] [E-FuzzEdge: Optimizing Embedded Device Security with Scalable In-Place Fuzzing](https://arxiv.org/abs/2510.01393)
*Davide Rusconi,Osama Yousef,Mirco Picca,Flavio Toffalini,Andrea Lanzi*

Main category: cs.CR

TL;DR: E-FuzzEdge是一种针对微控制器模糊测试的新型架构，通过优化执行速度来解决硬件在环模糊测试的效率问题，在可扩展性受限的环境中显著提升测试吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决微控制器硬件在环模糊测试中的效率低下问题，特别是在可扩展性不可用的场景下，提高模糊测试活动的吞吐量。

Method: 开发E-FuzzEdge架构，优化执行速度，专注于提升微控制器模糊测试的性能，并与执行设备测试而非固件仿真的嵌入式模糊测试技术兼容。

Result: 与最先进的基准测试相比，E-FuzzEdge展现出显著的性能改进，能够有效提升模糊测试活动的吞吐量。

Conclusion: E-FuzzEdge架构具有与现有嵌入式模糊测试技术的兼容性，可以被更广泛的嵌入式模糊测试社区集成到工作流程中，从而提升整体测试效率。

Abstract: In this paper we show E-FuzzEdge, a novel fuzzing architecture targeted
towards improving the throughput of fuzzing campaigns in contexts where
scalability is unavailable. E-FuzzEdge addresses the inefficiencies of
hardware-in-the-loop fuzzing for microcontrollers by optimizing execution
speed. We evaluated our system against state-of-the-art benchmarks,
demonstrating significant performance improvements. A key advantage of
E-FuzzEdgearchitecture is its compatibility with other embedded fuzzing
techniques that perform on device testing instead of firmware emulation. This
means that the broader embedded fuzzing community can integrate E-FuzzEdge into
their workflows to enhance overall testing efficiency.

</details>


### [7] [Securing IoT Devices in Smart Cities: A Review of Proposed Solutions](https://arxiv.org/abs/2510.01445)
*Andrés F. Betancur-López*

Main category: cs.CR

TL;DR: 本文综述了智能城市中物联网设备的安全保护方案，重点分析了轻量级密码学、物理不可克隆函数和区块链技术，指出了现有方法的优缺点及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 智能城市中的物联网设备由于计算资源有限且广泛部署，面临严重的安全威胁，需要有效的安全保护机制来保障用户隐私和数据安全。

Method: 通过分析近期关于设备级安全的文献，特别关注轻量级密码学、物理不可克隆函数和基于区块链的解决方案。

Result: 研究发现现有方法各有优缺点，轻量级密码学适合资源受限设备，物理不可克隆函数提供硬件级安全，区块链技术增强数据完整性，但都存在可扩展性和资源效率方面的挑战。

Conclusion: 需要开发更实用、可扩展且资源高效的安全机制，以确保护物联网生态系统中的用户隐私和数据保护。

Abstract: Privacy and security in Smart Cities remain at constant risk due to the
vulnerabilities introduced by Internet of Things (IoT) devices. The limited
computational resources of these devices make them especially susceptible to
attacks, while their widespread adoption increases the potential impact of
security breaches. This article presents a review of security proposals aimed
at protecting IoT devices in Smart City environments. The review was conducted
by analyzing recent literature on device-level security, with particular
emphasis on lightweight cryptography, physically unclonable functions (PUFs),
and blockchain-based solutions. Findings highlight both the strengths and
limitations of current approaches, as well as the need for more practical,
scalable, and resource-efficient mechanisms to ensure user privacy and data
protection in IoT ecosystems.

</details>


### [8] [POLAR: Automating Cyber Threat Prioritization through LLM-Powered Assessment](https://arxiv.org/abs/2510.01552)
*Luoxi Tang,Yuqiao Meng,Ankita Patra,Weicheng Ma,Muchao Ye,Zhaohan Xi*

Main category: cs.CR

TL;DR: 本文研究了LLM在网络安全威胁情报(CTI)应用中的内在脆弱性，揭示了三个主要问题：伪相关、知识矛盾性和受限泛化能力，这些限制了LLM在CTI任务中的实际效果。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM被广泛用于支持网络安全分析，但在实际部署中存在显著性能差距。本文旨在探究LLM在CTI中的内在脆弱性，这些挑战源于威胁环境本身而非模型架构。

Method: 采用大规模评估，涵盖多个CTI基准测试和真实威胁报告，引入集成分层、自回归精化和人机协同监督的新型分类方法，可靠分析失败案例。

Result: 通过广泛实验和人工检查，揭示了三个基本脆弱性：伪相关、矛盾知识和受限泛化，这些限制了LLM在有效支持CTI方面的能力。

Conclusion: 为设计更稳健的LLM驱动的CTI系统提供了可操作的见解，以促进未来研究。

Abstract: Large Language Models (LLMs) are intensively used to assist security analysts
in counteracting the rapid exploitation of cyber threats, wherein LLMs offer
cyber threat intelligence (CTI) to support vulnerability assessment and
incident response. While recent work has shown that LLMs can support a wide
range of CTI tasks such as threat analysis, vulnerability detection, and
intrusion defense, significant performance gaps persist in practical
deployments. In this paper, we investigate the intrinsic vulnerabilities of
LLMs in CTI, focusing on challenges that arise from the nature of the threat
landscape itself rather than the model architecture. Using large-scale
evaluations across multiple CTI benchmarks and real-world threat reports, we
introduce a novel categorization methodology that integrates stratification,
autoregressive refinement, and human-in-the-loop supervision to reliably
analyze failure instances. Through extensive experiments and human inspections,
we reveal three fundamental vulnerabilities: spurious correlations,
contradictory knowledge, and constrained generalization, that limit LLMs in
effectively supporting CTI. Subsequently, we provide actionable insights for
designing more robust LLM-powered CTI systems to facilitate future research.

</details>


### [9] [Position: Privacy Is Not Just Memorization!](https://arxiv.org/abs/2510.01645)
*Niloofar Mireshghallah,Tianshi Li*

Main category: cs.CR

TL;DR: 本文认为LLM隐私风险不仅限于训练数据记忆，还包括数据收集、推理时上下文泄露、自主代理能力和深度推理攻击等更紧迫的威胁，呼吁研究社区超越当前技术解决方案的狭隘关注。


<details>
  <summary>Details</summary>
Motivation: 当前关于LLM隐私风险的讨论过度关注训练数据的逐字记忆，而忽视了更直接和可扩展的隐私威胁，需要全面审视LLM系统中的隐私风险格局。

Method: 提出了LLM生命周期中隐私风险的全面分类法，通过案例研究展示当前隐私框架的不足，并对2016-2025年间1322篇AI/ML隐私论文进行纵向分析。

Result: 分析发现记忆问题在技术研究中受到过度关注，而最紧迫的隐私危害存在于其他领域，当前技术方法在这些领域几乎无法提供有效解决方案。

Conclusion: 呼吁研究社区从根本上改变处理LLM隐私的方法，超越当前技术解决方案的狭隘关注，采用跨学科方法应对这些新兴威胁的社会技术性质。

Abstract: The discourse on privacy risks in Large Language Models (LLMs) has
disproportionately focused on verbatim memorization of training data, while a
constellation of more immediate and scalable privacy threats remain
underexplored. This position paper argues that the privacy landscape of LLM
systems extends far beyond training data extraction, encompassing risks from
data collection practices, inference-time context leakage, autonomous agent
capabilities, and the democratization of surveillance through deep inference
attacks. We present a comprehensive taxonomy of privacy risks across the LLM
lifecycle -- from data collection through deployment -- and demonstrate through
case studies how current privacy frameworks fail to address these multifaceted
threats. Through a longitudinal analysis of 1,322 AI/ML privacy papers
published at leading conferences over the past decade (2016--2025), we reveal
that while memorization receives outsized attention in technical research, the
most pressing privacy harms lie elsewhere, where current technical approaches
offer little traction and viable paths forward remain unclear. We call for a
fundamental shift in how the research community approaches LLM privacy, moving
beyond the narrow focus of current technical solutions and embracing
interdisciplinary approaches that address the sociotechnical nature of these
emerging threats.

</details>


### [10] [Evaluating the Robustness of a Production Malware Detection System to Transferable Adversarial Attacks](https://arxiv.org/abs/2510.01676)
*Milad Nasr,Yanick Fratantonio,Luca Invernizzi,Ange Albertini,Loua Farah,Alex Petit-Bianco,Andreas Terzis,Kurt Thomas,Elie Bursztein,Nicholas Carlini*

Main category: cs.CR

TL;DR: 通过仅修改恶意软件样本的13个字节，可以成功绕过Gmail的Magika文件类型识别模型，在90%的情况下实现恶意软件检测规避。研究还开发了防御措施，将攻击成功率降至20%。


<details>
  <summary>Details</summary>
Motivation: 研究机器学习组件在恶意软件检测系统中的安全漏洞，特别是对抗性攻击如何通过欺骗单个ML模型来破坏整个生产系统的安全性。

Method: 针对Gmail的Magika文件类型识别模型设计对抗性样本，通过最小化字节修改来误导模型分类，从而绕过恶意软件检测管道。

Result: 仅修改13个字节即可在90%的情况下成功绕过Magika模型，使恶意软件能够通过Gmail发送。防御措施将攻击成功率降至20%，需要50字节修改。

Conclusion: 单个ML组件的安全漏洞可能对生产系统造成严重影响，需要开发有效的防御机制来缓解此类对抗性攻击。

Abstract: As deep learning models become widely deployed as components within larger
production systems, their individual shortcomings can create system-level
vulnerabilities with real-world impact. This paper studies how adversarial
attacks targeting an ML component can degrade or bypass an entire
production-grade malware detection system, performing a case study analysis of
Gmail's pipeline where file-type identification relies on a ML model.
  The malware detection pipeline in use by Gmail contains a machine learning
model that routes each potential malware sample to a specialized malware
classifier to improve accuracy and performance. This model, called Magika, has
been open sourced. By designing adversarial examples that fool Magika, we can
cause the production malware service to incorrectly route malware to an
unsuitable malware detector thereby increasing our chance of evading detection.
Specifically, by changing just 13 bytes of a malware sample, we can
successfully evade Magika in 90% of cases and thereby allow us to send malware
files over Gmail. We then turn our attention to defenses, and develop an
approach to mitigate the severity of these types of attacks. For our defended
production model, a highly resourced adversary requires 50 bytes to achieve
just a 20% attack success rate. We implement this defense, and, thanks to a
collaboration with Google engineers, it has already been deployed in production
for the Gmail classifier.

</details>


### [11] [Towards Imperceptible Adversarial Defense: A Gradient-Driven Shield against Facial Manipulations](https://arxiv.org/abs/2510.01699)
*Yue Li,Linying Xue,Dongdong Lin,Qiushi Li,Hui Tian,Hongxia Wang*

Main category: cs.CR

TL;DR: 提出了GRASP方法，通过梯度投影机制解决对抗性扰动中防御效果与视觉质量之间的平衡问题，在保持高图像质量的同时实现100%的深度伪造防御成功率。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的繁荣，被操纵的人脸图像越来越容易获取，引发了隐私侵权和社会信任问题。现有主动防御方法在不可感知性和防御效果之间存在权衡，强扰动会破坏伪造但降低视觉保真度。

Method: 提出基于梯度投影的对抗主动防御方法GRASP，首次成功整合结构相似性损失和低频损失来增强扰动不可感知性，通过梯度投影机制缓解防御效果损失与视觉质量损失之间的梯度冲突。

Result: 实验验证GRASP的有效性，PSNR超过40dB，SSIM达到0.99，对人脸属性操作实现100%的防御成功率，在视觉质量方面显著优于现有方法。

Conclusion: GRASP方法通过平衡优化实现了在保持图像保真度的同时不牺牲防御性能，为解决深度伪造防御中的视觉质量与防御效果平衡问题提供了有效解决方案。

Abstract: With the flourishing prosperity of generative models, manipulated facial
images have become increasingly accessible, raising concerns regarding privacy
infringement and societal trust. In response, proactive defense strategies
embed adversarial perturbations into facial images to counter deepfake
manipulation. However, existing methods often face a tradeoff between
imperceptibility and defense effectiveness-strong perturbations may disrupt
forgeries but degrade visual fidelity. Recent studies have attempted to address
this issue by introducing additional visual loss constraints, yet often
overlook the underlying gradient conflicts among losses, ultimately weakening
defense performance. To bridge the gap, we propose a gradient-projection-based
adversarial proactive defense (GRASP) method that effectively counters facial
deepfakes while minimizing perceptual degradation. GRASP is the first approach
to successfully integrate both structural similarity loss and low-frequency
loss to enhance perturbation imperceptibility. By analyzing gradient conflicts
between defense effectiveness loss and visual quality losses, GRASP pioneers
the design of the gradient-projection mechanism to mitigate these conflicts,
enabling balanced optimization that preserves image fidelity without
sacrificing defensive performance. Extensive experiments validate the efficacy
of GRASP, achieving a PSNR exceeding 40 dB, SSIM of 0.99, and a 100% defense
success rate against facial attribute manipulations, significantly
outperforming existing approaches in visual quality.

</details>


### [12] [Constructions of Efficiently Implementable Boolean Functions with Provable Nonlinearity/Resiliency/Algebraic Immunity Trade-Offs](https://arxiv.org/abs/2510.01720)
*Palash Sarkar*

Main category: cs.CR

TL;DR: 提出了多个布尔函数族，在弹性、非线性度和代数免疫性之间实现可证明的权衡，且具有线性复杂度的实现。


<details>
  <summary>Details</summary>
Motivation: 在密码学中，布尔函数需要在弹性、非线性度和代数免疫性等多个安全属性之间取得平衡，但现有方法难以同时优化这些属性。

Method: 构造了多个布尔函数族，通过参数化设计确保函数具有指定的弹性下界、非线性度上界和代数免疫性下界。

Result: 对于任意给定的参数m₀≥0、x₀≥1和a₀≥1，可以构造n变量函数，其弹性≥m₀、线性偏差≤2^{-x₀}、代数免疫性≥a₀，且n与参数呈线性关系，实现复杂度为O(n)门。

Conclusion: 该工作提供了在多个安全属性间实现可控权衡的高效布尔函数构造方法，为密码学应用提供了实用的函数设计方案。

Abstract: We describe several families of efficiently implementable Boolean functions
achieving provable trade-offs between resiliency, nonlinearity, and algebraic
immunity. In concrete terms, the following result holds for each of the
function families that we propose. Given integers $m_0\geq 0$, $x_0\geq 1$, and
$a_0\geq 1$, it is possible to construct an $n$-variable function which has
resiliency at least $m_0$, linear bias (which is an equivalent method of
expressing nonlinearity) at most $2^{-x_0}$ and algebraic immunity at least
$a_0$; further, $n$ is linear in $m_0$, $x_0$ and $a_0$, and the function can
be implemented using $O(n)$ gates.

</details>


### [13] [Secure Multi-Modal Data Fusion in Federated Digital Health Systems via MCP](https://arxiv.org/abs/2510.01780)
*Aueaphum Aueawatthanaphisut*

Main category: cs.CR

TL;DR: 提出了一种基于模型上下文协议(MCP)的多模态联邦学习框架，用于安全集成异构医疗数据，在保护隐私的同时提升诊断准确性并降低客户端掉线率。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习框架缺乏标准化机制来协调分布式资源受限环境中的多模态数据融合，医疗数据的安全互操作集成仍是一个重大挑战。

Method: 利用MCP作为互操作性层，构建包含多模态特征对齐、差分隐私安全聚合和能量感知调度的三支柱架构，实现安全的跨智能体通信。

Result: 在基准数据集和临床队列上的实验显示，相比基线联邦学习，诊断准确率提升9.8%，客户端掉线率降低54%，并在隐私-效用权衡方面达到临床可接受水平。

Conclusion: MCP支持的多模态融合为构建公平、可信赖的下一代联邦健康基础设施提供了可扩展的途径。

Abstract: Secure and interoperable integration of heterogeneous medical data remains a
grand challenge in digital health. Current federated learning (FL) frameworks
offer privacy-preserving model training but lack standardized mechanisms to
orchestrate multi-modal data fusion across distributed and resource-constrained
environments. This study introduces a novel framework that leverages the Model
Context Protocol (MCP) as an interoperability layer for secure, cross-agent
communication in multi-modal federated healthcare systems. The proposed
architecture unifies three pillars: (i) multi-modal feature alignment for
clinical imaging, electronic medical records, and wearable IoT data; (ii)
secure aggregation with differential privacy to protect patient-sensitive
updates; and (iii) energy-aware scheduling to mitigate dropouts in mobile
clients. By employing MCP as a schema-driven interface, the framework enables
adaptive orchestration of AI agents and toolchains while ensuring compliance
with privacy regulations. Experimental evaluation on benchmark datasets and
pilot clinical cohorts demonstrates up to 9.8\% improvement in diagnostic
accuracy compared with baseline FL, a 54\% reduction in client dropout rates,
and clinically acceptable privacy--utility trade-offs. These results highlight
MCP-enabled multi-modal fusion as a scalable and trustworthy pathway toward
equitable, next-generation federated health infrastructures.

</details>


### [14] [ZK-WAGON: Imperceptible Watermark for Image Generation Models using ZK-SNARKs](https://arxiv.org/abs/2510.01967)
*Aadarsh Anantha Ramakrishnan,Shubham Agarwal,Selvanayagam S,Kunwar Singh*

Main category: cs.CR

TL;DR: ZK-WAGON是首个使用ZK-SNARKs为图像生成模型添加水印的系统，通过选择性层电路转换和LSB隐写技术，在不暴露模型内部信息的情况下提供可验证的来源证明。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成模型变得强大且易得，合成媒体的真实性、所有权和滥用问题变得至关重要。传统水印方法会降低图像质量、易被移除或需要访问模型内部信息，无法满足安全可扩展部署的需求。

Method: 提出选择性层ZK电路创建(SL-ZKCC)方法，将图像生成模型的关键层转换为电路，显著减少证明生成时间。使用ZK-SNARKs生成可验证证明，并通过LSB隐写技术将证明嵌入生成图像中。

Result: 在GAN和扩散模型上验证了该系统，提供了一个安全、模型无关的可信AI图像生成流水线。

Conclusion: ZK-WAGON为图像生成模型提供了安全、可扩展的水印解决方案，能够在保护模型隐私的同时验证图像来源。

Abstract: As image generation models grow increasingly powerful and accessible,
concerns around authenticity, ownership, and misuse of synthetic media have
become critical. The ability to generate lifelike images indistinguishable from
real ones introduces risks such as misinformation, deepfakes, and intellectual
property violations. Traditional watermarking methods either degrade image
quality, are easily removed, or require access to confidential model internals
- making them unsuitable for secure and scalable deployment. We are the first
to introduce ZK-WAGON, a novel system for watermarking image generation models
using the Zero-Knowledge Succinct Non Interactive Argument of Knowledge
(ZK-SNARKs). Our approach enables verifiable proof of origin without exposing
model weights, generation prompts, or any sensitive internal information. We
propose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively
convert key layers of an image generation model into a circuit, reducing proof
generation time significantly. Generated ZK-SNARK proofs are imperceptibly
embedded into a generated image via Least Significant Bit (LSB) steganography.
We demonstrate this system on both GAN and Diffusion models, providing a
secure, model-agnostic pipeline for trustworthy AI image generation.

</details>


### [15] [Mirage Fools the Ear, Mute Hides the Truth: Precise Targeted Adversarial Attacks on Polyphonic Sound Event Detection Systems](https://arxiv.org/abs/2510.02158)
*Junjie Su,Weifei Jin,Yuxin Cao,Derui Wang,Kai Ye,Jie Hao*

Main category: cs.CR

TL;DR: 提出了Mirage和Mute Attack (M2A)框架，用于针对多音源声音事件检测系统的精确对抗攻击，通过保护损失确保非目标区域输出不变，并引入编辑精度(EP)评估指标。


<details>
  <summary>Details</summary>
Motivation: 声音事件检测系统在安全关键应用中部署增多，但其对抗攻击鲁棒性研究不足。现有攻击方法要么因SED的强上下文依赖性而效果不佳，要么因仅关注目标区域而缺乏精确性。

Method: 提出M2A框架，在优化过程中对非目标输出施加特定约束（保护损失），确保攻击不改变非目标区域的模型输出，从而实现精确攻击。

Result: 在两个最先进的SED模型上，M2A分别实现了94.56%和99.11%的编辑精度(EP)，表明该框架在保持有效性的同时显著提升了攻击精度。

Conclusion: M2A框架能够对多音源SED系统进行精确有效的对抗攻击，通过保护损失机制和EP评估指标，在攻击效果和精度之间实现了良好平衡。

Abstract: Sound Event Detection (SED) systems are increasingly deployed in
safety-critical applications such as industrial monitoring and audio
surveillance. However, their robustness against adversarial attacks has not
been well explored. Existing audio adversarial attacks targeting SED systems,
which incorporate both detection and localization capabilities, often lack
effectiveness due to SED's strong contextual dependencies or lack precision by
focusing solely on misclassifying the target region as the target event,
inadvertently affecting non-target regions. To address these challenges, we
propose the Mirage and Mute Attack (M2A) framework, which is designed for
targeted adversarial attacks on polyphonic SED systems. In our optimization
process, we impose specific constraints on the non-target output, which we
refer to as preservation loss, ensuring that our attack does not alter the
model outputs for non-target region, thus achieving precise attacks.
Furthermore, we introduce a novel evaluation metric Editing Precison (EP) that
balances effectiveness and precision, enabling our method to simultaneously
enhance both. Comprehensive experiments show that M2A achieves 94.56% and
99.11% EP on two state-of-the-art SED models, demonstrating that the framework
is sufficiently effective while significantly enhancing attack precision.

</details>


### [16] [NoMod: A Non-modular Attack on Module Learning With Errors](https://arxiv.org/abs/2510.02162)
*Cristian Bassotto,Ermes Franch,Marina Krček,Stjepan Picek*

Main category: cs.CR

TL;DR: NoMod ML-Attack是一种混合白盒密码分析方法，通过将模约简的环绕视为统计噪声，将秘密恢复转化为鲁棒线性估计问题，成功破解了基于Module-LWE的后量子密码方案。


<details>
  <summary>Details</summary>
Motivation: 量子计算的兴起威胁经典公钥密码学，NIST采用了基于Module-LWE问题的后量子方案，需要开发新的密码分析方法来评估这些方案的安全性。

Method: 结合优化的格预处理（包括减少向量保存和代数放大）与通过Tukey双权重损失训练的鲁棒估计器，将模约简的环绕建模为统计噪声。

Result: 实验显示NoMod能够完全恢复维度n=350的二进制秘密，恢复n=256的稀疏二项式秘密，并在CRYSTALS-Kyber参数(n,k)=(128,3)和(256,2)下成功恢复稀疏秘密。

Conclusion: NoMod方法成功绕过了模约简建模的挑战，为评估后量子密码方案的安全性提供了有效的分析工具。

Abstract: The advent of quantum computing threatens classical public-key cryptography,
motivating NIST's adoption of post-quantum schemes such as those based on the
Module Learning With Errors (Module-LWE) problem. We present NoMod ML-Attack, a
hybrid white-box cryptanalytic method that circumvents the challenge of
modeling modular reduction by treating wrap-arounds as statistical corruption
and casting secret recovery as robust linear estimation. Our approach combines
optimized lattice preprocessing--including reduced-vector saving and algebraic
amplification--with robust estimators trained via Tukey's Biweight loss.
Experiments show NoMod achieves full recovery of binary secrets for dimension
$n = 350$, recovery of sparse binomial secrets for $n = 256$, and successful
recovery of sparse secrets in CRYSTALS-Kyber settings with parameters $(n, k) =
(128, 3)$ and $(256, 2)$. We release our implementation in an anonymous
repository https://anonymous.4open.science/r/NoMod-3BD4.

</details>


### [17] [Testing Stability and Robustness in Three Cryptographic Chaotic Systems](https://arxiv.org/abs/2510.02184)
*N. A. Anagnostopoulos,K. Konstantinidis,A. N. Miliou,S. G. Stavrinides*

Main category: cs.CR

TL;DR: 测试三种已知密码学混沌系统的稳定性和鲁棒性，评估其在噪声环境下的同步性能与安全性


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，即使存在噪声，驱动-响应系统也需要始终保持同步，这对系统稳定性提出了要求

Method: 对三种不同的知名密码学混沌系统进行测试和比较分析

Result: 获得了不同混沌系统在噪声环境下的同步稳定性和鲁棒性表现结果

Conclusion: 通过比较分析，评估了这些混沌系统在安全应用中的适用性

Abstract: In practical applications, it is crucial that the drive-response systems,
although identical in all respects, are synchronized at all times, even if
there is noise present. In this work, we test the stability and robustness of
three distinct and well-known cryptographic chaotic systems, and compare the
results in relation to the desired security.

</details>


### [18] [Authentication Security of PRF GNSS Ranging](https://arxiv.org/abs/2510.02196)
*Jason Anderson*

Main category: cs.CR

TL;DR: 本文分析了基于伪随机函数(PRF)的GNSS测距在多种欺骗模型下的认证安全性，包括SCER欺骗器。当GNSS测距码来自仅广播者知道的秘密PRF时，欺骗者无法在广播前预测测距码，从而建立对GNSS伪距和PNT解决方案的信任。


<details>
  <summary>Details</summary>
Motivation: 研究GNSS测距在多种欺骗模型下的认证安全性，特别是针对SCER欺骗器的安全分析，为设计满足认证安全要求的PRF GNSS测距协议提供理论基础。

Method: 应用伪随机函数(PRF)理论分析GNSS测距认证安全性，针对Galileo的SAS服务使用加密的E6-C信号进行计算，分析不同欺骗模型下的安全边界。

Result: 计算得出在非SCER模型下，最多需要400ms的Galileo E6-C数据来确保128位认证安全性；对于SCER对手，预测了其所需的接收无线电设备来破坏认证安全性。

Conclusion: 该工作可用于设计PRF GNSS测距协议，通过计算漏检概率来满足实用的认证安全要求。

Abstract: This work derives the authentication security of pseudorandom function (PRF)
GNSS ranging under multiple GNSS spoofing models, including the Security Code
Estimation and Replay (SCER) spoofer. When GNSS ranging codes derive from a PRF
utilizing a secret known only to the broadcaster, the spoofer cannot predict
the ranging code before broadcast. Therefore, PRF ranging can be used to
establish trust in the GNSS pseudoranges and the resulting receiver position,
navigation, and timing (PNT) solution. I apply the methods herein to Galileo's
Signal Authentication Service (SAS) utilizing the encrypted Galileo E6-C signal
to compute that, at most, 400 ms of Galileo E6-C data to assert 128-bit
authentication security under non-SCER models. For the SCER adversary, I
predict the adversary's needed receiving radio equipment to break
authentication security. One can use this work to design a PRF GNSS ranging
protocol to meet useful authentication security requirements by computing the
probability of missed detection.

</details>


### [19] [An efficient quantum algorithm for computing $S$-units and its applications](https://arxiv.org/abs/2510.02280)
*Jean-Francois Biasse,Fang Song*

Main category: cs.CR

TL;DR: 本文详细证明了Biasse和Song提出的量子多项式时间算法，用于计算数域的S-单位群，该算法可应用于计算类群、S-类群、相对类群、单位群、射线类群，解决主理想问题、某些范数方程，以及分解理想类群中的理想类。


<details>
  <summary>Details</summary>
Motivation: 提供Biasse和Song量子多项式时间算法的详细证明，该算法能高效解决数论中的多个重要计算问题，包括类群计算和主理想问题的求解。

Method: 基于Biasse和Song的量子多项式时间算法，该算法直接计算数域的S-单位群，并利用该结果推导出其他数论问题的解决方案。

Result: 算法能够多项式时间内计算类群、S-类群、相对类群、单位群、射线类群，解决主理想问题、某些范数方程，分解理想类群中的理想类。结合其他研究成果，还能找到主理想的短生成元和分圆域理想格中的"适度短向量"。

Conclusion: 该量子多项式时间算法为数论中的多个重要计算问题提供了高效的解决方案，特别是在结合其他研究成果时，能够解决更广泛的计算问题。

Abstract: In this paper, we provide details on the proofs of the quantum polynomial
time algorithm of Biasse and Song (SODA 16) for computing the $S$-unit group of
a number field. This algorithm directly implies polynomial time methods to
calculate class groups, S-class groups, relative class group and the unit
group, ray class groups, solve the principal ideal problem, solve certain norm
equations, and decompose ideal classes in the ideal class group. Additionally,
combined with a result of Cramer, Ducas, Peikert and Regev (Eurocrypt 2016),
the resolution of the principal ideal problem allows one to find short
generators of a principal ideal. Likewise, methods due to Cramer, Ducas and
Wesolowski (Eurocrypt 2017) use the resolution of the principal ideal problem
and the decomposition of ideal classes to find so-called ``mildly short
vectors'' in ideal lattices of cyclotomic fields.

</details>
