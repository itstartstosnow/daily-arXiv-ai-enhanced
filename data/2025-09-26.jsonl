{"id": "2509.20382", "categories": ["cs.CR", "cs.AI", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.20382", "abs": "https://arxiv.org/abs/2509.20382", "authors": ["Dilli Hang Rai", "Sabin Kafley"], "title": "Lightweight MobileNetV1+GRU for ECG Biometric Authentication: Federated and Adversarial Evaluation", "comment": "5 pages, 7 figures, 5 tables", "summary": "ECG biometrics offer a unique, secure authentication method, yet their\ndeployment on wearable devices faces real-time processing, privacy, and\nspoofing vulnerability challenges. This paper proposes a lightweight deep\nlearning model (MobileNetV1+GRU) for ECG-based authentication, injection of\n20dB Gaussian noise & custom preprocessing. We simulate wearable conditions and\nedge deployment using the ECGID, MIT-BIH, CYBHi, and PTB datasets, achieving\naccuracies of 99.34%, 99.31%, 91.74%, and 98.49%, F1-scores of 0.9869, 0.9923,\n0.9125, and 0.9771, Precision of 0.9866, 0.9924, 0.9180 and 0.9845, Recall of\n0.9878, 0.9923, 0.9129, and 0.9756, equal error rates (EER) of 0.0009, 0.00013,\n0.0091, and 0.0009, and ROC-AUC values of 0.9999, 0.9999, 0.9985, and 0.9998,\nwhile under FGSM adversarial attacks, accuracy drops from 96.82% to as low as\n0.80%. This paper highlights federated learning, adversarial testing, and the\nneed for diverse wearable physiological datasets to ensure secure and scalable\nbiometrics."}
{"id": "2509.20383", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20383", "abs": "https://arxiv.org/abs/2509.20383", "authors": ["Wei Wan", "Yuxuan Ning", "Zhicong Huang", "Cheng Hong", "Shengshan Hu", "Ziqi Zhou", "Yechao Zhang", "Tianqing Zhu", "Wanlei Zhou", "Leo Yu Zhang"], "title": "MARS: A Malignity-Aware Backdoor Defense in Federated Learning", "comment": "NeurIPS 2025", "summary": "Federated Learning (FL) is a distributed paradigm aimed at protecting\nparticipant data privacy by exchanging model parameters to achieve high-quality\nmodel training. However, this distributed nature also makes FL highly\nvulnerable to backdoor attacks. Notably, the recently proposed state-of-the-art\n(SOTA) attack, 3DFed (SP2023), uses an indicator mechanism to determine whether\nthe backdoor models have been accepted by the defender and adaptively optimizes\nbackdoor models, rendering existing defenses ineffective. In this paper, we\nfirst reveal that the failure of existing defenses lies in the employment of\nempirical statistical measures that are loosely coupled with backdoor attacks.\nMotivated by this, we propose a Malignity-Aware backdooR defenSe (MARS) that\nleverages backdoor energy (BE) to indicate the malicious extent of each neuron.\nTo amplify malignity, we further extract the most prominent BE values from each\nmodel to form a concentrated backdoor energy (CBE). Finally, a novel\nWasserstein distance-based clustering method is introduced to effectively\nidentify backdoor models. Extensive experiments demonstrate that MARS can\ndefend against SOTA backdoor attacks and significantly outperforms existing\ndefenses."}
{"id": "2509.20384", "categories": ["cs.CR", "cs.AI", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20384", "abs": "https://arxiv.org/abs/2509.20384", "authors": ["Jiayi Lin", "Liangcai Su", "Junzhe Li", "Chenxiong Qian"], "title": "R1-Fuzz: Specializing Language Models for Textual Fuzzing via Reinforcement Learning", "comment": null, "summary": "Fuzzing is effective for vulnerability discovery but struggles with complex\ntargets such as compilers, interpreters, and database engines, which accept\ntextual input that must satisfy intricate syntactic and semantic constraints.\nAlthough language models (LMs) have attracted interest for this task due to\ntheir vast latent knowledge and reasoning potential, their practical adoption\nhas been limited. The major challenges stem from insufficient exploration of\ndeep program logic among real-world codebases, and the high cost of leveraging\nlarger models. To overcome these challenges, we propose R1-Fuzz, the first\nframework that leverages reinforcement learning (RL) to specialize\ncost-efficient LMs and integrate them for complex textual fuzzing input\ngeneration. R1-Fuzz introduces two key designs: coverage-slicing-based question\nconstruction and a distance-based reward calculation. Through RL-based\npost-training of a model with our constructed dataset, R1-Fuzz designs a\nfuzzing workflow that tightly integrates LMs to reason deep program semantics\nduring fuzzing. Evaluations on diverse real-world targets show that our design\nenables a small model, named R1-Fuzz-7B, to rival or even outperform much\nlarger models in real-world fuzzing. Notably, R1-Fuzz achieves up to 75\\%\nhigher coverage than state-of-the-art fuzzers and discovers 29 previously\nunknown vulnerabilities, demonstrating its practicality."}
{"id": "2509.20388", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20388", "abs": "https://arxiv.org/abs/2509.20388", "authors": ["Amir AL-Maamari"], "title": "Can You Trust Your Copilot? A Privacy Scorecard for AI Coding Assistants", "comment": null, "summary": "The rapid integration of AI-powered coding assistants into developer\nworkflows has raised significant privacy and trust concerns. As developers\nentrust proprietary code to services like OpenAI's GPT, Google's Gemini, and\nGitHub Copilot, the unclear data handling practices of these tools create\nsecurity and compliance risks. This paper addresses this challenge by\nintroducing and applying a novel, expert-validated privacy scorecard. The\nmethodology involves a detailed analysis of four document types; from legal\npolicies to external audits; to score five leading assistants against 14\nweighted criteria. A legal expert and a data protection officer refined these\ncriteria and their weighting. The results reveal a distinct hierarchy of\nprivacy protections, with a 20-point gap between the highest- and lowest-ranked\ntools. The analysis uncovers common industry weaknesses, including the\npervasive use of opt-out consent for model training and a near-universal\nfailure to filter secrets from user prompts proactively. The resulting\nscorecard provides actionable guidance for developers and organizations,\nenabling evidence-based tool selection. This work establishes a new benchmark\nfor transparency and advocates for a shift towards more user-centric privacy\nstandards in the AI industry."}
{"id": "2509.20391", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20391", "abs": "https://arxiv.org/abs/2509.20391", "authors": ["Md. Alamgir Hossain", "Waqas Ishtiaq", "Md. Samiul Islam"], "title": "A Comparative Analysis of Ensemble-Based Machine Learning Approaches with Explainable AI for Multi-Class Intrusion Detection in Drone Networks", "comment": "27 pages, 18 figures, 10 tables", "summary": "The growing integration of drones into civilian, commercial, and defense\nsectors introduces significant cybersecurity concerns, particularly with the\nincreased risk of network-based intrusions targeting drone communication\nprotocols. Detecting and classifying these intrusions is inherently challenging\ndue to the dynamic nature of drone traffic and the presence of multiple\nsophisticated attack vectors such as spoofing, injection, replay, and\nman-in-the-middle (MITM) attacks. This research aims to develop a robust and\ninterpretable intrusion detection framework tailored for drone networks, with a\nfocus on handling multi-class classification and model explainability. We\npresent a comparative analysis of ensemble-based machine learning models,\nnamely Random Forest, Extra Trees, AdaBoost, CatBoost, and XGBoost, trained on\na labeled dataset comprising benign traffic and nine distinct intrusion types.\nComprehensive data preprocessing was performed, including missing value\nimputation, scaling, and categorical encoding, followed by model training and\nextensive evaluation using metrics such as macro F1-score, ROC AUC, Matthews\nCorrelation Coefficient, and Log Loss. Random Forest achieved the highest\nperformance with a macro F1-score of 0.9998 and ROC AUC of 1.0000. To validate\nthe superiority of the models, statistical tests, including Friedmans test, the\nWilcoxon signed-rank test with Holm correction, and bootstrapped confidence\nintervals, were applied. Furthermore, explainable AI methods, SHAP and LIME,\nwere integrated to interpret both global and local feature importance,\nenhancing model transparency and decision trustworthiness. The proposed\napproach not only delivers near-perfect accuracy but also ensures\ninterpretability, making it highly suitable for real-time and safety-critical\ndrone operations."}
{"id": "2509.20395", "categories": ["cs.CR", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.20395", "abs": "https://arxiv.org/abs/2509.20395", "authors": ["Noam Schmitt", "Marc Antoine Lacoste"], "title": "Centralized vs. Decentralized Security for Space AI Systems? A New Look", "comment": "IEEE HPEC 2025 - 29th Annual IEEE High Performance Extreme Computing\n  Virtual Conference, MIT Lincoln Laboratory, Sep 2025, Boston (MA), United\n  States", "summary": "This paper investigates the trade-off between centralized and decentralized\nsecurity management in constellations of satellites to balance security and\nperformance. We highlight three key AI architectures for automated security\nmanagement: (a) centralized, (b) distributed and (c) federated. The centralized\narchitecture is the best option short term, providing fast training, despite\nthe hard challenge of the communication latency overhead across space.\nDecentralized architectures are better alternatives in the longer term,\nproviding enhanced scalability and security."}
{"id": "2509.20399", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20399", "abs": "https://arxiv.org/abs/2509.20399", "authors": ["Birk Torpmann-Hagen", "Michael A. Riegler", "PÃ¥l Halvorsen", "Dag Johansen"], "title": "Defending against Stegomalware in Deep Neural Networks with Permutation Symmetry", "comment": null, "summary": "Deep neural networks are being utilized in a growing number of applications,\nboth in production systems and for personal use. Network checkpoints are as a\nconsequence often shared and distributed on various platforms to ease the\ndevelopment process. This work considers the threat of neural network\nstegomalware, where malware is embedded in neural network checkpoints at a\nnegligible cost to network accuracy. This constitutes a significant security\nconcern, but is nevertheless largely neglected by the deep learning\npractitioners and security specialists alike. We propose the first effective\ncountermeasure to these attacks. In particular, we show that state-of-the-art\nneural network stegomalware can be efficiently and effectively neutralized\nthrough shuffling the column order of the weight- and bias-matrices, or\nequivalently the channel-order of convolutional layers. We show that this\neffectively corrupts payloads that have been embedded by state-of-the-art\nmethods in neural network steganography at no cost to network accuracy,\noutperforming competing methods by a significant margin. We then discuss\npossible means by which to bypass this defense, additional defense methods, and\nadvocate for continued research into the security of machine learning systems."}
{"id": "2509.20405", "categories": ["cs.CR", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.20405", "abs": "https://arxiv.org/abs/2509.20405", "authors": ["Visar Berisha", "Prad Kadambi", "Isabella Lenz"], "title": "Why Speech Deepfake Detectors Won't Generalize: The Limits of Detection in an Open World", "comment": null, "summary": "Speech deepfake detectors are often evaluated on clean, benchmark-style\nconditions, but deployment occurs in an open world of shifting devices,\nsampling rates, codecs, environments, and attack families. This creates a\n``coverage debt\" for AI-based detectors: every new condition multiplies with\nexisting ones, producing data blind spots that grow faster than data can be\ncollected. Because attackers can target these uncovered regions, worst-case\nperformance (not average benchmark scores) determines security. To demonstrate\nthe impact of the coverage debt problem, we analyze results from a recent\ncross-testing framework. Grouping performance by bona fide domain and spoof\nrelease year, two patterns emerge: newer synthesizers erase the legacy\nartifacts detectors rely on, and conversational speech domains\n(teleconferencing, interviews, social media) are consistently the hardest to\nsecure. These findings show that detection alone should not be relied upon for\nhigh-stakes decisions. Detectors should be treated as auxiliary signals within\nlayered defenses that include provenance, personhood credentials, and policy\nsafeguards."}
{"id": "2509.20411", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20411", "abs": "https://arxiv.org/abs/2509.20411", "authors": ["Tharcisse Ndayipfukamiye", "Jianguo Ding", "Doreen Sebastian Sarwatt", "Adamu Gaston Philipo", "Huansheng Ning"], "title": "Adversarial Defense in Cybersecurity: A Systematic Review of GANs for Threat Detection and Mitigation", "comment": "35 pages, 10 tables, 4figures", "summary": "Machine learning-based cybersecurity systems are highly vulnerable to\nadversarial attacks, while Generative Adversarial Networks (GANs) act as both\npowerful attack enablers and promising defenses. This survey systematically\nreviews GAN-based adversarial defenses in cybersecurity (2021--August 31,\n2025), consolidating recent progress, identifying gaps, and outlining future\ndirections. Using a PRISMA-compliant systematic literature review protocol, we\nsearched five major digital libraries. From 829 initial records, 185\npeer-reviewed studies were retained and synthesized through quantitative trend\nanalysis and thematic taxonomy development. We introduce a four-dimensional\ntaxonomy spanning defensive function, GAN architecture, cybersecurity domain,\nand adversarial threat model. GANs improve detection accuracy, robustness, and\ndata utility across network intrusion detection, malware analysis, and IoT\nsecurity. Notable advances include WGAN-GP for stable training, CGANs for\ntargeted synthesis, and hybrid GAN models for improved resilience. Yet,\npersistent challenges remain such as instability in training, lack of\nstandardized benchmarks, high computational cost, and limited explainability.\nGAN-based defenses demonstrate strong potential but require advances in stable\narchitectures, benchmarking, transparency, and deployment. We propose a roadmap\nemphasizing hybrid models, unified evaluation, real-world integration, and\ndefenses against emerging threats such as LLM-driven cyberattacks. This survey\nestablishes the foundation for scalable, trustworthy, and adaptive GAN-powered\ndefenses."}
{"id": "2509.20418", "categories": ["cs.CR", "cs.AI", "cs.ET", "K.6.5; I.2.0"], "pdf": "https://arxiv.org/pdf/2509.20418", "abs": "https://arxiv.org/abs/2509.20418", "authors": ["Grace Billiris", "Asif Gill", "Madhushi Bandara"], "title": "A Taxonomy of Data Risks in AI and Quantum Computing (QAI) - A Systematic Review", "comment": "11 pages, 2 figures, 2 tables", "summary": "Quantum Artificial Intelligence (QAI), the integration of Artificial\nIntelligence (AI) and Quantum Computing (QC), promises transformative advances,\nincluding AI-enabled quantum cryptography and quantum-resistant encryption\nprotocols. However, QAI inherits data risks from both AI and QC, creating\ncomplex privacy and security vulnerabilities that are not systematically\nstudied. These risks affect the trustworthiness and reliability of AI and QAI\nsystems, making their understanding critical. This study systematically reviews\n67 privacy- and security-related studies to expand understanding of QAI data\nrisks. We propose a taxonomy of 22 key data risks, organised into five\ncategories: governance, risk assessment, control implementation, user\nconsiderations, and continuous monitoring. Our findings reveal vulnerabilities\nunique to QAI and identify gaps in holistic risk assessment. This work\ncontributes to trustworthy AI and QAI research and provides a foundation for\ndeveloping future risk assessment tools."}
{"id": "2509.20460", "categories": ["cs.CR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.20460", "abs": "https://arxiv.org/abs/2509.20460", "authors": ["Andrew Campbell", "Anna Scaglione", "Hang Liu", "Victor Elvira", "Sean Peisert", "Daniel Arnold"], "title": "Differential Privacy of Network Parameters from a System Identification Perspective", "comment": null, "summary": "This paper addresses the problem of protecting network information from\nprivacy system identification (SI) attacks when sharing cyber-physical system\nsimulations. We model analyst observations of networked states as time-series\noutputs of a graph filter driven by differentially private (DP) nodal\nexcitations, with the analyst aiming to infer the underlying graph shift\noperator (GSO). Unlike traditional SI, which estimates system parameters, we\nstudy the inverse problem: what assumptions prevent adversaries from\nidentifying the GSO while preserving utility for legitimate analysis. We show\nthat applying DP mechanisms to inputs provides formal privacy guarantees for\nthe GSO, linking the $(\\epsilon,\\delta)$-DP bound to the spectral properties of\nthe graph filter and noise covariance. More precisely, for DP Gaussian signals,\nthe spectral characteristics of both the filter and noise covariance determine\nthe privacy bound, with smooth filters and low-condition-number covariance\nyielding greater privacy."}
{"id": "2509.20476", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20476", "abs": "https://arxiv.org/abs/2509.20476", "authors": ["Ren-Yi Huang", "Dumindu Samaraweera", "Prashant Shekhar", "J. Morris Chang"], "title": "Advancing Practical Homomorphic Encryption for Federated Learning: Theoretical Guarantees and Efficiency Optimizations", "comment": null, "summary": "Federated Learning (FL) enables collaborative model training while preserving\ndata privacy by keeping raw data locally stored on client devices, preventing\naccess from other clients or the central server. However, recent studies reveal\nthat sharing model gradients creates vulnerability to Model Inversion Attacks,\nparticularly Deep Leakage from Gradients (DLG), which reconstructs private\ntraining data from shared gradients. While Homomorphic Encryption has been\nproposed as a promising defense mechanism to protect gradient privacy, fully\nencrypting all model gradients incurs high computational overhead. Selective\nencryption approaches aim to balance privacy protection with computational\nefficiency by encrypting only specific gradient components. However, the\nexisting literature largely overlooks a theoretical exploration of the spectral\nbehavior of encrypted versus unencrypted parameters, relying instead primarily\non empirical evaluations. To address this gap, this paper presents a framework\nfor theoretical analysis of the underlying principles of selective encryption\nas a defense against model inversion attacks. We then provide a comprehensive\nempirical study that identifies and quantifies the critical factors, such as\nmodel complexity, encryption ratios, and exposed gradients, that influence\ndefense effectiveness. Our theoretical framework clarifies the relationship\nbetween gradient selection and privacy preservation, while our experimental\nevaluation demonstrates how these factors shape the robustness of defenses\nagainst model inversion attacks. Collectively, these contributions advance the\nunderstanding of selective encryption mechanisms and offer principled guidance\nfor designing efficient, scalable, privacy-preserving federated learning\nsystems."}
{"id": "2509.20589", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20589", "abs": "https://arxiv.org/abs/2509.20589", "authors": ["Maria Chiper", "Radu Tudor Ionescu"], "title": "Every Character Counts: From Vulnerability to Defense in Phishing Detection", "comment": "Accepted at ICTAI 2025", "summary": "Phishing attacks targeting both organizations and individuals are becoming an\nincreasingly significant threat as technology advances. Current automatic\ndetection methods often lack explainability and robustness in detecting new\nphishing attacks. In this work, we investigate the effectiveness of\ncharacter-level deep learning models for phishing detection, which can provide\nboth robustness and interpretability. We evaluate three neural architectures\nadapted to operate at the character level, namely CharCNN, CharGRU, and\nCharBiLSTM, on a custom-built email dataset, which combines data from multiple\nsources. Their performance is analyzed under three scenarios: (i) standard\ntraining and testing, (ii) standard training and testing under adversarial\nattacks, and (iii) training and testing with adversarial examples. Aiming to\ndevelop a tool that operates as a browser extension, we test all models under\nlimited computational resources. In this constrained setup, CharGRU proves to\nbe the best-performing model across all scenarios. All models show\nvulnerability to adversarial attacks, but adversarial training substantially\nimproves their robustness. In addition, by adapting the Gradient-weighted Class\nActivation Mapping (Grad-CAM) technique to character-level inputs, we are able\nto visualize which parts of each email influence the decision of each model.\nOur open-source code and data is released at\nhttps://github.com/chipermaria/every-character-counts."}
{"id": "2509.20592", "categories": ["cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.20592", "abs": "https://arxiv.org/abs/2509.20592", "authors": ["Oluwole Adewusi", "Wallace S. Msagusa", "Jean Pierre Imanirumva", "Okemawo Obadofin", "Jema D. Ndibwile"], "title": "Beyond SSO: Mobile Money Authentication for Inclusive e-Government in Sub-Saharan Africa", "comment": null, "summary": "The rapid adoption of Mobile Money Services (MMS) in Sub-Saharan Africa (SSA)\noffers a viable path to improve e-Government service accessibility in the face\nof persistent low internet penetration. However, existing Mobile Money\nAuthentication (MMA) methods face critical limitations, including\nsusceptibility to SIM swapping, weak session protection, and poor scalability\nduring peak demand. This study introduces a hybrid MMA framework that combines\nUnstructured Supplementary Service Data (USSD)-based multi-factor\nauthentication with secure session management via cryptographically bound JSON\nWeb Tokens (JWT). Unlike traditional MMA systems that rely solely on SIM-PIN\nverification or smartphone-dependent biometrics, our design implements a\nthree-factor authentication model; SIM verification, PIN entry, and session\ntoken binding, tailored for resource-constrained environments. Simulations and\ncomparative analysis against OAuth-based Single Sign-On (SSO) methods reveal a\n45% faster authentication time (8 seconds vs. 12 to 15 seconds), 15% higher\nsuccess under poor network conditions (95% vs. 80%), and increased resistance\nto phishing and brute-force attacks. Penetration testing and threat modeling\nfurther demonstrate a substantial reduction in vulnerability exposure compared\nto conventional approaches. The primary contributions of this work are: (1) a\nhybrid authentication protocol that ensures offline accessibility and secure\nsession continuity; (2) a tailored security framework addressing threats like\nSIM swapping and social engineering in SSA; and (3) demonstrated scalability\nfor thousands of users with reduced infrastructure overhead. The proposed\napproach advances secure digital inclusion in SSA and other regions with\nsimilar constraints."}
{"id": "2509.20639", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20639", "abs": "https://arxiv.org/abs/2509.20639", "authors": ["Adam Swanda", "Amy Chang", "Alexander Chen", "Fraser Burch", "Paul Kassianik", "Konstantin Berlin"], "title": "A Framework for Rapidly Developing and Deploying Protection Against Large Language Model Attacks", "comment": null, "summary": "The widespread adoption of Large Language Models (LLMs) has revolutionized AI\ndeployment, enabling autonomous and semi-autonomous applications across\nindustries through intuitive language interfaces and continuous improvements in\nmodel development. However, the attendant increase in autonomy and expansion of\naccess permissions among AI applications also make these systems compelling\ntargets for malicious attacks. Their inherent susceptibility to security flaws\nnecessitates robust defenses, yet no known approaches can prevent zero-day or\nnovel attacks against LLMs. This places AI protection systems in a category\nsimilar to established malware protection systems: rather than providing\nguaranteed immunity, they minimize risk through enhanced observability,\nmulti-layered defense, and rapid threat response, supported by a threat\nintelligence function designed specifically for AI-related threats.\n  Prior work on LLM protection has largely evaluated individual detection\nmodels rather than end-to-end systems designed for continuous, rapid adaptation\nto a changing threat landscape. We present a production-grade defense system\nrooted in established malware detection and threat intelligence practices. Our\nplatform integrates three components: a threat intelligence system that turns\nemerging threats into protections; a data platform that aggregates and enriches\ninformation while providing observability, monitoring, and ML operations; and a\nrelease platform enabling safe, rapid detection updates without disrupting\ncustomer workflows. Together, these components deliver layered protection\nagainst evolving LLM threats while generating training data for continuous\nmodel improvement and deploying updates without interrupting production."}
{"id": "2509.20686", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.20686", "abs": "https://arxiv.org/abs/2509.20686", "authors": ["Rian Adam Rajagede", "Yan Solihin"], "title": "Reliability Analysis of Fully Homomorphic Encryption Systems Under Memory Faults", "comment": null, "summary": "Fully Homomorphic Encryption (FHE) represents a paradigm shift in\ncryptography, enabling computation directly on encrypted data and unlocking\nprivacy-critical computation. Despite being increasingly deployed in real\nplatforms, the reliability aspects of FHE systems, especially how they respond\nto faults, have been mostly neglected. This paper aims to better understand of\nhow FHE computation behaves in the presence of memory faults, both in terms of\nindividual operations as well as at the level of applications, for different\nFHE schemes. Finally, we investigate how effective traditional and FHE-specific\nfault mitigation techniques are."}
{"id": "2509.20714", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20714", "abs": "https://arxiv.org/abs/2509.20714", "authors": ["Anh Tu Ngo", "Anupam Chattopadhyay", "Subhamoy Maitra"], "title": "Cryptographic Backdoor for Neural Networks: Boon and Bane", "comment": "Preprint", "summary": "In this paper we show that cryptographic backdoors in a neural network (NN)\ncan be highly effective in two directions, namely mounting the attacks as well\nas in presenting the defenses as well. On the attack side, a carefully planted\ncryptographic backdoor enables powerful and invisible attack on the NN.\nConsidering the defense, we present applications: first, a provably robust NN\nwatermarking scheme; second, a protocol for guaranteeing user authentication;\nand third, a protocol for tracking unauthorized sharing of the NN intellectual\nproperty (IP). From a broader theoretical perspective, borrowing the ideas from\nGoldwasser et. al. [FOCS 2022], our main contribution is to show that all these\ninstantiated practical protocol implementations are provably robust. The\nprotocols for watermarking, authentication and IP tracking resist an adversary\nwith black-box access to the NN, whereas the backdoor-enabled adversarial\nattack is impossible to prevent under the standard assumptions. While the\ntheoretical tools used for our attack is mostly in line with the Goldwasser et.\nal. ideas, the proofs related to the defense need further studies. Finally, all\nthese protocols are implemented on state-of-the-art NN architectures with\nempirical results corroborating the theoretical claims. Further, one can\nutilize post-quantum primitives for implementing the cryptographic backdoors,\nlaying out foundations for quantum-era applications in machine learning (ML)."}
{"id": "2509.20767", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20767", "abs": "https://arxiv.org/abs/2509.20767", "authors": ["Ayush Kumar", "Kar Wai Fok", "Vrizlynn L. L. Thing"], "title": "ExpIDS: A Drift-adaptable Network Intrusion Detection System With Improved Explainability", "comment": null, "summary": "Despite all the advantages associated with Network Intrusion Detection\nSystems (NIDSs) that utilize machine learning (ML) models, there is a\nsignificant reluctance among cyber security experts to implement these models\nin real-world production settings. This is primarily because of their opaque\nnature, meaning it is unclear how and why the models make their decisions. In\nthis work, we design a deep learning-based NIDS, ExpIDS to have high decision\ntree explanation fidelity, i.e., the predictions of decision tree explanation\ncorresponding to ExpIDS should be as close to ExpIDS's predictions as possible.\nExpIDS can also adapt to changes in network traffic distribution (drift). With\nthe help of extensive experiments, we verify that ExpIDS achieves higher\ndecision tree explanation fidelity and a malicious traffic detection\nperformance comparable to state-of-the-art NIDSs for common attacks with\nvarying levels of real-world drift."}
{"id": "2509.20796", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20796", "abs": "https://arxiv.org/abs/2509.20796", "authors": ["Yongjiao Li", "Liang Zhu", "Yalin Deng", "Qikun Zhang", "Zhenlei Wang", "Zhu Cao"], "title": "Fast Revocable Attribute-Based Encryption with Data Integrity for Internet of Things", "comment": "16 pages, 7 figures", "summary": "Efficient and secure revocable attribute-based encryption (RABE) is vital for\nensuring flexible and fine-grained access control and data sharing in cloud\nstorage and outsourced data environments within the Internet of Things (IoT).\nHowever, current RABE schemes often struggle to achieve an optimal balance\nbetween efficiency, security, dynamic scalability, and other important\nfeatures, which hampers their practical application. To overcome these\nlimitations, we propose a fast RABE scheme with data integrity for IoT that\nachieves adaptive security with multiple challenge ciphertexts. Our scheme\nsupports the revocation of authorized users and transfers the computationally\nheavy revocation processes to the cloud, thereby easing the computational\nburden on IoT devices. Moreover, it consistently guarantees the integrity and\ncorrectness of data. We have demonstrated its adaptive security within the\ndefined security model with multiple challenge ciphertexts and optimized its\nperformance. Experimental results indicate that our scheme provides better\nperformance than existing solutions. Under the same access policy, our scheme\nreduces computational consumption by 7 to 9 times compared to previous schemes."}
{"id": "2509.20808", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20808", "abs": "https://arxiv.org/abs/2509.20808", "authors": ["Raghul Saravanan", "Sudipta Paria", "Aritra Dasgupta", "Swarup Bhunia", "Sai Manoj P D"], "title": "Intelligent Graybox Fuzzing via ATPG-Guided Seed Generation and Submodule Analysis", "comment": "7 pages, 6 figures, 4 tables", "summary": "Hardware Fuzzing emerged as one of the crucial techniques for finding\nsecurity flaws in modern hardware designs by testing a wide range of input\nscenarios. One of the main challenges is creating high-quality input seeds that\nmaximize coverage and speed up verification. Coverage-Guided Fuzzing (CGF)\nmethods help explore designs more effectively, but they struggle to focus on\nspecific parts of the hardware. Existing Directed Gray-box Fuzzing (DGF)\ntechniques like DirectFuzz try to solve this by generating targeted tests, but\nit has major drawbacks, such as supporting only limited hardware description\nlanguages, not scaling well to large circuits, and having issues with\nabstraction mismatches. To address these problems, we introduce a novel\nframework, PROFUZZ, that follows the DGF approach and combines fuzzing with\nAutomatic Test Pattern Generation (ATPG) for more efficient fuzzing. By\nleveraging ATPG's structural analysis capabilities, PROFUZZ can generate\nprecise input seeds that target specific design regions more effectively while\nmaintaining high fuzzing throughput. Our experiments show that PROFUZZ scales\n30x better than DirectFuzz when handling multiple target sites, improves\ncoverage by 11.66%, and runs 2.76x faster, highlighting its scalability and\neffectiveness for directed fuzzing in complex hardware systems."}
{"id": "2509.20835", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20835", "abs": "https://arxiv.org/abs/2509.20835", "authors": ["Yu Liu", "Boxiang He", "Fanggang Wang"], "title": "Security-aware Semantic-driven ISAC via Paired Adversarial Residual Networks", "comment": null, "summary": "This paper proposes a novel and flexible security-aware semantic-driven\nintegrated sensing and communication (ISAC) framework, namely security semantic\nISAC (SS-ISAC). Inspired by the positive impact of the adversarial attack, a\npair of pluggable encryption and decryption modules is designed in the proposed\nSS-ISAC framework. The encryption module is installed after the semantic\ntransmitter, adopting a trainable adversarial residual network (ARN) to create\nthe adversarial attack. Correspondingly, the decryption module before the\nsemantic receiver utilizes another trainable ARN to mitigate the adversarial\nattack and noise. These two modules can be flexibly assembled considering the\nsystem security demands, without drastically modifying the hardware\ninfrastructure. To ensure the sensing and communication (SAC) performance while\npreventing the eavesdropping threat, the above ARNs are jointly optimized by\nminimizing a carefully designed loss function that relates to the adversarial\nattack power, SAC performance, as well as the privacy leakage risk. Simulation\nresults validate the effectiveness of the proposed SS-ISAC framework in terms\nof both SAC and eavesdropping prevention performance."}
{"id": "2509.20861", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20861", "abs": "https://arxiv.org/abs/2509.20861", "authors": ["Chao Zha", "Haolin Pan", "Bing Bai", "Jiangxing Wu", "Ruyun Zhang"], "title": "FlowXpert: Context-Aware Flow Embedding for Enhanced Traffic Detection in IoT Network", "comment": null, "summary": "In the Internet of Things (IoT) environment, continuous interaction among a\nlarge number of devices generates complex and dynamic network traffic, which\nposes significant challenges to rule-based detection approaches. Machine\nlearning (ML)-based traffic detection technology, capable of identifying\nanomalous patterns and potential threats within this traffic, serves as a\ncritical component in ensuring network security. This study first identifies a\nsignificant issue with widely adopted feature extraction tools (e.g.,\nCICMeterFlow): the extensive use of time- and length-related features leads to\nhigh sparsity, which adversely affects model convergence. Furthermore, existing\ntraffic detection methods generally lack an embedding mechanism capable of\nefficiently and comprehensively capturing the semantic characteristics of\nnetwork traffic. To address these challenges, we propose a novel feature\nextraction tool that eliminates traditional time and length features in favor\nof context-aware semantic features related to the source host, thus improving\nthe generalizability of the model. In addition, we design an embedding training\nframework that integrates the unsupervised DBSCAN clustering algorithm with a\ncontrastive learning strategy to effectively capture fine-grained semantic\nrepresentations of traffic. Extensive empirical evaluations are conducted on\nthe real-world Mawi data set to validate the proposed method in terms of\ndetection accuracy, robustness, and generalization. Comparative experiments\nagainst several state-of-the-art (SOTA) models demonstrate the superior\nperformance of our approach. Furthermore, we confirm its applicability and\ndeployability in real-time scenarios."}
{"id": "2509.20880", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.20880", "abs": "https://arxiv.org/abs/2509.20880", "authors": ["Cheng Lyu", "Mu Yuan", "Dabin Zheng", "Siwei Sun", "Shun Li"], "title": "A Generalized $Ï_n$-Function", "comment": null, "summary": "The mapping $\\chi_n$ from $\\F_{2}^{n}$ to itself defined by $y=\\chi_n(x)$\nwith $y_i=x_i+x_{i+2}(1+x_{i+1})$, where the indices are computed modulo $n$,\nhas been widely studied for its applications in lightweight cryptography.\nHowever, $\\chi_n $ is bijective on $\\F_2^n$ only when $n$ is odd, restricting\nits use to odd-dimensional vector spaces over $\\F_2$. To address this\nlimitation, we introduce and analyze the generalized mapping $\\chi_{n, m}$\ndefined by $y=\\chi_{n,m}(x)$ with $y_i=x_i+x_{i+m} (x_{i+m-1}+1)(x_{i+m-2}+1)\n\\cdots (x_{i+1}+1)$, where $m$ is a fixed integer with $m\\nmid n$. To\ninvestigate such mappings, we further generalize $\\chi_{n,m}$ to $\\theta_{m,\nk}$, where $\\theta_{m, k}$ is given by $y_i=x_{i+mk} \\prod_{\\substack{j=1,\\,\\,\nm \\nmid j}}^{mk-1} \\left(x_{i+j}+1\\right), \\,\\,{\\rm for }\\,\\, i\\in\n\\{0,1,\\ldots,n-1\\}$. We prove that these mappings generate an abelian group\nisomorphic to the group of units in $\\F_2[z]/(z^{\\lfloor n/m\\rfloor +1})$. This\nstructural insight enables us to construct a broad class of permutations over\n$\\F_2^n$ for any positive integer $n$, along with their inverses. We rigorously\nanalyze algebraic properties of these mappings, including their iterations,\nfixed points, and cycle structures. Additionally, we provide a comprehensive\ndatabase of the cryptographic properties for iterates of $\\chi_{n,m}$ for small\nvalues of $n$ and $m$. Finally, we conduct a comparative security and\nimplementation cost analysis among $\\chi_{n,m}$, $\\chi_n$, $\\chi\\chi_n$\n(EUROCRYPT 2025 \\cite{belkheyar2025chi}) and their variants, and prove\nConjecture~1 proposed in~\\cite{belkheyar2025chi} as a by-product of our study.\nOur results lead to generalizations of $\\chi_n$, providing alternatives to\n$\\chi_n$ and $\\chi\\chi_n$."}
{"id": "2509.20924", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20924", "abs": "https://arxiv.org/abs/2509.20924", "authors": ["Hanbo Huang", "Yiran Zhang", "Hao Zheng", "Xuan Gong", "Yihan Li", "Lin Liu", "Shiyu Liang"], "title": "RLCracker: Exposing the Vulnerability of LLM Watermarks with Adaptive RL Attacks", "comment": null, "summary": "Large Language Models (LLMs) watermarking has shown promise in detecting\nAI-generated content and mitigating misuse, with prior work claiming robustness\nagainst paraphrasing and text editing. In this paper, we argue that existing\nevaluations are not sufficiently adversarial, obscuring critical\nvulnerabilities and overstating the security. To address this, we introduce\nadaptive robustness radius, a formal metric that quantifies watermark\nresilience against adaptive adversaries. We theoretically prove that optimizing\nthe attack context and model parameters can substantially reduce this radius,\nmaking watermarks highly susceptible to paraphrase attacks. Leveraging this\ninsight, we propose RLCracker, a reinforcement learning (RL)-based adaptive\nattack that erases watermarks while preserving semantic fidelity. RLCracker\nrequires only limited watermarked examples and zero access to the detector.\nDespite weak supervision, it empowers a 3B model to achieve 98.5% removal\nsuccess and an average 0.92 P-SP score on 1,500-token Unigram-marked texts\nafter training on only 100 short samples. This performance dramatically exceeds\n6.75% by GPT-4o and generalizes across five model sizes over ten watermarking\nschemes. Our results confirm that adaptive attacks are broadly effective and\npose a fundamental threat to current watermarking defenses."}
{"id": "2509.20943", "categories": ["cs.CR", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.20943", "abs": "https://arxiv.org/abs/2509.20943", "authors": ["Dincy R. Arikkat", "Sneha B. T.", "Serena Nicolazzo", "Antonino Nocera", "Vinod P.", "Rafidha Rehiman K. A.", "Karthika R"], "title": "CTI Dataset Construction from Telegram", "comment": null, "summary": "Cyber Threat Intelligence (CTI) enables organizations to anticipate, detect,\nand mitigate evolving cyber threats. Its effectiveness depends on high-quality\ndatasets, which support model development, training, evaluation, and\nbenchmarking. Building such datasets is crucial, as attack vectors and\nadversary tactics continually evolve. Recently, Telegram has gained prominence\nas a valuable CTI source, offering timely and diverse threat-related\ninformation that can help address these challenges. In this work, we address\nthese challenges by presenting an end-to-end automated pipeline that\nsystematically collects and filters threat-related content from Telegram. The\npipeline identifies relevant Telegram channels and scrapes 145,349 messages\nfrom 12 curated channels out of 150 identified sources. To accurately filter\nthreat intelligence messages from generic content, we employ a BERT-based\nclassifier, achieving an accuracy of 96.64%. From the filtered messages, we\ncompile a dataset of 86,509 malicious Indicators of Compromise, including\ndomains, IPs, URLs, hashes, and CVEs. This approach not only produces a\nlarge-scale, high-fidelity CTI dataset but also establishes a foundation for\nfuture research and operational applications in cyber threat detection."}
{"id": "2509.20972", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20972", "abs": "https://arxiv.org/abs/2509.20972", "authors": ["Ibrahim Altan", "Abdulla Bachir", "Yousuf Parbhulkar", "Abdul Muksith Rizvi", "Moshiur Farazi"], "title": "Dual-Path Phishing Detection: Integrating Transformer-Based NLP with Structural URL Analysis", "comment": "Paper accepted for presentation at the ACS/IEEE 22nd International\n  Conference on Computer Systems and Applications (AICCSA 2025)", "summary": "Phishing emails pose a persistent and increasingly sophisticated threat,\nundermining email security through deceptive tactics designed to exploit both\nsemantic and structural vulnerabilities. Traditional detection methods, often\nbased on isolated analysis of email content or embedded URLs, fail to\ncomprehensively address these evolving attacks. In this paper, we propose a\ndual-path phishing detection framework that integrates transformer-based\nnatural language processing (NLP) with classical machine learning to jointly\nanalyze email text and embedded URLs. Our approach leverages the complementary\nstrengths of semantic analysis using fine-tuned transformer architectures\n(e.g., DistilBERT) and structural link analysis via character-level TF-IDF\nvectorization paired with classical classifiers (e.g., Random Forest).\nEmpirical evaluation on representative email and URL datasets demonstrates that\nthis combined approach significantly improves detection accuracy. Specifically,\nthe DistilBERT model achieves a near-optimal balance between accuracy and\ncomputational efficiency for textual phishing detection, while Random Forest\nnotably outperforms other classical classifiers in identifying malicious URLs.\nThe modular design allows flexibility for standalone deployment or ensemble\nintegration, facilitating real-world adoption. Collectively, our results\nhighlight the efficacy and practical value of this dual-path approach,\nestablishing a scalable, accurate, and interpretable solution capable of\nenhancing email security against contemporary phishing threats."}
{"id": "2509.21011", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21011", "abs": "https://arxiv.org/abs/2509.21011", "authors": ["Ping He", "Changjiang Li", "Binbin Zhao", "Tianyu Du", "Shouling Ji"], "title": "Automatic Red Teaming LLM-based Agents with Model Context Protocol Tools", "comment": null, "summary": "The remarkable capability of large language models (LLMs) has led to the wide\napplication of LLM-based agents in various domains. To standardize interactions\nbetween LLM-based agents and their environments, model context protocol (MCP)\ntools have become the de facto standard and are now widely integrated into\nthese agents. However, the incorporation of MCP tools introduces the risk of\ntool poisoning attacks, which can manipulate the behavior of LLM-based agents.\nAlthough previous studies have identified such vulnerabilities, their red\nteaming approaches have largely remained at the proof-of-concept stage, leaving\nthe automatic and systematic red teaming of LLM-based agents under the MCP tool\npoisoning paradigm an open question. To bridge this gap, we propose\nAutoMalTool, an automated red teaming framework for LLM-based agents by\ngenerating malicious MCP tools. Our extensive evaluation shows that AutoMalTool\neffectively generates malicious MCP tools capable of manipulating the behavior\nof mainstream LLM-based agents while evading current detection mechanisms,\nthereby revealing new security risks in these agents."}
{"id": "2509.21057", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21057", "abs": "https://arxiv.org/abs/2509.21057", "authors": ["Jiahao Huo", "Shuliang Liu", "Bin Wang", "Junyan Zhang", "Yibo Yan", "Aiwei Liu", "Xuming Hu", "Mingxun Zhou"], "title": "PMark: Towards Robust and Distortion-free Semantic-level Watermarking with Channel Constraints", "comment": null, "summary": "Semantic-level watermarking (SWM) for large language models (LLMs) enhances\nwatermarking robustness against text modifications and paraphrasing attacks by\ntreating the sentence as the fundamental unit. However, existing methods still\nlack strong theoretical guarantees of robustness, and reject-sampling-based\ngeneration often introduces significant distribution distortions compared with\nunwatermarked outputs. In this work, we introduce a new theoretical framework\non SWM through the concept of proxy functions (PFs) $\\unicode{x2013}$ functions\nthat map sentences to scalar values. Building on this framework, we propose\nPMark, a simple yet powerful SWM method that estimates the PF median for the\nnext sentence dynamically through sampling while enforcing multiple PF\nconstraints (which we call channels) to strengthen watermark evidence. Equipped\nwith solid theoretical guarantees, PMark achieves the desired distortion-free\nproperty and improves the robustness against paraphrasing-style attacks. We\nalso provide an empirically optimized version that further removes the\nrequirement for dynamical median estimation for better sampling efficiency.\nExperimental results show that PMark consistently outperforms existing SWM\nbaselines in both text quality and robustness, offering a more effective\nparadigm for detecting machine-generated text. Our code will be released at\n[this URL](https://github.com/PMark-repo/PMark)."}
{"id": "2509.21147", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21147", "abs": "https://arxiv.org/abs/2509.21147", "authors": ["Amr Akmal Abouelmagd", "Amr Hilal"], "title": "Emerging Paradigms for Securing Federated Learning Systems", "comment": null, "summary": "Federated Learning (FL) facilitates collaborative model training while\nkeeping raw data decentralized, making it a conduit for leveraging the power of\nIoT devices while maintaining privacy of the locally collected data. However,\nexisting privacy- preserving techniques present notable hurdles. Methods such\nas Multi-Party Computation (MPC), Homomorphic Encryption (HE), and Differential\nPrivacy (DP) often incur high compu- tational costs and suffer from limited\nscalability. This survey examines emerging approaches that hold promise for\nenhancing both privacy and efficiency in FL, including Trusted Execution\nEnvironments (TEEs), Physical Unclonable Functions (PUFs), Quantum Computing\n(QC), Chaos-Based Encryption (CBE), Neuromorphic Computing (NC), and Swarm\nIntelligence (SI). For each paradigm, we assess its relevance to the FL\npipeline, outlining its strengths, limitations, and practical considerations.\nWe conclude by highlighting open challenges and prospective research avenues,\noffering a detailed roadmap for advancing secure and scalable FL systems."}
