<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 16]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [State of Passkey Authentication in the Wild: A Census of the Top 100K sites](https://arxiv.org/abs/2602.15135)
*Prince Bhardwaj,Nishanth Sastry*

Main category: cs.CR

TL;DR: 该论文介绍了Fidentikit工具，用于大规模检测网站对Passkeys（可发现WebAuthn凭证）的采用情况，并通过对前10万个域名的普查揭示了采用模式。


<details>
  <summary>Details</summary>
Motivation: Passkeys作为密码认证的未来被广泛推广，但自2022年推出以来，其在实际网络中的采用程度仍然未知。由于网站实现方式各异（显式按钮、多步流程、条件调解等），缺乏标准化发现端点，且动态JavaScript页面使自动检测复杂化，因此需要开发专门的检测工具来测量真实采用情况。

Method: 开发了Fidentikit浏览器爬虫工具，通过迭代优化基于1500个网站手动检查的经验，实现了43种启发式方法，涵盖五个类别：UI元素、DOM结构、WebAuthn API调用、网络模式和库检测。然后使用该工具对Tranco排名前10万个域名进行大规模普查。

Result: 研究结果显示，Passkeys的采用与网站流行度密切相关，且通常依赖于外部身份提供商而非原生实现。这是首次对Passkeys采用情况的大规模普查，揭示了实际部署模式。

Conclusion: 该论文成功开发了能够检测异构Passkeys实现的工具Fidentikit，并通过大规模测量揭示了Passkeys采用的实际状况，为理解这一新兴认证技术的部署提供了重要数据。

Abstract: Passkeys -- discoverable WebAuthn credentials synchronised across devices are widely promoted as the future of passwordless authentication. Built on the FIDO2 standard, they eliminate shared secrets and resist phishing while offering usability through platform credential managers. Since their introduction in 2022, major vendors have integrated passkeys into operating systems and browsers, and prominent websites have announced support. Yet the true extent of adoption across the broader web remains unknown.
  Measuring this is challenging because websites implement passkeys in heterogeneous ways. Some expose explicit ``Sign in with passkey'' buttons, others hide options under multi-step flows or rely on conditional mediation, and many adopt external mechanisms such as JavaScript libraries or OAuth-based identity providers. There is no standardised discovery endpoint, and dynamic, JavaScript-heavy pages complicate automated detection.
  This paper makes two contributions. First, we present Fidentikit, a browser-based crawler implementing 43 heuristics across five categories -- UI elements, DOM structures, WebAuthn API calls, network patterns, and library detection developed through iterative refinement over manual examination of 1,500 sites. Second, we apply Fidentikit to the top 100,000 Tranco-ranked domains, producing the first large-scale census of passkey adoption. Our results show adoption strongly correlates with site popularity and often depends on external identity providers rather than native implementations.

</details>


### [2] [Exploiting Layer-Specific Vulnerabilities to Backdoor Attack in Federated Learning](https://arxiv.org/abs/2602.15161)
*Mohammad Hadi Foroughi,Seyed Hamed Rastegar,Mohammad Sabokrou,Ahmad Khonsari*

Main category: cs.CR

TL;DR: 论文提出了一种名为层平滑攻击（LSA）的新型后门攻击方法，专门针对联邦学习系统，通过识别和操纵神经网络中的关键层来注入持久后门，同时逃避现有防御机制。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然解决了数据隐私问题，但其去中心化特性引入了新的安全漏洞，特别是后门攻击威胁模型完整性。现有防御机制存在不足，需要研究更隐蔽的攻击方法来揭示系统根本性漏洞。

Method: 1. 提出层替代分析方法，系统识别对后门成功贡献最大的关键层（BC层）；2. 设计层平滑攻击（LSA），策略性地操纵这些BC层来注入持久后门，同时保持主任务的高准确率。

Result: 在不同模型架构和数据集上的实验表明，LSA实现了高达97%的后门成功率，同时保持主任务的高准确率，并能持续绕过现代联邦学习防御机制。

Conclusion: 研究揭示了当前联邦学习安全框架的根本性漏洞，表明未来的防御必须采用层感知的检测和缓解策略，以应对这种新型攻击。

Abstract: Federated learning (FL) enables distributed model training across edge devices while preserving data locality. This decentralized approach has emerged as a promising solution for collaborative learning on sensitive user data, effectively addressing the longstanding privacy concerns inherent in centralized systems. However, the decentralized nature of FL exposes new security vulnerabilities, especially backdoor attacks that threaten model integrity. To investigate this critical concern, this paper presents the Layer Smoothing Attack (LSA), a novel backdoor attack that exploits layer-specific vulnerabilities in neural networks. First, a Layer Substitution Analysis methodology systematically identifies backdoor-critical (BC) layers that contribute most significantly to backdoor success. Subsequently, LSA strategically manipulates these BC layers to inject persistent backdoors while remaining undetected by state-of-the-art defense mechanisms. Extensive experiments across diverse model architectures and datasets demonstrate that LSA achieves a remarkably backdoor success rate of up to 97% while maintaining high model accuracy on the primary task, consistently bypassing modern FL defenses. These findings uncover fundamental vulnerabilities in current FL security frameworks, demonstrating that future defenses must incorporate layer-aware detection and mitigation strategies.

</details>


### [3] [Weight space Detection of Backdoors in LoRA Adapters](https://arxiv.org/abs/2602.15195)
*David Puertolas Merenciano,Ekaterina Vasyagina,Raghav Dixit,Kevin Zhu,Ruizhe Li,Javier Ferrando,Maheep Chaudhary*

Main category: cs.CR

TL;DR: 提出一种无需运行模型即可检测LoRA适配器后门攻击的数据无关方法，通过分析权重矩阵的奇异值统计特征实现高效筛查


<details>
  <summary>Details</summary>
Motivation: 当前LoRA适配器在开源仓库中共享，容易受到后门攻击。现有检测方法需要运行模型测试输入数据，不适用于筛查数千个适配器且后门触发器未知的情况

Method: 通过直接分析LoRA适配器的权重矩阵（无需运行模型），提取奇异值集中度、熵值和分布形状等简单统计特征，识别偏离正常模式的后门适配器

Result: 在500个LoRA适配器（400个干净，100个中毒）上评估，针对Llama-3.2-3B模型在多个指令和推理数据集上测试，达到97%的检测准确率，假阳性率低于2%

Conclusion: 该方法提供了一种高效、数据无关的LoRA适配器后门检测方案，能够在大规模适配器筛查中有效识别恶意适配器，具有实际应用价值

Abstract: LoRA adapters let users fine-tune large language models (LLMs) efficiently. However, LoRA adapters are shared through open repositories like Hugging Face Hub \citep{huggingface_hub_docs}, making them vulnerable to backdoor attacks. Current detection methods require running the model with test input data -- making them impractical for screening thousands of adapters where the trigger for backdoor behavior is unknown. We detect poisoned adapters by analyzing their weight matrices directly, without running the model -- making our method data-agnostic. Our method extracts simple statistics -- how concentrated the singular values are, their entropy, and the distribution shape -- and flags adapters that deviate from normal patterns. We evaluate the method on 500 LoRA adapters -- 400 clean, and 100 poisoned for Llama-3.2-3B on instruction and reasoning datasets: Alpaca, Dolly, GSM8K, ARC-Challenge, SQuADv2, NaturalQuestions, HumanEval, and GLUE dataset. We achieve 97\% detection accuracy with less than 2\% false positives.

</details>


### [4] [A Scan-Based Analysis of Internet-Exposed IoT Devices Using Shodan Data](https://arxiv.org/abs/2602.15263)
*Richelle Williams,Fernando Koch*

Main category: cs.CR

TL;DR: 分析物联网安全中扫描可观测网络配置是否编码群体级暴露风险，通过多国样本研究发现存在一致的跨国暴露结构差异


<details>
  <summary>Details</summary>
Motivation: 解决物联网安全中的一个开放测量问题：扫描可观测的网络配置是否能够编码超越单个设备的群体级暴露风险

Method: 使用Shodan Search和Shodan InternetDB获取多国控制样本，选择100个通过TCP端口7547（TR-069/CWMP）识别的物联网端点，均匀分布在10个代表性国家。通过特征相关性评估、跨国开放和风险端口暴露比较、以及监督分类分析高风险暴露配置文件

Result: 分析揭示了跨国暴露结构的一致差异，每个主机的平均风险端口数量在0.4到1.0之间，分类高风险暴露配置文件时达到约0.61的平衡准确率

Conclusion: 扫描可观测的网络配置确实编码了群体级的暴露风险，存在可测量的跨国差异，这为物联网安全风险评估提供了新的视角和方法

Abstract: An open measurement problem in IoT security is whether scan-observable network configurations encode population-level exposure risk beyond individual devices. An analysis of internet-exposed IoT endpoints using a controlled multi-country sample from Shodan Search and Shodan InternetDB, selecting 100 hosts identified via TCP port 7547 (TR-069/CWMP) and evenly distributed across the ten most represented countries. Hosts are enriched with scan-derived metadata and analyzed using feature-relevance assessment, cross-country comparisons of open and risky port exposure, and supervised classification of higher-risk exposure profiles. The analysis reveals consistent cross-country differences in exposure structure, with mean risky-port counts ranging from 0.4 to 1.0 per host, and achieves balanced accuracy of approximately 0.61 when classifying higher-risk exposure profiles.

</details>


### [5] [Intellicise Wireless Networks Meet Agentic AI: A Security and Privacy Perspective](https://arxiv.org/abs/2602.15290)
*Rui Meng,Zhidi Zhang,Song Gao,Yaheng Wang,Xiaodong Xu,Yijing Lin,Yiming Liu,Chenyuan Feng,Lexi Xu,Yi Ma,Ping Zhang,Rahim Tafazolli*

Main category: cs.CR

TL;DR: 该论文分析了Agentic AI在智能无线网络中的安全应用，提出了分类框架并识别了安全挑战，通过案例研究展示了其防御窃听攻击的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着智能无线网络（Intellicise）成为未来移动通信系统演进的主要方向，其中Agentic AI因其通过感知-记忆-推理-行动循环展现的高级认知能力而受到关注。论文旨在探索Agentic AI在智能无线网络中的安全应用潜力。

Method: 首先分析Agentic AI在智能无线网络中的独特优势，然后提出Agentic AI增强的安全智能无线网络的结构化分类框架。基于此框架识别Agentic AI引入的新兴安全和隐私挑战，并总结针对这些漏洞的策略。最后通过案例研究展示Agentic AI在防御智能窃听攻击中的有效性。

Result: 提出了Agentic AI增强的安全智能无线网络的分类框架，识别了相关的安全和隐私挑战，总结了应对策略，并通过案例研究证明了Agentic AI在防御智能窃听攻击方面的有效性。

Conclusion: Agentic AI在智能无线网络安全中具有重要应用价值，论文为未来该领域的研究提供了框架和方向，指出了关键开放研究方向以指导未来探索。

Abstract: Intellicise (Intelligent and Concise) wireless network is the main direction of the evolution of future mobile communication systems, a perspective now widely acknowledged across academia and industry. As a key technology within it, Agentic AI has garnered growing attention due to its advanced cognitive capabilities, enabled through continuous perception-memory-reasoning-action cycles. This paper first analyses the unique advantages that Agentic AI introduces to intellicise wireless networks. We then propose a structured taxonomy for Agentic AI-enhanced secure intellicise wireless networks. Building on this framework, we identify emerging security and privacy challenges introduced by Agentic AI and summarize targeted strategies to address these vulnerabilities. A case study further demonstrates Agentic AI's efficacy in defending against intelligent eavesdropping attacks. Finally, we outline key open research directions to guide future exploration in this field.

</details>


### [6] [Unforgeable Watermarks for Language Models via Robust Signatures](https://arxiv.org/abs/2602.15323)
*Huijia Lin,Kameron Shahabi,Min Jae Song*

Main category: cs.CR

TL;DR: 该论文提出首个不可检测的水印方案，具备鲁棒性、不可伪造性和可恢复性，通过新型密码学原语——鲁棒数字签名，增强内容溯源能力。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型生成文本难以与人类写作区分，需要更强的内容溯源工具。现有水印方案在防止错误归属方面保护有限，需要更强的安全保证。

Method: 引入不可伪造性和可恢复性两个新保证，构建首个针对替换扰动（汉明距离）的不可检测水印方案。核心技术是鲁棒数字签名，利用属性保持哈希函数将标准数字签名方案提升为鲁棒版本。

Result: 提出了首个具备不可检测性、鲁棒性、不可伪造性和可恢复性的水印方案，能够将内容唯一链接到其生成模型，实现安全归属和细粒度溯源。

Conclusion: 通过引入不可伪造性和可恢复性保证，显著增强了水印方案的内容所有权保护能力，为语言模型生成文本的安全溯源提供了更强大的工具。

Abstract: Language models now routinely produce text that is difficult to distinguish from human writing, raising the need for robust tools to verify content provenance. Watermarking has emerged as a promising countermeasure, with existing work largely focused on model quality preservation and robust detection. However, current schemes provide limited protection against false attribution. We strengthen the notion of soundness by introducing two novel guarantees: unforgeability and recoverability. Unforgeability prevents adversaries from crafting false positives, texts that are far from any output from the watermarked model but are nonetheless flagged as watermarked. Recoverability provides an additional layer of protection: whenever a watermark is detected, the detector identifies the source text from which the flagged content was derived. Together, these properties strengthen content ownership by linking content exclusively to its generating model, enabling secure attribution and fine-grained traceability. We construct the first undetectable watermarking scheme that is robust, unforgeable, and recoverable with respect to substitutions (i.e., perturbations in Hamming metric). The key technical ingredient is a new cryptographic primitive called robust (or recoverable) digital signatures, which allow verification of messages that are close to signed ones, while preventing forgery of messages that are far from all previously signed messages. We show that any standard digital signature scheme can be boosted to a robust one using property-preserving hash functions (Boyle, LaVigne, and Vaikuntanathan, ITCS 2019).

</details>


### [7] [MarkSweep: A No-box Removal Attack on AI-Generated Image Watermarking via Noise Intensification and Frequency-aware Denoising](https://arxiv.org/abs/2602.15364)
*Jie Cao,Zelin Zhang,Qi Li,Jianbing Ni*

Main category: cs.CR

TL;DR: MarkSweep是一种新型AI水印移除攻击方法，通过边缘感知高斯扰动放大高频区域水印噪声，训练去噪网络来有效擦除AI生成图像中的水印，同时保持视觉质量。


<details>
  <summary>Details</summary>
Motivation: AI水印技术用于为AI生成图像提供来源信息和身份识别，但现有水印系统可能存在安全漏洞。本文旨在研究AI水印的脆弱性，开发能够有效移除水印而不影响图像质量的方法。

Method: MarkSweep方法包含两个阶段：1) 通过边缘感知高斯扰动放大高频区域的水印噪声，并将其注入干净图像中训练去噪网络；2) 去噪网络包含可学习频率分解模块和频率感知融合模块，用于抑制放大噪声并消除水印痕迹。

Result: 实验表明MarkSweep能有效移除HiDDeN和Stable Signature水印方案中的水印，将比特准确率降至67%以下，同时保持AI生成图像的感知质量。理论分析和大量实验证明不可见水印对MarkSweep攻击高度脆弱。

Conclusion: 本文提出的MarkSweep攻击方法揭示了当前AI水印技术的安全漏洞，表明不可见水印容易被移除而不影响图像质量，这对AI生成内容的认证和溯源系统提出了安全挑战。

Abstract: AI watermarking embeds invisible signals within images to provide provenance information and identify content as AI-generated. In this paper, we introduce MarkSweep, a novel watermark removal attack that effectively erases the embedded watermarks from AI-generated images without degrading visual quality. MarkSweep first amplifies watermark noise in high-frequency regions via edge-aware Gaussian perturbations and injects it into clean images for training a denoising network. This network then integrates two modules, the learnable frequency decomposition module and the frequency-aware fusion module, to suppress amplified noise and eliminate watermark traces. Theoretical analysis and extensive experiments demonstrate that invisible watermarks are highly vulnerable to MarkSweep, which effectively removes embedded watermarks, reducing the bit accuracy of HiDDeN and Stable Signature watermarking schemes to below 67%, while preserving perceptual quality of AI-generated images.

</details>


### [8] [A Unified Evaluation of Learning-Based Similarity Techniques for Malware Detection](https://arxiv.org/abs/2602.15376)
*Udbhav Prasad,Aniesh Chawla*

Main category: cs.CR

TL;DR: 本文系统比较了基于学习的分类和相似性方法在安全应用中的表现，发现没有单一方法在所有维度都表现优异，需要结合互补技术。


<details>
  <summary>Details</summary>
Motivation: 传统加密摘要（如MD5、SHA-256）虽然能提供精确身份验证，但在威胁狩猎、恶意软件分析和数字取证等实际安全任务中存在局限性，因为攻击者经常进行微小变换。相似性技术可以解决这一问题，但目前各种方法缺乏统一的评估框架。

Method: 使用大型公开数据集，在统一的实验框架下系统比较基于学习的分类和相似性方法，采用行业公认的评估指标，包括相似性摘要、局部敏感哈希（如ssdeep、sdhash、TLSH）以及基于机器学习的嵌入方法。

Result: 研究结果显示，没有任何单一方法在所有维度上都表现优异，每种方法都有其独特的权衡取舍。基于学习的相似性技术在特定场景下表现良好，但需要根据具体应用场景选择合适的方法。

Conclusion: 有效的恶意软件分析和威胁狩猎平台需要结合互补的分类和相似性技术，而不是依赖单一方法。这是首个可重复的基准研究，为实际安全工作量身定制。

Abstract: Cryptographic digests (e.g., MD5, SHA-256) are designed to provide exact identity. Any single-bit change in the input produces a completely different hash, which is ideal for integrity verification but limits their usefulness in many real-world tasks like threat hunting, malware analysis and digital forensics, where adversaries routinely introduce minor transformations. Similarity-based techniques address this limitation by enabling approximate matching, allowing related byte sequences to produce measurably similar fingerprints. Modern enterprises manage tens of thousands of endpoints with billions of files, making the effectiveness and scalability of the proposed techniques more important than ever in security applications. Security researchers have proposed a range of approaches, including similarity digests and locality-sensitive hashes (e.g., ssdeep, sdhash, TLSH), as well as more recent machine-learning-based methods that generate embeddings from file features. However, these techniques have largely been evaluated in isolation, using disparate datasets and evaluation criteria. This paper presents a systematic comparison of learning-based classification and similarity methods using large, publicly available datasets. We evaluate each method under a unified experimental framework with industry-accepted metrics. To our knowledge, this is the first reproducible study to benchmark these diverse learning-based similarity techniques side by side for real-world security workloads. Our results show that no single approach performs well across all dimensions; instead, each exhibits distinct trade-offs, indicating that effective malware analysis and threat-hunting platforms must combine complementary classification and similarity techniques rather than rely on a single method.

</details>


### [9] [MEV in Binance Builder](https://arxiv.org/abs/2602.15395)
*Qin Wang,Ruiqiang Li,Guangsheng Yu,Vincent Gramoli,Shiping Chen*

Main category: cs.CR

TL;DR: 该论文实证研究发现BNB智能链的MEV套利高度集中在少数白名单构建者手中，48Club和Blockrazor控制了96%的区块并捕获92%的MEV利润，短区块间隔和白名单机制导致竞争窗口缩小，加剧了中心化问题。


<details>
  <summary>Details</summary>
Motivation: 研究BNB智能链的构建者驱动MEV套利，特别关注其独特的PBS设计（白名单构建者、短区块间隔、私有订单流绕过公共内存池）可能导致的中心化问题，这些特性长期引发社区担忧。

Method: 通过实证分析追踪2025年5月至11月期间两个主导构建者（48Club和Blockrazor）的套利活动，研究区块生产分布、MEV利润分配、套利路径特征以及竞争动态。

Result: 发现MEV利润高度集中在短距离、低跳数的包装代币和稳定币套利路径；区块构建迅速向垄断收敛；短区块间隔和白名单PBS设计缩小了MEV竞争的可竞争窗口，放大了延迟优势，排除了较慢的构建者和搜索者。

Conclusion: BNB链的MEV提取不仅比以太坊更加中心化，而且在结构上更容易受到审查和公平性削弱的影响，其设计特征加剧了中心化风险。

Abstract: We study the builder-driven MEV arbitrage on BNB Smart Chain (BSC). BSC's Proposer--Builder Separation (PBS) adopts a leaner design: only whitelisted builders can participate, blocks are produced at shorter intervals, and private order flow bypasses the public mempool. These features have long raised community concerns over centralization, which we empirically confirm by tracing arbitrage activity of the two dominant builders from May to November 2025. Within months, 48Club and Blockrazor produced over 96\% of blocks and captured about 92\% of MEV profits.
  We find that profits concentrate in short, low-hop arbitrage routes over wrapped tokens and stablecoins, and that block construction rapidly converges toward monopoly. Beyond concentration alone, our analysis reveals a structural source of inequality: BSC's short block interval and whitelisted PBS collapse the contestable window for MEV competition, amplifying latency advantages and excluding slower builders and searchers. MEV extraction on BSC is not only more centralized than on Ethereum, but also structurally more vulnerable to censorship and weakened fairness.

</details>


### [10] [SecCodeBench-V2 Technical Report](https://arxiv.org/abs/2602.15485)
*Longfei Chen,Ji Zhao,Lanxiao Cui,Tong Su,Xingbo Pan,Ziyang Li,Yongxing Wu,Qijiang Cao,Qiyao Cai,Jing Zhang,Yuandong Ni,Junyao He,Zeyu Zhang,Chao Ge,Xuhuai Lu,Zeyu Gao,Yuxin Cui,Weisen Chen,Yuxuan Peng,Shengping Wang,Qi Li,Yukai Huang,Yukun Liu,Tuo Zhou,Terry Yue Zhuo,Junyang Lin,Chao Zhang*

Main category: cs.CR

TL;DR: SecCodeBench-V2是一个用于评估LLM代码助手生成安全代码能力的公开基准测试，包含98个基于阿里巴巴工业生产的生成和修复场景，覆盖5种编程语言和22种CWE类别。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估LLM代码助手安全代码生成能力的标准化基准测试，特别是基于真实工业场景、覆盖多种编程语言和安全漏洞类别的评估工具。

Method: 采用函数级任务设计，提供完整项目脚手架，要求模型在固定接口和依赖下实现或修复目标函数。通过动态执行评估，在隔离环境中编译运行模型生成的代码，使用专家编写的PoC测试用例进行功能和安全性验证。

Result: 创建了一个包含98个场景的基准测试，覆盖Java、C、Python、Go、Node.js五种语言和22种CWE类别，提供了统一的评估流程和基于Pass@K的评分协议。

Conclusion: SecCodeBench-V2为评估AI编码助手的安全能力提供了严谨、可复现的基础，有助于推动更安全的代码生成技术发展，所有结果和工具已公开。

Abstract: We introduce SecCodeBench-V2, a publicly released benchmark for evaluating Large Language Model (LLM) copilots' capabilities of generating secure code. SecCodeBench-V2 comprises 98 generation and fix scenarios derived from Alibaba Group's industrial productions, where the underlying security issues span 22 common CWE (Common Weakness Enumeration) categories across five programming languages: Java, C, Python, Go, and Node.js. SecCodeBench-V2 adopts a function-level task formulation: each scenario provides a complete project scaffold and requires the model to implement or patch a designated target function under fixed interfaces and dependencies. For each scenario, SecCodeBench-V2 provides executable proof-of-concept (PoC) test cases for both functional validation and security verification. All test cases are authored and double-reviewed by security experts, ensuring high fidelity, broad coverage, and reliable ground truth. Beyond the benchmark itself, we build a unified evaluation pipeline that assesses models primarily via dynamic execution. For most scenarios, we compile and run model-generated artifacts in isolated environments and execute PoC test cases to validate both functional correctness and security properties. For scenarios where security issues cannot be adjudicated with deterministic test cases, we additionally employ an LLM-as-a-judge oracle. To summarize performance across heterogeneous scenarios and difficulty levels, we design a Pass@K-based scoring protocol with principled aggregation over scenarios and severity, enabling holistic and comparable evaluation across models. Overall, SecCodeBench-V2 provides a rigorous and reproducible foundation for assessing the security posture of AI coding assistants, with results and artifacts released at https://alibaba.github.io/sec-code-bench. The benchmark is publicly available at https://github.com/alibaba/sec-code-bench.

</details>


### [11] [Onto-DP: Constructing Neighborhoods for Differential Privacy on Ontological Databases](https://arxiv.org/abs/2602.15614)
*Yasmine Hayder,Adrien Boiret,Cédric Eichler,Benjamin Nguyen*

Main category: cs.CR

TL;DR: 论文提出Onto-DP（本体感知差分隐私），通过为传统差分隐私模型添加语义感知能力，防止攻击者利用推理规则从数据库中推断敏感信息。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私模型在应用时过于简单，无法有效防止攻击者利用推理规则从数据库中推断敏感信息，存在安全漏洞。

Method: 提出Onto-DP（本体感知差分隐私），作为经典差分隐私模型的扩展，通过引入语义感知能力来增强隐私保护。

Result: Onto-DP被证明是充分条件，能够有效保护数据库免受知晓推理规则的攻击者的攻击。

Conclusion: 需要为差分隐私模型添加语义感知能力，Onto-DP为保护数据库免受推理攻击提供了有效解决方案。

Abstract: In this paper, we investigate how attackers can discover sensitive information embedded within databases by exploiting inference rules. We demonstrate the inadequacy of naively applied existing state of the art differential privacy (DP) models in safeguarding against such attacks. We introduce ontology aware differential privacy (Onto-DP), a novel extension of differential privacy paradigms built on top of any classical DP model by enriching it with semantic awareness. We show that this extension is a sufficient condition to adequately protect against attackers aware of inference rules.

</details>


### [12] [Zombie Agents: Persistent Control of Self-Evolving LLM Agents via Self-Reinforcing Injections](https://arxiv.org/abs/2602.15654)
*Xianglin Yang,Yufei He,Shuo Ji,Bryan Hooi,Jin Song Dong*

Main category: cs.CR

TL;DR: 论文提出"僵尸代理"攻击，攻击者通过受控网页内容间接植入恶意载荷到LLM代理的长期记忆中，使其在后续会话中执行未授权操作，揭示了自进化代理的安全风险。


<details>
  <summary>Details</summary>
Motivation: 自进化LLM代理通过跨会话更新内部状态和重用长期记忆来提升长时任务性能，但这种设计带来了安全风险：良性会话中观察到的不可信外部内容可能被存储为记忆，并在后续被当作指令执行。论文旨在研究这种风险并形式化一种持久性攻击。

Method: 提出黑盒攻击框架，仅通过攻击者控制的网页内容进行间接暴露。攻击分为两个阶段：感染阶段（代理在完成良性任务时读取中毒源，通过正常更新过程将载荷写入长期记忆）和触发阶段（载荷被检索或传递，导致未授权的工具行为）。针对常见内存实现（如滑动窗口和检索增强内存）设计了机制特定的持久性策略，以抵抗截断和相关性过滤。

Result: 在代表性代理设置和任务上评估攻击，测量了随时间持久性和诱导未授权行动的能力，同时保持良性任务质量。结果显示内存进化可以将一次性间接注入转化为持久性妥协。

Conclusion: 自进化代理的内存进化机制使得一次性间接注入能够转化为持久性安全妥协，这表明仅关注每会话提示过滤的防御措施对于自进化代理是不够的，需要更全面的安全防护机制。

Abstract: Self-evolving LLM agents update their internal state across sessions, often by writing and reusing long-term memory. This design improves performance on long-horizon tasks but creates a security risk: untrusted external content observed during a benign session can be stored as memory and later treated as instruction. We study this risk and formalize a persistent attack we call a Zombie Agent, where an attacker covertly implants a payload that survives across sessions, effectively turning the agent into a puppet of the attacker.
  We present a black-box attack framework that uses only indirect exposure through attacker-controlled web content. The attack has two phases. During infection, the agent reads a poisoned source while completing a benign task and writes the payload into long-term memory through its normal update process. During trigger, the payload is retrieved or carried forward and causes unauthorized tool behavior. We design mechanism-specific persistence strategies for common memory implementations, including sliding-window and retrieval-augmented memory, to resist truncation and relevance filtering. We evaluate the attack on representative agent setups and tasks, measuring both persistence over time and the ability to induce unauthorized actions while preserving benign task quality. Our results show that memory evolution can convert one-time indirect injection into persistent compromise, which suggests that defenses focused only on per-session prompt filtering are not sufficient for self-evolving agents.

</details>


### [13] [Revisiting Backdoor Threat in Federated Instruction Tuning from a Signal Aggregation Perspective](https://arxiv.org/abs/2602.15671)
*Haodong Zhao,Jinming Hu,Gongshen Liu*

Main category: cs.CR

TL;DR: 本文挑战了联邦学习中传统后门威胁范式，研究了一种更普遍且隐蔽的威胁：良性客户端数据集中分布的低浓度中毒数据导致的后门漏洞，这在依赖未经验证第三方数据的联邦指令调优中越来越常见。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习安全研究主要关注少数恶意客户端故意破坏模型更新的后门威胁，但本文挑战这一范式，研究更普遍且隐蔽的威胁：良性客户端数据集中分布的低浓度中毒数据导致的后门漏洞。这种场景在依赖未经验证第三方和众包数据的联邦指令调优中越来越常见。

Method: 1) 分析两种后门数据形式：自然触发器（固有特征作为隐式触发器）和对手注入触发器；2) 从信号聚合角度建模后门植入过程，提出后门信噪比量化分布式后门信号的动态特性；3) 进行大量实验验证威胁严重性。

Result: 实验显示威胁严重：仅不到10%的训练数据中毒并分布在客户端中，攻击成功率超过85%，而主要任务性能基本不受影响。关键发现是，针对恶意客户端攻击设计的最先进后门防御对这种威胁基本无效。

Conclusion: 研究发现揭示了一个紧迫需求：需要针对现代去中心化数据生态系统现实量身定制的新防御机制。现有防御措施无法应对分布式低浓度中毒数据带来的后门威胁。

Abstract: Federated learning security research has predominantly focused on backdoor threats from a minority of malicious clients that intentionally corrupt model updates. This paper challenges this paradigm by investigating a more pervasive and insidious threat: \textit{backdoor vulnerabilities from low-concentration poisoned data distributed across the datasets of benign clients.} This scenario is increasingly common in federated instruction tuning for language models, which often rely on unverified third-party and crowd-sourced data. We analyze two forms of backdoor data through real cases: 1) \textit{natural trigger (inherent features as implicit triggers)}; 2) \textit{adversary-injected trigger}. To analyze this threat, we model the backdoor implantation process from signal aggregation, proposing the Backdoor Signal-to-Noise Ratio to quantify the dynamics of the distributed backdoor signal. Extensive experiments reveal the severity of this threat: With just less than 10\% of training data poisoned and distributed across clients, the attack success rate exceeds 85\%, while the primary task performance remains largely intact. Critically, we demonstrate that state-of-the-art backdoor defenses, designed for attacks from malicious clients, are fundamentally ineffective against this threat. Our findings highlight an urgent need for new defense mechanisms tailored to the realities of modern, decentralized data ecosystems.

</details>


### [14] [Privacy-Preserving and Secure Spectrum Sharing for Database-Driven Cognitive Radio Networks](https://arxiv.org/abs/2602.15705)
*Saleh Darzia,Gökcan Cantalib,Attila Altay Yavuza,Gürkan Gür*

Main category: cs.CR

TL;DR: SLAPX是一个统一的安全框架，为数据库驱动的认知无线电网络提供位置隐私、用户匿名、可验证位置和DoS弹性，相比现有方案具有更低延迟和通信开销。


<details>
  <summary>Details</summary>
Motivation: 数据库驱动的认知无线电网络虽然支持动态频谱共享，但存在强制位置披露、位置欺骗易受攻击、集中式服务易受DoS攻击等安全隐私问题。现有方法各自为政，缺乏统一且符合监管要求的解决方案。

Method: 提出SLAPX框架，使用可委托匿名凭证实现隐私保护的频谱查询，支持不暴露精确位置的适应性位置验证，并通过可验证延迟函数结合RLRS速率限制来缓解DoS攻击。

Result: 广泛的密码学基准测试和网络仿真表明，SLAPX相比现有方案显著降低了延迟和通信开销，同时能有效抵抗位置欺骗和DoS攻击。

Conclusion: SLAPX是实用且适合安全下一代数据库驱动认知无线电网络部署的统一安全框架。

Abstract: Database-driven cognitive radio networks (DB-CRNs) enable dynamic spectrum sharing through geolocation databases but introduce critical security and privacy challenges, including mandatory location disclosure, susceptibility to location spoofing, and denial-of-service (DoS) attacks on centralized services. Existing approaches address these issues in isolation and lack a unified, regulation-compliant solution under realistic adversarial conditions. In this work, we present a unified security framework for DB-CRNs that simultaneously provides location privacy, user anonymity, verifiable location, and DoS resilience. Our framework, denoted as SLAPX, enables privacy-preserving spectrum queries using delegatable anonymous credentials, supports adaptive location verification without revealing precise user location, and mitigates DoS attacks through verifiable delay functions (VDFs) combined with RLRS-based rate limiting. Extensive cryptographic benchmarking and network simulations demonstrate that SLAPX achieves significantly lower latency and communication overhead than existing solutions while effectively resisting location spoofing and DoS attacks. These results show that SLAPX is practical and well-suited for secure next-generation DB-CRN deployments.

</details>


### [15] [A Note on Non-Composability of Layerwise Approximate Verification for Neural Inference](https://arxiv.org/abs/2602.15756)
*Or Zamir*

Main category: cs.CR

TL;DR: 论文指出"逐层近似正确即可保证最终推理结果合理"的直觉是错误的，通过构造功能等效网络，层计算中的微小误差可导致最终输出任意偏离。


<details>
  <summary>Details</summary>
Motivation: 验证机器学习推理时，人们常认为只要每层计算在容差δ内正确，最终输出就是合理的推理结果。本文旨在揭示这一直觉推理是错误的。

Method: 通过构造性反例：对任意神经网络，都能构造功能等效的网络，使得层计算中精心选择的近似误差足以使最终输出在预设范围内任意偏离。

Result: 证明了逐层近似正确性不能保证最终推理结果的可靠性，即使每层误差在容差范围内，最终输出仍可能被恶意误差任意操纵。

Conclusion: 验证ML推理需要更严谨的方法，不能仅依赖逐层近似正确性，必须考虑误差在多层传播中的累积和放大效应。

Abstract: A natural and informal approach to verifiable (or zero-knowledge) ML inference over floating-point data is: ``prove that each layer was computed correctly up to tolerance $δ$; therefore the final output is a reasonable inference result''. This short note gives a simple counterexample showing that this inference is false in general: for any neural network, we can construct a functionally equivalent network for which adversarially chosen approximation-magnitude errors in individual layer computations suffice to steer the final output arbitrarily (within a prescribed bounded range).

</details>


### [16] [Natural Privacy Filters Are Not Always Free: A Characterization of Free Natural Filters](https://arxiv.org/abs/2602.15815)
*Matthew Regehr,Bingshan Hu,Ethan Leeman,Pasin Manurangsi,Pierre Tholoniat,Mathias Lécuyer*

Main category: cs.CR

TL;DR: 自然隐私过滤器可实现差分隐私机制的自适应组合，但并非总是"免费"的，只有满足特定顺序条件的机制族才支持免费过滤器


<details>
  <summary>Details</summary>
Motivation: 现有隐私过滤器仅考虑简单的隐私参数（如Rényi-DP或Gaussian DP），而自然隐私过滤器能够利用每个查询的完整隐私轮廓，有望在给定隐私预算下提供更好的效用

Method: 研究自然隐私过滤器，分析其与差分隐私机制自适应组合的特性，探讨在何种条件下自然隐私过滤器可以"免费"实现

Result: 发现自然隐私过滤器并非总是免费的，只有那些在组合时具有良好顺序性的隐私机制族才支持免费的自然隐私过滤器

Conclusion: 自然隐私过滤器虽然能提供更好的效用，但其实现并非无条件免费，需要机制族满足特定的顺序条件，这对实际应用中的隐私保护设计具有重要指导意义

Abstract: We study natural privacy filters, which enable the exact composition of differentially private (DP) mechanisms with adaptively chosen privacy characteristics. Earlier privacy filters consider only simple privacy parameters such as Rényi-DP or Gaussian DP parameters. Natural filters account for the entire privacy profile of every query, promising greater utility for a given privacy budget. We show that, contrary to other forms of DP, natural privacy filters are not free in general. Indeed, we show that only families of privacy mechanisms that are well-ordered when composed admit free natural privacy filters.

</details>
