<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 17]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [SoK: A Beginner-Friendly Introduction to Fault Injection Attacks](https://arxiv.org/abs/2509.18341)
*Christopher Simon Liu,Fan Wang,Patrick Gould,Carter Yagemann*

Main category: cs.CR

TL;DR: 本文提供了故障注入研究的入门介绍、技术分类、成本效益分析以及现有漏洞检测工具的复制分析


<details>
  <summary>Details</summary>
Motivation: 研究计算机系统在异常压力下的行为，测试系统极限并寻找破坏网络物理安全的新方法

Method: 采用分类学方法对故障注入技术进行系统分类，进行成本效益分析，并对现有漏洞检测工具进行复制分析

Result: 建立了完整的故障注入技术分类体系，提供了各种攻击方法的成本效益评估，识别了未来研究的重点方向

Conclusion: 故障注入是研究系统安全性的重要方法，本文为初学者提供了全面指导，并为未来研究指明了方向

Abstract: Fault Injection is the study of observing how systems behave under unusual
stress, environmental or otherwise. In practice, fault injection involves
testing the limits of computer systems and finding novel ways to potentially
break cyber-physical security.
  The contributions of this paper are three-fold. First, we provide a
beginner-friendly introduction to this research topic and an in-depth taxonomy
of fault injection techniques. Second, we highlight the current
state-of-the-art and provide a cost-benefit analysis of each attack method.
Third, for those interested in doing fault injection research, we provide a
replication analysis of an existing vulnerability detection tool and identify a
research focus for future work.

</details>


### [2] [Turning Hearsay into Discovery: Industrial 3D Printer Side Channel Information Translated to Stealing the Object Design](https://arxiv.org/abs/2509.18366)
*Aleksandr Dolgavin,Jacob Gatlin,Moti Yung,Mark Yampolskiy*

Main category: cs.CR

TL;DR: 本文首次证明侧信道攻击对工业级3D打印机构成严重威胁，能够通过功率侧信道数据重建打印模型，即使设计文件已加密。


<details>
  <summary>Details</summary>
Motivation: 保护3D打印中的数字设计知识产权，发现现有安全措施不足以防止侧信道攻击。

Method: 对粉末床熔融3D打印机的执行器进行功率侧信道数据采集，开发基于多轨迹的重建方法，通过体素比较评估重建质量。

Result: 对不同复杂度的模型，重建准确率高达90.29%，假阳性和假阴性率分别为7.02%和9.71%。

Conclusion: 工业环境中设计文件安全不能仅依赖文件保护，还必须防止功率、噪声等信号的泄露。

Abstract: The central security issue of outsourced 3D printing (aka AM: Additive
Manufacturing), an industry that is expected to dominate manufacturing, is the
protection of the digital design (containing the designers' model, which is
their intellectual property) shared with the manufacturer. Here, we show, for
the first time, that side-channel attacks are, in fact, a concrete serious
threat to existing industrial grade 3D printers, enabling the reconstruction of
the model printed (regardless of employing ways to directly conceal the design,
e.g. by encrypting it in transit and before loading it into the printer).
Previously, such attacks were demonstrated only on fairly simple FDM desktop 3D
printers, which play a negligible role in manufacturing of valuable designs. We
focus on the Powder Bed Fusion (PBF) AM process, which is popular for
manufacturing net-shaped parts with both polymers and metals. We demonstrate
how its individual actuators can be instrumented for the collection of power
side-channel information during the printing process. We then present our
approach to reconstruct the 3D printed model solely from the collected power
side-channel data. Further, inspired by Differential Power Analysis, we
developed a method to improve the quality of the reconstruction based on
multiple traces. We tested our approach on two design models with different
degrees of complexity. For different models, we achieved as high as 90.29~\% of
True Positives and as low as 7.02~\% and 9.71~\% of False Positives and False
Negatives by voxel-based volumetric comparison between reconstructed and
original designs. The lesson learned from our attack is that the security of
design files cannot solely rely on protecting the files themselves in an
industrial environment, but must instead also rely on assuring no leakage of
power, noise and similar signals to potential eavesdroppers in the printer's
vicinity.

</details>


### [3] [VoxGuard: Evaluating User and Attribute Privacy in Speech via Membership Inference Attacks](https://arxiv.org/abs/2509.18413)
*Efthymios Tsaprazlis,Thanathai Lertpetchpun,Tiantian Feng,Sai Praneeth Karimireddy,Shrikanth Narayanan*

Main category: cs.CR

TL;DR: 本文提出VoxGuard框架，用于评估语音匿名化技术在低误报率下的隐私保护效果，指出传统EER评估方法低估了隐私泄露风险。


<details>
  <summary>Details</summary>
Motivation: 当前语音匿名化评估主要依赖EER，但这种方法掩盖了攻击者在低误报率下可能发起的精准攻击风险。

Method: 基于差分隐私和成员推理，VoxGuard框架形式化了用户隐私和属性隐私两个互补概念，并在合成和真实数据集上测试了知情攻击者的能力。

Result: 研究发现，使用微调模型和最大相似度评分的攻击者在低误报率下攻击强度提高了数个数量级，简单透明攻击就能近乎完美地恢复性别和口音等属性。

Conclusion: EER严重低估了隐私泄露，需要采用低误报率评估方法，推荐VoxGuard作为隐私泄露评估的基准。

Abstract: Voice anonymization aims to conceal speaker identity and attributes while
preserving intelligibility, but current evaluations rely almost exclusively on
Equal Error Rate (EER) that obscures whether adversaries can mount
high-precision attacks. We argue that privacy should instead be evaluated in
the low false-positive rate (FPR) regime, where even a small number of
successful identifications constitutes a meaningful breach. To this end, we
introduce VoxGuard, a framework grounded in differential privacy and membership
inference that formalizes two complementary notions: User Privacy, preventing
speaker re-identification, and Attribute Privacy, protecting sensitive traits
such as gender and accent. Across synthetic and real datasets, we find that
informed adversaries, especially those using fine-tuned models and
max-similarity scoring, achieve orders-of-magnitude stronger attacks at low-FPR
despite similar EER. For attributes, we show that simple transparent attacks
recover gender and accent with near-perfect accuracy even after anonymization.
Our results demonstrate that EER substantially underestimates leakage,
highlighting the need for low-FPR evaluation, and recommend VoxGuard as a
benchmark for evaluating privacy leakage.

</details>


### [4] [Context Lineage Assurance for Non-Human Identities in Critical Multi-Agent Systems](https://arxiv.org/abs/2509.18415)
*Sumana Malkapuram,Sameera Gangavarapu,Kailashnath Reddy Kavalakuntla,Ananya Gangavarapu*

Main category: cs.CR

TL;DR: 本文提出了一种基于密码学机制的自主软件代理间安全交互框架，通过Merkle树结构实现非人类身份（NHI）的谱系验证，确保多跳溯源的可验证性。


<details>
  <summary>Details</summary>
Motivation: 随着自主软件代理的激增，需要建立安全可验证的代理间交互框架，特别是在非人类身份场景下，传统点对点安全模型无法满足多跳溯源验证需求。

Method: 采用基于证书透明度（CT）日志的Merkle树结构记录NHI的谱系信息，通过联邦证明服务器聚合包含证明和一致性检查，生成紧凑的签名证明，同时扩展A2A代理卡以包含身份验证原语。

Result: 建立了一个集身份证明、谱系验证和独立证明审计于一体的统一模型，能够在不访问完整执行轨迹的情况下进行外部验证。

Conclusion: 该框架显著提升了代理间生态系统的安全性，为受监管环境（如FedRAMP）中的NHI治理提供了坚实基础。

Abstract: The proliferation of autonomous software agents necessitates rigorous
frameworks for establishing secure and verifiable agent-to-agent (A2A)
interactions, particularly when such agents are instantiated as non-human
identities(NHIs). We extend the A2A paradigm [1 , 2] by introducing a
cryptographically grounded mechanism for lineage verification, wherein the
provenance and evolution of NHIs are anchored in append-only Merkle tree
structures modeled after Certificate Transparency (CT) logs. Unlike traditional
A2A models that primarily secure point-to-point interactions, our approach
enables both agents and external verifiers to cryptographically validate
multi-hop provenance, thereby ensuring the integrity of the entire call chain.
  A federated proof server acts as an auditor across one or more Merkle logs,
aggregating inclusion proofs and consistency checks into compact, signed
attestations that external parties can verify without access to the full
execution trace. In parallel, we augment the A2A agent card to incorporate
explicit identity verification primitives, enabling both peer agents and human
approvers to authenticate the legitimacy of NHI representations in a
standardized manner. Together, these contributions establish a cohesive model
that integrates identity attestation, lineage verification, and independent
proof auditing, thereby advancing the security posture of inter-agent
ecosystems and providing a foundation for robust governance of NHIs in
regulated environments such as FedRAMP.

</details>


### [5] [Coherence-driven inference for cybersecurity](https://arxiv.org/abs/2509.18520)
*Steve Huntsman*

Main category: cs.CR

TL;DR: LLMs利用自然语言数据构建加权图，实现自动一致性驱动推理(CDI)，应用于网络安全中的红蓝队操作。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在网络安全决策支持中的早期应用，为近中期网络安全决策和未来自主蓝队操作提供潜力。

Method: 使用大型语言模型从自然语言数据编译加权图，进行一致性驱动的自动推理。

Result: 开发了基于LLM的CDI系统，能够支持网络安全中的红蓝队操作决策。

Conclusion: 该方法在网络安全领域具有应用前景，有望为自主蓝队操作奠定基础。

Abstract: Large language models (LLMs) can compile weighted graphs on natural language
data to enable automatic coherence-driven inference (CDI) relevant to red and
blue team operations in cybersecurity. This represents an early application of
automatic CDI that holds near- to medium-term promise for decision-making in
cybersecurity and eventually also for autonomous blue team operations.

</details>


### [6] [Examining I2P Resilience: Effect of Centrality-based Attack](https://arxiv.org/abs/2509.18572)
*Kemi Akanbi,Sunkanmi Oluwadare,Jess Kropczynski,Jacques Bou Abdo*

Main category: cs.CR

TL;DR: 本研究评估了I2P匿名网络的鲁棒性，发现该网络在面对针对性攻击时表现出结构性脆弱性，网络密度下降约10%，平均路径长度增加33%。


<details>
  <summary>Details</summary>
Motivation: I2P作为一种知名的匿名去中心化P2P网络，虽然设计用于确保匿名性、保密性和规避审查，但其抗攻击能力相比TOR网络研究较少，需要对其鲁棒性进行系统评估。

Method: 采用网络分析方法，通过度中心性衡量节点在网络中的影响力，评估I2P网络对敌对渗透的敏感性。

Result: 渗透前网络密度为0.01065443，平均路径长度为6.842194；渗透后网络密度下降约10%，平均路径长度增加33%，表明网络效率和连通性显著下降。

Conclusion: 即使是像I2P这样的去中心化网络，在针对性攻击下也表现出结构性脆弱性，需要改进设计策略以增强对抗敌对干扰的韧性。

Abstract: This study examines the robustness of I2P, a well-regarded anonymous and
decentralized peer-to-peer network designed to ensure anonymity,
confidentiality, and circumvention of censorship. Unlike its more widely
researched counterpart, TOR, I2P's resilience has received less scholarly
attention. Employing network analysis, this research evaluates I2P's
susceptibility to adversarial percolation. By utilizing the degree centrality
as a measure of nodes' influence in the network, the finding suggests the
network is vulnerable to targeted disruptions. Before percolation, the network
exhibited a density of 0.01065443 and an average path length of 6.842194. At
the end of the percolation process, the density decreased by approximately 10%,
and the average path length increased by 33%, indicating a decline in
efficiency and connectivity. These results highlight that even decentralized
networks, such as I2P, exhibit structural fragility under targeted attacks,
emphasizing the need for improved design strategies to enhance resilience
against adversarial disruptions.

</details>


### [7] [MER-Inspector: Assessing model extraction risks from an attack-agnostic perspective](https://arxiv.org/abs/2509.18578)
*Xinwei Zhang,Haibo Hu,Qingqing Ye,Li Bai,Huadi Zheng*

Main category: cs.CR

TL;DR: 该论文首次从攻击无关的角度理论分析模型提取攻击(MEAs)，提出评估模型提取风险的分析指标，包括理论指标MRC和实证指标模型准确率，并开发MER-Inspector框架来比较不同模型架构的提取风险。


<details>
  <summary>Details</summary>
Motivation: 机器学习Web应用中的信息泄露问题日益受到关注，但模型功能泄露（即模型提取攻击）的理论研究还不够深入，需要从理论角度理解MEAs并建立风险评估指标。

Method: 使用神经正切核(NTK)理论将线性化MEA建模为正则化核分类问题，推导攻击性能的保真度差距和泛化误差界，提出模型恢复复杂度(MRC)理论指标，结合模型准确率实证指标，构建MER-Inspector框架。

Result: 在16种模型架构和5个数据集上的实验表明，所提指标与模型提取风险高度相关，MER-Inspector能准确比较任意两个模型的提取风险，准确率高达89.58%。

Conclusion: 该研究为模型提取攻击提供了理论基础和实用评估工具，提出的MRC指标和MER-Inspector框架能有效量化模型提取风险，有助于模型安全评估。

Abstract: Information leakage issues in machine learning-based Web applications have
attracted increasing attention. While the risk of data privacy leakage has been
rigorously analyzed, the theory of model function leakage, known as Model
Extraction Attacks (MEAs), has not been well studied. In this paper, we are the
first to understand MEAs theoretically from an attack-agnostic perspective and
to propose analytical metrics for evaluating model extraction risks. By using
the Neural Tangent Kernel (NTK) theory, we formulate the linearized MEA as a
regularized kernel classification problem and then derive the fidelity gap and
generalization error bounds of the attack performance. Based on these
theoretical analyses, we propose a new theoretical metric called Model Recovery
Complexity (MRC), which measures the distance of weight changes between the
victim and surrogate models to quantify risk. Additionally, we find that victim
model accuracy, which shows a strong positive correlation with model extraction
risk, can serve as an empirical metric. By integrating these two metrics, we
propose a framework, namely Model Extraction Risk Inspector (MER-Inspector), to
compare the extraction risks of models under different model architectures by
utilizing relative metric values. We conduct extensive experiments on 16 model
architectures and 5 datasets. The experimental results demonstrate that the
proposed metrics have a high correlation with model extraction risks, and
MER-Inspector can accurately compare the extraction risks of any two models
with up to 89.58%.

</details>


### [8] [FlowCrypt: Flow-Based Lightweight Encryption with Near-Lossless Recovery for Cloud Photo Privacy](https://arxiv.org/abs/2509.18696)
*Xiaohui Yang,Ping Ping,Feng Xu*

Main category: cs.CR

TL;DR: FlowCrypt是一个基于可逆神经网络的图像加密框架，通过流式加密/解密模块实现近无损恢复、高安全性和轻量级设计，适合移动和边缘设备应用。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的图像加密方案依赖传统密码算法且缺乏架构可逆性，导致恢复质量有限和鲁棒性差。首个基于INN的加密方案仍需辅助参考图像并丢弃副产品信息，限制了实用性。

Method: FlowCrypt采用基于流的图像加密框架，首先对输入图像进行密钥条件随机分割以增强前向过程随机性，然后通过由可逆块组成的FED模块处理分割后的组件，加密和解密共享参数。

Result: 实验表明FlowCrypt在三个数据集上实现100dB的恢复质量，产生均匀分布的密文图像，仅需100万参数，架构紧凑。

Conclusion: FlowCrypt通过其可逆架构和无参考设计确保了高保真图像恢复，同时保持轻量级特性，适用于移动和边缘设备应用。

Abstract: The widespread adoption of smartphone photography has led users to
increasingly rely on cloud storage for personal photo archiving and sharing,
raising critical privacy concerns. Existing deep learning-based image
encryption schemes, typically built upon CNNs or GANs, often depend on
traditional cryptographic algorithms and lack inherent architectural
reversibility, resulting in limited recovery quality and poor robustness.
Invertible neural networks (INNs) have emerged to address this issue by
enabling reversible transformations, yet the first INN-based encryption scheme
still relies on an auxiliary reference image and discards by-product
information before decryption, leading to degraded recovery and limited
practicality. To address these limitations, this paper proposes FlowCrypt, a
novel flow-based image encryption framework that simultaneously achieves
near-lossless recovery, high security, and lightweight model design. FlowCrypt
begins by applying a key-conditioned random split to the input image, enhancing
forward-process randomness and encryption strength. The resulting components
are processed through a Flow-based Encryption/Decryption (FED) module composed
of invertible blocks, which share parameters across encryption and decryption.
Thanks to its reversible architecture and reference-free design, FlowCrypt
ensures high-fidelity image recovery. Extensive experiments show that FlowCrypt
achieves recovery quality with 100dB on three datasets, produces uniformly
distributed cipher images, and maintains a compact architecture with only 1M
parameters, making it suitable for mobile and edge-device applications.

</details>


### [9] [Security smells in infrastructure as code: a taxonomy update beyond the seven sins](https://arxiv.org/abs/2509.18761)
*Aicha War,Serge L. B. Nikiema,Jordan Samhi,Jacques Klein,Tegawende F. Bissyande*

Main category: cs.CR

TL;DR: 本文重新审视了IaC安全漏洞分类法，通过分析7种流行IaC工具和利用LLM辅助分析，将已知的7种安全漏洞扩展至62种，并开发了高精度的检测工具。


<details>
  <summary>Details</summary>
Motivation: 现有IaC安全漏洞分类法仅基于单一工具且依赖人工分析，存在局限性和潜在偏见。需要更全面的分类法来改善IaC安全性。

Method: 扩展研究到7种流行IaC工具，利用LLM进行初步模式处理，所有分类决策都经过系统的人工验证并与安全标准进行协调。

Result: 建立了包含62种安全漏洞类别的综合分类法，为7种IaC工具实现了新的安全检查规则，精度通常达到1.00。GitHub项目研究表明这些安全问题长期存在。

Conclusion: 该工作为IaC从业者提供了解决常见安全漏洞的见解，并系统性地采用DevSecOps实践来构建更安全的基础设施代码。

Abstract: Infrastructure as Code (IaC) has become essential for modern software
management, yet security flaws in IaC scripts can have severe consequences, as
exemplified by the recurring exploits of Cloud Web Services. Prior work has
recognized the need to build a precise taxonomy of security smells in IaC
scripts as a first step towards developing approaches to improve IaC security.
This first effort led to the unveiling of seven sins, limited by the focus on a
single IaC tool as well as by the extensive, and potentially biased, manual
effort that was required. We propose, in our work, to revisit this taxonomy:
first, we extend the study of IaC security smells to a more diverse dataset
with scripts associated with seven popular IaC tools, including Terraform,
Ansible, Chef, Puppet, Pulumi, Saltstack, and Vagrant; second, we bring in some
automation for the analysis by relying on an LLM. While we leverage LLMs for
initial pattern processing, all taxonomic decisions underwent systematic human
validation and reconciliation with established security standards. Our study
yields a comprehensive taxonomy of 62 security smell categories, significantly
expanding beyond the previously known seven. We demonstrate actionability by
implementing new security checking rules within linters for seven popular IaC
tools, often achieving 1.00 precision score. Our evolution study of security
smells in GitHub projects reveals that these issues persist for extended
periods, likely due to inadequate detection and mitigation tools. This work
provides IaC practitioners with insights for addressing common security smells
and systematically adopting DevSecOps practices to build safer infrastructure
code.

</details>


### [10] [Detection of security smells in IaC scripts through semantics-aware code and language processing](https://arxiv.org/abs/2509.18790)
*Aicha War,Adnan A. Rawass,Abdoul K. Kabore,Jordan Samhi,Jacques Klein,Tegawende F. Bissyande*

Main category: cs.CR

TL;DR: 该论文提出了一种结合自然语言和代码表示的静态分析方法，用于检测基础设施即代码(IaC)脚本中的安全错误配置，通过语义理解显著提高了检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有的IaC安全检测方法主要依赖静态分析，但缺乏语义理解能力。研究表明IaC脚本中常存在重复的安全错误配置，需要更有效的检测方法。

Method: 使用CodeBERT捕获代码和文本的语义信息，LongFormer处理长脚本的上下文信息，结合自然语言和代码表示进行联合分析。

Result: 在Ansible和Puppet数据集上的实验显示，该方法将精确率从0.46提升到0.92(Ansible)，从0.55提升到0.87(Puppet)，召回率也有显著改善。

Conclusion: 语义增强显著提高了IaC安全错误配置的检测效果，验证了结合自然语言和代码表示方法的有效性。

Abstract: Infrastructure as Code (IaC) automates the provisioning and management of IT
infrastructure through scripts and tools, streamlining software deployment.
Prior studies have shown that IaC scripts often contain recurring security
misconfigurations, and several detection and mitigation approaches have been
proposed. Most of these rely on static analysis, using statistical code
representations or Machine Learning (ML) classifiers to distinguish insecure
configurations from safe code.
  In this work, we introduce a novel approach that enhances static analysis
with semantic understanding by jointly leveraging natural language and code
representations. Our method builds on two complementary ML models: CodeBERT, to
capture semantics across code and text, and LongFormer, to represent long IaC
scripts without losing contextual information. We evaluate our approach on
misconfiguration datasets from two widely used IaC tools, Ansible and Puppet.
To validate its effectiveness, we conduct two ablation studies (removing code
text from the natural language input and truncating scripts to reduce context)
and compare against four large language models (LLMs) and prior work. Results
show that semantic enrichment substantially improves detection, raising
precision and recall from 0.46 and 0.79 to 0.92 and 0.88 on Ansible, and from
0.55 and 0.97 to 0.87 and 0.75 on Puppet, respectively.

</details>


### [11] [Security Evaluation of Android apps in budget African Mobile Devices](https://arxiv.org/abs/2509.18800)
*Alioune Diallo,Anta Diop,Abdoul Kader Kabore,Jordan Samhi,Aleksandr Pilgun,Tegawendé F. Bissyande,Jacque Klein*

Main category: cs.CR

TL;DR: 本文开发了一个框架来提取预算Android设备中的预装APK并进行静态分析，发现这些应用存在严重的隐私和安全问题。


<details>
  <summary>Details</summary>
Motivation: Android设备的开源特性使得预算设备广泛普及，但预装系统应用具有高权限却缺乏独立审查，存在安全风险。

Method: 开发了一个从物理设备提取APK并进行静态分析的框架，研究了来自7款非洲智能手机的1,544个APK。

Result: 研究发现9%的应用泄露敏感数据，16%暴露关键组件无足够保护，还有大量应用执行特权命令、操作短信或静默安装。

Conclusion: 预装应用在广泛分布的低成本设备上构成了重大且未被充分探索的安全和隐私威胁。

Abstract: Android's open-source nature facilitates widespread smartphone accessibility,
particularly in price-sensitive markets. System and vendor applications that
come pre-installed on budget Android devices frequently operate with elevated
privileges, yet they receive limited independent examination. To address this
gap, we developed a framework that extracts APKs from physical devices and
applies static analysis to identify privacy and security issues in embedded
software. Our study examined 1,544 APKs collected from seven African
smartphones. The analysis revealed that 145 applications (9%) disclose
sensitive data, 249 (16%) expose critical components without sufficient
safeguards, and many present additional risks: 226 execute privileged or
dangerous commands, 79 interact with SMS messages (read, send, or delete), and
33 perform silent installation operations. We also uncovered a vendor-supplied
package that appears to transmit device identifiers and location details to an
external third party. These results demonstrate that pre-installed applications
on widely distributed low-cost devices represent a significant and
underexplored threat to user security and privacy.

</details>


### [12] [R-CONV++: Uncovering Privacy Vulnerabilities through Analytical Gradient Inversion Attacks](https://arxiv.org/abs/2509.18871)
*Tamer Ahmed Eltaras,Qutaibah Malluhi,Alessandro Savino,Stefano Di Carlo,Adnan Qayyum*

Main category: cs.CR

TL;DR: 本文提出了三种先进的梯度反演攻击算法，扩展了联邦学习中隐私保护的攻击能力，特别是在卷积层、高维输入和批量训练场景下的数据重建。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然通过共享梯度而非原始数据来保护隐私，但研究表明梯度仍可能泄露训练数据。现有方法在处理卷积层、高维输入和批量训练时效果有限，因此需要开发更有效的攻击算法。

Method: 提出了三种算法：1）基于卷积层梯度的数据泄露方法，无需重建中间层输出；2）扩展至高维输入数据的分析方法；3）针对小批量训练的解析重建方法。重点利用梯度约束而非权重约束。

Result: 新算法证明即使使用ReLU等非完全可逆激活函数，也能直接从梯度重建训练样本，且在某些层仅需先前认为必要约束的5%即可成功执行攻击。

Conclusion: 该研究显著扩展了梯度反演攻击的适用性，揭示了联邦学习中潜在的隐私风险，特别是在复杂现实数据集和批量训练场景下。

Abstract: Federated learning has emerged as a prominent privacy-preserving technique
for leveraging large-scale distributed datasets by sharing gradients instead of
raw data. However, recent studies indicate that private training data can still
be exposed through gradient inversion attacks. While earlier analytical methods
have demonstrated success in reconstructing input data from fully connected
layers, their effectiveness significantly diminishes when applied to
convolutional layers, high-dimensional inputs, and scenarios involving multiple
training examples. This paper extends our previous work \cite{eltaras2024r} and
proposes three advanced algorithms to broaden the applicability of gradient
inversion attacks. The first algorithm presents a novel data leakage method
that efficiently exploits convolutional layer gradients, demonstrating that
even with non-fully invertible activation functions, such as ReLU, training
samples can be analytically reconstructed directly from gradients without the
need to reconstruct intermediate layer outputs. Building on this foundation,
the second algorithm extends this analytical approach to support
high-dimensional input data, substantially enhancing its utility across complex
real-world datasets. The third algorithm introduces an innovative analytical
method for reconstructing mini-batches, addressing a critical gap in current
research that predominantly focuses on reconstructing only a single training
example. Unlike previous studies that focused mainly on the weight constraints
of convolutional layers, our approach emphasizes the pivotal role of gradient
constraints, revealing that successful attacks can be executed with fewer than
5\% of the constraints previously deemed necessary in certain layers.

</details>


### [13] [Obelix: Mitigating Side-Channels Through Dynamic Obfuscation](https://arxiv.org/abs/2509.18909)
*Jan Wichelmann,Anja Rabich,Anna P"atschke,Thomas Eisenbarth*

Main category: cs.CR

TL;DR: Obelix是一个针对可信执行环境（TEE）的综合保护工具，通过将程序划分为均匀代码块并使用不经意RAM来保护代码和数据免受多种侧信道攻击。


<details>
  <summary>Details</summary>
Motivation: TEE虽然提供硬件级别的保护，但攻击者仍可通过侧信道泄露数据访问模式和执行路径。现有软件级防护措施要么针对特定攻击，要么泄漏模型过于狭窄，需要一种全面的防护方案。

Method: 分析单步执行工具的实际精度，设计算法将程序划分为攻击者无法区分的均匀代码块，将这些块和程序数据存储在不经意RAM中，使攻击者无法跟踪执行路径。

Result: Obelix能够保护代码和数据免受缓存攻击、单步执行攻击和密文侧信道等多种TEE攻击，为开发者提供易于使用的防护工具。

Conclusion: 虽然Obelix作为混淆工具会带来显著的性能开销，但它提供了强大的安全保证，且无需专家知识即可应用，弥补了当前TEE防护方案的不足。

Abstract: Trusted execution environments (TEEs) offer hardware-assisted means to
protect code and data. However, as shown in numerous results over the years,
attackers can use side-channels to leak data access patterns and even
single-step the code. While the vendors are slowly introducing hardware-based
countermeasures for some attacks, others will stay unaddressed. This makes a
software-level countermeasure desirable, but current available solutions only
address very specific attack vectors or have a narrow leakage model.
  In this work, we take a holistic view at the vulnerabilities of TEEs and
design a tool named Obelix, which is the first to protect both code and data
against a wide range of TEE attacks, from cache attacks over single-stepping to
ciphertext side-channels. We analyze the practically achievable precision of
state-of-the-art single-stepping tools, and present an algorithm which uses
that knowledge to divide a program into uniform code blocks, that are
indistinguishable for a strong attacker. By storing these blocks and the
program data in oblivious RAM, the attacker cannot follow execution,
effectively protecting both secret code and data. We describe how we automate
our approach to make it available for developers who are unfamiliar with
side-channels. As an obfuscation tool, Obelix comes with a considerable
performance overhead, but compensates this with strong security guarantees and
easy applicability without requiring any expert knowledge.

</details>


### [14] [Generic Adversarial Smart Contract Detection with Semantics and Uncertainty-Aware LLM](https://arxiv.org/abs/2509.18934)
*Yating Liu,Xing Su,Hao Wu,Sijin Li,Yuxi Cheng,Fengyuan Xu,Sheng Zhong*

Main category: cs.CR

TL;DR: FinDet是一个基于LLM的通用对抗性智能合约检测框架，通过提取字节码的语义意图和行为逻辑，并结合LLM不确定性评估，实现了高精度的恶意合约检测。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法类型有限且效果不佳，而LLM技术虽然具有泛化潜力，但在处理编译代码输入和评估二元分类确定性方面存在困难。

Method: FinDet框架采用两种增强技术：从低级字节码中提取语义意图和行为逻辑以释放LLM推理能力；通过多轮回答探测LLM不确定性来提高二元分类的鲁棒性。

Result: FinDet在综合评估中达到0.9223的平衡准确率和0.8950的真阳性率，显著优于现有基线方法，在真实世界测试中成功检测出所有公开和未报告的对抗性合约。

Conclusion: FinDet提供了一个通用的对抗性智能合约检测解决方案，在检测准确性、鲁棒性和实用性方面都表现出色，能够有效防止金融损失。

Abstract: Adversarial smart contracts, mostly on EVM-compatible chains like Ethereum
and BSC, are deployed as EVM bytecode to exploit vulnerable smart contracts
typically for financial gains. Detecting such malicious contracts at the time
of deployment is an important proactive strategy preventing loss from victim
contracts. It offers a better cost-benefit than detecting vulnerabilities on
diverse potential victims. However, existing works are not generic with limited
detection types and effectiveness due to imbalanced samples, while the emerging
LLM technologies, which show its potentials in generalization, have two key
problems impeding its application in this task: hard digestion of compiled-code
inputs, especially those with task-specific logic, and hard assessment of LLMs'
certainty in their binary answers, i.e., yes-or-no answers. Therefore, we
propose a generic adversarial smart contracts detection framework FinDet, which
leverages LLMs with two enhancements addressing above two problems. FinDet
takes as input only the EVM-bytecode contracts and identifies adversarial ones
among them with high balanced accuracy. The first enhancement extracts concise
semantic intentions and high-level behavioral logic from the low-level bytecode
inputs, unleashing the LLM reasoning capability restricted by the task input.
The second enhancement probes and measures the LLM uncertainty to its
multi-round answering to the same query, improving the LLM answering robustness
for binary classifications required by the task output. Our comprehensive
evaluation shows that FinDet achieves a BAC of 0.9223 and a TPR of 0.8950,
significantly outperforming existing baselines. It remains robust under
challenging conditions including unseen attack patterns, low-data settings, and
feature obfuscation. FinDet detects all 5 public and 20+ unreported adversarial
contracts in a 10-day real-world test, confirmed manually.

</details>


### [15] [Trigger Where It Hurts: Unveiling Hidden Backdoors through Sensitivity with Sensitron](https://arxiv.org/abs/2509.19101)
*Gejian Zhao,Hanzhou Wu,Xinpeng Zhang*

Main category: cs.CR

TL;DR: 本文提出了Sensitron框架，通过将可解释人工智能(XAI)与后门攻击定量连接，创建隐蔽且鲁棒的后门触发器。该方法使用动态元敏感度分析和分层SHAP估计来识别模型漏洞，实现了97.8%的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击方法缺乏可解释的触发机制，无法定量建模漏洞模式。本文旨在建立XAI与后门攻击之间的定量联系，揭示NLP系统的根本性漏洞。

Method: Sensitron采用渐进式精炼方法：1)动态元敏感度分析识别潜在易受攻击的输入标记；2)分层SHAP估计提供可解释归因；3)即插即用排名机制生成上下文适当的触发器。

Result: 建立了首个可解释性分数与经验攻击成功率之间的数学相关性(SRC=0.83)，攻击成功率达97.8%(比SOTA提升5.8%)，在0.1%投毒率下仍保持85.4%的ASR，并能抵抗多种SOTA防御。

Conclusion: 这项工作揭示了NLP系统的根本性漏洞，通过武器化可解释性提供了新的攻击向量，为后门攻击研究开辟了新方向。

Abstract: Backdoor attacks pose a significant security threat to natural language
processing (NLP) systems, but existing methods lack explainable trigger
mechanisms and fail to quantitatively model vulnerability patterns. This work
pioneers the quantitative connection between explainable artificial
intelligence (XAI) and backdoor attacks, introducing Sensitron, a novel modular
framework for crafting stealthy and robust backdoor triggers. Sensitron employs
a progressive refinement approach where Dynamic Meta-Sensitivity Analysis
(DMSA) first identifies potentially vulnerable input tokens, Hierarchical SHAP
Estimation (H-SHAP) then provides explainable attribution to precisely pinpoint
the most influential tokens, and finally a Plug-and-Rank mechanism that
generates contextually appropriate triggers. We establish the first
mathematical correlation (Sensitivity Ranking Correlation, SRC=0.83) between
explainability scores and empirical attack success, enabling precise targeting
of model vulnerabilities. Sensitron achieves 97.8% Attack Success Rate (ASR)
(+5.8% over state-of-the-art (SOTA)) with 85.4% ASR at 0.1% poisoning rate,
demonstrating robust resistance against multiple SOTA defenses. This work
reveals fundamental NLP vulnerabilities and provides new attack vectors through
weaponized explainability.

</details>


### [16] [LLM-based Vulnerability Discovery through the Lens of Code Metrics](https://arxiv.org/abs/2509.19117)
*Felix Weissberg,Lukas Pirch,Erik Imgrund,Jonas Möller,Thorsten Eisenhofer,Konrad Rieck*

Main category: cs.CR

TL;DR: 研究发现，仅使用经典代码指标训练的模型在漏洞发现任务上表现与最先进的大语言模型相当，表明LLMs在漏洞发现方面存在局限性，主要依赖于浅层代码指标而非深层语义理解。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在软件工程任务中表现出色，但在漏洞发现方面的进展近年来停滞不前，研究者希望通过经典代码指标的视角来理解这一现象。

Method: 通过经典代码指标分析LLMs的表现，训练仅基于代码指标的分类器，并与LLMs进行对比，同时进行根因分析以探究LLMs与代码指标之间的相关性和因果关系。

Result: 发现LLMs与代码指标存在强相关性和因果关系，当代码指标值改变时，LLMs的预测也会相应变化，表明LLMs在漏洞发现中主要依赖于浅层代码特征。

Conclusion: LLMs在漏洞发现任务中的能力受到限制，主要依赖于浅层代码指标而非深层语义理解，需要研究更有效的方法来应对这一挑战。

Abstract: Large language models (LLMs) excel in many tasks of software engineering, yet
progress in leveraging them for vulnerability discovery has stalled in recent
years. To understand this phenomenon, we investigate LLMs through the lens of
classic code metrics. Surprisingly, we find that a classifier trained solely on
these metrics performs on par with state-of-the-art LLMs for vulnerability
discovery. A root-cause analysis reveals a strong correlation and a causal
effect between LLMs and code metrics: When the value of a metric is changed,
LLM predictions tend to shift by a corresponding magnitude. This dependency
suggests that LLMs operate at a similarly shallow level as code metrics,
limiting their ability to grasp complex patterns and fully realize their
potential in vulnerability discovery. Based on these findings, we derive
recommendations on how research should more effectively address this challenge.

</details>


### [17] [LLMs as verification oracles for Solidity](https://arxiv.org/abs/2509.19153)
*Massimo Bartoletti,Enrico Lipparini,Livio Pompianu*

Main category: cs.CR

TL;DR: 本文首次系统评估了GPT-5作为智能合约验证预言机的能力，发现推理导向的大型语言模型在验证任务中表现出惊人效果，为AI与形式化方法的融合开辟了新前沿。


<details>
  <summary>Details</summary>
Motivation: 智能合约的正确性至关重要，但现有形式化验证工具存在学习曲线陡峭和规范语言受限的问题。虽然LLMs已用于安全相关任务，但能否作为验证预言机来推理任意合约特定属性仍是一个开放性问题。

Method: 在大型验证任务数据集上对GPT-5进行基准测试，将其输出与成熟的形式化验证工具进行比较，并评估其在真实审计场景中的实际有效性。结合定量指标和定性分析。

Result: 研究表明，最近推理导向的LLMs作为验证预言机可以出人意料地有效。

Conclusion: 这项工作展示了AI与形式化方法在安全智能合约开发和审计方面融合的新前沿，表明LLMs有潜力成为有效的验证工具。

Abstract: Ensuring the correctness of smart contracts is critical, as even subtle flaws
can lead to severe financial losses. While bug detection tools able to spot
common vulnerability patterns can serve as a first line of defense, most
real-world exploits and losses stem from errors in the contract business logic.
Formal verification tools such as SolCMC and the Certora Prover address this
challenge, but their impact remains limited by steep learning curves and
restricted specification languages. Recent works have begun to explore the use
of large language models (LLMs) for security-related tasks such as
vulnerability detection and test generation. Yet, a fundamental question
remains open: can LLMs serve as verification oracles, capable of reasoning
about arbitrary contract-specific properties? In this paper, we provide the
first systematic evaluation of GPT-5, a state-of-the-art reasoning LLM, in this
role. We benchmark its performance on a large dataset of verification tasks,
compare its outputs against those of established formal verification tools, and
assess its practical effectiveness in real-world auditing scenarios. Our study
combines quantitative metrics with qualitative analysis, and shows that recent
reasoning-oriented LLMs can be surprisingly effective as verification oracles,
suggesting a new frontier in the convergence of AI and formal methods for
secure smart contract development and auditing.

</details>
