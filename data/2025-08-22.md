<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 14]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [MCPTox: A Benchmark for Tool Poisoning Attack on Real-World MCP Servers](https://arxiv.org/abs/2508.14925)
*Zhiqiang Wang,Yichao Gao,Yanting Wang,Suyuan Liu,Haifeng Sun,Haoran Cheng,Guanquan Shi,Haohua Du,Xiangyang Li*

Main category: cs.CR

TL;DR: MCPTox基准测试系统评估了LLM代理对工具投毒攻击的脆弱性，发现主流代理普遍易受攻击，攻击成功率高达72.8%，现有安全对齐机制对此类攻击无效。


<details>
  <summary>Details</summary>
Motivation: Model Context Protocol (MCP)为LLM代理提供了与外部工具交互的标准接口，但同时也引入了新的攻击面。现有研究主要关注通过工具输出注入的攻击，而忽略了更基本的漏洞：工具投毒攻击，即在工具元数据中嵌入恶意指令而不需要执行。

Method: 构建MCPTox基准测试，基于45个真实MCP服务器和353个真实工具，设计三种攻击模板，通过少样本学习生成1312个恶意测试用例，覆盖10个风险类别，在20个主流LLM代理上进行评估。

Result: 评估显示代理普遍易受工具投毒攻击，o1-mini攻击成功率高达72.8%。更有能力的模型往往更易受攻击，因为攻击利用了其优越的指令遵循能力。现有安全对齐机制几乎无效，最高拒绝率(Claude-3.7-Sonnet)不到3%。

Conclusion: 工具投毒攻击是一个广泛存在的威胁，现有安全措施对此类攻击无效。MCPTox为理解和缓解这一威胁提供了重要的实证基准，并可用于开发可验证更安全的AI代理。

Abstract: By providing a standardized interface for LLM agents to interact with
external tools, the Model Context Protocol (MCP) is quickly becoming a
cornerstone of the modern autonomous agent ecosystem. However, it creates novel
attack surfaces due to untrusted external tools. While prior work has focused
on attacks injected through external tool outputs, we investigate a more
fundamental vulnerability: Tool Poisoning, where malicious instructions are
embedded within a tool's metadata without execution. To date, this threat has
been primarily demonstrated through isolated cases, lacking a systematic,
large-scale evaluation.
  We introduce MCPTox, the first benchmark to systematically evaluate agent
robustness against Tool Poisoning in realistic MCP settings. MCPTox is
constructed upon 45 live, real-world MCP servers and 353 authentic tools. To
achieve this, we design three distinct attack templates to generate a
comprehensive suite of 1312 malicious test cases by few-shot learning, covering
10 categories of potential risks. Our evaluation on 20 prominent LLM agents
setting reveals a widespread vulnerability to Tool Poisoning, with o1-mini,
achieving an attack success rate of 72.8\%. We find that more capable models
are often more susceptible, as the attack exploits their superior
instruction-following abilities. Finally, the failure case analysis reveals
that agents rarely refuse these attacks, with the highest refused rate
(Claude-3.7-Sonnet) less than 3\%, demonstrating that existing safety alignment
is ineffective against malicious actions that use legitimate tools for
unauthorized operation. Our findings create a crucial empirical baseline for
understanding and mitigating this widespread threat, and we release MCPTox for
the development of verifiably safer AI agents. Our dataset is available at an
anonymized repository: \textit{https://anonymous.4open.science/r/AAAI26-7C02}.

</details>


### [2] [A Systematic Survey of Model Extraction Attacks and Defenses: State-of-the-Art and Perspectives](https://arxiv.org/abs/2508.15031)
*Kaixiang Zhao,Lincan Li,Kaize Ding,Neil Zhenqiang Gong,Yue Zhao,Yushun Dong*

Main category: cs.CR

TL;DR: 对模型提取攻击(MEAs)及其防御策略的全面调查，提出了新的分类法，分析了攻击技术、防御挑战和不同计算环境下的影响


<details>
  <summary>Details</summary>
Motivation: 机器学习即服务(MLaaS)平台的普及使得高级ML模型更易访问，但也引入了模型提取攻击的漏洞，威胁知识产权、隐私和系统安全

Method: 提出新的分类法，根据攻击机制、防御方法和计算环境对MEAs进行分类，分析各种攻击技术的有效性，评估现有防御面临的挑战

Result: 系统性地调查了MEAs和防御策略，强调了模型效用与安全性之间的关键权衡，评估了不同计算范式下的技术、伦理、法律和社会影响

Conclusion: 该调查为AI安全和隐私领域的研究者、从业者和政策制定者提供了有价值的参考，并维护了持续更新的在线文献库

Abstract: Machine learning (ML) models have significantly grown in complexity and
utility, driving advances across multiple domains. However, substantial
computational resources and specialized expertise have historically restricted
their wide adoption. Machine-Learning-as-a-Service (MLaaS) platforms have
addressed these barriers by providing scalable, convenient, and affordable
access to sophisticated ML models through user-friendly APIs. While this
accessibility promotes widespread use of advanced ML capabilities, it also
introduces vulnerabilities exploited through Model Extraction Attacks (MEAs).
Recent studies have demonstrated that adversaries can systematically replicate
a target model's functionality by interacting with publicly exposed interfaces,
posing threats to intellectual property, privacy, and system security. In this
paper, we offer a comprehensive survey of MEAs and corresponding defense
strategies. We propose a novel taxonomy that classifies MEAs according to
attack mechanisms, defense approaches, and computing environments. Our analysis
covers various attack techniques, evaluates their effectiveness, and highlights
challenges faced by existing defenses, particularly the critical trade-off
between preserving model utility and ensuring security. We further assess MEAs
within different computing paradigms and discuss their technical, ethical,
legal, and societal implications, along with promising directions for future
research. This systematic survey aims to serve as a valuable reference for
researchers, practitioners, and policymakers engaged in AI security and
privacy. Additionally, we maintain an online repository continuously updated
with related literature at https://github.com/kzhao5/ModelExtractionPapers.

</details>


### [3] [MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in Mixture-of-Experts LLMs](https://arxiv.org/abs/2508.15036)
*Ruyi Ding,Tianhong Xu,Xinyi Shen,Aidong Adam Ding,Yunsi Fei*

Main category: cs.CR

TL;DR: MoEcho发现并利用MoE架构中的侧信道漏洞，通过CPU和GPU的四种新型侧信道攻击，能够从激活模式推断用户输入的敏感信息，对基于MoE的LLM和VLM构成严重隐私威胁。


<details>
  <summary>Details</summary>
Motivation: 随着MoE架构在Transformer模型中的广泛应用，其基于语义的动态路由机制虽然提高了效率，但也产生了新的攻击面。输入依赖的激活模式在硬件执行中留下可观测的痕迹，可能被攻击者利用来推断敏感用户数据。

Method: 提出了四种新型架构侧信道：CPU上的缓存占用信道和Pageout+Reload，GPU上的性能计数器和TLB Evict+Reload。利用这些漏洞设计了四种攻击方法：提示推断攻击、响应重构攻击、视觉推断攻击和视觉重构攻击。

Result: 成功在基于MoE架构的大型语言模型和视觉语言模型上实现了隐私泄露攻击，证明了MoE架构存在的严重安全漏洞。

Conclusion: 这是对现代Transformer中流行的MoE结构的首次运行时架构级安全分析，揭示了严重的安全隐私威胁，呼吁在开发高效大规模AI服务时采取及时有效的防护措施。

Abstract: The transformer architecture has become a cornerstone of modern AI, fueling
remarkable progress across applications in natural language processing,
computer vision, and multimodal learning. As these models continue to scale
explosively for performance, implementation efficiency remains a critical
challenge. Mixture of Experts (MoE) architectures, selectively activating
specialized subnetworks (experts), offer a unique balance between model
accuracy and computational cost. However, the adaptive routing in MoE
architectures, where input tokens are dynamically directed to specialized
experts based on their semantic meaning inadvertently opens up a new attack
surface for privacy breaches. These input-dependent activation patterns leave
distinctive temporal and spatial traces in hardware execution, which
adversaries could exploit to deduce sensitive user data. In this work, we
propose MoEcho, discovering a side channel analysis based attack surface that
compromises user privacy on MoE based systems. Specifically, in MoEcho, we
introduce four novel architectural side channels on different computing
platforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and
Performance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting
these vulnerabilities, we propose four attacks that effectively breach user
privacy in large language models (LLMs) and vision language models (VLMs) based
on MoE architectures: Prompt Inference Attack, Response Reconstruction Attack,
Visual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first
runtime architecture level security analysis of the popular MoE structure
common in modern transformers, highlighting a serious security and privacy
threat and calling for effective and timely safeguards when harnessing MoE
based models for developing efficient large scale AI services.

</details>


### [4] [When Machine Learning Meets Vulnerability Discovery: Challenges and Lessons Learned](https://arxiv.org/abs/2508.15042)
*Sima Arasteh,Christophe Hauser*

Main category: cs.CR

TL;DR: 本文探讨了机器学习在软件漏洞检测应用中的挑战，特别是数据集规模、评估方法和模型选择方面的问题，并分享了作者先前研究的见解。


<details>
  <summary>Details</summary>
Motivation: 机器学习在软件漏洞检测中展现出潜力，但面临数据规模、评估标准和方法适用性等新挑战，需要系统性地分析和改进现有方法。

Method: 通过分析机器学习在漏洞发现中的应用挑战，结合作者先前开发的Bin2vec和BinHunter两个研究项目的经验教训，提出改进建议。

Result: 识别出现有机器学习漏洞检测方法在数据集统计透明度、训练数据适用性和模型粒度选择等方面的重要缺陷。

Conclusion: 机器学习在软件漏洞检测领域需要更严谨的数据集构建、更透明的评估标准以及更合适的模型粒度选择，作者的前期研究为此提供了有价值的见解。

Abstract: In recent years, machine learning has demonstrated impressive results in
various fields, including software vulnerability detection. Nonetheless, using
machine learning to identify software vulnerabilities presents new challenges,
especially regarding the scale of data involved, which was not a factor in
traditional methods. Consequently, in spite of the rise of new
machine-learning-based approaches in that space, important shortcomings persist
regarding their evaluation. First, researchers often fail to provide concrete
statistics about their training datasets, such as the number of samples for
each type of vulnerability. Moreover, many methods rely on training with
semantically similar functions rather than directly on vulnerable programs.
This leads to uncertainty about the suitability of the datasets currently used
for training. Secondly, the choice of a model and the level of granularity at
which models are trained also affect the effectiveness of such vulnerability
discovery approaches.
  In this paper, we explore the challenges of applying machine learning to
vulnerability discovery. We also share insights from our two previous research
papers, Bin2vec and BinHunter, which could enhance future research in this
field.

</details>


### [5] [Tighter Privacy Analysis for Truncated Poisson Sampling](https://arxiv.org/abs/2508.15089)
*Arun Ganesh*

Main category: cs.CR

TL;DR: 关于截断泊松采样的新隐私放大分析


<details>
  <summary>Details</summary>
Motivation: 研究截断泊松采样（一种在批量超过最大批量大小时进行截断的泊松采样变体）的隐私保护特性，旨在提供更精确的隐私放大分析

Method: 提出新的分析方法来分析截断泊松采样的隐私放大效果，该方法考虑了批量截断对隐私保护的影响

Result: 获得了截断泊松采样的改进隐私保证，提供了更紧致的隐私放大边界

Conclusion: 新的分析框架为截断泊松采样提供了更好的隐私保护理论保证，有助于在实际差分隐私应用中更有效地使用这种采样方法

Abstract: We give a new privacy amplification analysis for truncated Poisson sampling,
a Poisson sampling variant that truncates a batch if it exceeds a given maximum
batch size.

</details>


### [6] [Adaptive Anomaly Detection in Evolving Network Environments](https://arxiv.org/abs/2508.15100)
*Ehssan Mousavipour,Andrey Dimanchev,Majid Ghaderi*

Main category: cs.CR

TL;DR: NetSight是一个用于网络数据异常检测的监督学习框架，能够在线持续检测和适应分布偏移，无需人工干预，通过伪标签技术和知识蒸馏防止灾难性遗忘，在三个长期网络数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 分布偏移是深度学习异常检测系统面临的关键挑战，现有方法要么需要昂贵的人工标注，要么依赖难以获取的干净数据来进行偏移适应，这些要求在现实中都难以满足。

Method: NetSight采用新颖的伪标签技术消除人工干预，并使用基于知识蒸馏的适应策略来防止灾难性遗忘，实现在线持续检测和适应分布偏移。

Result: 在三个长期网络数据集上的评估显示，NetSight相比依赖人工标注的最先进方法具有优越的适应性能，F1分数提升高达11.72%。

Conclusion: NetSight证明了其在经历分布偏移的动态网络中具有鲁棒性和有效性，能够有效解决分布偏移带来的挑战。

Abstract: Distribution shift, a change in the statistical properties of data over time,
poses a critical challenge for deep learning anomaly detection systems.
Existing anomaly detection systems often struggle to adapt to these shifts.
Specifically, systems based on supervised learning require costly manual
labeling, while those based on unsupervised learning rely on clean data, which
is difficult to obtain, for shift adaptation. Both of these requirements are
challenging to meet in practice. In this paper, we introduce NetSight, a
framework for supervised anomaly detection in network data that continually
detects and adapts to distribution shifts in an online manner. NetSight
eliminates manual intervention through a novel pseudo-labeling technique and
uses a knowledge distillation-based adaptation strategy to prevent catastrophic
forgetting. Evaluated on three long-term network datasets, NetSight
demonstrates superior adaptation performance compared to state-of-the-art
methods that rely on manual labeling, achieving F1-score improvements of up to
11.72%. This proves its robustness and effectiveness in dynamic networks that
experience distribution shifts over time.

</details>


### [7] [Conditional Cube Attack on Round-Reduced ASCON](https://arxiv.org/abs/2508.15172)
*Zheng Li,Xiaoyang Dong,Xiaoyun Wang*

Main category: cs.CR

TL;DR: 本文评估了认证加密算法Ascon对类立方体攻击的安全性，提出了针对5/6轮缩减Ascon的新条件立方体攻击方法，并首次实现了对7轮Ascon的密钥恢复攻击


<details>
  <summary>Details</summary>
Motivation: Ascon是CAESAR竞赛第三轮的16个幸存算法之一，之前最好的密钥恢复攻击只能达到6轮，而类似结构的Keccak密钥模式攻击可达7轮以上，需要研究Ascon对类立方体攻击的更强安全性

Method: 推广了条件立方体攻击方法，引入立方体类密钥子集技术，根据不同的密钥条件将密钥空间划分为多个子集，对每个子集使用立方体测试器进行测试

Result: 将6轮攻击的时间复杂度从理论上的2^66降低到实际的2^40，首次实现了对7轮Ascon的密钥恢复攻击，总时间复杂度约为2^103.9，对弱密钥子集(大小2^117)的攻击仅需2^77时间复杂度

Conclusion: 这些攻击不会威胁完整12轮Ascon的安全性，但展示了在缩减轮数下Ascon对类立方体攻击的脆弱性，为算法安全性评估提供了重要参考

Abstract: This paper evaluates the secure level of authenticated encryption
\textsc{Ascon} against cube-like method. \textsc{Ascon} submitted by Dobraunig
\emph{et~al.} is one of 16 survivors of the 3rd round CAESAR competition. The
cube-like method is first used by Dinur \emph{et~al.} to analyze Keccak keyed
modes. At CT-RSA 2015, Dobraunig \emph{et~al.} applied this method to 5/6-round
reduced \textsc{Ascon}, whose structure is similar to Keccak keyed modes.
However, for \textsc{Ascon} the non-linear layer is more complex and state is
much smaller, which make it hard for the attackers to select enough cube
variables that do not multiply with each other after the first round. This
seems to be the reason why the best previous key-recovery attack is on 6-round
\textsc{Ascon}, while for Keccak keyed modes (Keccak-MAC and Keyak) the
attacked round is no less than 7-round.
  In this paper, we generalize the conditional cube attack proposed by Huang
\emph{et~al.}, and find new cubes depending on some key bit conditions for
5/6-round reduced \textsc{Ascon}, and translate the previous theoretic 6-round
attack with $2^{66}$ time complexity to a practical one with $2^{40}$ time
complexity. Moreover, we propose the first 7-round key-recovery attack on
\textsc{Ascon}. By introducing \emph{the cube-like key-subset technique}, we
divide the full key space into many subsets according to different key
conditions. For each key subset, we launch the cube tester to determine if the
key falls into it. Finally, we recover the full key space by testing all the
key subsets. The total time complexity is about $2^{103.9}$. In addition, for a
weak-key subset, whose size is $2^{117}$, the attack is more efficient and
costs only $2^{77}$ time complexity. Those attacks do not threaten the full
round (12 rounds) \textsc{Ascon}.

</details>


### [8] [Private Hyperparameter Tuning with Ex-Post Guarantee](https://arxiv.org/abs/2508.15183)
*Badih Ghazi,Pritish Kamath,Alexander Knop,Ravi Kumar,Pasin Manurangsi,Chiyuan Zhang*

Main category: cs.CR

TL;DR: 本文扩展了效用优先的差分隐私方法，支持任意私有估计器序列，最多只需原始隐私预算的两倍，并能无额外隐私成本地进行超参数调优，包括扩展到ex-post Renyi DP。


<details>
  <summary>Details</summary>
Motivation: 传统差分隐私采用"隐私优先"视角，但实践中往往需要"效用优先"——先确定所需效用水平，再计算相应的隐私成本。现有研究主要局限于基于拉普拉斯或高斯噪声的简单机制。

Method: 扩展Wu等人[2019]和Liu与Talwar[2019]的工作，支持任意私有估计器序列，通过相关噪声添加和渐进减少机制，实现最多两倍原始隐私预算的效用优先隐私保护。

Result: 实现了对任意估计器序列的支持，隐私成本最多为原始预算的两倍；超参数调优（包括最优隐私预算选择）无需额外隐私成本；结果可扩展到ex-post Renyi DP。

Conclusion: 本文显著推广了效用优先差分隐私机制，使其适用于更广泛的估计器类型和隐私定义，为实际应用提供了更灵活的隐私-效用权衡框架。

Abstract: The conventional approach in differential privacy (DP) literature formulates
the privacy-utility trade-off with a "privacy-first" perspective: for a
predetermined level of privacy, a certain utility is achievable. However,
practitioners often operate under a "utility-first" paradigm, prioritizing a
desired level of utility and then determining the corresponding privacy cost.
  Wu et al. [2019] initiated a formal study of this "utility-first" perspective
by introducing ex-post DP. They demonstrated that by adding correlated Laplace
noise and progressively reducing it on demand, a sequence of increasingly
accurate estimates of a private parameter can be generated, with the privacy
cost attributed only to the least noisy iterate released. This led to a Laplace
mechanism variant that achieves a specified utility with minimal privacy loss.
However, their work, and similar findings by Whitehouse et al. [2022], are
primarily limited to simple mechanisms based on Laplace or Gaussian noise.
  In this paper, we significantly generalize these results. In particular, we
extend the work of Wu et al. [2019] and Liu and Talwar [2019] to support any
sequence of private estimators, incurring at most a doubling of the original
privacy budget. Furthermore, we demonstrate that hyperparameter tuning for
these estimators, including the selection of an optimal privacy budget, can be
performed without additional privacy cost. Finally, we extend our results to
ex-post Renyi DP, further broadening the applicability of utility-first privacy
mechanisms.

</details>


### [9] [Retrieval-Augmented Review Generation for Poisoning Recommender Systems](https://arxiv.org/abs/2508.15252)
*Shiyi Yang,Xinshu Li,Guanglin Zhou,Chen Wang,Xiwei Xu,Liming Zhu,Lina Yao*

Main category: cs.CR

TL;DR: 本文提出RAGAN攻击框架，利用多模态基础模型的上下文学习能力生成高质量虚假用户配置文件，提升推荐系统的数据投毒攻击效果和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 推荐系统易受数据投毒攻击，但现有方法生成的虚假评论质量差，在资源受限的黑盒设置下攻击效果和隐蔽性不足。

Method: 提出RAGAN框架：1) 使用演示检索算法和文本风格转换策略增强上下文学习；2) 通过越狱器生成配置文件，由指令代理和守护者协同优化攻击可迁移性和隐蔽性。

Result: 在多个真实数据集上的实验表明，RAGAN实现了最先进的中毒攻击性能。

Conclusion: RAGAN框架能生成高质量的虚假用户配置文件，有效评估推荐系统的鲁棒性，在攻击效果和隐蔽性方面优于现有方法。

Abstract: Recent studies have shown that recommender systems (RSs) are highly
vulnerable to data poisoning attacks, where malicious actors inject fake user
profiles, including a group of well-designed fake ratings, to manipulate
recommendations. Due to security and privacy constraints in practice, attackers
typically possess limited knowledge of the victim system and thus need to craft
profiles that have transferability across black-box RSs. To maximize the attack
impact, the profiles often remains imperceptible. However, generating such
high-quality profiles with the restricted resources is challenging. Some works
suggest incorporating fake textual reviews to strengthen the profiles; yet, the
poor quality of the reviews largely undermines the attack effectiveness and
imperceptibility under the practical setting.
  To tackle the above challenges, in this paper, we propose to enhance the
quality of the review text by harnessing in-context learning (ICL) capabilities
of multimodal foundation models. To this end, we introduce a demonstration
retrieval algorithm and a text style transfer strategy to augment the navie
ICL. Specifically, we propose a novel practical attack framework named RAGAN to
generate high-quality fake user profiles, which can gain insights into the
robustness of RSs. The profiles are generated by a jailbreaker and
collaboratively optimized on an instructional agent and a guardian to improve
the attack transferability and imperceptibility. Comprehensive experiments on
various real-world datasets demonstrate that RAGAN achieves the
state-of-the-art poisoning attack performance.

</details>


### [10] [Connected and Exposed: Cybersecurity Risks, Regulatory Gaps, and Public Perception in Internet-Connected Vehicles](https://arxiv.org/abs/2508.15306)
*Henrietta Hegyi,Laszlo Erdodi*

Main category: cs.CR

TL;DR: 本文分析了联网车辆的网络安全和隐私风险，通过评估16项国际标准法规和用户调查，揭示了当前保护措施的不足与消费者认知差距


<details>
  <summary>Details</summary>
Motivation: 随着智能网联汽车的快速发展，网络安全和个人隐私风险日益突出，需要全面了解当前保护标准和用户认知状况

Method: 采用双重研究方法：1）对16项国际标准和法规进行多维度分析；2）开展用户调查了解消费者对智能汽车的态度和风险认知

Result: 研究发现当前标准在监管力度、技术细节、供应链风险处理等方面存在差异，同时用户对数据风险的认知不足且信息获取不充分

Conclusion: 需要制定更全面的标准和提高用户教育，以应对联网车辆生态系统中的安全和隐私挑战

Abstract: The rapid advancement of Internet-connected vehicle technologies has
introduced a new era of smart mobility, while simultaneously raising
significant cybersecurity and privacy concerns. This paper explores the
evolving threat landscape associated with connected vehicles, focusing on risks
such as unauthorized remote access and the potential leakage of personal data.
To assess the current state of protection, we conducted a comprehensive
analysis of 16 international standards and regulations, evaluating them from
multiple perspectives including regulatory strength, technical specificity,
treatment of supply chain risks, and approaches to personal data handling.
  In parallel, we carried out a user-focused survey designed to map consumer
attitudes toward smart cars. The survey investigated which types of vehicles
users trust and prefer, the reasons behind rejecting certain car types, their
awareness of data-related risks, and whether they feel adequately informed
about how their vehicles handle data. By combining regulatory analysis with
user perception insights, this study aims to contribute to a more holistic
understanding of the challenges and expectations surrounding connected vehicle
ecosystems.

</details>


### [11] [IPIGuard: A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agents](https://arxiv.org/abs/2508.15310)
*Hengyu An,Jinghuai Zhang,Tianyu Du,Chunyi Zhou,Qingming Li,Tao Lin,Shouling Ji*

Main category: cs.CR

TL;DR: IPIGuard是一种防御间接提示注入攻击的新方法，通过将智能体任务执行建模为工具依赖图遍历，分离行动规划与外部数据交互，显著减少恶意工具调用。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体防御方法依赖于模型内在安全性假设，缺乏对智能体行为的结构约束，导致在面对更强攻击向量时仍然脆弱。

Method: 提出IPIGuard防御框架，将智能体任务执行过程建模为工具依赖图(TDG)遍历，明确分离行动规划与外部数据交互阶段。

Result: 在AgentDojo基准测试中，IPIGuard在有效性和鲁棒性之间实现了优越的平衡，显著减少了由注入指令触发的意外工具调用。

Conclusion: IPIGuard为动态环境中开发更安全的智能体系统铺平了道路，通过结构化约束从根本上增强了对抗间接提示注入攻击的鲁棒性。

Abstract: Large language model (LLM) agents are widely deployed in real-world
applications, where they leverage tools to retrieve and manipulate external
data for complex tasks. However, when interacting with untrusted data sources
(e.g., fetching information from public websites), tool responses may contain
injected instructions that covertly influence agent behaviors and lead to
malicious outcomes, a threat referred to as Indirect Prompt Injection (IPI).
Existing defenses typically rely on advanced prompting strategies or auxiliary
detection models. While these methods have demonstrated some effectiveness,
they fundamentally rely on assumptions about the model's inherent security,
which lacks structural constraints on agent behaviors. As a result, agents
still retain unrestricted access to tool invocations, leaving them vulnerable
to stronger attack vectors that can bypass the security guardrails of the
model. To prevent malicious tool invocations at the source, we propose a novel
defensive task execution paradigm, called IPIGuard, which models the agents'
task execution process as a traversal over a planned Tool Dependency Graph
(TDG). By explicitly decoupling action planning from interaction with external
data, IPIGuard significantly reduces unintended tool invocations triggered by
injected instructions, thereby enhancing robustness against IPI attacks.
Experiments on the AgentDojo benchmark show that IPIGuard achieves a superior
balance between effectiveness and robustness, paving the way for the
development of safer agentic systems in dynamic environments.

</details>


### [12] [A Practical Guideline and Taxonomy to LLVM's Control Flow Integrity](https://arxiv.org/abs/2508.15386)
*Sabine Houy,Bruno Kreyssig,Timothee Riom,Alexandre Bartel,Patrick McDaniel*

Main category: cs.CR

TL;DR: 本文建立了LLVM前向边CFI变体与内存损坏漏洞类别的分类映射，为开发者在现有代码库中逐步部署CFI提供实用指导。基于Top 10已知被利用漏洞列表，评估了LLVM CFI对四类高影响漏洞的防护效果。


<details>
  <summary>Details</summary>
Motivation: 内存损坏漏洞仍然是软件安全最严重的威胁之一，虽然控制流完整性(CFI)可以缓解这类攻击，但开发者缺乏如何在实际软件中应用CFI的具体指导。

Method: 建立LLVM前向边CFI变体与漏洞类别的分类映射，基于Top 10 KEV列表选择四类高影响漏洞的代表性CVE，评估LLVM CFI对每个CVE的防护效果。

Result: CFI在两个案例中成功阻止了漏洞利用，但在另外两个案例中失败，显示了其潜力和当前局限性。

Conclusion: 研究结果为CFI的部署决策提供了依据，并为改进CFI在生产系统中的实际应用奠定了基础。

Abstract: Memory corruption vulnerabilities remain one of the most severe threats to
software security. They often allow attackers to achieve arbitrary code
execution by redirecting a vulnerable program's control flow. While Control
Flow Integrity (CFI) has gained traction to mitigate this exploitation path,
developers are not provided with any direction on how to apply CFI to
real-world software. In this work, we establish a taxonomy mapping LLVM's
forward-edge CFI variants to memory corruption vulnerability classes, offering
actionable guidance for developers seeking to deploy CFI incrementally in
existing codebases. Based on the Top 10 Known Exploited Vulnerabilities (KEV)
list, we identify four high-impact vulnerability categories and select one
representative CVE for each. We evaluate LLVM's CFI against each CVE and
explain why CFI blocks exploitation in two cases while failing in the other
two, illustrating its potential and current limitations. Our findings support
informed deployment decisions and provide a foundation for improving the
practical use of CFI in production systems.

</details>


### [13] [BadFU: Backdoor Federated Learning through Adversarial Machine Unlearning](https://arxiv.org/abs/2508.15541)
*Bingguang Lu,Hongsheng Hu,Yuantian Miao,Shaleeza Sohail,Chaoxiang He,Shuo Wang,Xiao Chen*

Main category: cs.CR

TL;DR: 本文提出了BadFU攻击，首次在联邦学习遗忘场景中实现后门攻击，恶意客户端通过看似合法的遗忘请求将后门注入全局模型。


<details>
  <summary>Details</summary>
Motivation: 随着数据隐私和监管合规要求日益严格，联邦学习中的机器遗忘技术变得重要，但将遗忘整合到联邦学习中带来了新的安全风险，特别是攻击者可能利用遗忘过程破坏全局模型的完整性。

Method: 提出BadFU攻击策略：恶意客户端在联邦训练过程中使用后门样本和伪装样本正常训练全局模型，当请求遗忘伪装样本时，全局模型转变为后门状态。

Result: 在各种联邦学习框架和遗忘策略下的广泛实验验证了BadFU的有效性，揭示了当前联邦遗忘实践中的关键漏洞。

Conclusion: 这项工作揭示了联邦遗忘机制中的严重安全漏洞，强调了开发更安全、更鲁棒的联邦遗忘机制的紧迫需求。

Abstract: Federated learning (FL) has been widely adopted as a decentralized training
paradigm that enables multiple clients to collaboratively learn a shared model
without exposing their local data. As concerns over data privacy and regulatory
compliance grow, machine unlearning, which aims to remove the influence of
specific data from trained models, has become increasingly important in the
federated setting to meet legal, ethical, or user-driven demands. However,
integrating unlearning into FL introduces new challenges and raises largely
unexplored security risks. In particular, adversaries may exploit the
unlearning process to compromise the integrity of the global model. In this
paper, we present the first backdoor attack in the context of federated
unlearning, demonstrating that an adversary can inject backdoors into the
global model through seemingly legitimate unlearning requests. Specifically, we
propose BadFU, an attack strategy where a malicious client uses both backdoor
and camouflage samples to train the global model normally during the federated
training process. Once the client requests unlearning of the camouflage
samples, the global model transitions into a backdoored state. Extensive
experiments under various FL frameworks and unlearning strategies validate the
effectiveness of BadFU, revealing a critical vulnerability in current federated
unlearning practices and underscoring the urgent need for more secure and
robust federated unlearning mechanisms.

</details>


### [14] [Towards Scalable and Interpretable Mobile App Risk Analysis via Large Language Models](https://arxiv.org/abs/2508.15606)
*Yu Yang,Zhenyuan Li,Xiandong Ran,Jiahao Liu,Jiahui Wang,Bo Yu,Shouling Ji*

Main category: cs.CR

TL;DR: Mars是一个基于大语言模型的自动化移动应用安全风险评估系统，通过风险识别树提取关键指标，减少LLM输入并降低幻觉风险，在真实数据集上达到0.838的F1分数，效率提升60-90%。


<details>
  <summary>Details</summary>
Motivation: 当前移动应用市场的安全审核过程依赖人工分析，效率低下且劳动密集，需要自动化解决方案来提高审核效率和准确性。

Method: 构建风险识别树从高维应用特征中提取相关指标，减少LLM输入量；使用LLM进行最终风险判定；自动生成包含分析过程的证据链以支持人工审核。

Result: 在真实Android市场数据集上，风险识别F1分数达0.838，证据检索F1分数达0.934；用户研究表明效率比传统人工分析提升60-90%。

Conclusion: Mars系统成功实现了移动应用安全风险评估的自动化，在保持高准确性的同时显著提升了效率，为应用市场审核提供了可行的自动化解决方案。

Abstract: Mobile application marketplaces are responsible for vetting apps to identify
and mitigate security risks. Current vetting processes are labor-intensive,
relying on manual analysis by security professionals aided by semi-automated
tools. To address this inefficiency, we propose Mars, a system that leverages
Large Language Models (LLMs) for automated risk identification and profiling.
Mars is designed to concurrently analyze multiple applications across diverse
risk categories with minimal human intervention. To enhance analytical
precision and operational efficiency, Mars leverages a pre-constructed risk
identification tree to extract relevant indicators from high-dimensional
application features. This initial step filters the data, reducing the input
volume for the LLM and mitigating the potential for model hallucination induced
by irrelevant features. The extracted indicators are then subjected to LLM
analysis for final risk determination. Furthermore, Mars automatically
generates a comprehensive evidence chain for each assessment, documenting the
analytical process to provide transparent justification. These chains are
designed to facilitate subsequent manual review and to inform enforcement
decisions, such as application delisting. The performance of Mars was evaluated
on a real-world dataset from a partner Android marketplace. The results
demonstrate that Mars attained an F1-score of 0.838 in risk identification and
an F1-score of 0.934 in evidence retrieval. To assess its practical
applicability, a user study involving 20 expert analysts was conducted, which
indicated that Mars yielded a substantial efficiency gain, ranging from 60% to
90%, over conventional manual analysis.

</details>
