<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 24]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Social Engineering Attacks: A Systemisation of Knowledge on People Against Humans](https://arxiv.org/abs/2601.04215)
*Scott Thomson,Michael Bewong,Arash Mahboubi,Tanveer Zia*

Main category: cs.CR

TL;DR: 本文系统化分析了社会工程攻击（SEA），提出了三层分析框架（人类因素、组织文化、攻击者动机），开发了风险加权的HAISQ元分析，并设计了分段的适应性培训蓝图。


<details>
  <summary>Details</summary>
Motivation: 社会工程攻击日益严重，通过利用泄露的个人数据和行为线索绕过技术控制，对机场旅客和智慧城市居民等物理网络空间构成重大威胁。近期Optus、Medibank和Qantas等大规模数据泄露事件凸显了问题的紧迫性。

Method: 1. 三层系统化：统一知识-能力-行为（KAB）指标、文化驱动因素和攻击者经济学的分析框架
2. 风险加权HAISQ元分析：对近期实地研究的HAISQ分数进行标准化和排名，识别高风险集群
3. 适应性"分段与模拟"培训蓝图：基于聚类证据设计差异化培训方案

Result: 1. 识别出三个关键维度：人类KAB因素、组织文化和非正式规范、攻击者动机与技术
2. 揭示了互联网和社交媒体使用等持续高风险集群
3. 提出了从描述性转向预测性的风险加权方法
4. 开发了针对低、中、高风险用户群体的差异化培训方案

Conclusion: 该研究为理解社会工程攻击提供了统一的分析框架，使从业者能够看到漏洞、规范和威胁激励如何共同演化。提出的风险加权方法和适应性培训蓝图有助于将反制措施嵌入未来智慧城市战略，提高网络安全防护的有效性。

Abstract: Our systematisation of knowledge on Social Engineering Attacks (SEAs), identifies the human, organisational, and adversarial dimensions of cyber threats. It addresses the growing risks posed by SEAs, highly relevant in the context physical cyber places, such as travellers at airports and residents in smart cities, and synthesizes findings from peer reviewed studies, industry and government reports to inform effective countermeasures that can be embedded into future smart city strategies. SEAs increasingly sidestep technical controls by weaponising leaked personal data and behavioural cues, an urgency underscored by the Optus, Medibank and now Qantas (2025) mega breaches that placed millions of personal records in criminals' hands. Our review surfaces three critical dimensions: (i) human factors of knowledge, abilities and behaviours (KAB) (ii) organisational culture and informal norms that shape those behaviours and (iii) attacker motivations, techniques and return on investment calculations. Our contributions are threefold: (1) TriLayer Systematisation: to the best of our knowledge, we are the first to unify KAB metrics, cultural drivers and attacker economics into a single analytical lens, enabling practitioners to see how vulnerabilities, norms and threat incentives coevolve. (2) Risk Weighted HAISQ Meta analysis: By normalising and ranking HAISQ scores across recent field studies, we reveal persistent high risk clusters (Internet and Social Media use) and propose impact weightings that make the instrument predictive rather than descriptive. (3) Adaptive 'Segment and Simulate' Training Blueprint: Building on clustering evidence, we outline a differentiated programme that matches low, medium, high risk user cohorts to experiential learning packages including phishing simulations, gamified challenges and realtime feedback thereby aligning effort with measured exposure.

</details>


### [2] [Integrating Multi-Agent Simulation, Behavioral Forensics, and Trust-Aware Machine Learning for Adaptive Insider Threat Detection](https://arxiv.org/abs/2601.04243)
*Firdous Kausar,Asmah Muallem,Naw Safrin Sattar,Mohamed Zakaria Kurdi*

Main category: cs.CR

TL;DR: 提出混合框架用于自适应内部威胁检测，集成多智能体模拟、分层SIEM关联、行为与通信取证、信任感知机器学习和心智理论推理，通过四种系统变体验证认知上下文提升敏感性，证据门控实现高精度低噪声检测。


<details>
  <summary>Details</summary>
Motivation: 传统内部威胁检测系统往往缺乏对恶意行为者认知意图的理解，难以平衡检测敏感性和误报率。需要结合认知推理和证据验证机制来提高检测精度和效率。

Method: 开发混合框架，整合多智能体模拟生成行为和认知信号，构建四种系统变体：分层SIEM核心基线、认知增强SIEM（加入心智理论和通信取证）、证据门控SIEM（引入精度验证机制）、以及Enron增强的证据门控SIEM（加入预训练邮件取证模块）。

Result: 认知增强SIEM实现完美召回率（1.000），将参与者级F1从0.521提升至0.774；证据门控SIEM将参与者级F1提升至0.922，确认警报精度达0.997，每轮误报降至0.2；Enron增强版本保持高精度（1.000确认警报精度，0.0误报），参与者级F1微升至0.933，检测延迟从15.20步降至10.26步。

Conclusion: 认知上下文能提升检测敏感性，证据门控验证可实现高精度低噪声检测，预训练的通信校准能加速高置信度的内部威胁识别。该框架为平衡检测精度和效率提供了有效解决方案。

Abstract: We present a hybrid framework for adaptive insider-threat detection that tightly integrates multi-agent simulation (MAS), layered Security Information and Event Management (SIEM) correlation, behavioral and communication forensics, trust-aware machine learning, and Theory-of-Mind (ToM) reasoning. Intelligent agents operate in a simulated enterprise environment, generating both behavioral events and cognitive intent signals that are ingested by a centralized SIEM. We evaluate four system variants: a Layered SIEM-Core (LSC) baseline, a Cognitive-Enriched SIEM (CE-SIEM) incorporating ToM and communication forensics, an Evidence-Gated SIEM (EG-SIEM) introducing precision-focused validation mechanisms, and an Enron-enabled EG-SIEM (EG-SIEM-Enron) that augments evidence gating with a pretrained email forensics module calibrated on Enron corpora. Across ten simulation runs involving eight malicious insiders, CE-SIEM achieves perfect recall (1.000) and improves actor-level F1 from 0.521 (LSC) to 0.774. EG-SIEM raises actor-level F1 to 0.922 and confirmed-alert precision to 0.997 while reducing false positives to 0.2 per run. EG-SIEM-Enron preserves high precision (1.000 confirmed-alert precision; 0.0 false positives per run), slightly improves actor-level F1 to 0.933, and reduces detection latency (average TTD 10.26 steps versus 15.20 for EG-SIEM). These results demonstrate that cognitive context improves sensitivity, evidence-gated validation enables high-precision, low-noise detection, and pretrained communication calibration can further accelerate high-confidence insider threat identification.

</details>


### [3] [Beyond Immediate Activation: Temporally Decoupled Backdoor Attacks on Time Series Forecasting](https://arxiv.org/abs/2601.04247)
*Zhixin Liu,Xuanlin Liu,Sihan Xu,Yaqiong Qiao,Ying Zhang,Xiangrui Cai*

Main category: cs.CR

TL;DR: 提出TDBA框架，实现多元时间序列预测中的时间解耦后门攻击，允许触发器和目标模式在不同时间和维度上异步激活


<details>
  <summary>Details</summary>
Motivation: 现有多元时间序列预测的后门攻击要求触发器和目标模式在时间和维度上严格同步激活，但现实场景需要延迟和变量特定的激活方式，这一关键需求尚未得到满足

Method: TDBA框架包含两个核心模块：1) 位置引导的触发器生成机制，利用平滑高斯先验生成与预定目标模式位置相关的触发器；2) 位置感知优化模块，基于触发器完整性、模式覆盖度和时间偏移分配软权重，实现有针对性和隐蔽的攻击优化

Result: 在真实世界数据集上的大量实验表明，TDBA在保持良好隐蔽性的同时，在攻击效果上始终优于现有基线方法。消融研究证实了其设计的可控性和鲁棒性

Conclusion: TDBA解决了多元时间序列预测中后门攻击的关键限制，实现了时间解耦的灵活攻击，为现实场景中的异步激活需求提供了有效解决方案

Abstract: Existing backdoor attacks on multivariate time series (MTS) forecasting enforce strict temporal and dimensional coupling between triggers and target patterns, requiring synchronous activation at fixed positions across variables. However, realistic scenarios often demand delayed and variable-specific activation. We identify this critical unmet need and propose TDBA, a temporally decoupled backdoor attack framework for MTS forecasting. By injecting triggers that encode the expected location of the target pattern, TDBA enables the activation of the target pattern at any positions within the forecasted data, with the activation position flexibly varying across different variable dimensions. TDBA introduces two core modules: (1) a position-guided trigger generation mechanism that leverages smoothed Gaussian priors to generate triggers that are position-related to the predefined target pattern; and (2) a position-aware optimization module that assigns soft weights based on trigger completeness, pattern coverage, and temporal offset, facilitating targeted and stealthy attack optimization. Extensive experiments on real-world datasets show that TDBA consistently outperforms existing baselines in effectiveness while maintaining good stealthiness. Ablation studies confirm the controllability and robustness of its design.

</details>


### [4] [Inhibitory Attacks on Backdoor-based Fingerprinting for Large Language Models](https://arxiv.org/abs/2601.04261)
*Hang Fu,Wanli Peng,Yinghan Zhou,Jiaxuan Wu,Juan Wen,Yiming Xue*

Main category: cs.CR

TL;DR: 本文提出两种针对LLM指纹识别的攻击方法（TFA和SVA），专门针对LLM集成场景，能有效抑制指纹响应同时保持集成性能，揭示了现有指纹识别技术的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在商业和研究领域的广泛应用，知识产权保护需求日益迫切。基于后门的LLM指纹识别成为有前景的解决方案，但LLM集成技术（多模型协作）的流行使得现有指纹识别在集成场景下的脆弱性尚未被探索，需要全面评估其鲁棒性。

Method: 提出两种新颖的指纹识别攻击方法：1) 令牌过滤攻击(TFA)：在每个解码步骤中，从令牌过滤机制创建的统一令牌集中获取下一个令牌；2) 句子验证攻击(SVA)：基于困惑度和投票的句子验证机制过滤掉指纹响应。

Result: 实验表明，所提方法能有效抑制指纹响应，同时保持LLM集成的性能。与现有最先进的攻击方法相比，本文方法能取得更好的性能表现。

Conclusion: 研究发现揭示了LLM指纹识别在集成场景下的脆弱性，强调需要增强LLM指纹识别的鲁棒性，以应对实际应用中的安全挑战。

Abstract: The widespread adoption of Large Language Model (LLM) in commercial and research settings has intensified the need for robust intellectual property protection. Backdoor-based LLM fingerprinting has emerged as a promising solution for this challenge. In practical application, the low-cost multi-model collaborative technique, LLM ensemble, combines diverse LLMs to leverage their complementary strengths, garnering significant attention and practical adoption. Unfortunately, the vulnerability of existing LLM fingerprinting for the ensemble scenario is unexplored. In order to comprehensively assess the robustness of LLM fingerprinting, in this paper, we propose two novel fingerprinting attack methods: token filter attack (TFA) and sentence verification attack (SVA). The TFA gets the next token from a unified set of tokens created by the token filter mechanism at each decoding step. The SVA filters out fingerprint responses through a sentence verification mechanism based on perplexity and voting. Experimentally, the proposed methods effectively inhibit the fingerprint response while maintaining ensemble performance. Compared with state-of-the-art attack methods, the proposed method can achieve better performance. The findings necessitate enhanced robustness in LLM fingerprinting.

</details>


### [5] [You Only Anonymize What Is Not Intent-Relevant: Suppressing Non-Intent Privacy Evidence](https://arxiv.org/abs/2601.04265)
*Weihao Shen,Yaxin Xu,Shuang Li,Wei Chen,Yuqin Lan,Meng Yuan,Fuzhen Zhuang*

Main category: cs.CR

TL;DR: IntentAnony：一种基于意图的匿名化方法，通过建模语用意图和隐私推理证据链，在保护隐私的同时保留文本的语义、语用和情感功能，相比现有方法在隐私-效用权衡上提升约30%。


<details>
  <summary>Details</summary>
Motivation: 现有匿名化方法通常对所有属性采用统一处理，这会与沟通意图冲突并掩盖必要信息，特别是当个人属性对表达或语用目标至关重要时。核心挑战在于确定保护哪些属性、保护到什么程度，同时保持语义和语用功能。

Method: 提出IntentAnony方法，进行意图条件化的暴露控制：1) 建模语用意图；2) 构建隐私推理证据链，捕捉分布式线索如何支持属性推理；3) 基于意图为每个属性分配暴露预算；4) 选择性抑制非意图推理路径，同时保留意图相关内容、语义结构、情感细微差别和交互功能。

Result: 评估显示在隐私推理成功率、文本效用指标和人工评估方面，IntentAnony相比现有最先进方法在整体隐私-效用权衡上提升约30%，且匿名化文本的可用性显著更强。

Conclusion: IntentAnony通过意图条件化的暴露控制，实现了更好的隐私保护与文本效用平衡，解决了传统匿名化方法忽视语用意图的问题，为实用隐私保护提供了新思路。

Abstract: Anonymizing sensitive information in user text is essential for privacy, yet existing methods often apply uniform treatment across attributes, which can conflict with communicative intent and obscure necessary information. This is particularly problematic when personal attributes are integral to expressive or pragmatic goals. The central challenge lies in determining which attributes to protect, and to what extent, while preserving semantic and pragmatic functions. We propose IntentAnony, a utility-preserving anonymization approach that performs intent-conditioned exposure control. IntentAnony models pragmatic intent and constructs privacy inference evidence chains to capture how distributed cues support attribute inference. Conditioned on intent, it assigns each attribute an exposure budget and selectively suppresses non-intent inference pathways while preserving intent-relevant content, semantic structure, affective nuance, and interactional function. We evaluate IntentAnony using privacy inference success rates, text utility metrics, and human evaluation. The results show an approximately 30% improvement in the overall privacy--utility trade-off, with notably stronger usability of anonymized text compared to prior state-of-the-art methods. Our code is available at https://github.com/Nevaeh7/IntentAnony.

</details>


### [6] [State Backdoor: Towards Stealthy Real-world Poisoning Attack on Vision-Language-Action Model in State Space](https://arxiv.org/abs/2601.04266)
*Ji Guo,Wenbo Jiang,Yansong Lin,Yijing Liu,Ruichen Zhang,Guomin Lu,Aiguo Chen,Xinshuo Han,Hongwei Li,Dusit Niyato*

Main category: cs.CR

TL;DR: 提出一种针对视觉-语言-动作模型的新型后门攻击方法——状态后门，利用机器人初始状态作为触发器，通过偏好引导遗传算法优化，在保持正常任务性能的同时实现高攻击成功率


<details>
  <summary>Details</summary>
Motivation: VLA模型广泛应用于机器人等安全关键领域，但其复杂的多模态交互暴露了新的安全漏洞。现有后门方法依赖视觉触发器，在真实环境中鲁棒性差且易被察觉。需要开发更隐蔽、更鲁棒的后门攻击方法。

Method: 提出状态后门攻击，将机器人手臂的初始状态作为触发器。设计偏好引导遗传算法（PGA），在状态空间中高效搜索最小但有效的触发器，平衡隐蔽性和攻击效果。

Result: 在5个代表性VLA模型和5个真实世界任务上的实验表明，该方法攻击成功率超过90%，且不影响正常任务性能，揭示了具身AI系统中未充分探索的安全漏洞。

Conclusion: 状态后门攻击是一种实用且有效的新型后门攻击方法，利用机器人初始状态作为隐蔽触发器，通过PGA优化实现高攻击成功率，为VLA模型的安全防护提供了重要警示。

Abstract: Vision-Language-Action (VLA) models are widely deployed in safety-critical embodied AI applications such as robotics. However, their complex multimodal interactions also expose new security vulnerabilities. In this paper, we investigate a backdoor threat in VLA models, where malicious inputs cause targeted misbehavior while preserving performance on clean data. Existing backdoor methods predominantly rely on inserting visible triggers into visual modality, which suffer from poor robustness and low insusceptibility in real-world settings due to environmental variability. To overcome these limitations, we introduce the State Backdoor, a novel and practical backdoor attack that leverages the robot arm's initial state as the trigger. To optimize trigger for insusceptibility and effectiveness, we design a Preference-guided Genetic Algorithm (PGA) that efficiently searches the state space for minimal yet potent triggers. Extensive experiments on five representative VLA models and five real-world tasks show that our method achieves over 90% attack success rate without affecting benign task performance, revealing an underexplored vulnerability in embodied AI systems.

</details>


### [7] [Shadow Unlearning: A Neuro-Semantic Approach to Fidelity-Preserving Faceless Forgetting in LLMs](https://arxiv.org/abs/2601.04275)
*Dinesh Srivasthav P,Ashok Urlana,Rahul Mishra,Bala Mallikarjunarao Garlapati,Ponnurangam Kumaraguru*

Main category: cs.CR

TL;DR: 提出Shadow Unlearning新范式，通过匿名化数据实现隐私保护的机器遗忘，避免暴露个人可识别信息，比传统方法计算效率高10倍以上。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法需要访问要删除的数据，这会暴露数据给成员推理攻击和PII滥用风险，违反了GDPR等隐私法规的"被遗忘权"要求。

Method: 提出Shadow Unlearning范式和Neuro-Semantic Projector Unlearning (NSPU)框架，在匿名化的遗忘数据上执行机器遗忘，不暴露PII。使用Multi-domain Fictitious Unlearning (MuFU)数据集和评估栈来量化知识保留与遗忘效果的权衡。

Result: 在各种LLM上的实验表明，NSPU实现了优异的遗忘性能，保持了模型效用，增强了用户隐私，且计算效率比标准遗忘方法至少高10倍。

Conclusion: 该方法为隐私感知的机器遗忘开辟了新方向，平衡了数据保护和模型保真度，满足隐私法规要求的同时保持模型性能。

Abstract: Machine unlearning aims to selectively remove the influence of specific training samples to satisfy privacy regulations such as the GDPR's 'Right to be Forgotten'. However, many existing methods require access to the data being removed, exposing it to membership inference attacks and potential misuse of Personally Identifiable Information (PII). We address this critical challenge by proposing Shadow Unlearning, a novel paradigm of approximate unlearning, that performs machine unlearning on anonymized forget data without exposing PII. We further propose a novel privacy-preserving framework, Neuro-Semantic Projector Unlearning (NSPU) to achieve Shadow unlearning. To evaluate our method, we compile Multi-domain Fictitious Unlearning (MuFU) forget set across five diverse domains and introduce an evaluation stack to quantify the trade-off between knowledge retention and unlearning effectiveness. Experimental results on various LLMs show that NSPU achieves superior unlearning performance, preserves model utility, and enhances user privacy. Additionally, the proposed approach is at least 10 times more computationally efficient than standard unlearning approaches. Our findings foster a new direction for privacy-aware machine unlearning that balances data protection and model fidelity.

</details>


### [8] [A Privacy-Preserving Localization Scheme with Node Selection in Mobile Networks](https://arxiv.org/abs/2601.04280)
*Liangbo Xie,Mude Cai,Xiaolong Yang,Mu Zhou,Jiacheng Wang,Dusit Niyato*

Main category: cs.CR

TL;DR: 提出PPLZN隐私保护定位方案，在众包定位中保护目标和锚节点的位置隐私，同时提高定位精度并降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 移动网络定位应用广泛，但传统定位方法中负责位置估计的实体可能导致目标和锚节点的位置信息泄露，存在严重安全风险。现有隐私保护定位算法面临定位精度不足和通信开销过大的挑战。

Method: 提出名为PPLZN的隐私保护定位方案，在众包定位环境中同时保护目标和锚节点的位置隐私。该方法旨在实现准确位置估计而不泄露位置信息。

Result: 仿真结果验证了PPLZN的有效性，能够实现准确的位置估计且无位置泄露，在定位精度和通信开销方面优于现有先进方法。在大规模部署中显著降低计算和通信开销。

Conclusion: PPLZN方案适用于资源受限网络中的实际隐私保护定位，在保护位置隐私的同时实现了高精度定位和低通信开销的平衡。

Abstract: Localization in mobile networks has been widely applied in many scenarios. However, an entity responsible for location estimation exposes both the target and anchors to potential location leakage at any time, creating serious security risks. Although existing studies have proposed privacy-preserving localization algorithms, they still face challenges of insufficient positioning accuracy and excessive communication overhead. In this article, we propose a privacy-preserving localization scheme, named PPLZN. PPLZN protects protects the location privacy of both the target and anchor nodes in crowdsourced localization. Simulation results validate the effectiveness of PPLZN. Evidently, it can achieve accurate position estimation without location leakage and outperform state-of-the-art approaches in both positioning accuracy and communication overhead. In addition, PPLZN significantly reduces computational and communication overhead in large-scale deployments, making it well-fitted for practical privacy-preserving localization in resource-constrained networks.

</details>


### [9] [A Longitudinal Measurement Study of Log4Shell Exploitation from an Active Network Telescope](https://arxiv.org/abs/2601.04281)
*Aakash Singh,Kuldeep Singh Yadav,V. Anil Kumar,Samiran Ghosh,Pranita Baro,Basavala Bhanu Prasanth*

Main category: cs.CR

TL;DR: 对Log4Shell漏洞的长期纵向测量研究，基于印度网络望远镜2021年12月至2025年10月的观测数据，揭示了漏洞披露后数年仍持续存在的扫描和利用活动演变规律。


<details>
  <summary>Details</summary>
Motivation: 先前研究主要关注Log4Shell漏洞披露初期的欧美网络望远镜数据，缺乏长期和地理多样性的观测。需要了解漏洞利用行为的长期演变及其区域特征，以全面理解关键软件漏洞的生命周期。

Method: 使用部署在印度的主动网络望远镜，对2021年12月至2025年10月期间的Log4Shell相关流量进行纵向测量研究。分析扫描广度、基础设施重用、载荷构造和目标选择的变化，并与基准研究进行对比分析。

Result: Log4Shell漏洞利用在披露后数年仍持续存在，活动逐渐集中在较小的重复扫描器和回调基础设施集合上。观察到载荷混淆增加、协议和端口使用变化。与基准研究的对比验证了相关时间趋势和因观测点位置差异导致的系统性差异。

Conclusion: Log4Shell漏洞在初始披露期后仍保持活跃，强调了长期、地理多样性的测量对于理解关键软件漏洞完整生命周期的重要性。研究结果为漏洞管理提供了重要见解。

Abstract: The disclosure of the Log4Shell vulnerability in December 2021 led to an unprecedented wave of global scanning and exploitation activity. A recent study provided important initial insights, but was largely limited in duration and geography, focusing primarily on European and U.S. network telescope deployments and covering the immediate aftermath of disclosure. As a result, the longer-term evolution of exploitation behavior and its regional characteristics has remained insufficiently understood. In this paper, we present a longitudinal measurement study of Log4Shell-related traffic observed between December 2021 and October 2025 by an active network telescope deployed in India. This vantage point enables examination of sustained exploitation dynamics beyond the initial outbreak phase, including changes in scanning breadth, infrastructure reuse, payload construction, and destination targeting. Our analysis reveals that Log4Shell exploitation persists for several years after disclosure, with activity gradually concentrating around a smaller set of recurring scanner and callback infrastructures, accompanied by an increase in payload obfuscation and shifts in protocol and port usage. A comparative analysis and observations with the benchmark study validate both correlated temporal trends and systematic differences attributable to vantage point placement and coverage. Subsequently, these results demonstrate that Log4Shell remains active well beyond its initial disclosure period, underscoring the value of long-term, geographically diverse measurement for understanding the full lifecycle of critical software vulnerabilities.

</details>


### [10] [Privacy at Scale in Networked Healthcare](https://arxiv.org/abs/2601.04298)
*M. Amin Rahimian,Benjamin Panny,James Joshi*

Main category: cs.CR

TL;DR: 论文主张在医疗健康领域实施以决策理论差分隐私为核心的隐私设计规模化，通过隐私预算账本、控制平面等工具实现合规、可信的数据共享。


<details>
  <summary>Details</summary>
Motivation: 数字化医疗网络虽然带来早期检测、精准治疗等好处，但也扩大了隐私泄露和合规风险。当前应用级保护措施零散孤立，需要转向规模化隐私设计。

Method: 提出三个核心方法：1) 基于决策理论差分隐私的全生命周期保护；2) 考虑人员、传感器、组织相互依赖的网络感知隐私计算；3) 将合规要求代码化的工具，使医疗机构能共享证据并展示监管合规。

Result: 论文综合分析了医疗领域的隐私增强技术现状，识别实践差距，并提出了可部署的议程：隐私预算账本、跨站点PET组件协调控制平面、共享测试平台和PET素养培养。

Conclusion: 通过分布式推理作为多机构学习的主要工具，在明确隐私预算下实现合法、可信的数据共享成为默认模式，并以多中心试验、基因组学、疾病监测等用例进行说明。

Abstract: Digitized, networked healthcare promises earlier detection, precision therapeutics, and continuous care; yet, it also expands the surface for privacy loss and compliance risk. We argue for a shift from siloed, application-specific protections to privacy-by-design at scale, centered on decision-theoretic differential privacy (DP) across the full healthcare data lifecycle; network-aware privacy accounting for interdependence in people, sensors, and organizations; and compliance-as-code tooling that lets health systems share evidence while demonstrating regulatory due care. We synthesize the privacy-enhancing technology (PET) landscape in health (federated analytics, DP, cryptographic computation), identify practice gaps, and outline a deployable agenda involving privacy-budget ledgers, a control plane to coordinate PET components across sites, shared testbeds, and PET literacy, to make lawful, trustworthy sharing the default. We illustrate with use cases (multi-site trials, genomics, disease surveillance, mHealth) and highlight distributed inference as a workhorse for multi-institution learning under explicit privacy budgets.

</details>


### [11] [Large Language Models for Detecting Cyberattacks on Smart Grid Protective Relays](https://arxiv.org/abs/2601.04443)
*Ahmad Mohammad Saber,Saeed Jafari,Zhengmao Ouyang,Paul Budnarain,Amr Youssef,Deepa Kundur*

Main category: cs.CR

TL;DR: 本文提出基于大语言模型的变压器电流差动保护网络攻击检测框架，使用DistilBERT等紧凑LLM区分网络攻击与实际故障，检测率达97.6%，推理延迟低于6ms。


<details>
  <summary>Details</summary>
Motivation: 变压器电流差动保护若遭受网络攻击可能导致误跳闸，威胁关键变压器安全运行。现有方法难以有效检测针对保护系统的网络攻击，需要开发智能检测框架。

Method: 提出基于LLM的检测框架，将多维TCDR电流测量数据文本化，使用DistilBERT等紧凑LLM进行微调，区分网络攻击与实际故障。还评估了GPT-2和DistilBERT+LoRA等变体。

Result: DistilBERT检测率达到97.6%，不损害TCDR可靠性，推理延迟低于6ms。框架对时间同步和虚假数据注入攻击具有鲁棒性，对测量噪声有抗性，在不同提示变体中保持稳定。

Conclusion: LLM在智能电网网络安全中具有巨大潜力，紧凑LLM如DistilBERT可有效检测TCDR网络攻击，提供高性能、低延迟的解决方案。研究提供完整数据集支持可重复性。

Abstract: This paper presents a large language model (LLM)-based framework for detecting cyberattacks on transformer current differential relays (TCDRs), which, if undetected, may trigger false tripping of critical transformers. The proposed approach adapts and fine-tunes compact LLMs such as DistilBERT to distinguish cyberattacks from actual faults using textualized multidimensional TCDR current measurements recorded before and after tripping. Our results demonstrate that DistilBERT detects 97.6% of cyberattacks without compromising TCDR dependability and achieves inference latency below 6 ms on a commercial workstation. Additional evaluations confirm the framework's robustness under combined time-synchronization and false-data-injection attacks, resilience to measurement noise, and stability across prompt formulation variants. Furthermore, GPT-2 and DistilBERT+LoRA achieve comparable performance, highlighting the potential of LLMs for enhancing smart grid cybersecurity. We provide the full dataset used in this study for reproducibility.

</details>


### [12] [Decision-Aware Trust Signal Alignment for SOC Alert Triage](https://arxiv.org/abs/2601.04486)
*Israt Jahan Chowdhury,Md Abu Yousuf Tanvir*

Main category: cs.CR

TL;DR: 提出一个决策敏感的信任信号对应方案，用于SOC警报分类，结合校准后的置信度、轻量级不确定性提示和成本敏感决策阈值，以改善分析师决策支持。


<details>
  <summary>Details</summary>
Motivation: 当前SOC中的机器学习检测系统通常显示概率结果或置信度分数，但这些分数校准不佳且在压力下难以解读。警报质量差和警报过载增加了分析师负担，特别是当工具输出与决策需求不一致时。主要限制在于模型置信度显示通常未考虑决策中的不对称成本（漏报比误报危害更大）。

Method: 提出决策敏感的信任信号对应框架，结合：1）使用已知后处理方法进行校准的置信度；2）轻量级不确定性提示，在模型确定性低时提供保守保护；3）成本敏感决策阈值。该框架作为决策支持层，无需修改检测模型。使用Logistic回归和随机森林分类器在UNSW-NB15入侵检测基准上进行评估。

Result: 模拟结果显示：未对齐的置信度显示会显著增加漏报率；而具有决策对齐信任信号的模型使成本加权损失降低了数量级。这表明决策对齐的信任信号能有效改善警报分类性能。

Conclusion: 提出的决策敏感信任信号对应方案能有效改善SOC警报分类中的决策支持，通过校准置信度、不确定性提示和成本敏感阈值来减少漏报并降低决策成本。计划进行人机交互研究以实证评估对齐与未对齐信任界面对分析师决策的影响。

Abstract: Detection systems that utilize machine learning are progressively implemented at Security Operations Centers (SOCs) to help an analyst to filter through high volumes of security alerts. Practically, such systems tend to reveal probabilistic results or confidence scores which are ill-calibrated and hard to read when under pressure. Qualitative and survey based studies of SOC practice done before reveal that poor alert quality and alert overload greatly augment the burden on the analyst, especially when tool outputs are not coherent with decision requirements, or signal noise. One of the most significant limitations is that model confidence is usually shown without expressing that there are asymmetric costs in decision making where false alarms are much less harmful than missed attacks. The present paper presents a decision-sensitive trust signal correspondence scheme of SOC alert triage. The framework combines confidence that has been calibrated, lightweight uncertainty cues, and cost-sensitive decision thresholds into coherent decision-support layer, instead of making changes to detection models. To enhance probabilistic consistency, the calibration is done using the known post-hoc methods and the uncertainty cues give conservative protection in situations where model certainty is low. To measure the model-independent performance of the suggested model, we apply the Logistic Regression and the Random Forest classifiers to the UNSW-NB15 intrusion detection benchmark. According to simulation findings, false negatives are greatly amplified by the presence of misaligned displays of confidence, whereas cost weighted loss decreases by orders of magnitude between models with decision aligned trust signals. Lastly, we describe a human-in-the-loop study plan that would allow empirically assessing the decision-making of the analysts with aligned and misaligned trust interfaces.

</details>


### [13] [Application of Hybrid Chain Storage Framework in Energy Trading and Carbon Asset Management](https://arxiv.org/abs/2601.04512)
*Yinghan Hou,Zongyou Yang,Xiaokun Yang*

Main category: cs.CR

TL;DR: 提出混合链上链下结算框架，解决分布式能源交易与碳资产管理中的高频小额结算问题，在保证审计可信度的同时显著降低链上成本


<details>
  <summary>Details</summary>
Motivation: 分布式能源交易和碳资产管理需要高频、小额结算，且对审计要求高。完全链上方案成本过高，而纯链下方案缺乏可验证的一致性保证

Method: 设计混合链上链下结算框架：将结算承诺和关键约束锚定在链上，通过确定性摘要和可重放审计将链下记录链接起来

Result: 在公开约束的工作负载下实验表明，该框架显著降低了链上执行和存储成本，同时保持了审计可信度

Conclusion: 混合链上链下框架有效平衡了分布式能源交易和碳资产管理中的成本与审计可信度需求

Abstract: Distributed energy trading and carbon asset management involve high-frequency, small-value settlements with strong audit requirements. Fully on-chain designs incur excessive cost, while purely off-chain approaches lack verifiable consistency. This paper presents a hybrid on-chain and off-chain settlement framework that anchors settlement commitments and key constraints on-chain and links off-chain records through deterministic digests and replayable auditing. Experiments under publicly constrained workloads show that the framework significantly reduces on-chain execution and storage cost while preserving audit trustworthiness.

</details>


### [14] [Deep Dive into the Abuse of DL APIs To Create Malicious AI Models and How to Detect Them](https://arxiv.org/abs/2601.04553)
*Mohamed Nabeel,Oleksii Starov*

Main category: cs.CR

TL;DR: 论文揭示AI模型供应链安全风险：攻击者可滥用TensorFlow等框架的隐藏API功能（文件读写、网络通信）植入恶意代码，现有扫描工具无法有效检测，并提出基于LLM的检测方法。


<details>
  <summary>Details</summary>
Motivation: 随着70%以上组织将在2025年前集成AI模型到工作流程，从Hugging Face等模型中心获取预训练模型成为常态，但这引入了严重的安全风险。攻击者可在上传的模型中注入恶意代码，利用深度学习API的隐藏功能发起攻击，而现有扫描工具缺乏语义理解能力，无法检测此类隐蔽滥用。

Method: 首先展示如何滥用TensorFlow API的隐藏功能（如文件读写、网络收发）及其持久化API发起攻击；然后提出使用大型语言模型（LLM）识别潜在可滥用的隐藏API功能，并构建检测此类滥用的扫描器。

Result: 研究发现现有模型中心（如Hugging Face、TensorFlow Hub）的扫描工具无法检测某些隐蔽的API滥用攻击，因为这些工具仅基于语法层面的可疑功能识别，缺乏语义级理解。通过LLM方法能够有效识别潜在的可滥用API功能。

Conclusion: AI模型供应链面临严重安全威胁，攻击者可滥用深度学习框架的隐藏API功能发起多种攻击。需要开发基于语义理解的检测工具，而LLM为识别和检测此类API滥用提供了有效途径，对保障AI模型安全至关重要。

Abstract: According to Gartner, more than 70% of organizations will have integrated AI models into their workflows by the end of 2025. In order to reduce cost and foster innovation, it is often the case that pre-trained models are fetched from model hubs like Hugging Face or TensorFlow Hub. However, this introduces a security risk where attackers can inject malicious code into the models they upload to these hubs, leading to various kinds of attacks including remote code execution (RCE), sensitive data exfiltration, and system file modification when these models are loaded or executed (predict function). Since AI models play a critical role in digital transformation, this would drastically increase the number of software supply chain attacks. While there are several efforts at detecting malware when deserializing pickle based saved models (hiding malware in model parameters), the risk of abusing DL APIs (e.g. TensorFlow APIs) is understudied. Specifically, we show how one can abuse hidden functionalities of TensorFlow APIs such as file read/write and network send/receive along with their persistence APIs to launch attacks. It is concerning to note that existing scanners in model hubs like Hugging Face and TensorFlow Hub are unable to detect some of the stealthy abuse of such APIs. This is because scanning tools only have a syntactically identified set of suspicious functionality that is being analysed. They often do not have a semantic-level understanding of the functionality utilized. After demonstrating the possible attacks, we show how one may identify potentially abusable hidden API functionalities using LLMs and build scanners to detect such abuses.

</details>


### [15] [Constitutional Classifiers++: Efficient Production-Grade Defenses against Universal Jailbreaks](https://arxiv.org/abs/2601.04603)
*Hoagy Cunningham,Jerry Wei,Zihan Wang,Andrew Persic,Alwin Peng,Jordan Abderrachid,Raj Agarwal,Bobby Chen,Austin Cohen,Andy Dau,Alek Dimitriev,Rob Gilson,Logan Howard,Yijin Hua,Jared Kaplan,Jan Leike,Mu Lin,Christopher Liu,Vladimir Mikulik,Rohit Mittapalli,Clare O'Hara,Jin Pan,Nikhil Saxena,Alex Silverstein,Yue Song,Xunjie Yu,Giulio Zhou,Ethan Perez,Mrinank Sharma*

Main category: cs.CR

TL;DR: 增强版宪法分类器：通过上下文评估、两阶段级联和高效线性探针，实现生产级越狱防护，计算成本降低40倍，拒绝率仅0.05%


<details>
  <summary>Details</summary>
Motivation: 解决现有防御系统在计算成本高、拒绝率高以及孤立评估输出等方面的不足，开发实用高效的大型语言模型安全防护系统

Method: 1. 开发交换分类器，在完整对话上下文中评估模型响应；2. 实现两阶段级联分类器，轻量级分类器筛选所有流量，可疑交换才升级到更昂贵的分类器；3. 训练高效线性探针分类器并与外部分类器集成

Result: 计算成本相比基线交换分类器降低40倍，生产流量拒绝率仅0.05%，通过1700多小时的红队测试，成功抵御通用越狱攻击

Conclusion: 宪法分类器已成为实用高效的大型语言模型安全防护系统，为生产环境提供了可行的越狱防御解决方案

Abstract: We introduce enhanced Constitutional Classifiers that deliver production-grade jailbreak robustness with dramatically reduced computational costs and refusal rates compared to previous-generation defenses. Our system combines several key insights. First, we develop exchange classifiers that evaluate model responses in their full conversational context, which addresses vulnerabilities in last-generation systems that examine outputs in isolation. Second, we implement a two-stage classifier cascade where lightweight classifiers screen all traffic and escalate only suspicious exchanges to more expensive classifiers. Third, we train efficient linear probe classifiers and ensemble them with external classifiers to simultaneously improve robustness and reduce computational costs. Together, these techniques yield a production-grade system achieving a 40x computational cost reduction compared to our baseline exchange classifier, while maintaining a 0.05% refusal rate on production traffic. Through extensive red-teaming comprising over 1,700 hours, we demonstrate strong protection against universal jailbreaks -- no attack on this system successfully elicited responses to all eight target queries comparable in detail to an undefended model. Our work establishes Constitutional Classifiers as practical and efficient safeguards for large language models.

</details>


### [16] [DP-MGTD: Privacy-Preserving Machine-Generated Text Detection via Adaptive Differentially Private Entity Sanitization](https://arxiv.org/abs/2601.04641)
*Lionel Z. Wang,Yusheng Zhao,Jiabin Luo,Xinfeng Li,Lixu Wang,Yinan Peng,Haoyang Li,XiaoFeng Wang,Wei Dong*

Main category: cs.CR

TL;DR: DP-MGTD框架通过自适应差分隐私实体净化算法，在保护用户隐私的同时实现机器生成文本检测，意外发现DP噪声能增强人机文本区分度。


<details>
  <summary>Details</summary>
Motivation: 机器生成文本检测系统需要处理敏感用户数据，导致作者身份验证与隐私保护之间存在根本冲突。标准匿名化技术会破坏语言流畅性，而严格的差分隐私机制通常会降低检测所需的统计信号。

Method: 提出DP-MGTD框架，采用自适应差分隐私实体净化算法，使用两阶段机制：进行噪声频率估计并动态校准隐私预算，分别对数值和文本实体应用拉普拉斯和指数机制。

Result: 在MGTBench-2.0数据集上的实验表明，该方法实现了接近完美的检测准确率，显著优于非隐私基线，同时满足严格的隐私保证。发现了一个反直觉现象：DP噪声的应用通过暴露对扰动的不同敏感性模式，增强了人机文本的可区分性。

Conclusion: DP-MGTD框架成功解决了机器生成文本检测中的隐私保护与检测准确性之间的冲突，通过差分隐私机制不仅保护了隐私，反而增强了检测性能。

Abstract: The deployment of Machine-Generated Text (MGT) detection systems necessitates processing sensitive user data, creating a fundamental conflict between authorship verification and privacy preservation. Standard anonymization techniques often disrupt linguistic fluency, while rigorous Differential Privacy (DP) mechanisms typically degrade the statistical signals required for accurate detection. To resolve this dilemma, we propose \textbf{DP-MGTD}, a framework incorporating an Adaptive Differentially Private Entity Sanitization algorithm. Our approach utilizes a two-stage mechanism that performs noisy frequency estimation and dynamically calibrates privacy budgets, applying Laplace and Exponential mechanisms to numerical and textual entities respectively. Crucially, we identify a counter-intuitive phenomenon where the application of DP noise amplifies the distinguishability between human and machine text by exposing distinct sensitivity patterns to perturbation. Extensive experiments on the MGTBench-2.0 dataset show that our method achieves near-perfect detection accuracy, significantly outperforming non-private baselines while satisfying strict privacy guarantees.

</details>


### [17] [Unified Framework for Qualifying Security Boundary of PUFs Against Machine Learning Attacks](https://arxiv.org/abs/2601.04697)
*Hongming Fei,Zilong Hu,Prosanta Gope,Biplab Sikdar*

Main category: cs.CR

TL;DR: 本文提出了一种评估物理不可克隆函数（PUF）安全性的统一形式化框架，通过提供与具体攻击模型无关的安全下界，系统量化PUF对建模攻击的抵抗能力。


<details>
  <summary>Details</summary>
Motivation: 现有的延迟型PUF容易受到机器学习攻击，但目前缺乏有效的安全评估指标。现有评估方法依赖经验性建模实验，缺乏理论保证且对机器学习技术进步高度敏感，存在PUF设计与安全评估之间的根本性差距。

Method: 提出一个形式化、统一的框架，通过数学刻画攻击者仅基于观察到的挑战-响应对（CRPs）预测未见挑战响应的优势，将问题表述为候选PUF空间上的条件概率估计问题，从而提供与具体攻击算法无关的安全下界。

Result: 对先前被"攻破"的PUF（如Arbiter PUF、XOR PUF、Feed-Forward PUF）进行了首次形式化比较，评估了当前"安全"的CT PUF并确定了其安全边界。证明该框架能系统量化PUF抗性、捕捉细微安全差异。

Conclusion: 所提出的方法为PUF安全性提供了可操作、理论上有依据的安全保证，能够支持PUF在实际部署中的安全评估，填补了PUF设计与安全认证之间的根本性差距。

Abstract: Physical Unclonable Functions (PUFs) serve as lightweight, hardware-intrinsic entropy sources widely deployed in IoT security applications. However, delay-based PUFs are vulnerable to Machine Learning Attacks (MLAs), undermining their assumed unclonability. There are no valid metrics for evaluating PUF MLA resistance, but empirical modelling experiments, which lack theoretical guarantees and are highly sensitive to advances in machine learning techniques. To address the fundamental gap between PUF designs and security qualifications, this work proposes a novel, formal, and unified framework for evaluating PUF security against modelling attacks by providing security lower bounds, independent of specific attack models or learning algorithms. We mathematically characterise the adversary's advantage in predicting responses to unseen challenges based solely on observed challenge-response pairs (CRPs), formulating the problem as a conditional probability estimation over the space of candidate PUFs. We present our analysis on previous "broken" PUFs, e.g., Arbiter PUFs, XOR PUFs, Feed-Forward PUFs, and for the first time compare their MLA resistance in a formal way. In addition, we evaluate the currently "secure" CT PUF, and show its security boundary. We demonstrate that the proposed approach systematically quantifies PUF resilience, captures subtle security differences, and provides actionable, theoretically grounded security guarantees for the practical deployment of PUFs.

</details>


### [18] [Quantum Secure Biometric Authentication in Decentralised Systems](https://arxiv.org/abs/2601.04852)
*Tooba Qasim,Vasilios A. Siris,Izak Oosthuizen,Muttukrishnan Rajarajan,Sujit Biswas*

Main category: cs.CR

TL;DR: 提出一种基于增强型量子密钥分发系统的量子安全通信协议，用于去中心化生物识别系统，解决量子攻击威胁下的身份认证问题


<details>
  <summary>Details</summary>
Motivation: 智能城市中的生物识别认证系统面临集中式架构的隐私和可扩展性问题，而去中心化身份框架虽然提供更好的数据主权，但存在分布式设备间相互信任的安全问题。现有基于经典公钥基础设施的认证方案易受量子攻击威胁

Method: 提出量子安全通信协议，基于增强型量子密钥分发系统，在QKD的经典层和量子层都实现量子弹性认证：使用后量子密码学保护经典信道，使用认证量子比特验证量子信道完整性。建立信任后，QKD生成对称密钥用于加密传输中的生物识别数据

Result: Qiskit模拟显示密钥生成速率为15比特/秒，效率达到89%。这种分层量子弹性方法为下一代智能城市基础设施提供了可扩展、鲁棒的身份认证

Conclusion: 该协议通过量子安全通信解决了去中心化生物识别系统中的量子攻击威胁，为智能城市基础设施提供了可扩展、鲁棒的身份认证方案

Abstract: Biometric authentication has become integral to digital identity systems, particularly in smart cities where it en-ables secure access to services across governance, trans-portation, and public infrastructure. Centralised archi-tectures, though widely used, pose privacy and scalabil-ity challenges due to the aggregation of sensitive biomet-ric data. Decentralised identity frameworks offer better data sovereignty and eliminate single points of failure but introduce new security concerns, particularly around mu-tual trust among distributed devices. In such environments, biometric sensors and verification agents must authenticate one another before sharing sensitive biometric data. Ex-isting authentication schemes rely on classical public key infrastructure, which is increasingly susceptible to quan-tum attacks. This work addresses this gap by propos-ing a quantum-secure communication protocol for decen-tralised biometric systems, built upon an enhanced Quan-tum Key Distribution (QKD) system. The protocol incorpo-rates quantum-resilient authentication at both the classical and quantum layers of QKD: post-quantum cryptography (PQC) is used to secure the classical channel, while authen-tication qubits verify the integrity of the quantum channel. Once trust is established, QKD generates symmetric keys for encrypting biometric data in transit. Qiskit-based sim-ulations show a key generation rate of 15 bits/sec and 89% efficiency. This layered, quantum-resilient approach offers scalable, robust authentication for next-generation smart city infrastructures.

</details>


### [19] [Decentralized Privacy-Preserving Federal Learning of Computer Vision Models on Edge Devices](https://arxiv.org/abs/2601.04912)
*Damian Harenčák,Lukáš Gajdošech,Martin Madaras*

Main category: cs.CR

TL;DR: 该论文分析了联邦学习中客户端数据隐私保护方法，包括同态加密、梯度压缩、梯度噪声等技术，评估了这些方法对卷积神经网络准确性的影响，并在边缘设备上进行了概念验证。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然通过不共享原始数据来保护隐私，但研究表明仅通过模型参数信息仍可能重建私有数据。现有隐私保护方法主要关注服务器端风险，假设其他客户端不会恶意行为，但实际中其他客户端也可能构成威胁。

Method: 分析了多种提高客户端数据隐私保护的方法：同态加密、梯度压缩、梯度噪声，并讨论了改进的联邦学习系统如分割学习、群体学习或完全加密模型。评估了梯度压缩和梯度噪声对卷积神经网络分类准确性的负面影响，展示了分割网络中数据重建的难度，并在NVIDIA Jetson TX2边缘设备模块上实现了概念验证。

Result: 研究发现梯度压缩和梯度噪声会对卷积神经网络的分类准确性产生负面影响。分割网络中的数据重建难度较大。成功在边缘设备上实现了联邦学习过程的概念验证。

Conclusion: 需要综合考虑多种隐私保护方法来应对联邦学习中的隐私风险，包括服务器端和其他客户端的威胁。虽然某些方法会影响模型准确性，但通过适当的技术组合可以在隐私保护和模型性能之间取得平衡。

Abstract: Collaborative training of a machine learning model comes with a risk of sharing sensitive or private data. Federated learning offers a way of collectively training a single global model without the need to share client data, by sharing only the updated parameters from each client's local model. A central server is then used to aggregate parameters from all clients and redistribute the aggregated model back to the clients. Recent findings have shown that even in this scenario, private data can be reconstructed only using information about model parameters. Current efforts to mitigate this are mainly focused on reducing privacy risks on the server side, assuming that other clients will not act maliciously. In this work, we analyzed various methods for improving the privacy of client data concerning both the server and other clients for neural networks. Some of these methods include homomorphic encryption, gradient compression, gradient noising, and discussion on possible usage of modified federated learning systems such as split learning, swarm learning or fully encrypted models. We have analyzed the negative effects of gradient compression and gradient noising on the accuracy of convolutional neural networks used for classification. We have shown the difficulty of data reconstruction in the case of segmentation networks. We have also implemented a proof of concept on the NVIDIA Jetson TX2 module used in edge devices and simulated a federated learning process.

</details>


### [20] [CurricuLLM: Designing Personalized and Workforce-Aligned Cybersecurity Curricula Using Fine-Tuned LLMs](https://arxiv.org/abs/2601.04940)
*Arthur Nijdam,Harri Kähkönen,Valtteri Niemi,Paul Stankovski Wagner,Sara Ramezanian*

Main category: cs.CR

TL;DR: 提出CurricuLLM框架，利用LLM自动设计和分析网络安全课程，解决传统课程设计成本高、与行业需求脱节的问题。


<details>
  <summary>Details</summary>
Motivation: 网络安全领域快速发展，但传统网络安全教育课程设计成本高、劳动密集，且毕业生技能与行业需求存在脱节，需要更高效的课程设计方法。

Method: 采用两阶段方法：PreprocessLM标准化输入数据，ClassifyLM将课程内容分类到9个网络安全知识领域。系统评估多种NLP架构和微调策略，最终选择微调的BERT模型作为ClassifyLM。

Result: 首次通过人类专家验证方法，证明CurricuLLM能有效替代劳动密集的课程分析。课程内容分类后可整合基于角色的权重，实现教育项目与特定职位、劳动力类别或市场需求的对接。

Conclusion: CurricuLLM为个性化、与劳动力需求对齐的网络安全课程奠定了基础，帮助学生应对网络安全领域不断变化的需求。

Abstract: The cybersecurity landscape is constantly evolving, driven by increased digitalization and new cybersecurity threats. Cybersecurity programs often fail to equip graduates with skills demanded by the workforce, particularly concerning recent developments in cybersecurity, as curriculum design is costly and labor-intensive. To address this misalignment, we present a novel Large Language Model (LLM)-based framework for automated design and analysis of cybersecurity curricula, called CurricuLLM. Our approach provides three key contributions: (1) automation of personalized curriculum design, (2) a data-driven pipeline aligned with industry demands, and (3) a comprehensive methodology for leveraging fine-tuned LLMs in curriculum development.
  CurricuLLM utilizes a two-tier approach consisting of PreprocessLM, which standardizes input data, and ClassifyLM, which assigns course content to nine Knowledge Areas in cybersecurity. We systematically evaluated multiple Natural Language Processing (NLP) architectures and fine-tuning strategies, ultimately selecting the Bidirectional Encoder Representations from Transformers (BERT) model as ClassifyLM, fine-tuned on foundational cybersecurity concepts and workforce competencies.
  We are the first to validate our method with human experts who analyzed real-world cybersecurity curricula and frameworks, motivating that CurricuLLM is an efficient solution to replace labor-intensive curriculum analysis. Moreover, once course content has been classified, it can be integrated with established cybersecurity role-based weights, enabling alignment of the educational program with specific job roles, workforce categories, or general market needs. This lays the foundation for personalized, workforce-aligned cybersecurity curricula that prepare students for the evolving demands in cybersecurity.

</details>


### [21] [Knowledge-to-Data: LLM-Driven Synthesis of Structured Network Traffic for Testbed-Free IDS Evaluation](https://arxiv.org/abs/2601.05022)
*Konstantinos E. Kampourakis,Vyron Kampourakis,Efstratios Chatzoglou,Georgios Kambourakis,Stefanos Gritzalis*

Main category: cs.CR

TL;DR: LLMs can generate realistic synthetic network traffic datasets for IDS research without needing real data or testbeds, achieving high classification performance on real samples.


<details>
  <summary>Details</summary>
Motivation: 缺乏真实、大规模、标注良好的网络安全数据集阻碍了入侵检测系统的训练和评估，主要由于隐私限制、数据敏感性和构建测试环境的成本。

Method: 提出一种结合协议文档、攻击语义和统计规则的方法来约束LLMs生成结构化网络流量数据集，无需微调或原始样本。使用AWID3基准进行案例研究，通过多级验证框架评估生成数据质量。

Result: 在明确约束下，LLM生成的数据集能近似真实网络流量的统计和结构特征，梯度提升分类器在真实样本上F1分数可达0.956。

Conclusion: 受约束的LLM驱动生成可为IDS实验提供按需、隐私保护的数据集替代方案，克服传统物理流量收集和手动标注的瓶颈。

Abstract: Realistic, large-scale, and well-labeled cybersecurity datasets are essential for training and evaluating Intrusion Detection Systems (IDS). However, they remain difficult to obtain due to privacy constraints, data sensitivity, and the cost of building controlled collection environments such as testbeds and cyber ranges. This paper investigates whether Large Language Models (LLMs) can operate as controlled knowledge-to-data engines for generating structured synthetic network traffic datasets suitable for IDS research. We propose a methodology that combines protocol documentation, attack semantics, and explicit statistical rules to condition LLMs without fine-tuning or access to raw samples. Using the AWID3 IEEE~802.11 benchmark as a demanding case study, we generate labeled datasets with four state-of-the-art LLMs and assess fidelity through a multi-level validation framework including global similarity metrics, per-feature distribution testing, structural comparison, and cross-domain classification. Results show that, under explicit constraints, LLM-generated datasets can closely approximate the statistical and structural characteristics of real network traffic, enabling gradient-boosting classifiers to achieve F1-scores up to 0.956 when evaluated on real samples. Overall, the findings suggest that constrained LLM-driven generation can facilitate on-demand IDS experimentation, providing a testbed-free, privacy-preserving alternative that overcomes the traditional bottlenecks of physical traffic collection and manual labeling.

</details>


### [22] [Supporting Secured Integration of Microarchitectural Defenses](https://arxiv.org/abs/2601.05057)
*Kartik Ramkrishnan,Stephen McCamant,Antonia Zhai,Pen-Chung Yew*

Main category: cs.CR

TL;DR: 提出MDAV问题（微架构防御假设违反）及两阶段检测方法：Maestro建模框架进行形式验证，GEM5模拟器进行实际攻击测试


<details>
  <summary>Details</summary>
Motivation: 微架构级攻击众多，相应防御措施也很多，但防御措施之间的集成可能导致安全漏洞，即一个防御措施可能破坏另一个防御措施的安全假设

Method: 提出两阶段方法：1) 使用Maestro事件建模框架进行形式化验证和模型检查；2) 使用GEM5模拟器进行实际攻击测试

Result: Maestro发现8个MDAV，代码表达更紧凑（~15倍），支持语义组合，消除性能下降（>100倍）。GEM5测试显示修复后的集成防御能抵抗隐蔽信道攻击

Conclusion: 微架构防御集成存在MDAV问题，提出的两阶段方法能有效检测和修复此类漏洞，确保集成防御的安全性

Abstract: There has been a plethora of microarchitectural-level attacks leading to many proposed countermeasures. This has created an unexpected and unaddressed security issue where naive integration of those defenses can potentially lead to security vulnerabilities. This occurs when one defense changes an aspect of a microarchitecture that is crucial for the security of another defense. We refer to this problem as a microarchitectural defense assumption violation} (MDAV).
  We propose a two-step methodology to screen for potential MDAVs in the early-stage of integration. The first step is to design and integrate a composed model, guided by bounded model checking of security properties. The second step is to implement the model concretely on a simulator and to evaluate with simulated attacks. As a contribution supporting the first step, we propose an event-based modeling framework, called Maestro, for testing and evaluating microarchitectural models with integrated defenses. In our evaluation, Maestro reveals MDAVs (8), supports compact expression (~15x Alloy LoC ratio), enables semantic composability and eliminates performance degradations (>100x).
  As a contribution supporting the second step, we use an event-based simulator (GEM5) for investigating integrated microarchitectural defenses. We show that a covert channel attack is possible on a naively integrated implementation of some state-of-the-art defenses, and a repaired implementation using our integration methodology is resilient to the attack.

</details>


### [23] [$PC^2$: Politically Controversial Content Generation via Jailbreaking Attacks on GPT-based Text-to-Image Models](https://arxiv.org/abs/2601.05150)
*Wonwoo Choi,Minjae Seo,Minkyoo Song,Hwanjo Heo,Seungwon Shin,Myoungsung You*

Main category: cs.CR

TL;DR: PC²是一个针对文本到图像模型的黑盒政治越狱框架，通过身份保持描述映射和地缘政治远端翻译绕过安全过滤器，成功率达到86%


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型的快速发展带来了安全风险，特别是政治有害内容的生成。当前T2I安全过滤器对政治动机的对抗性提示的鲁棒性尚未得到充分研究，需要探索如何防止政治敏感内容的生成。

Method: PC²采用两种策略：1）身份保持描述映射 - 将敏感关键词模糊化为中性描述；2）地缘政治远端翻译 - 将这些描述映射到碎片化的低敏感性语言中。这种方法防止过滤器在提示中构建政治实体之间的毒性关系。

Result: 在包含36位公众人物的240个政治敏感提示基准测试中，所有原始提示都被商业T2I模型（特别是GPT系列）阻止，但PC²实现了高达86%的攻击成功率。

Conclusion: PC²揭示了T2I安全过滤器在评估政治敏感性时存在基于语言上下文的漏洞，表明当前的安全机制对政治越狱攻击存在脆弱性，需要更强大的防御措施。

Abstract: The rapid evolution of text-to-image (T2I) models has enabled high-fidelity visual synthesis on a global scale. However, these advancements have introduced significant security risks, particularly regarding the generation of harmful content. Politically harmful content, such as fabricated depictions of public figures, poses severe threats when weaponized for fake news or propaganda. Despite its criticality, the robustness of current T2I safety filters against such politically motivated adversarial prompting remains underexplored. In response, we propose $PC^2$, the first black-box political jailbreaking framework for T2I models. It exploits a novel vulnerability where safety filters evaluate political sensitivity based on linguistic context. $PC^2$ operates through: (1) Identity-Preserving Descriptive Mapping to obfuscate sensitive keywords into neutral descriptions, and (2) Geopolitically Distal Translation to map these descriptions into fragmented, low-sensitivity languages. This strategy prevents filters from constructing toxic relationships between political entities within prompts, effectively bypassing detection. We construct a benchmark of 240 politically sensitive prompts involving 36 public figures. Evaluation on commercial T2I models, specifically GPT-series, shows that while all original prompts are blocked, $PC^2$ achieves attack success rates of up to 86%.

</details>


### [24] [The Adverse Effects of Omitting Records in Differential Privacy: How Sampling and Suppression Degrade the Privacy-Utility Tradeoff (Long Version)](https://arxiv.org/abs/2601.05180)
*Àlex Miranda-Pascual,Javier Parra-Arnau,Thorsten Strufe*

Main category: cs.CR

TL;DR: 采样在差分隐私中通常被认为能提升效用，但本文证明采样作为预处理在所有经典DP机制下都会导致效用损失，抑制策略也无法改善隐私-效用权衡，均匀采样反而是相对较好的方法。


<details>
  <summary>Details</summary>
Motivation: 挑战差分隐私实践中关于采样作为预处理能改善效用-隐私权衡的常见假设。虽然采样已知能增强隐私保护，但通常假设它能通过减少噪声来提升效用，本文旨在验证这一假设的正确性。

Method: 1) 分析采样作为预处理在拉普拉斯、高斯、指数和报告噪声最大等经典DP机制下的效用影响；2) 将抑制作为广义的记录选择/省略方法进行理论分析；3) 推导无界近似DP下任意抑制策略的隐私边界；4) 测试不同抑制策略对隐私-效用权衡的影响。

Result: 1) 在相同隐私水平下，采样作为预处理在所有测试的DP机制中都导致效用损失（因省略记录）；2) 测试的抑制策略也无法改善隐私-效用权衡；3) 均匀采样反而是相对较好的抑制方法，尽管仍有负面影响；4) 采样在聚类等应用中也存在类似问题。

Conclusion: 采样作为预处理并不能改善差分隐私机制的效用-隐私权衡，反而会降低效用。均匀采样是相对较好的方法但仍有负面影响。这些发现对DP实践中的常见预处理假设提出了质疑。

Abstract: Sampling is renowned for its privacy amplification in differential privacy (DP), and is often assumed to improve the utility of a DP mechanism by allowing a noise reduction. In this paper, we further show that this last assumption is flawed: When measuring utility at equal privacy levels, sampling as preprocessing consistently yields penalties due to utility loss from omitting records over all canonical DP mechanisms -- Laplace, Gaussian, exponential, and report noisy max -- as well as recent applications of sampling, such as clustering.
  Extending this analysis, we investigate suppression as a generalized method of choosing, or omitting, records. Developing a theoretical analysis of this technique, we derive privacy bounds for arbitrary suppression strategies under unbounded approximate DP. We find that our tested suppression strategy also fails to improve the privacy-utility tradeoff. Surprisingly, uniform sampling emerges as one of the best suppression methods -- despite its still degrading effect. Our results call into question common preprocessing assumptions in DP practice.

</details>
