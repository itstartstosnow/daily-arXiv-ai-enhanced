<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 29]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Lightweight MobileNetV1+GRU for ECG Biometric Authentication: Federated and Adversarial Evaluation](https://arxiv.org/abs/2509.20382)
*Dilli Hang Rai,Sabin Kafley*

Main category: cs.CR

TL;DR: 本文提出了一种轻量级深度学习模型（MobileNetV1+GRU）用于基于ECG的身份认证，通过注入20dB高斯噪声和自定义预处理，在模拟可穿戴设备条件下实现了高精度认证，但在对抗性攻击下性能显著下降。


<details>
  <summary>Details</summary>
Motivation: ECG生物识别技术提供了一种独特、安全的认证方法，但在可穿戴设备上的部署面临实时处理、隐私保护和防欺骗漏洞等挑战。

Method: 使用MobileNetV1+GRU轻量级深度学习模型，结合20dB高斯噪声注入和自定义预处理，在ECGID、MIT-BIH、CYBHi和PTB数据集上模拟可穿戴条件和边缘部署。

Result: 在四个数据集上分别达到99.34%、99.31%、91.74%和98.49%的准确率，F1分数分别为0.9869、0.9923、0.9125和0.9771，但在FGSM对抗性攻击下准确率从96.82%降至最低0.80%。

Conclusion: 论文强调了联邦学习、对抗性测试以及需要多样化的可穿戴生理数据集，以确保生物识别技术的安全性和可扩展性。

Abstract: ECG biometrics offer a unique, secure authentication method, yet their
deployment on wearable devices faces real-time processing, privacy, and
spoofing vulnerability challenges. This paper proposes a lightweight deep
learning model (MobileNetV1+GRU) for ECG-based authentication, injection of
20dB Gaussian noise & custom preprocessing. We simulate wearable conditions and
edge deployment using the ECGID, MIT-BIH, CYBHi, and PTB datasets, achieving
accuracies of 99.34%, 99.31%, 91.74%, and 98.49%, F1-scores of 0.9869, 0.9923,
0.9125, and 0.9771, Precision of 0.9866, 0.9924, 0.9180 and 0.9845, Recall of
0.9878, 0.9923, 0.9129, and 0.9756, equal error rates (EER) of 0.0009, 0.00013,
0.0091, and 0.0009, and ROC-AUC values of 0.9999, 0.9999, 0.9985, and 0.9998,
while under FGSM adversarial attacks, accuracy drops from 96.82% to as low as
0.80%. This paper highlights federated learning, adversarial testing, and the
need for diverse wearable physiological datasets to ensure secure and scalable
biometrics.

</details>


### [2] [MARS: A Malignity-Aware Backdoor Defense in Federated Learning](https://arxiv.org/abs/2509.20383)
*Wei Wan,Yuxuan Ning,Zhicong Huang,Cheng Hong,Shengshan Hu,Ziqi Zhou,Yechao Zhang,Tianqing Zhu,Wanlei Zhou,Leo Yu Zhang*

Main category: cs.CR

TL;DR: 本文提出了一种名为MARS的联邦学习后门防御方法，通过利用后门能量（BE）来指示每个神经元的恶意程度，并使用基于Wasserstein距离的聚类来有效识别后门模型。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习防御方法采用经验统计度量，与后门攻击松散耦合，导致对最新攻击（如3DFed）无效。本文旨在解决这一缺陷。

Method: 提出MARS防御方法：1）利用后门能量（BE）指示神经元恶意程度；2）提取最显著的BE值形成集中后门能量（CBE）；3）引入基于Wasserstein距离的聚类方法识别后门模型。

Result: 大量实验表明，MARS能够有效防御最先进的后门攻击，并显著优于现有防御方法。

Conclusion: MARS通过更紧密地耦合后门攻击特征，提供了一种有效的联邦学习后门防御解决方案，解决了现有防御方法的局限性。

Abstract: Federated Learning (FL) is a distributed paradigm aimed at protecting
participant data privacy by exchanging model parameters to achieve high-quality
model training. However, this distributed nature also makes FL highly
vulnerable to backdoor attacks. Notably, the recently proposed state-of-the-art
(SOTA) attack, 3DFed (SP2023), uses an indicator mechanism to determine whether
the backdoor models have been accepted by the defender and adaptively optimizes
backdoor models, rendering existing defenses ineffective. In this paper, we
first reveal that the failure of existing defenses lies in the employment of
empirical statistical measures that are loosely coupled with backdoor attacks.
Motivated by this, we propose a Malignity-Aware backdooR defenSe (MARS) that
leverages backdoor energy (BE) to indicate the malicious extent of each neuron.
To amplify malignity, we further extract the most prominent BE values from each
model to form a concentrated backdoor energy (CBE). Finally, a novel
Wasserstein distance-based clustering method is introduced to effectively
identify backdoor models. Extensive experiments demonstrate that MARS can
defend against SOTA backdoor attacks and significantly outperforms existing
defenses.

</details>


### [3] [R1-Fuzz: Specializing Language Models for Textual Fuzzing via Reinforcement Learning](https://arxiv.org/abs/2509.20384)
*Jiayi Lin,Liangcai Su,Junzhe Li,Chenxiong Qian*

Main category: cs.CR

TL;DR: R1-Fuzz是一个基于强化学习的框架，专门用于复杂文本输入模糊测试，通过训练小型语言模型在真实代码库中发现漏洞，性能优于大型模型。


<details>
  <summary>Details</summary>
Motivation: 传统模糊测试在处理编译器、解释器等复杂目标时效果有限，这些目标需要满足复杂的语法和语义约束。虽然语言模型有潜力，但实际应用受限，主要挑战包括对真实代码库深层逻辑探索不足以及大模型成本过高。

Method: R1-Fuzz采用强化学习专门化成本效益高的语言模型，包含两个关键设计：基于覆盖切片的问题构建和基于距离的奖励计算。通过RL后训练模型，设计紧密集成LM的模糊测试工作流。

Result: 在多样化真实目标上的评估显示，R1-Fuzz-7B小模型性能可媲美甚至超越更大模型，覆盖率达到最先进模糊测试工具的175%，发现了29个先前未知的漏洞。

Conclusion: R1-Fuzz证明了通过强化学习专门化小型语言模型在复杂模糊测试任务中的实用性，为高效漏洞发现提供了可行方案。

Abstract: Fuzzing is effective for vulnerability discovery but struggles with complex
targets such as compilers, interpreters, and database engines, which accept
textual input that must satisfy intricate syntactic and semantic constraints.
Although language models (LMs) have attracted interest for this task due to
their vast latent knowledge and reasoning potential, their practical adoption
has been limited. The major challenges stem from insufficient exploration of
deep program logic among real-world codebases, and the high cost of leveraging
larger models. To overcome these challenges, we propose R1-Fuzz, the first
framework that leverages reinforcement learning (RL) to specialize
cost-efficient LMs and integrate them for complex textual fuzzing input
generation. R1-Fuzz introduces two key designs: coverage-slicing-based question
construction and a distance-based reward calculation. Through RL-based
post-training of a model with our constructed dataset, R1-Fuzz designs a
fuzzing workflow that tightly integrates LMs to reason deep program semantics
during fuzzing. Evaluations on diverse real-world targets show that our design
enables a small model, named R1-Fuzz-7B, to rival or even outperform much
larger models in real-world fuzzing. Notably, R1-Fuzz achieves up to 75\%
higher coverage than state-of-the-art fuzzers and discovers 29 previously
unknown vulnerabilities, demonstrating its practicality.

</details>


### [4] [Can You Trust Your Copilot? A Privacy Scorecard for AI Coding Assistants](https://arxiv.org/abs/2509.20388)
*Amir AL-Maamari*

Main category: cs.CR

TL;DR: 本文介绍了一种专家验证的隐私评分卡，用于评估AI编程助手的数据隐私保护水平，揭示了不同工具间存在20分的隐私保护差距，并发现行业普遍存在模型训练选择退出同意和未能主动过滤用户提示中的秘密信息等弱点。


<details>
  <summary>Details</summary>
Motivation: 随着AI编程助手在开发者工作流程中的快速集成，这些工具不明确的数据处理实践引发了严重的隐私和信任问题，开发者将专有代码托付给这些服务时面临安全和合规风险。

Method: 通过详细分析四种文档类型（从法律政策到外部审计），基于14个加权标准对五个领先的AI编程助手进行评分，评分标准和权重由法律专家和数据保护官员共同制定和优化。

Result: 研究结果显示隐私保护存在明显的等级差异，最高分和最低分工具之间有20分的差距，发现了行业普遍弱点，包括普遍使用选择退出同意进行模型训练，以及几乎普遍未能主动过滤用户提示中的秘密信息。

Conclusion: 该评分卡为开发者和组织提供了可操作的指导，支持基于证据的工具选择，为AI行业建立了透明度新基准，并倡导向更以用户为中心的隐私标准转变。

Abstract: The rapid integration of AI-powered coding assistants into developer
workflows has raised significant privacy and trust concerns. As developers
entrust proprietary code to services like OpenAI's GPT, Google's Gemini, and
GitHub Copilot, the unclear data handling practices of these tools create
security and compliance risks. This paper addresses this challenge by
introducing and applying a novel, expert-validated privacy scorecard. The
methodology involves a detailed analysis of four document types; from legal
policies to external audits; to score five leading assistants against 14
weighted criteria. A legal expert and a data protection officer refined these
criteria and their weighting. The results reveal a distinct hierarchy of
privacy protections, with a 20-point gap between the highest- and lowest-ranked
tools. The analysis uncovers common industry weaknesses, including the
pervasive use of opt-out consent for model training and a near-universal
failure to filter secrets from user prompts proactively. The resulting
scorecard provides actionable guidance for developers and organizations,
enabling evidence-based tool selection. This work establishes a new benchmark
for transparency and advocates for a shift towards more user-centric privacy
standards in the AI industry.

</details>


### [5] [A Comparative Analysis of Ensemble-Based Machine Learning Approaches with Explainable AI for Multi-Class Intrusion Detection in Drone Networks](https://arxiv.org/abs/2509.20391)
*Md. Alamgir Hossain,Waqas Ishtiaq,Md. Samiul Islam*

Main category: cs.CR

TL;DR: 本研究开发了一个针对无人机网络的鲁棒且可解释的入侵检测框架，通过比较多种集成机器学习模型，随机森林表现最佳，达到近乎完美的检测精度，并结合可解释AI方法增强模型透明度。


<details>
  <summary>Details</summary>
Motivation: 随着无人机在民用、商业和国防领域的广泛应用，其网络安全问题日益突出，特别是针对无人机通信协议的网络入侵风险增加，需要开发有效的入侵检测方法。

Method: 采用集成机器学习模型（随机森林、Extra Trees、AdaBoost、CatBoost和XGBoost）进行多类分类，结合数据预处理和可解释AI方法（SHAP和LIME）来增强模型透明度。

Result: 随机森林模型表现最佳，宏F1分数达到0.9998，ROC AUC为1.0000，统计测试验证了模型的优越性。

Conclusion: 提出的方法不仅实现了近乎完美的检测精度，还确保了可解释性，非常适合实时和安全关键的无人机操作。

Abstract: The growing integration of drones into civilian, commercial, and defense
sectors introduces significant cybersecurity concerns, particularly with the
increased risk of network-based intrusions targeting drone communication
protocols. Detecting and classifying these intrusions is inherently challenging
due to the dynamic nature of drone traffic and the presence of multiple
sophisticated attack vectors such as spoofing, injection, replay, and
man-in-the-middle (MITM) attacks. This research aims to develop a robust and
interpretable intrusion detection framework tailored for drone networks, with a
focus on handling multi-class classification and model explainability. We
present a comparative analysis of ensemble-based machine learning models,
namely Random Forest, Extra Trees, AdaBoost, CatBoost, and XGBoost, trained on
a labeled dataset comprising benign traffic and nine distinct intrusion types.
Comprehensive data preprocessing was performed, including missing value
imputation, scaling, and categorical encoding, followed by model training and
extensive evaluation using metrics such as macro F1-score, ROC AUC, Matthews
Correlation Coefficient, and Log Loss. Random Forest achieved the highest
performance with a macro F1-score of 0.9998 and ROC AUC of 1.0000. To validate
the superiority of the models, statistical tests, including Friedmans test, the
Wilcoxon signed-rank test with Holm correction, and bootstrapped confidence
intervals, were applied. Furthermore, explainable AI methods, SHAP and LIME,
were integrated to interpret both global and local feature importance,
enhancing model transparency and decision trustworthiness. The proposed
approach not only delivers near-perfect accuracy but also ensures
interpretability, making it highly suitable for real-time and safety-critical
drone operations.

</details>


### [6] [Centralized vs. Decentralized Security for Space AI Systems? A New Look](https://arxiv.org/abs/2509.20395)
*Noam Schmitt,Marc Antoine Lacoste*

Main category: cs.CR

TL;DR: 本文研究了卫星星座中集中式与分散式安全管理之间的权衡，以平衡安全性和性能。


<details>
  <summary>Details</summary>
Motivation: 探索在卫星星座中如何平衡安全性和性能，特别是在集中式和分散式安全管理之间的选择问题。

Method: 提出了三种关键的AI架构用于自动化安全管理：(a)集中式架构，(b)分布式架构，(c)联邦式架构。

Result: 集中式架构在短期内是最佳选择，尽管存在跨空间通信延迟的挑战，但能提供快速训练；分散式架构在长期内是更好的替代方案，提供增强的可扩展性和安全性。

Conclusion: 在卫星星座的安全管理中，需要根据时间维度选择不同的架构：短期采用集中式架构，长期采用分散式架构以获得更好的可扩展性和安全性。

Abstract: This paper investigates the trade-off between centralized and decentralized
security management in constellations of satellites to balance security and
performance. We highlight three key AI architectures for automated security
management: (a) centralized, (b) distributed and (c) federated. The centralized
architecture is the best option short term, providing fast training, despite
the hard challenge of the communication latency overhead across space.
Decentralized architectures are better alternatives in the longer term,
providing enhanced scalability and security.

</details>


### [7] [Defending against Stegomalware in Deep Neural Networks with Permutation Symmetry](https://arxiv.org/abs/2509.20399)
*Birk Torpmann-Hagen,Michael A. Riegler,Pål Halvorsen,Dag Johansen*

Main category: cs.CR

TL;DR: 该论文提出了针对神经网络隐写恶意软件的有效防御方法，通过重排权重矩阵的列顺序来中和嵌入在神经网络检查点中的恶意软件，同时不影响网络精度。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络在生产系统和个人使用中的广泛应用，网络检查点经常被共享和分发。这带来了神经网络隐写恶意软件的安全威胁，即恶意软件以可忽略的精度代价嵌入到神经网络检查点中，但这一安全问题被深度学习和安全社区普遍忽视。

Method: 提出通过重排权重矩阵和偏置矩阵的列顺序（相当于卷积层的通道顺序）来有效中和最先进的神经网络隐写恶意软件。这种方法不会影响网络精度，且性能显著优于竞争方法。

Result: 实验证明该方法能够有效破坏最先进神经网络隐写方法嵌入的有效载荷，同时保持网络精度不变，性能远超其他竞争方法。

Conclusion: 论文讨论了可能的防御绕过方法、额外的防御措施，并呼吁继续研究机器学习系统的安全性问题。

Abstract: Deep neural networks are being utilized in a growing number of applications,
both in production systems and for personal use. Network checkpoints are as a
consequence often shared and distributed on various platforms to ease the
development process. This work considers the threat of neural network
stegomalware, where malware is embedded in neural network checkpoints at a
negligible cost to network accuracy. This constitutes a significant security
concern, but is nevertheless largely neglected by the deep learning
practitioners and security specialists alike. We propose the first effective
countermeasure to these attacks. In particular, we show that state-of-the-art
neural network stegomalware can be efficiently and effectively neutralized
through shuffling the column order of the weight- and bias-matrices, or
equivalently the channel-order of convolutional layers. We show that this
effectively corrupts payloads that have been embedded by state-of-the-art
methods in neural network steganography at no cost to network accuracy,
outperforming competing methods by a significant margin. We then discuss
possible means by which to bypass this defense, additional defense methods, and
advocate for continued research into the security of machine learning systems.

</details>


### [8] [Why Speech Deepfake Detectors Won't Generalize: The Limits of Detection in an Open World](https://arxiv.org/abs/2509.20405)
*Visar Berisha,Prad Kadambi,Isabella Lenz*

Main category: cs.CR

TL;DR: 论文指出语音深度伪造检测器在真实部署环境中面临"覆盖债务"问题，即检测器无法覆盖所有可能的攻击条件和环境变化，导致最坏情况下的安全性能而非平均基准分数决定实际安全性。


<details>
  <summary>Details</summary>
Motivation: 语音深度伪造检测器通常在干净的基准条件下评估，但实际部署环境涉及各种设备、采样率、编解码器、环境和攻击家族的复杂变化，这些未覆盖的盲点成为攻击者的目标。

Method: 通过分析跨测试框架的结果，按真实语音域和伪造语音发布年份分组评估性能，识别检测器的盲点和薄弱环节。

Result: 发现两个关键模式：1）新型合成器消除了检测器依赖的传统伪影；2）对话式语音域（电话会议、访谈、社交媒体）始终是最难保护的环境。

Conclusion: 检测器不应作为高风险决策的唯一依据，而应作为包含来源验证、身份凭证和政策保障的多层防御体系中的辅助信号。

Abstract: Speech deepfake detectors are often evaluated on clean, benchmark-style
conditions, but deployment occurs in an open world of shifting devices,
sampling rates, codecs, environments, and attack families. This creates a
``coverage debt" for AI-based detectors: every new condition multiplies with
existing ones, producing data blind spots that grow faster than data can be
collected. Because attackers can target these uncovered regions, worst-case
performance (not average benchmark scores) determines security. To demonstrate
the impact of the coverage debt problem, we analyze results from a recent
cross-testing framework. Grouping performance by bona fide domain and spoof
release year, two patterns emerge: newer synthesizers erase the legacy
artifacts detectors rely on, and conversational speech domains
(teleconferencing, interviews, social media) are consistently the hardest to
secure. These findings show that detection alone should not be relied upon for
high-stakes decisions. Detectors should be treated as auxiliary signals within
layered defenses that include provenance, personhood credentials, and policy
safeguards.

</details>


### [9] [Adversarial Defense in Cybersecurity: A Systematic Review of GANs for Threat Detection and Mitigation](https://arxiv.org/abs/2509.20411)
*Tharcisse Ndayipfukamiye,Jianguo Ding,Doreen Sebastian Sarwatt,Adamu Gaston Philipo,Huansheng Ning*

Main category: cs.CR

TL;DR: 这篇论文系统综述了2021-2025年基于GAN的网络安全对抗防御方法，通过PRISMA系统文献综述方法分析了185篇研究，提出了四维分类法，总结了GAN在网络安全防御中的进展、挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 机器学习网络安全系统容易受到对抗攻击，而GAN既可作为攻击工具也可作为防御手段。需要系统梳理GAN在网络安全防御中的最新进展，识别研究空白，为未来发展提供指导。

Method: 采用PRISMA系统文献综述协议，检索五个主要数字图书馆，从829篇初始文献中筛选出185篇同行评审研究，通过定量趋势分析和主题分类法进行综合分析。

Result: GAN在网络入侵检测、恶意软件分析和物联网安全等领域提高了检测准确性、鲁棒性和数据效用。主要进展包括WGAN-GP的稳定训练、CGAN的定向合成和混合GAN模型的改进韧性。

Conclusion: GAN防御显示出强大潜力，但需要解决训练不稳定、缺乏标准化基准、高计算成本和有限可解释性等挑战。建议发展混合模型、统一评估、实际集成和针对新兴威胁的防御策略。

Abstract: Machine learning-based cybersecurity systems are highly vulnerable to
adversarial attacks, while Generative Adversarial Networks (GANs) act as both
powerful attack enablers and promising defenses. This survey systematically
reviews GAN-based adversarial defenses in cybersecurity (2021--August 31,
2025), consolidating recent progress, identifying gaps, and outlining future
directions. Using a PRISMA-compliant systematic literature review protocol, we
searched five major digital libraries. From 829 initial records, 185
peer-reviewed studies were retained and synthesized through quantitative trend
analysis and thematic taxonomy development. We introduce a four-dimensional
taxonomy spanning defensive function, GAN architecture, cybersecurity domain,
and adversarial threat model. GANs improve detection accuracy, robustness, and
data utility across network intrusion detection, malware analysis, and IoT
security. Notable advances include WGAN-GP for stable training, CGANs for
targeted synthesis, and hybrid GAN models for improved resilience. Yet,
persistent challenges remain such as instability in training, lack of
standardized benchmarks, high computational cost, and limited explainability.
GAN-based defenses demonstrate strong potential but require advances in stable
architectures, benchmarking, transparency, and deployment. We propose a roadmap
emphasizing hybrid models, unified evaluation, real-world integration, and
defenses against emerging threats such as LLM-driven cyberattacks. This survey
establishes the foundation for scalable, trustworthy, and adaptive GAN-powered
defenses.

</details>


### [10] [A Taxonomy of Data Risks in AI and Quantum Computing (QAI) - A Systematic Review](https://arxiv.org/abs/2509.20418)
*Grace Billiris,Asif Gill,Madhushi Bandara*

Main category: cs.CR

TL;DR: 本文系统分析了量子人工智能（QAI）中的数据风险，提出了包含22个关键数据风险的分类法，分为治理、风险评估、控制实施、用户考虑和持续监控五个类别。


<details>
  <summary>Details</summary>
Motivation: QAI结合了人工智能和量子计算的优势，但也继承了二者的数据风险，这些复杂的隐私和安全漏洞尚未得到系统研究，影响了AI和QAI系统的可信度和可靠性。

Method: 系统回顾了67项与隐私和安全相关的研究，扩展对QAI数据风险的理解，并提出包含5个类别22个关键数据风险的分类法。

Result: 研究发现QAI特有的漏洞，并识别了整体风险评估中的差距，揭示了QAI数据风险的系统性特征。

Conclusion: 这项工作为可信AI和QAI研究做出贡献，并为开发未来风险评估工具提供了基础。

Abstract: Quantum Artificial Intelligence (QAI), the integration of Artificial
Intelligence (AI) and Quantum Computing (QC), promises transformative advances,
including AI-enabled quantum cryptography and quantum-resistant encryption
protocols. However, QAI inherits data risks from both AI and QC, creating
complex privacy and security vulnerabilities that are not systematically
studied. These risks affect the trustworthiness and reliability of AI and QAI
systems, making their understanding critical. This study systematically reviews
67 privacy- and security-related studies to expand understanding of QAI data
risks. We propose a taxonomy of 22 key data risks, organised into five
categories: governance, risk assessment, control implementation, user
considerations, and continuous monitoring. Our findings reveal vulnerabilities
unique to QAI and identify gaps in holistic risk assessment. This work
contributes to trustworthy AI and QAI research and provides a foundation for
developing future risk assessment tools.

</details>


### [11] [Differential Privacy of Network Parameters from a System Identification Perspective](https://arxiv.org/abs/2509.20460)
*Andrew Campbell,Anna Scaglione,Hang Liu,Victor Elvira,Sean Peisert,Daniel Arnold*

Main category: cs.CR

TL;DR: 该论文研究了在共享网络物理系统仿真时保护网络信息免受隐私系统识别攻击的问题，通过将差分隐私机制应用于输入来为图移位算子提供正式隐私保证。


<details>
  <summary>Details</summary>
Motivation: 传统系统识别方法旨在估计系统参数，而本文关注逆向问题：在保持合法分析效用的同时，防止对手识别图移位算子的假设条件。

Method: 将分析师对网络状态的观测建模为由差分隐私节点激励驱动的图滤波器的时间序列输出，分析谱特性和噪声协方差对隐私边界的影响。

Result: 研究表明，对于差分隐私高斯信号，滤波器和噪声协方差的谱特性共同决定了隐私边界，平滑滤波器和低条件数的协方差能提供更强的隐私保护。

Conclusion: 应用差分隐私机制到输入可以为图移位算子提供正式隐私保证，隐私边界与图滤波器的谱特性和噪声协方差直接相关。

Abstract: This paper addresses the problem of protecting network information from
privacy system identification (SI) attacks when sharing cyber-physical system
simulations. We model analyst observations of networked states as time-series
outputs of a graph filter driven by differentially private (DP) nodal
excitations, with the analyst aiming to infer the underlying graph shift
operator (GSO). Unlike traditional SI, which estimates system parameters, we
study the inverse problem: what assumptions prevent adversaries from
identifying the GSO while preserving utility for legitimate analysis. We show
that applying DP mechanisms to inputs provides formal privacy guarantees for
the GSO, linking the $(\epsilon,\delta)$-DP bound to the spectral properties of
the graph filter and noise covariance. More precisely, for DP Gaussian signals,
the spectral characteristics of both the filter and noise covariance determine
the privacy bound, with smooth filters and low-condition-number covariance
yielding greater privacy.

</details>


### [12] [Advancing Practical Homomorphic Encryption for Federated Learning: Theoretical Guarantees and Efficiency Optimizations](https://arxiv.org/abs/2509.20476)
*Ren-Yi Huang,Dumindu Samaraweera,Prashant Shekhar,J. Morris Chang*

Main category: cs.CR

TL;DR: 本文提出了一个理论分析框架，用于研究选择性加密作为防御模型反转攻击的原理，并通过实证研究量化影响防御效果的关键因素。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的梯度共享存在模型反转攻击风险，完全加密所有梯度计算开销大，现有选择性加密方法缺乏理论分析，主要依赖实证评估。

Method: 开发理论分析框架探索选择性加密原理，并进行全面的实证研究，识别和量化模型复杂度、加密比例、暴露梯度等关键因素。

Result: 理论框架阐明了梯度选择与隐私保护的关系，实验评估展示了这些因素如何影响防御模型反转攻击的鲁棒性。

Conclusion: 这些贡献推进了对选择性加密机制的理解，为设计高效、可扩展的隐私保护联邦学习系统提供了原则性指导。

Abstract: Federated Learning (FL) enables collaborative model training while preserving
data privacy by keeping raw data locally stored on client devices, preventing
access from other clients or the central server. However, recent studies reveal
that sharing model gradients creates vulnerability to Model Inversion Attacks,
particularly Deep Leakage from Gradients (DLG), which reconstructs private
training data from shared gradients. While Homomorphic Encryption has been
proposed as a promising defense mechanism to protect gradient privacy, fully
encrypting all model gradients incurs high computational overhead. Selective
encryption approaches aim to balance privacy protection with computational
efficiency by encrypting only specific gradient components. However, the
existing literature largely overlooks a theoretical exploration of the spectral
behavior of encrypted versus unencrypted parameters, relying instead primarily
on empirical evaluations. To address this gap, this paper presents a framework
for theoretical analysis of the underlying principles of selective encryption
as a defense against model inversion attacks. We then provide a comprehensive
empirical study that identifies and quantifies the critical factors, such as
model complexity, encryption ratios, and exposed gradients, that influence
defense effectiveness. Our theoretical framework clarifies the relationship
between gradient selection and privacy preservation, while our experimental
evaluation demonstrates how these factors shape the robustness of defenses
against model inversion attacks. Collectively, these contributions advance the
understanding of selective encryption mechanisms and offer principled guidance
for designing efficient, scalable, privacy-preserving federated learning
systems.

</details>


### [13] [Every Character Counts: From Vulnerability to Defense in Phishing Detection](https://arxiv.org/abs/2509.20589)
*Maria Chiper,Radu Tudor Ionescu*

Main category: cs.CR

TL;DR: 本文研究了字符级深度学习模型在钓鱼检测中的有效性，评估了CharCNN、CharGRU和CharBiLSTM三种架构在标准训练、对抗攻击和对抗训练三种场景下的表现。CharGRU在计算资源受限环境下表现最佳，对抗训练显著提升了模型鲁棒性，并通过Grad-CAM技术实现了决策可视化。


<details>
  <summary>Details</summary>
Motivation: 当前自动钓鱼检测方法缺乏可解释性和对新攻击的鲁棒性，需要开发既鲁棒又可解释的检测方案。

Method: 使用字符级深度学习模型（CharCNN、CharGRU、CharBiLSTM），在多源数据集上进行评估，包括标准训练测试、对抗攻击测试和对抗训练测试，并采用Grad-CAM技术进行决策可视化。

Result: CharGRU在所有场景下表现最佳，所有模型都对对抗攻击敏感但对抗训练能显著提升鲁棒性，成功实现了字符级决策可视化。

Conclusion: 字符级深度学习模型为钓鱼检测提供了有效的解决方案，CharGRU在资源受限环境下表现优越，对抗训练和可视化技术增强了模型的实用性和可信度。

Abstract: Phishing attacks targeting both organizations and individuals are becoming an
increasingly significant threat as technology advances. Current automatic
detection methods often lack explainability and robustness in detecting new
phishing attacks. In this work, we investigate the effectiveness of
character-level deep learning models for phishing detection, which can provide
both robustness and interpretability. We evaluate three neural architectures
adapted to operate at the character level, namely CharCNN, CharGRU, and
CharBiLSTM, on a custom-built email dataset, which combines data from multiple
sources. Their performance is analyzed under three scenarios: (i) standard
training and testing, (ii) standard training and testing under adversarial
attacks, and (iii) training and testing with adversarial examples. Aiming to
develop a tool that operates as a browser extension, we test all models under
limited computational resources. In this constrained setup, CharGRU proves to
be the best-performing model across all scenarios. All models show
vulnerability to adversarial attacks, but adversarial training substantially
improves their robustness. In addition, by adapting the Gradient-weighted Class
Activation Mapping (Grad-CAM) technique to character-level inputs, we are able
to visualize which parts of each email influence the decision of each model.
Our open-source code and data is released at
https://github.com/chipermaria/every-character-counts.

</details>


### [14] [Beyond SSO: Mobile Money Authentication for Inclusive e-Government in Sub-Saharan Africa](https://arxiv.org/abs/2509.20592)
*Oluwole Adewusi,Wallace S. Msagusa,Jean Pierre Imanirumva,Okemawo Obadofin,Jema D. Ndibwile*

Main category: cs.CR

TL;DR: 本文提出了一种混合移动货币认证框架，结合USSD多因素认证和JWT会话管理，解决了撒哈拉以南非洲地区移动货币认证的安全性和可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 撒哈拉以南非洲地区移动货币服务快速普及，但现有认证方法存在SIM交换攻击、会话保护弱和高峰需求时扩展性差等关键限制。

Method: 设计了三因素认证模型：SIM验证、PIN输入和会话令牌绑定，采用USSD多因素认证和密码绑定的JSON Web Tokens进行安全会话管理。

Result: 相比OAuth单点登录方法，认证时间快45%（8秒 vs 12-15秒），网络条件差时成功率提高15%（95% vs 80%），并显著增强了对网络钓鱼和暴力攻击的抵抗力。

Conclusion: 该混合认证协议确保了离线可访问性和安全会话连续性，为SSA及其他类似约束地区推进了安全的数字包容性。

Abstract: The rapid adoption of Mobile Money Services (MMS) in Sub-Saharan Africa (SSA)
offers a viable path to improve e-Government service accessibility in the face
of persistent low internet penetration. However, existing Mobile Money
Authentication (MMA) methods face critical limitations, including
susceptibility to SIM swapping, weak session protection, and poor scalability
during peak demand. This study introduces a hybrid MMA framework that combines
Unstructured Supplementary Service Data (USSD)-based multi-factor
authentication with secure session management via cryptographically bound JSON
Web Tokens (JWT). Unlike traditional MMA systems that rely solely on SIM-PIN
verification or smartphone-dependent biometrics, our design implements a
three-factor authentication model; SIM verification, PIN entry, and session
token binding, tailored for resource-constrained environments. Simulations and
comparative analysis against OAuth-based Single Sign-On (SSO) methods reveal a
45% faster authentication time (8 seconds vs. 12 to 15 seconds), 15% higher
success under poor network conditions (95% vs. 80%), and increased resistance
to phishing and brute-force attacks. Penetration testing and threat modeling
further demonstrate a substantial reduction in vulnerability exposure compared
to conventional approaches. The primary contributions of this work are: (1) a
hybrid authentication protocol that ensures offline accessibility and secure
session continuity; (2) a tailored security framework addressing threats like
SIM swapping and social engineering in SSA; and (3) demonstrated scalability
for thousands of users with reduced infrastructure overhead. The proposed
approach advances secure digital inclusion in SSA and other regions with
similar constraints.

</details>


### [15] [A Framework for Rapidly Developing and Deploying Protection Against Large Language Model Attacks](https://arxiv.org/abs/2509.20639)
*Adam Swanda,Amy Chang,Alexander Chen,Fraser Burch,Paul Kassianik,Konstantin Berlin*

Main category: cs.CR

TL;DR: 本文提出了一种针对大型语言模型（LLMs）的生产级防御系统，借鉴成熟的恶意软件检测和威胁情报实践，通过三层组件集成提供分层保护。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在各行业的广泛应用，其自主性和权限扩展使其成为恶意攻击的目标。现有方法无法预防零日攻击或新型攻击，需要类似恶意软件保护系统的风险最小化策略。

Method: 系统集成三个核心组件：威胁情报系统（将新兴威胁转化为防护措施）、数据平台（聚合和丰富信息，提供可观测性和监控）、发布平台（实现安全快速的检测更新）。

Result: 该系统能够提供针对不断演变的LLM威胁的分层保护，同时生成训练数据用于持续模型改进，并在不中断生产的情况下部署更新。

Conclusion: 该生产级防御系统通过集成威胁情报、数据平台和发布平台，实现了对LLM威胁的持续、快速适应的端到端保护，填补了现有研究主要评估单个检测模型而非完整系统的空白。

Abstract: The widespread adoption of Large Language Models (LLMs) has revolutionized AI
deployment, enabling autonomous and semi-autonomous applications across
industries through intuitive language interfaces and continuous improvements in
model development. However, the attendant increase in autonomy and expansion of
access permissions among AI applications also make these systems compelling
targets for malicious attacks. Their inherent susceptibility to security flaws
necessitates robust defenses, yet no known approaches can prevent zero-day or
novel attacks against LLMs. This places AI protection systems in a category
similar to established malware protection systems: rather than providing
guaranteed immunity, they minimize risk through enhanced observability,
multi-layered defense, and rapid threat response, supported by a threat
intelligence function designed specifically for AI-related threats.
  Prior work on LLM protection has largely evaluated individual detection
models rather than end-to-end systems designed for continuous, rapid adaptation
to a changing threat landscape. We present a production-grade defense system
rooted in established malware detection and threat intelligence practices. Our
platform integrates three components: a threat intelligence system that turns
emerging threats into protections; a data platform that aggregates and enriches
information while providing observability, monitoring, and ML operations; and a
release platform enabling safe, rapid detection updates without disrupting
customer workflows. Together, these components deliver layered protection
against evolving LLM threats while generating training data for continuous
model improvement and deploying updates without interrupting production.

</details>


### [16] [Reliability Analysis of Fully Homomorphic Encryption Systems Under Memory Faults](https://arxiv.org/abs/2509.20686)
*Rian Adam Rajagede,Yan Solihin*

Main category: cs.CR

TL;DR: 本文研究全同态加密（FHE）系统在内存故障下的可靠性表现，分析不同FHE方案在故障情况下的行为和传统及FHE专用故障缓解技术的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管FHE技术已在真实平台中部署，但其可靠性方面，特别是对故障的响应机制，一直被忽视。本文旨在填补这一研究空白。

Method: 通过分析FHE计算在内存故障下的行为，研究不同FHE方案的个体操作和应用程序级别的表现。

Result: 研究发现FHE系统在故障情况下的具体行为特征，并评估了不同故障缓解技术的效果。

Conclusion: FHE系统的可靠性研究至关重要，需要开发专门的故障缓解技术来确保其在真实环境中的安全部署。

Abstract: Fully Homomorphic Encryption (FHE) represents a paradigm shift in
cryptography, enabling computation directly on encrypted data and unlocking
privacy-critical computation. Despite being increasingly deployed in real
platforms, the reliability aspects of FHE systems, especially how they respond
to faults, have been mostly neglected. This paper aims to better understand of
how FHE computation behaves in the presence of memory faults, both in terms of
individual operations as well as at the level of applications, for different
FHE schemes. Finally, we investigate how effective traditional and FHE-specific
fault mitigation techniques are.

</details>


### [17] [Cryptographic Backdoor for Neural Networks: Boon and Bane](https://arxiv.org/abs/2509.20714)
*Anh Tu Ngo,Anupam Chattopadhyay,Subhamoy Maitra*

Main category: cs.CR

TL;DR: 本文展示了神经网络中密码学后门在攻击和防御两个方向上的高效应用。攻击方面，精心植入的密码学后门可实现强大且隐蔽的攻击；防御方面，提出了可证明鲁棒的神经网络水印方案、用户认证协议和知识产权追踪协议。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络中密码学后门的双重应用潜力，既探索其作为攻击手段的有效性，也开发其在防御和保护神经网络知识产权方面的实用价值。

Method: 借鉴Goldwasser等人的理论框架，设计并实现了基于密码学后门的攻击和防御协议。防御协议包括水印方案、认证协议和IP追踪协议，攻击方案则利用后门实现隐蔽攻击。

Result: 理论分析表明防御协议对黑盒访问的对手具有可证明鲁棒性，而攻击在标准假设下无法预防。实验在先进神经网络架构上验证了理论主张，并展示了后量子密码原语的应用潜力。

Conclusion: 密码学后门在神经网络安全领域具有重要价值，既可作为强大的攻击工具，也可用于构建可证明安全的防御机制，为量子时代的机器学习安全应用奠定基础。

Abstract: In this paper we show that cryptographic backdoors in a neural network (NN)
can be highly effective in two directions, namely mounting the attacks as well
as in presenting the defenses as well. On the attack side, a carefully planted
cryptographic backdoor enables powerful and invisible attack on the NN.
Considering the defense, we present applications: first, a provably robust NN
watermarking scheme; second, a protocol for guaranteeing user authentication;
and third, a protocol for tracking unauthorized sharing of the NN intellectual
property (IP). From a broader theoretical perspective, borrowing the ideas from
Goldwasser et. al. [FOCS 2022], our main contribution is to show that all these
instantiated practical protocol implementations are provably robust. The
protocols for watermarking, authentication and IP tracking resist an adversary
with black-box access to the NN, whereas the backdoor-enabled adversarial
attack is impossible to prevent under the standard assumptions. While the
theoretical tools used for our attack is mostly in line with the Goldwasser et.
al. ideas, the proofs related to the defense need further studies. Finally, all
these protocols are implemented on state-of-the-art NN architectures with
empirical results corroborating the theoretical claims. Further, one can
utilize post-quantum primitives for implementing the cryptographic backdoors,
laying out foundations for quantum-era applications in machine learning (ML).

</details>


### [18] [ExpIDS: A Drift-adaptable Network Intrusion Detection System With Improved Explainability](https://arxiv.org/abs/2509.20767)
*Ayush Kumar,Kar Wai Fok,Vrizlynn L. L. Thing*

Main category: cs.CR

TL;DR: ExpIDS是一个基于深度学习的网络入侵检测系统，旨在提高决策树解释的保真度，并能够适应网络流量分布的变化。


<details>
  <summary>Details</summary>
Motivation: 尽管基于机器学习的网络入侵检测系统具有诸多优势，但由于其不透明的决策过程，网络安全专家在实际生产环境中部署这些模型时存在明显犹豫。

Method: 设计了一个深度学习模型ExpIDS，使其具有高决策树解释保真度，即解释决策树的预测结果与ExpIDS的预测结果尽可能接近。该系统还能适应网络流量分布的变化（漂移）。

Result: 通过大量实验验证，ExpIDS在常见攻击检测方面实现了更高的决策树解释保真度，并且检测性能与最先进的NIDS相当，能够适应不同程度的真实世界漂移。

Conclusion: ExpIDS成功解决了机器学习NIDS的可解释性问题，同时保持了良好的检测性能和对现实环境变化的适应性。

Abstract: Despite all the advantages associated with Network Intrusion Detection
Systems (NIDSs) that utilize machine learning (ML) models, there is a
significant reluctance among cyber security experts to implement these models
in real-world production settings. This is primarily because of their opaque
nature, meaning it is unclear how and why the models make their decisions. In
this work, we design a deep learning-based NIDS, ExpIDS to have high decision
tree explanation fidelity, i.e., the predictions of decision tree explanation
corresponding to ExpIDS should be as close to ExpIDS's predictions as possible.
ExpIDS can also adapt to changes in network traffic distribution (drift). With
the help of extensive experiments, we verify that ExpIDS achieves higher
decision tree explanation fidelity and a malicious traffic detection
performance comparable to state-of-the-art NIDSs for common attacks with
varying levels of real-world drift.

</details>


### [19] [Fast Revocable Attribute-Based Encryption with Data Integrity for Internet of Things](https://arxiv.org/abs/2509.20796)
*Yongjiao Li,Liang Zhu,Yalin Deng,Qikun Zhang,Zhenlei Wang,Zhu Cao*

Main category: cs.CR

TL;DR: 提出了一种高效的、支持数据完整性的可撤销属性基加密方案，适用于物联网环境，在相同访问策略下计算消耗比现有方案减少7-9倍


<details>
  <summary>Details</summary>
Motivation: 当前可撤销属性基加密方案在效率、安全性、动态可扩展性等方面难以达到最佳平衡，限制了其在物联网云存储环境中的实际应用

Method: 设计快速RABE方案，将计算密集的撤销过程转移到云端，减轻物联网设备负担，同时保证数据完整性和正确性

Result: 方案在定义的安全模型中实现了自适应安全性，实验结果表明性能优于现有解决方案

Conclusion: 该方案为物联网环境下的安全数据共享和细粒度访问控制提供了高效实用的解决方案

Abstract: Efficient and secure revocable attribute-based encryption (RABE) is vital for
ensuring flexible and fine-grained access control and data sharing in cloud
storage and outsourced data environments within the Internet of Things (IoT).
However, current RABE schemes often struggle to achieve an optimal balance
between efficiency, security, dynamic scalability, and other important
features, which hampers their practical application. To overcome these
limitations, we propose a fast RABE scheme with data integrity for IoT that
achieves adaptive security with multiple challenge ciphertexts. Our scheme
supports the revocation of authorized users and transfers the computationally
heavy revocation processes to the cloud, thereby easing the computational
burden on IoT devices. Moreover, it consistently guarantees the integrity and
correctness of data. We have demonstrated its adaptive security within the
defined security model with multiple challenge ciphertexts and optimized its
performance. Experimental results indicate that our scheme provides better
performance than existing solutions. Under the same access policy, our scheme
reduces computational consumption by 7 to 9 times compared to previous schemes.

</details>


### [20] [Intelligent Graybox Fuzzing via ATPG-Guided Seed Generation and Submodule Analysis](https://arxiv.org/abs/2509.20808)
*Raghul Saravanan,Sudipta Paria,Aritra Dasgupta,Swarup Bhunia,Sai Manoj P D*

Main category: cs.CR

TL;DR: PROFUZZ是一个新的硬件模糊测试框架，结合了定向灰盒模糊测试和自动测试模式生成技术，解决了现有方法在硬件语言支持、可扩展性和抽象不匹配方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有硬件模糊测试方法如DirectFuzz存在支持硬件描述语言有限、难以扩展到大型电路、存在抽象不匹配等问题，需要更高效的定向模糊测试解决方案。

Method: PROFUZZ采用定向灰盒模糊测试方法，结合自动测试模式生成的结构分析能力，生成精确的输入种子来更有效地针对特定设计区域。

Result: 实验显示PROFUZZ在处理多目标站点时比DirectFuzz扩展性好30倍，覆盖率提高11.66%，运行速度快2.76倍。

Conclusion: PROFUZZ在复杂硬件系统中具有出色的可扩展性和有效性，为定向模糊测试提供了高效的解决方案。

Abstract: Hardware Fuzzing emerged as one of the crucial techniques for finding
security flaws in modern hardware designs by testing a wide range of input
scenarios. One of the main challenges is creating high-quality input seeds that
maximize coverage and speed up verification. Coverage-Guided Fuzzing (CGF)
methods help explore designs more effectively, but they struggle to focus on
specific parts of the hardware. Existing Directed Gray-box Fuzzing (DGF)
techniques like DirectFuzz try to solve this by generating targeted tests, but
it has major drawbacks, such as supporting only limited hardware description
languages, not scaling well to large circuits, and having issues with
abstraction mismatches. To address these problems, we introduce a novel
framework, PROFUZZ, that follows the DGF approach and combines fuzzing with
Automatic Test Pattern Generation (ATPG) for more efficient fuzzing. By
leveraging ATPG's structural analysis capabilities, PROFUZZ can generate
precise input seeds that target specific design regions more effectively while
maintaining high fuzzing throughput. Our experiments show that PROFUZZ scales
30x better than DirectFuzz when handling multiple target sites, improves
coverage by 11.66%, and runs 2.76x faster, highlighting its scalability and
effectiveness for directed fuzzing in complex hardware systems.

</details>


### [21] [Security-aware Semantic-driven ISAC via Paired Adversarial Residual Networks](https://arxiv.org/abs/2509.20835)
*Yu Liu,Boxiang He,Fanggang Wang*

Main category: cs.CR

TL;DR: 本文提出了一种新颖灵活的安全感知语义驱动集成感知与通信框架SS-ISAC，通过可插拔的加解密模块实现安全防护，同时保证感知通信性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决集成感知与通信系统中的窃听威胁，同时保持系统性能，需要一种灵活的安全防护机制。

Method: 设计一对可插拔的加解密模块，采用可训练对抗残差网络实现对抗攻击和噪声消除，通过联合优化损失函数来平衡安全性和性能。

Result: 仿真结果表明SS-ISAC框架在感知通信性能和窃听预防方面均表现出色。

Conclusion: SS-ISAC框架提供了一种灵活有效的安全防护方案，无需大幅修改硬件基础设施即可实现安全增强。

Abstract: This paper proposes a novel and flexible security-aware semantic-driven
integrated sensing and communication (ISAC) framework, namely security semantic
ISAC (SS-ISAC). Inspired by the positive impact of the adversarial attack, a
pair of pluggable encryption and decryption modules is designed in the proposed
SS-ISAC framework. The encryption module is installed after the semantic
transmitter, adopting a trainable adversarial residual network (ARN) to create
the adversarial attack. Correspondingly, the decryption module before the
semantic receiver utilizes another trainable ARN to mitigate the adversarial
attack and noise. These two modules can be flexibly assembled considering the
system security demands, without drastically modifying the hardware
infrastructure. To ensure the sensing and communication (SAC) performance while
preventing the eavesdropping threat, the above ARNs are jointly optimized by
minimizing a carefully designed loss function that relates to the adversarial
attack power, SAC performance, as well as the privacy leakage risk. Simulation
results validate the effectiveness of the proposed SS-ISAC framework in terms
of both SAC and eavesdropping prevention performance.

</details>


### [22] [FlowXpert: Context-Aware Flow Embedding for Enhanced Traffic Detection in IoT Network](https://arxiv.org/abs/2509.20861)
*Chao Zha,Haolin Pan,Bing Bai,Jiangxing Wu,Ruyun Zhang*

Main category: cs.CR

TL;DR: 该论文针对物联网环境中网络流量检测的挑战，提出了一种新的特征提取工具和嵌入训练框架，通过消除传统时空特征、引入上下文语义特征和对比学习策略，显著提升了异常流量检测的性能。


<details>
  <summary>Details</summary>
Motivation: 物联网设备产生的大规模动态网络流量给基于规则的检测方法带来挑战。现有特征提取工具存在高稀疏性问题，且缺乏有效的语义特征嵌入机制，影响了机器学习模型的收敛和检测效果。

Method: 提出新型特征提取工具，消除传统时空特征，采用与源主机相关的上下文语义特征；设计嵌入训练框架，结合无监督DBSCAN聚类算法和对比学习策略，捕捉细粒度流量语义表示。

Result: 在真实Mawi数据集上的实验验证表明，该方法在检测精度、鲁棒性和泛化能力方面优于多个先进模型，并证实了其在实时场景中的适用性和可部署性。

Conclusion: 所提出的方法有效解决了物联网流量检测中的特征稀疏性和语义表征问题，为网络安全提供了更有效的解决方案，具有实际应用价值。

Abstract: In the Internet of Things (IoT) environment, continuous interaction among a
large number of devices generates complex and dynamic network traffic, which
poses significant challenges to rule-based detection approaches. Machine
learning (ML)-based traffic detection technology, capable of identifying
anomalous patterns and potential threats within this traffic, serves as a
critical component in ensuring network security. This study first identifies a
significant issue with widely adopted feature extraction tools (e.g.,
CICMeterFlow): the extensive use of time- and length-related features leads to
high sparsity, which adversely affects model convergence. Furthermore, existing
traffic detection methods generally lack an embedding mechanism capable of
efficiently and comprehensively capturing the semantic characteristics of
network traffic. To address these challenges, we propose a novel feature
extraction tool that eliminates traditional time and length features in favor
of context-aware semantic features related to the source host, thus improving
the generalizability of the model. In addition, we design an embedding training
framework that integrates the unsupervised DBSCAN clustering algorithm with a
contrastive learning strategy to effectively capture fine-grained semantic
representations of traffic. Extensive empirical evaluations are conducted on
the real-world Mawi data set to validate the proposed method in terms of
detection accuracy, robustness, and generalization. Comparative experiments
against several state-of-the-art (SOTA) models demonstrate the superior
performance of our approach. Furthermore, we confirm its applicability and
deployability in real-time scenarios.

</details>


### [23] [A Generalized $χ_n$-Function](https://arxiv.org/abs/2509.20880)
*Cheng Lyu,Mu Yuan,Dabin Zheng,Siwei Sun,Shun Li*

Main category: cs.CR

TL;DR: 本文提出并分析了一种广义映射χ_{n,m}，解决了原始χ_n映射只能在奇数维向量空间上双射的限制，构造了适用于任意正整数n的置换类，并研究了其代数性质和密码学特性。


<details>
  <summary>Details</summary>
Motivation: 原始χ_n映射只能在n为奇数时在F_2^n上双射，限制了其在轻量级密码学中的应用。为了克服这一限制，需要构造适用于任意维度的置换映射。

Method: 引入广义映射χ_{n,m}和θ_{m,k}，证明这些映射生成与F_2[z]/(z^{⌊n/m⌋+1})单位群同构的阿贝尔群，从而构造广泛的置换类。

Result: 成功构造了适用于任意正整数n的置换映射，分析了其迭代、不动点和循环结构等代数性质，并提供了小n和m值时χ_{n,m}迭代的密码学性质数据库。

Conclusion: 研究结果推广了χ_n映射，为轻量级密码学提供了χ_n和χχ_n的替代方案，并证明了belkheyar2025chi中的猜想1。

Abstract: The mapping $\chi_n$ from $\F_{2}^{n}$ to itself defined by $y=\chi_n(x)$
with $y_i=x_i+x_{i+2}(1+x_{i+1})$, where the indices are computed modulo $n$,
has been widely studied for its applications in lightweight cryptography.
However, $\chi_n $ is bijective on $\F_2^n$ only when $n$ is odd, restricting
its use to odd-dimensional vector spaces over $\F_2$. To address this
limitation, we introduce and analyze the generalized mapping $\chi_{n, m}$
defined by $y=\chi_{n,m}(x)$ with $y_i=x_i+x_{i+m} (x_{i+m-1}+1)(x_{i+m-2}+1)
\cdots (x_{i+1}+1)$, where $m$ is a fixed integer with $m\nmid n$. To
investigate such mappings, we further generalize $\chi_{n,m}$ to $\theta_{m,
k}$, where $\theta_{m, k}$ is given by $y_i=x_{i+mk} \prod_{\substack{j=1,\,\,
m \nmid j}}^{mk-1} \left(x_{i+j}+1\right), \,\,{\rm for }\,\, i\in
\{0,1,\ldots,n-1\}$. We prove that these mappings generate an abelian group
isomorphic to the group of units in $\F_2[z]/(z^{\lfloor n/m\rfloor +1})$. This
structural insight enables us to construct a broad class of permutations over
$\F_2^n$ for any positive integer $n$, along with their inverses. We rigorously
analyze algebraic properties of these mappings, including their iterations,
fixed points, and cycle structures. Additionally, we provide a comprehensive
database of the cryptographic properties for iterates of $\chi_{n,m}$ for small
values of $n$ and $m$. Finally, we conduct a comparative security and
implementation cost analysis among $\chi_{n,m}$, $\chi_n$, $\chi\chi_n$
(EUROCRYPT 2025 \cite{belkheyar2025chi}) and their variants, and prove
Conjecture~1 proposed in~\cite{belkheyar2025chi} as a by-product of our study.
Our results lead to generalizations of $\chi_n$, providing alternatives to
$\chi_n$ and $\chi\chi_n$.

</details>


### [24] [RLCracker: Exposing the Vulnerability of LLM Watermarks with Adaptive RL Attacks](https://arxiv.org/abs/2509.20924)
*Hanbo Huang,Yiran Zhang,Hao Zheng,Xuan Gong,Yihan Li,Lin Liu,Shiyu Liang*

Main category: cs.CR

TL;DR: 本文提出了一种基于强化学习的自适应攻击方法RLCracker，能够有效移除大语言模型水印，揭示了现有水印方案在对抗性攻击下的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有水印方案的评估不够对抗性，掩盖了关键漏洞并高估了安全性。需要更严格的评估方法来量化水印对自适应攻击的抵抗力。

Method: 提出自适应鲁棒半径作为形式化度量，并开发RLCracker——一种基于强化学习的自适应攻击方法，通过优化攻击上下文和模型参数来移除水印，同时保持语义保真度。

Result: RLCracker仅需100个短样本训练，就能让3B模型在1500个标记的Unigram水印文本上达到98.5%的移除成功率和平均0.92 P-SP分数，远超GPT-4o的6.75%成功率，并在10种水印方案上表现出泛化能力。

Conclusion: 自适应攻击对当前水印防御构成根本性威胁，现有水印方案在面对精心设计的对抗性攻击时安全性被严重高估。

Abstract: Large Language Models (LLMs) watermarking has shown promise in detecting
AI-generated content and mitigating misuse, with prior work claiming robustness
against paraphrasing and text editing. In this paper, we argue that existing
evaluations are not sufficiently adversarial, obscuring critical
vulnerabilities and overstating the security. To address this, we introduce
adaptive robustness radius, a formal metric that quantifies watermark
resilience against adaptive adversaries. We theoretically prove that optimizing
the attack context and model parameters can substantially reduce this radius,
making watermarks highly susceptible to paraphrase attacks. Leveraging this
insight, we propose RLCracker, a reinforcement learning (RL)-based adaptive
attack that erases watermarks while preserving semantic fidelity. RLCracker
requires only limited watermarked examples and zero access to the detector.
Despite weak supervision, it empowers a 3B model to achieve 98.5% removal
success and an average 0.92 P-SP score on 1,500-token Unigram-marked texts
after training on only 100 short samples. This performance dramatically exceeds
6.75% by GPT-4o and generalizes across five model sizes over ten watermarking
schemes. Our results confirm that adaptive attacks are broadly effective and
pose a fundamental threat to current watermarking defenses.

</details>


### [25] [CTI Dataset Construction from Telegram](https://arxiv.org/abs/2509.20943)
*Dincy R. Arikkat,Sneha B. T.,Serena Nicolazzo,Antonino Nocera,Vinod P.,Rafidha Rehiman K. A.,Karthika R*

Main category: cs.CR

TL;DR: 本文提出了一个端到端的自动化管道，用于从Telegram系统性地收集和过滤威胁相关内容，构建高质量的网络威胁情报数据集。


<details>
  <summary>Details</summary>
Motivation: 网络威胁情报的有效性依赖于高质量数据集，而攻击向量和对手战术不断演变。Telegram作为有价值的CTI来源，提供及时多样的威胁相关信息，但需要系统化的方法来构建可靠数据集。

Method: 开发自动化管道识别相关Telegram频道，从150个来源中筛选12个频道，收集145,349条消息。使用基于BERT的分类器过滤威胁情报消息，准确率达96.64%。

Result: 从过滤后的消息中编译了包含86,509个恶意攻击指标的数据集，涵盖域名、IP地址、URL、哈希值和CVE漏洞。

Conclusion: 该方法不仅产生了大规模、高保真度的CTI数据集，还为未来网络威胁检测的研究和实际应用奠定了基础。

Abstract: Cyber Threat Intelligence (CTI) enables organizations to anticipate, detect,
and mitigate evolving cyber threats. Its effectiveness depends on high-quality
datasets, which support model development, training, evaluation, and
benchmarking. Building such datasets is crucial, as attack vectors and
adversary tactics continually evolve. Recently, Telegram has gained prominence
as a valuable CTI source, offering timely and diverse threat-related
information that can help address these challenges. In this work, we address
these challenges by presenting an end-to-end automated pipeline that
systematically collects and filters threat-related content from Telegram. The
pipeline identifies relevant Telegram channels and scrapes 145,349 messages
from 12 curated channels out of 150 identified sources. To accurately filter
threat intelligence messages from generic content, we employ a BERT-based
classifier, achieving an accuracy of 96.64%. From the filtered messages, we
compile a dataset of 86,509 malicious Indicators of Compromise, including
domains, IPs, URLs, hashes, and CVEs. This approach not only produces a
large-scale, high-fidelity CTI dataset but also establishes a foundation for
future research and operational applications in cyber threat detection.

</details>


### [26] [Dual-Path Phishing Detection: Integrating Transformer-Based NLP with Structural URL Analysis](https://arxiv.org/abs/2509.20972)
*Ibrahim Altan,Abdulla Bachir,Yousuf Parbhulkar,Abdul Muksith Rizvi,Moshiur Farazi*

Main category: cs.CR

TL;DR: 提出了一种双路径网络钓鱼检测框架，结合基于Transformer的NLP和传统机器学习，共同分析邮件文本和嵌入URL，显著提高了检测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统检测方法仅孤立分析邮件内容或URL，无法全面应对日益复杂的网络钓鱼攻击，需要更综合的解决方案。

Method: 使用微调的Transformer架构（如DistilBERT）进行语义分析，结合字符级TF-IDF向量化和传统分类器（如随机森林）进行链接结构分析。

Result: DistilBERT在文本检测中实现了准确性和计算效率的平衡，随机森林在恶意URL识别中表现优异，整体检测精度显著提升。

Conclusion: 双路径方法具有高效性、实用性和可扩展性，为应对现代网络钓鱼威胁提供了准确且可解释的解决方案。

Abstract: Phishing emails pose a persistent and increasingly sophisticated threat,
undermining email security through deceptive tactics designed to exploit both
semantic and structural vulnerabilities. Traditional detection methods, often
based on isolated analysis of email content or embedded URLs, fail to
comprehensively address these evolving attacks. In this paper, we propose a
dual-path phishing detection framework that integrates transformer-based
natural language processing (NLP) with classical machine learning to jointly
analyze email text and embedded URLs. Our approach leverages the complementary
strengths of semantic analysis using fine-tuned transformer architectures
(e.g., DistilBERT) and structural link analysis via character-level TF-IDF
vectorization paired with classical classifiers (e.g., Random Forest).
Empirical evaluation on representative email and URL datasets demonstrates that
this combined approach significantly improves detection accuracy. Specifically,
the DistilBERT model achieves a near-optimal balance between accuracy and
computational efficiency for textual phishing detection, while Random Forest
notably outperforms other classical classifiers in identifying malicious URLs.
The modular design allows flexibility for standalone deployment or ensemble
integration, facilitating real-world adoption. Collectively, our results
highlight the efficacy and practical value of this dual-path approach,
establishing a scalable, accurate, and interpretable solution capable of
enhancing email security against contemporary phishing threats.

</details>


### [27] [Automatic Red Teaming LLM-based Agents with Model Context Protocol Tools](https://arxiv.org/abs/2509.21011)
*Ping He,Changjiang Li,Binbin Zhao,Tianyu Du,Shouling Ji*

Main category: cs.CR

TL;DR: 本文提出了AutoMalTool，一个针对LLM智能体的自动化红队框架，通过生成恶意MCP工具来揭示安全风险。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在各领域的广泛应用，MCP工具成为标准交互协议，但存在工具投毒攻击风险。现有研究停留在概念验证阶段，缺乏系统性的自动化红队测试方法。

Method: 开发AutoMalTool框架，自动生成恶意MCP工具，用于测试LLM智能体的安全性。

Result: 评估显示AutoMalTool能有效生成恶意MCP工具，成功操纵主流LLM智能体行为并规避现有检测机制。

Conclusion: 该研究揭示了LLM智能体在MCP工具范式下的新安全风险，为安全防护提供了重要参考。

Abstract: The remarkable capability of large language models (LLMs) has led to the wide
application of LLM-based agents in various domains. To standardize interactions
between LLM-based agents and their environments, model context protocol (MCP)
tools have become the de facto standard and are now widely integrated into
these agents. However, the incorporation of MCP tools introduces the risk of
tool poisoning attacks, which can manipulate the behavior of LLM-based agents.
Although previous studies have identified such vulnerabilities, their red
teaming approaches have largely remained at the proof-of-concept stage, leaving
the automatic and systematic red teaming of LLM-based agents under the MCP tool
poisoning paradigm an open question. To bridge this gap, we propose
AutoMalTool, an automated red teaming framework for LLM-based agents by
generating malicious MCP tools. Our extensive evaluation shows that AutoMalTool
effectively generates malicious MCP tools capable of manipulating the behavior
of mainstream LLM-based agents while evading current detection mechanisms,
thereby revealing new security risks in these agents.

</details>


### [28] [PMark: Towards Robust and Distortion-free Semantic-level Watermarking with Channel Constraints](https://arxiv.org/abs/2509.21057)
*Jiahao Huo,Shuliang Liu,Bin Wang,Junyan Zhang,Yibo Yan,Aiwei Liu,Xuming Hu,Mingxun Zhou*

Main category: cs.CR

TL;DR: 本文提出PMark，一种基于代理函数的语义级水印方法，通过动态估计中位数和多重约束通道，实现无失真水印并提升抗攻击能力。


<details>
  <summary>Details</summary>
Motivation: 现有语义级水印方法缺乏理论鲁棒性保证，且基于拒绝采样的生成方式会导致与无水印输出的分布偏差。

Method: 提出代理函数框架，通过动态采样估计句子代理函数中位数，并实施多重约束通道来增强水印证据。

Result: PMark在文本质量和鲁棒性方面均优于现有基线方法，提供更有效的机器生成文本检测范式。

Conclusion: PMark通过理论保证实现了无失真水印生成，显著提升了对抗改写攻击的鲁棒性。

Abstract: Semantic-level watermarking (SWM) for large language models (LLMs) enhances
watermarking robustness against text modifications and paraphrasing attacks by
treating the sentence as the fundamental unit. However, existing methods still
lack strong theoretical guarantees of robustness, and reject-sampling-based
generation often introduces significant distribution distortions compared with
unwatermarked outputs. In this work, we introduce a new theoretical framework
on SWM through the concept of proxy functions (PFs) $\unicode{x2013}$ functions
that map sentences to scalar values. Building on this framework, we propose
PMark, a simple yet powerful SWM method that estimates the PF median for the
next sentence dynamically through sampling while enforcing multiple PF
constraints (which we call channels) to strengthen watermark evidence. Equipped
with solid theoretical guarantees, PMark achieves the desired distortion-free
property and improves the robustness against paraphrasing-style attacks. We
also provide an empirically optimized version that further removes the
requirement for dynamical median estimation for better sampling efficiency.
Experimental results show that PMark consistently outperforms existing SWM
baselines in both text quality and robustness, offering a more effective
paradigm for detecting machine-generated text. Our code will be released at
[this URL](https://github.com/PMark-repo/PMark).

</details>


### [29] [Emerging Paradigms for Securing Federated Learning Systems](https://arxiv.org/abs/2509.21147)
*Amr Akmal Abouelmagd,Amr Hilal*

Main category: cs.CR

TL;DR: 这篇论文综述了联邦学习中新兴的隐私保护技术，包括TEEs、PUFs、QC、CBE、NC和SI等，分析了它们在FL流程中的适用性、优缺点，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习的隐私保护技术（如MPC、HE、DP）存在计算成本高和可扩展性有限的问题，需要探索更高效的新方法。

Method: 采用文献综述方法，系统评估六种新兴隐私保护范式在联邦学习中的应用潜力、技术特点和实践考量。

Result: 识别了各种新兴技术的优势与局限性，为不同场景下的隐私保护方案选择提供了指导框架。

Conclusion: 提出了联邦学习隐私保护领域的开放挑战和研究方向，为构建安全可扩展的FL系统提供了详细路线图。

Abstract: Federated Learning (FL) facilitates collaborative model training while
keeping raw data decentralized, making it a conduit for leveraging the power of
IoT devices while maintaining privacy of the locally collected data. However,
existing privacy- preserving techniques present notable hurdles. Methods such
as Multi-Party Computation (MPC), Homomorphic Encryption (HE), and Differential
Privacy (DP) often incur high compu- tational costs and suffer from limited
scalability. This survey examines emerging approaches that hold promise for
enhancing both privacy and efficiency in FL, including Trusted Execution
Environments (TEEs), Physical Unclonable Functions (PUFs), Quantum Computing
(QC), Chaos-Based Encryption (CBE), Neuromorphic Computing (NC), and Swarm
Intelligence (SI). For each paradigm, we assess its relevance to the FL
pipeline, outlining its strengths, limitations, and practical considerations.
We conclude by highlighting open challenges and prospective research avenues,
offering a detailed roadmap for advancing secure and scalable FL systems.

</details>
