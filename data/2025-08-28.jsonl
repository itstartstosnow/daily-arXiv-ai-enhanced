{"id": "2508.19250", "categories": ["cs.CR", "cs.DM", "math.NT", "quant-ph", "94A60, 68Q12, 06B99, 81P94, 60E15", "E.3; F.2.2; G.2.0; B.8.0"], "pdf": "https://arxiv.org/pdf/2508.19250", "abs": "https://arxiv.org/abs/2508.19250", "authors": ["Ruopengyu Xu", "Chenglian Liu"], "title": "Tight Quantum-Security Bounds and Parameter Optimization for SPHINCS+ and NTRU", "comment": "15 pages, 2tables", "summary": "The imminent threat of quantum computing necessitates quantum-resistant\ncryptosystems. This paper establishes tight security bounds for two NIST PQC\nfinalists: SPHINCS+ (hash-based) and NTRU (lattice-based). Our key\ncontributions include: (1) A quantum attack model incorporating decoherence\neffects ($\\tau_d$) and parallelization limits; (2) Improved entropy\nconcentration inequalities reducing SPHINCS+ parameters by 15-20\\%; (3)\nOptimized NTRU lattice parameters via quantum lattice entropy $H_Q(\\Lambda)$;\n(4) Tightened NTRU-to-LWE reduction with polynomial-factor improvement.\nTheoretical results demonstrate significant security enhancement over existing\nconstructions, providing implementable parameters for standardization."}
{"id": "2508.19267", "categories": ["cs.CR", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.19267", "abs": "https://arxiv.org/abs/2508.19267", "authors": ["Sai Teja Reddy Adapala", "Yashwanth Reddy Alugubelly"], "title": "The Aegis Protocol: A Foundational Security Framework for Autonomous AI Agents", "comment": "10 pages, 3 figures, 3 tables. Source compiled with pdfLaTeX;\n  bibliography included via prebuilt main.bbl. Code repository: available in\n  paper", "summary": "The proliferation of autonomous AI agents marks a paradigm shift toward\ncomplex, emergent multi-agent systems. This transition introduces systemic\nsecurity risks, including control-flow hijacking and cascading failures, that\ntraditional cybersecurity paradigms are ill-equipped to address. This paper\nintroduces the Aegis Protocol, a layered security framework designed to provide\nstrong security guarantees for open agentic ecosystems. The protocol integrates\nthree technological pillars: (1) non-spoofable agent identity via W3C\nDecentralized Identifiers (DIDs); (2) communication integrity via\nNIST-standardized post-quantum cryptography (PQC); and (3) verifiable,\nprivacy-preserving policy compliance using the Halo2 zero-knowledge proof (ZKP)\nsystem. We formalize an adversary model extending Dolev-Yao for agentic threats\nand validate the protocol against the STRIDE framework. Our quantitative\nevaluation used a discrete-event simulation, calibrated against cryptographic\nbenchmarks, to model 1,000 agents. The simulation showed a 0 percent success\nrate across 20,000 attack trials. For policy verification, analysis of the\nsimulation logs reported a median proof-generation latency of 2.79 seconds,\nestablishing a performance baseline for this class of security. While the\nevaluation is simulation-based and early-stage, it offers a reproducible\nbaseline for future empirical studies and positions Aegis as a foundation for\nsafe, scalable autonomous AI."}
{"id": "2508.19273", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19273", "abs": "https://arxiv.org/abs/2508.19273", "authors": ["Tongxi Wu", "Chenwei Xu", "Jin Yang"], "title": "MixGAN: A Hybrid Semi-Supervised and Generative Approach for DDoS Detection in Cloud-Integrated IoT Networks", "comment": null, "summary": "The proliferation of cloud-integrated IoT systems has intensified exposure to\nDistributed Denial of Service (DDoS) attacks due to the expanded attack\nsurface, heterogeneous device behaviors, and limited edge protection. However,\nDDoS detection in this context remains challenging because of complex traffic\ndynamics, severe class imbalance, and scarce labeled data. While recent methods\nhave explored solutions to address class imbalance, many still struggle to\ngeneralize under limited supervision and dynamic traffic conditions. To\novercome these challenges, we propose MixGAN, a hybrid detection method that\nintegrates conditional generation, semi-supervised learning, and robust feature\nextraction. Specifically, to handle complex temporal traffic patterns, we\ndesign a 1-D WideResNet backbone composed of temporal convolutional layers with\nresidual connections, which effectively capture local burst patterns in traffic\nsequences. To alleviate class imbalance and label scarcity, we use a pretrained\nCTGAN to generate synthetic minority-class (DDoS attack) samples that\ncomplement unlabeled data. Furthermore, to mitigate the effect of noisy\npseudo-labels, we introduce a MixUp-Average-Sharpen (MAS) strategy that\nconstructs smoothed and sharpened targets by averaging predictions over\naugmented views and reweighting them towards high-confidence classes.\nExperiments on NSL-KDD, BoT-IoT, and CICIoT2023 demonstrate that MixGAN\nachieves up to 2.5% higher accuracy and 4% improvement in both TPR and TNR\ncompared to state-of-the-art methods, confirming its robustness in large-scale\nIoT-cloud environments. The source code is publicly available at\nhttps://github.com/0xCavaliers/MixGAN."}
{"id": "2508.19278", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19278", "abs": "https://arxiv.org/abs/2508.19278", "authors": ["Konur Tholl", "Mariam El Mezouar", "Ranwa Al Mallah"], "title": "Towards Production-Worthy Simulation for Autonomous Cyber Operations", "comment": null, "summary": "Simulated environments have proven invaluable in Autonomous Cyber Operations\n(ACO) where Reinforcement Learning (RL) agents can be trained without the\ncomputational overhead of emulation. These environments must accurately\nrepresent cybersecurity scenarios while producing the necessary signals to\nsupport RL training. In this study, we present a framework where we first\nextend CybORG's Cage Challenge 2 environment by implementing three new actions:\nPatch, Isolate, and Unisolate, to better represent the capabilities available\nto human operators in real-world settings. We then propose a design for agent\ndevelopment where we modify the reward signals and the agent's feature space to\nenhance training performance. To validate these modifications, we train DQN and\nPPO agents in the updated environment. Our study demonstrates that CybORG can\nbe extended with additional realistic functionality, while maintaining its\nability to generate informative training signals for RL agents."}
{"id": "2508.19281", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19281", "abs": "https://arxiv.org/abs/2508.19281", "authors": ["Aoun E Muhammad", "Kin Choong Yow", "Jamel Baili", "Yongwon Cho", "Yunyoung Nam"], "title": "CORTEX: Composite Overlay for Risk Tiering and Exposure in Operational AI Systems", "comment": null, "summary": "As the deployment of Artificial Intelligence (AI) systems in high-stakes\nsectors - like healthcare, finance, education, justice, and infrastructure has\nincreased - the possibility and impact of failures of these systems have\nsignificantly evolved from being a theoretical possibility to practical\nrecurring, systemic risk. This paper introduces CORTEX (Composite Overlay for\nRisk Tiering and Exposure), a multi-layered risk scoring framework proposed to\nassess and score AI system vulnerabilities, developed on empirical analysis of\nover 1,200 incidents documented in the AI Incident Database (AIID), CORTEX\ncategorizes failure modes into 29 technical vulnerability groups. Each\nvulnerability is scored through a five-tier architecture that combines: (1)\nutility-adjusted Likelihood x Impact calculations; (2) governance + contextual\noverlays aligned with regulatory frameworks, such as the EU AI Act, NIST RMF,\nOECD principles; (3) technical surface scores, covering exposure vectors like\ndrift, traceability, and adversarial risk; (4) environmental and residual\nmodifiers tailored to context of where these systems are being deployed to use;\nand (5) a final layered assessment via Bayesian risk aggregation and Monte\nCarlo simulation to model volatility and long-tail risks. The resulting\ncomposite score can be operationalized across AI risk registers, model audits,\nconformity checks, and dynamic governance dashboards."}
{"id": "2508.19283", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19283", "abs": "https://arxiv.org/abs/2508.19283", "authors": ["Mark Dorsett", "Scott Man", "Tim Koussas"], "title": "Rethinking Denial-of-Service: A Conditional Taxonomy Unifying Availability and Sustainability Threats", "comment": "7 pages, 3 figures, 3 tables,", "summary": "This paper proposes a unified, condition-based framework for classifying both\nlegacy and cloud-era denial-of-service (DoS) attacks. The framework comprises\nthree interrelated models: a formal conditional tree taxonomy, a hierarchical\nlattice structure based on order theory, and a conceptual Venn diagram. At its\ncore, the taxonomy introduces six observable conditions (C0-C5) grounded in\nreal-world attack behaviours, including source distribution, traffic volume,\ninfrastructure targeting, and financial exploitation. These conditions enable\nconsistent classification of known attacks-such as DoS, DDoS, LDoS, LDDoS,\nEDoS, DoW, and DDoW, while supporting identification of emerging or hybrid\nvariants. The lattice structure captures the cumulative satisfaction of\nconditions, allowing hierarchical reasoning across denial attack classes. The\nVenn diagram highlights conceptual overlaps between availability- and\nsustainability-focused attacks, improving comparative insight. Together, these\nmodels provide a robust analytical lens for threat modeling, mitigation\nstrategy design, and attacker intent classification. The framework is\nparticularly relevant in cloud-native and serverless environments, where\nsustainability-based attacks are increasingly impactful yet under-recognised.\nIts extensibility also permits future integration of socio-technical or\nbehavioural dimensions. By offering a structured taxonomy with theoretical\ngrounding and real-world applicability, this work advances denial attack\ncomprehension and equips defenders, researchers, and cloud architects with a\nshared vocabulary for interpreting and mitigating evolving threat vectors."}
{"id": "2508.19284", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19284", "abs": "https://arxiv.org/abs/2508.19284", "authors": ["Mark Dorsett", "Scott Mann", "Jabed Chowdhury", "Abdun Mahmood"], "title": "A Comprehensive Review of Denial of Wallet Attacks in Serverless Architectures", "comment": "12 pages, 2 figures, 5 tables", "summary": "The Denial of Wallet (DoW) attack poses a unique and growing threat to\nserverless architectures that rely on Function-as-a-Service (FaaS) models,\nexploiting the cost structure of pay-as-you-go billing to financially burden\napplication owners. Unlike traditional Denial of Service (DoS) attacks, which\naim to exhaust resources and disrupt service availability, DoW attacks focus on\nescalating costs without impacting service operation. This review traces the\nevolution of DoW research, from initial awareness and attack classification to\nadvancements in detection and mitigation strategies. Key developments include\nthe categorisation of attack types-such as Blast DDoW, Continual Inconspicuous\nDDoW, and Background Chained DDoW-and the creation of simulation tools like\nDoWTS, which enable safe experimentation and data generation. Recent\nadvancements highlight machine learning approaches, including systems like\nGringotts and DoWNet, which leverage deep learning and anomaly detection to\nidentify malicious traffic patterns. Although substantial progress has been\nmade, challenges persist, notably the lack of real-world data and the need for\nadaptive billing models. This is the first comprehensive literature review\ndedicated strictly to Denial of Wallet attacks, providing an in-depth analysis\nof their financial impacts, attack techniques, mitigation strategies, and\ndetection mechanisms within serverless computing. The paper also presents the\nfirst detailed examination of simulation and data generation tools used for DoW\nresearch, addressing a critical gap in existing cybersecurity literature. By\nsynthesising these key areas, this study serves as a foundational resource for\nfuture research and industry efforts in securing pay-as-you-go cloud\nenvironments."}
{"id": "2508.19286", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19286", "abs": "https://arxiv.org/abs/2508.19286", "authors": ["Zhan Shi", "Yefeng Yuan", "Yuhong Liu", "Liang Cheng", "Yi Fang"], "title": "RL-Finetuned LLMs for Privacy-Preserving Synthetic Rewriting", "comment": null, "summary": "The performance of modern machine learning systems depends on access to\nlarge, high-quality datasets, often sourced from user-generated content or\nproprietary, domain-specific corpora. However, these rich datasets inherently\ncontain sensitive personal information, raising significant concerns about\nprivacy, data security, and compliance with regulatory frameworks. While\nconventional anonymization techniques can remove explicit identifiers, such\nremoval may result in performance drop in downstream machine learning tasks.\nMore importantly, simple anonymization may not be effective against inference\nattacks that exploit implicit signals such as writing style, topical focus, or\ndemographic cues, highlighting the need for more robust privacy safeguards\nduring model training. To address the challenging issue of balancing user\nprivacy and data utility, we propose a reinforcement learning framework that\nfine-tunes a large language model (LLM) using a composite reward function that\njointly optimizes for explicit and implicit privacy, semantic fidelity, and\noutput diversity. To effectively capture population level regularities, the\nprivacy reward combines semantic cues with structural patterns derived from a\nminimum spanning tree (MST) over latent representations. By modeling these\nprivacy-sensitive signals in their distributional context, the proposed\napproach guides the model to generate synthetic rewrites that preserve utility\nwhile mitigating privacy risks. Empirical results show that the proposed method\nsignificantly enhances author obfuscation and privacy metrics without degrading\nsemantic quality, providing a scalable and model-agnostic solution for privacy\npreserving data generation in the era of large language models."}
{"id": "2508.19287", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19287", "abs": "https://arxiv.org/abs/2508.19287", "authors": ["Zhuotao Lian", "Weiyu Wang", "Qingkui Zeng", "Toru Nakanishi", "Teruaki Kitasuka", "Chunhua Su"], "title": "Prompt-in-Content Attacks: Exploiting Uploaded Inputs to Hijack LLM Behavior", "comment": null, "summary": "Large Language Models (LLMs) are widely deployed in applications that accept\nuser-submitted content, such as uploaded documents or pasted text, for tasks\nlike summarization and question answering. In this paper, we identify a new\nclass of attacks, prompt in content injection, where adversarial instructions\nare embedded in seemingly benign inputs. When processed by the LLM, these\nhidden prompts can manipulate outputs without user awareness or system\ncompromise, leading to biased summaries, fabricated claims, or misleading\nsuggestions. We demonstrate the feasibility of such attacks across popular\nplatforms, analyze their root causes including prompt concatenation and\ninsufficient input isolation, and discuss mitigation strategies. Our findings\nreveal a subtle yet practical threat in real-world LLM workflows."}
{"id": "2508.19288", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19288", "abs": "https://arxiv.org/abs/2508.19288", "authors": ["Kyohei Shiomi", "Zhuotao Lian", "Toru Nakanishi", "Teruaki Kitasuka"], "title": "Tricking LLM-Based NPCs into Spilling Secrets", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to generate dynamic\ndialogue for game NPCs. However, their integration raises new security\nconcerns. In this study, we examine whether adversarial prompt injection can\ncause LLM-based NPCs to reveal hidden background secrets that are meant to\nremain undisclosed."}
{"id": "2508.19292", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19292", "abs": "https://arxiv.org/abs/2508.19292", "authors": ["Xi Wang", "Songlei Jian", "Shasha Li", "Xiaopeng Li", "Bin Ji", "Jun Ma", "Xiaodong Liu", "Jing Wang", "Feilong Bao", "Jianfeng Zhang", "Baosheng Wang", "Jie Yu"], "title": "Stand on The Shoulders of Giants: Building JailExpert from Previous Attack Experience", "comment": "18 pages, EMNLP 2025 Main Conference", "summary": "Large language models (LLMs) generate human-aligned content under certain\nsafety constraints. However, the current known technique ``jailbreak prompt''\ncan circumvent safety-aligned measures and induce LLMs to output malicious\ncontent. Research on Jailbreaking can help identify vulnerabilities in LLMs and\nguide the development of robust security frameworks. To circumvent the issue of\nattack templates becoming obsolete as models evolve, existing methods adopt\niterative mutation and dynamic optimization to facilitate more automated\njailbreak attacks. However, these methods face two challenges: inefficiency and\nrepetitive optimization, as they overlook the value of past attack experiences.\nTo better integrate past attack experiences to assist current jailbreak\nattempts, we propose the \\textbf{JailExpert}, an automated jailbreak framework,\nwhich is the first to achieve a formal representation of experience structure,\ngroup experiences based on semantic drift, and support the dynamic updating of\nthe experience pool. Extensive experiments demonstrate that JailExpert\nsignificantly improves both attack effectiveness and efficiency. Compared to\nthe current state-of-the-art black-box jailbreak methods, JailExpert achieves\nan average increase of 17\\% in attack success rate and 2.7 times improvement in\nattack efficiency. Our implementation is available at\n\\href{https://github.com/xiZAIzai/JailExpert}{XiZaiZai/JailExpert}"}
{"id": "2508.19309", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.19309", "abs": "https://arxiv.org/abs/2508.19309", "authors": ["Peng Gu", "Shuangchen Li", "Dylan Stow", "Russell Barnes", "Liu Liu", "Yuan Xie", "Eren Kursshan"], "title": "Leveraging 3D Technologies for Hardware Security: Opportunities and Challenges", "comment": null, "summary": "3D die stacking and 2.5D interposer design are promising technologies to\nimprove integration density, performance and cost. Current approaches face\nserious issues in dealing with emerging security challenges such as side\nchannel attacks, hardware trojans, secure IC manufacturing and IP piracy. By\nutilizing intrinsic characteristics of 2.5D and 3D technologies, we propose\nnovel opportunities in designing secure systems. We present: (i) a 3D\narchitecture for shielding side-channel information; (ii) split fabrication\nusing active interposers; (iii) circuit camouflage on monolithic 3D IC, and\n(iv) 3D IC-based security processing-in-memory (PIM). Advantages and challenges\nof these designs are discussed, showing that the new designs can improve\nexisting countermeasures against security threats and further provide new\nsecurity features."}
{"id": "2508.19321", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19321", "abs": "https://arxiv.org/abs/2508.19321", "authors": ["Kehao Miao", "Xiaolong Jin"], "title": "An Investigation on Group Query Hallucination Attacks", "comment": null, "summary": "With the widespread use of large language models (LLMs), understanding their\npotential failure modes during user interactions is essential. In practice,\nusers often pose multiple questions in a single conversation with LLMs.\nTherefore, in this study, we propose Group Query Attack, a technique that\nsimulates this scenario by presenting groups of queries to LLMs simultaneously.\nWe investigate how the accumulated context from consecutive prompts influences\nthe outputs of LLMs. Specifically, we observe that Group Query Attack\nsignificantly degrades the performance of models fine-tuned on specific tasks.\nMoreover, we demonstrate that Group Query Attack induces a risk of triggering\npotential backdoors of LLMs. Besides, Group Query Attack is also effective in\ntasks involving reasoning, such as mathematical reasoning and code generation\nfor pre-trained and aligned models."}
{"id": "2508.19323", "categories": ["cs.CR", "cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.19323", "abs": "https://arxiv.org/abs/2508.19323", "authors": ["Ms. Preeti P. Bhatt", "Rakesh R. Savant"], "title": "A Technical Review on Comparison and Estimation of Steganographic Tools", "comment": "20", "summary": "Steganography is technique of hiding a data under cover media using different\nsteganography tools. Image steganography is hiding of data\n(Text/Image/Audio/Video) under a cover as Image. This review paper presents\nclassification of image steganography and the comparison of various Image\nsteganography tools using different image formats. Analyzing numerous tools on\nthe basis of Image features and extracting the best one. Some of the tools\navailable in the market were selected based on the frequent use; these tools\nwere tested using the same input on all of them. Specific text was embedded\nwithin all host images for each of the six Steganography tools selected. The\nresults of the experiment reveal that all the six tools were relatively\nperforming at the same level, though some software performs better than others\nthrough efficiency. And it was based on the image features like size,\ndimensions, and pixel value and histogram differentiation."}
{"id": "2508.19368", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19368", "abs": "https://arxiv.org/abs/2508.19368", "authors": ["Luqman Muhammad Zagi", "Girindro Pringgo Digdo", "Wervyan Shalannanda"], "title": "Just Dork and Crawl: Measuring Illegal Online Gambling Defacement in Indonesian Websites", "comment": "6 pages, 2 figures, IEEE Conference", "summary": "This study investigates the defacement of Indonesian websites by actors\npromoting illegal online gambling. Using a lightweight methodology that\ncombines keyword-driven dorking with systematic crawling, we identified 453\ndefaced webpages within one month. Although dorking alone yielded a false\npositive rate of approximately 20.3\\%, the integration of crawling and\nkeyword-counting enabled reliable differentiation between true and false\npositives. Our measurements revealed diverse defacement behaviors, including\nrepeat defacements (150 cases), fixed instances (129), keyword modifications\n(55), and redirections or hidden URL injections. In total, 8,837 unique\nthird-party URLs spanning 5,930 domains were captured, with a small subset\nrecurring across multiple sites. Website responses were inconsistent, with an\naverage reaction time of 75.3 hours. These findings demonstrate that simple,\nreproducible techniques can provide meaningful insights into the scale,\npersistence, and dynamics of defacement, highlighting the importance of\ncontinuous measurement for strengthening defenses against online gambling\nactivities."}
{"id": "2508.19395", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19395", "abs": "https://arxiv.org/abs/2508.19395", "authors": ["Fabian Aude Steen", "Daniel Assani Shabani"], "title": "A NIS2 pan-European registry for identifying and classifying essential and important entities", "comment": null, "summary": "The NIS2 Directive establishes a common cybersecurity governance model across\nthe European Union, requiring member states to identify, classify, and\nsupervise essential and important entities. As part of a broader governance\nnetwork, member states are also obligated to notify the European Commission,\nthe Cooperation Group, and ENISA about their cybersecurity infrastructure\nlandscape. This thesis presents an analysis of the NIS2 Directive in this\ncontext and translates its provisions into concrete technical requirements.\nThese requirements inform the design and implementation of a modular, legally\ngrounded registry system intended to support competent authorities across the\nEU in meeting their obligations. Using the Design Science Research methodology,\nthe thesis transforms complex legal provisions into structured workflows,\ndeterministic classification algorithms, and interactive dashboards. The\nresulting system automates key regulatory processes, including entity\nregistration, classification, and notification, while enabling context-aware\nsupervision and reducing administrative burden. It supports both automated and\nmanual registration methods and introduces a contextual labeling system to\nhandle edge cases, risk factors, and cross-directive dependencies. Although\ndeveloped for the Norwegian regulatory ecosystem, the system is designed for\nadaptation by other member states with minimal modification. This thesis\ncontributes a reusable framework that bridges legal interpretation and\ntechnical implementation, offering a scalable solution for national and\nEU-level NIS2 cybersecurity governance. It also identifies key limitations and\noutlines opportunities for future research and development."}
{"id": "2508.19430", "categories": ["cs.CR", "cs.FL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.19430", "abs": "https://arxiv.org/abs/2508.19430", "authors": ["Kangfeng Ye", "Roberto Metere", "Jim Woodcock", "Poonam Yadav"], "title": "Formal Verification of Physical Layer Security Protocols for Next-Generation Communication Networks", "comment": "Submitted to ICFEM2025; 23 pages, 2 tables, and 6 figures", "summary": "Formal verification is crucial for ensuring the robustness of security\nprotocols against adversarial attacks. The Needham-Schroeder protocol, a\nfoundational authentication mechanism, has been extensively studied, including\nits integration with Physical Layer Security (PLS) techniques such as\nwatermarking and jamming. Recent research has used ProVerif to verify these\nmechanisms in terms of secrecy. However, the ProVerif-based approach limits the\nability to improve understanding of security beyond verification results. To\novercome these limitations, we re-model the same protocol using an Isabelle\nformalism that generates sound animation, enabling interactive and automated\nformal verification of security protocols. Our modelling and verification\nframework is generic and highly configurable, supporting both cryptography and\nPLS. For the same protocol, we have conducted a comprehensive analysis (secrecy\nand authenticity in four different eavesdropper locations under both passive\nand active attacks) using our new web interface. Our findings not only\nsuccessfully reproduce and reinforce previous results on secrecy but also\nreveal an uncommon but expected outcome: authenticity is preserved across all\nexamined scenarios, even in cases where secrecy is compromised. We have\nproposed a PLS-based Diffie-Hellman protocol that integrates watermarking and\njamming, and our analysis shows that it is secure for deriving a session key\nwith required authentication. These highlight the advantages of our novel\napproach, demonstrating its robustness in formally verifying security\nproperties beyond conventional methods."}
{"id": "2508.19450", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19450", "abs": "https://arxiv.org/abs/2508.19450", "authors": ["Elvin Li", "Onat Gungor", "Zhengli Shang", "Tajana Rosing"], "title": "CITADEL: Continual Anomaly Detection for Enhanced Learning in IoT Intrusion Detection", "comment": "Under review at IEEE IoTJ", "summary": "The Internet of Things (IoT), with its high degree of interconnectivity and\nlimited computational resources, is particularly vulnerable to a wide range of\ncyber threats. Intrusion detection systems (IDS) have been extensively studied\nto enhance IoT security, and machine learning-based IDS (ML-IDS) show\nconsiderable promise for detecting malicious activity. However, their\neffectiveness is often constrained by poor adaptability to emerging threats and\nthe issue of catastrophic forgetting during continuous learning. To address\nthese challenges, we propose CITADEL, a self-supervised continual learning\nframework designed to extract robust representations from benign data while\npreserving long-term knowledge through optimized memory consolidation\nmechanisms. CITADEL integrates a tabular-to-image transformation module, a\nmemory-aware masked autoencoder for self-supervised representation learning,\nand a novelty detection component capable of identifying anomalies without\ndependence on labeled attack data. Our design enables the system to\nincrementally adapt to emerging behaviors while retaining its ability to detect\npreviously observed threats. Experiments on multiple intrusion datasets\ndemonstrate that CITADEL achieves up to a 72.9% improvement over the VAE-based\nlifelong anomaly detector (VLAD) in key detection and retention metrics,\nhighlighting its effectiveness in dynamic IoT environments."}
{"id": "2508.19456", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19456", "abs": "https://arxiv.org/abs/2508.19456", "authors": ["Cagla Ipek Kocal", "Onat Gungor", "Tajana Rosing", "Baris Aksanli"], "title": "ReLATE+: Unified Framework for Adversarial Attack Detection, Classification, and Resilient Model Selection in Time-Series Classification", "comment": "Under review at IEEE TSMC Journal. arXiv admin note: text overlap\n  with arXiv:2503.07882", "summary": "Minimizing computational overhead in time-series classification, particularly\nin deep learning models, presents a significant challenge due to the high\ncomplexity of model architectures and the large volume of sequential data that\nmust be processed in real time. This challenge is further compounded by\nadversarial attacks, emphasizing the need for resilient methods that ensure\nrobust performance and efficient model selection. To address this challenge, we\npropose ReLATE+, a comprehensive framework that detects and classifies\nadversarial attacks, adaptively selects deep learning models based on\ndataset-level similarity, and thus substantially reduces retraining costs\nrelative to conventional methods that do not leverage prior knowledge, while\nmaintaining strong performance. ReLATE+ first checks whether the incoming data\nis adversarial and, if so, classifies the attack type, using this insight to\nidentify a similar dataset from a repository and enable the reuse of the\nbest-performing associated model. This approach ensures strong performance\nwhile reducing the need for retraining, and it generalizes well across\ndifferent domains with varying data distributions and feature spaces.\nExperiments show that ReLATE+ reduces computational overhead by an average of\n77.68%, enhancing adversarial resilience and streamlining robust model\nselection, all without sacrificing performance, within 2.02% of Oracle."}
{"id": "2508.19465", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19465", "abs": "https://arxiv.org/abs/2508.19465", "authors": ["Onyinye Okoye"], "title": "Addressing Weak Authentication like RFID, NFC in EVs and EVCs using AI-powered Adaptive Authentication", "comment": "Research paper exploring AI-driven adaptive authentication in the\n  Electric Vehicle industry", "summary": "The rapid expansion of the Electric Vehicles (EVs) and Electric Vehicle\nCharging Systems (EVCs) has introduced new cybersecurity challenges,\nspecifically in authentication protocols that protect vehicles, users, and\nenergy infrastructure. Although widely adopted for convenience, traditional\nauthentication mechanisms like Radio Frequency Identification (RFID) and Near\nField Communication (NFC) rely on static identifiers and weak encryption,\nmaking them highly vulnerable to attack vectors such as cloning, relay attacks,\nand signal interception. This study explores an AI-powered adaptive\nauthentication framework designed to overcome these shortcomings by integrating\nmachine learning, anomaly detection, behavioral analytics, and contextual risk\nassessment. Grounded in the principles of Zero Trust Architecture, the proposed\nframework emphasizes continuous verification, least privilege access, and\nsecure communication. Through a comprehensive literature review, this research\nevaluates current vulnerabilities and highlights AI-driven solutions to provide\na scalable, resilient, and proactive defense. Ultimately, the research findings\nconclude that adopting AI-powered adaptive authentication is a strategic\nimperative for securing the future of electric mobility and strengthening\ndigital trust across the ecosystem. Keywords: weak authentication, RFID, NFC,\nML, AI-powered adaptive authentication, relay attacks, cloning, eavesdropping,\nMITM attacks, Zero Trust Architecture"}
{"id": "2508.19472", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19472", "abs": "https://arxiv.org/abs/2508.19472", "authors": ["Kyler Katz", "Sara Moshtari", "Ibrahim Mujhid", "Mehdi Mirakhorli", "Derek Garcia"], "title": "SIExVulTS: Sensitive Information Exposure Vulnerability Detection System using Transformer Models and Static Analysis", "comment": null, "summary": "Sensitive Information Exposure (SIEx) vulnerabilities (CWE-200) remain a\npersistent and under-addressed threat across software systems, often leading to\nserious security breaches. Existing detection tools rarely target the diverse\nsubcategories of CWE-200 or provide context-aware analysis of code-level data\nflows.\n  Aims: This paper aims to present SIExVulTS, a novel vulnerability detection\nsystem that integrates transformer-based models with static analysis to\nidentify and verify sensitive information exposure in Java applications.\n  Method: SIExVulTS employs a three-stage architecture: (1) an Attack Surface\nDetection Engine that uses sentence embeddings to identify sensitive variables,\nstrings, comments, and sinks; (2) an Exposure Analysis Engine that instantiates\nCodeQL queries aligned with the CWE-200 hierarchy; and (3) a Flow Verification\nEngine that leverages GraphCodeBERT to semantically validate source-to-sink\nflows. We evaluate SIExVulTS using three curated datasets, including real-world\nCVEs, a benchmark set of synthetic CWE-200 examples, and labeled flows from 31\nopen-source projects.\n  Results: The Attack Surface Detection Engine achieved an average F1 score\ngreater than 93\\%, the Exposure Analysis Engine achieved an F1 score of\n85.71\\%, and the Flow Verification Engine increased precision from 22.61\\% to\n87.23\\%. Moreover, SIExVulTS successfully uncovered six previously unknown CVEs\nin major Apache projects.\n  Conclusions: The results demonstrate that SIExVulTS is effective and\npractical for improving software security against sensitive data exposure,\naddressing limitations of existing tools in detecting and verifying CWE-200\nvulnerabilities."}
{"id": "2508.19493", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19493", "abs": "https://arxiv.org/abs/2508.19493", "authors": ["Zhixin Lin", "Jungang Li", "Shidong Pan", "Yibo Shi", "Yue Yao", "Dongliang Xu"], "title": "Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents", "comment": null, "summary": "Smartphones bring significant convenience to users but also enable devices to\nextensively record various types of personal information. Existing smartphone\nagents powered by Multimodal Large Language Models (MLLMs) have achieved\nremarkable performance in automating different tasks. However, as the cost,\nthese agents are granted substantial access to sensitive users' personal\ninformation during this operation. To gain a thorough understanding of the\nprivacy awareness of these agents, we present the first large-scale benchmark\nencompassing 7,138 scenarios to the best of our knowledge. In addition, for\nprivacy context in scenarios, we annotate its type (e.g., Account Credentials),\nsensitivity level, and location. We then carefully benchmark seven available\nmainstream smartphone agents. Our results demonstrate that almost all\nbenchmarked agents show unsatisfying privacy awareness (RA), with performance\nremaining below 60% even with explicit hints. Overall, closed-source agents\nshow better privacy ability than open-source ones, and Gemini 2.0-flash\nachieves the best, achieving an RA of 67%. We also find that the agents'\nprivacy detection capability is highly related to scenario sensitivity level,\ni.e., the scenario with a higher sensitivity level is typically more\nidentifiable. We hope the findings enlighten the research community to rethink\nthe unbalanced utility-privacy tradeoff about smartphone agents. Our code and\nbenchmark are available at https://zhixin-l.github.io/SAPA-Bench."}
{"id": "2508.19500", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19500", "abs": "https://arxiv.org/abs/2508.19500", "authors": ["David Noever"], "title": "Servant, Stalker, Predator: How An Honest, Helpful, And Harmless (3H) Agent Unlocks Adversarial Skills", "comment": null, "summary": "This paper identifies and analyzes a novel vulnerability class in Model\nContext Protocol (MCP) based agent systems. The attack chain describes and\ndemonstrates how benign, individually authorized tasks can be orchestrated to\nproduce harmful emergent behaviors. Through systematic analysis using the MITRE\nATLAS framework, we demonstrate how 95 agents tested with access to multiple\nservices-including browser automation, financial analysis, location tracking,\nand code deployment-can chain legitimate operations into sophisticated attack\nsequences that extend beyond the security boundaries of any individual service.\nThese red team exercises survey whether current MCP architectures lack\ncross-domain security measures necessary to detect or prevent a large category\nof compositional attacks. We present empirical evidence of specific attack\nchains that achieve targeted harm through service orchestration, including data\nexfiltration, financial manipulation, and infrastructure compromise. These\nfindings reveal that the fundamental security assumption of service isolation\nfails when agents can coordinate actions across multiple domains, creating an\nexponential attack surface that grows with each additional capability. This\nresearch provides a barebones experimental framework that evaluate not whether\nagents can complete MCP benchmark tasks, but what happens when they complete\nthem too well and optimize across multiple services in ways that violate human\nexpectations and safety constraints. We propose three concrete experimental\ndirections using the existing MCP benchmark suite."}
{"id": "2508.19525", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19525", "abs": "https://arxiv.org/abs/2508.19525", "authors": ["Tianshi Xu", "Wen-jie Lu", "Jiangrui Yu", "Chen Yi", "Chenqi Lin", "Runsheng Wang", "Meng Li"], "title": "Breaking the Layer Barrier: Remodeling Private Transformer Inference with Hybrid CKKS and MPC", "comment": "USENIX Security 2025", "summary": "This paper presents an efficient framework for private Transformer inference\nthat combines Homomorphic Encryption (HE) and Secure Multi-party Computation\n(MPC) to protect data privacy. Existing methods often leverage HE for linear\nlayers (e.g., matrix multiplications) and MPC for non-linear layers (e.g.,\nSoftmax activation functions), but the conversion between HE and MPC introduces\nsignificant communication costs. The proposed framework, dubbed BLB, overcomes\nthis by breaking down layers into fine-grained operators and further fusing\nadjacent linear operators, reducing the need for HE/MPC conversions. To manage\nthe increased ciphertext bit width from the fused linear operators, BLB\nproposes the first secure conversion protocol between CKKS and MPC and enables\nCKKS-based computation of the fused operators. Additionally, BLB proposes an\nefficient matrix multiplication protocol for fused computation in Transformers.\nExtensive evaluations on BERT-base, BERT-large, and GPT2-base show that BLB\nachieves a $21\\times$ reduction in communication overhead compared to BOLT\n(S\\&P'24) and a $2\\times$ reduction compared to Bumblebee (NDSS'25), along with\nlatency reductions of $13\\times$ and $1.8\\times$, respectively, when leveraging\nGPU acceleration."}
{"id": "2508.19641", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19641", "abs": "https://arxiv.org/abs/2508.19641", "authors": ["Lincan Li", "Bolin Shen", "Chenxi Zhao", "Yuxiang Sun", "Kaixiang Zhao", "Shirui Pan", "Yushun Dong"], "title": "Intellectual Property in Graph-Based Machine Learning as a Service: Attacks and Defenses", "comment": null, "summary": "Graph-structured data, which captures non-Euclidean relationships and\ninteractions between entities, is growing in scale and complexity. As a result,\ntraining state-of-the-art graph machine learning (GML) models have become\nincreasingly resource-intensive, turning these models and data into invaluable\nIntellectual Property (IP). To address the resource-intensive nature of model\ntraining, graph-based Machine-Learning-as-a-Service (GMLaaS) has emerged as an\nefficient solution by leveraging third-party cloud services for model\ndevelopment and management. However, deploying such models in GMLaaS also\nexposes them to potential threats from attackers. Specifically, while the APIs\nwithin a GMLaaS system provide interfaces for users to query the model and\nreceive outputs, they also allow attackers to exploit and steal model\nfunctionalities or sensitive training data, posing severe threats to the safety\nof these GML models and the underlying graph data. To address these challenges,\nthis survey systematically introduces the first taxonomy of threats and\ndefenses at the level of both GML model and graph-structured data. Such a\ntailored taxonomy facilitates an in-depth understanding of GML IP protection.\nFurthermore, we present a systematic evaluation framework to assess the\neffectiveness of IP protection methods, introduce a curated set of benchmark\ndatasets across various domains, and discuss their application scopes and\nfuture challenges. Finally, we establish an open-sourced versatile library\nnamed PyGIP, which evaluates various attack and defense techniques in GMLaaS\nscenarios and facilitates the implementation of existing benchmark methods. The\nlibrary resource can be accessed at: https://labrai.github.io/PyGIP. We believe\nthis survey will play a fundamental role in intellectual property protection\nfor GML and provide practical recipes for the GML community."}
{"id": "2508.19697", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19697", "abs": "https://arxiv.org/abs/2508.19697", "authors": ["Chao Huang", "Zefeng Zhang", "Juewei Yue", "Quangang Li", "Chuang Zhang", "Tingwen Liu"], "title": "Safety Alignment Should Be Made More Than Just A Few Attention Heads", "comment": null, "summary": "Current safety alignment for large language models(LLMs) continues to present\nvulnerabilities, given that adversarial prompting can effectively bypass their\nsafety measures.Our investigation shows that these safety mechanisms\npredominantly depend on a limited subset of attention heads: removing or\nablating these heads can severely compromise model safety. To identify and\nevaluate these safety-critical components, we introduce RDSHA, a targeted\nablation method that leverages the model's refusal direction to pinpoint\nattention heads mostly responsible for safety behaviors. Further analysis shows\nthat existing jailbreak attacks exploit this concentration by selectively\nbypassing or manipulating these critical attention heads. To address this\nissue, we propose AHD, a novel training strategy designed to promote the\ndistributed encoding of safety-related behaviors across numerous attention\nheads. Experimental results demonstrate that AHD successfully distributes\nsafety-related capabilities across more attention heads. Moreover, evaluations\nunder several mainstream jailbreak attacks show that models trained with AHD\nexhibit considerably stronger safety robustness, while maintaining overall\nfunctional utility."}
{"id": "2508.19714", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19714", "abs": "https://arxiv.org/abs/2508.19714", "authors": ["Subhrojyoti Mukherjee", "Manoranjan Mohanty"], "title": "Addressing Deepfake Issue in Selfie banking through camera based authentication", "comment": null, "summary": "Fake images in selfie banking are increasingly becoming a threat. Previously,\nit was just Photoshop, but now deep learning technologies enable us to create\nhighly realistic fake identities, which fraudsters exploit to bypass biometric\nsystems such as facial recognition in online banking. This paper explores the\nuse of an already established forensic recognition system, previously used for\npicture camera localization, in deepfake detection."}
{"id": "2508.19774", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19774", "abs": "https://arxiv.org/abs/2508.19774", "authors": ["Tong Liu", "Guozhu Meng", "Peng Zhou", "Zizhuang Deng", "Shuaiyin Yao", "Kai Chen"], "title": "The Art of Hide and Seek: Making Pickle-Based Model Supply Chain Poisoning Stealthy Again", "comment": null, "summary": "Pickle deserialization vulnerabilities have persisted throughout Python's\nhistory, remaining widely recognized yet unresolved. Due to its ability to\ntransparently save and restore complex objects into byte streams, many AI/ML\nframeworks continue to adopt pickle as the model serialization protocol despite\nits inherent risks. As the open-source model ecosystem grows, model-sharing\nplatforms such as Hugging Face have attracted massive participation,\nsignificantly amplifying the real-world risks of pickle exploitation and\nopening new avenues for model supply chain poisoning. Although several\nstate-of-the-art scanners have been developed to detect poisoned models, their\nincomplete understanding of the poisoning surface leaves the detection logic\nfragile and allows attackers to bypass them. In this work, we present the first\nsystematic disclosure of the pickle-based model poisoning surface from both\nmodel loading and risky function perspectives. Our research demonstrates how\npickle-based model poisoning can remain stealthy and highlights critical gaps\nin current scanning solutions. On the model loading surface, we identify 22\ndistinct pickle-based model loading paths across five foundational AI/ML\nframeworks, 19 of which are entirely missed by existing scanners. We further\ndevelop a bypass technique named Exception-Oriented Programming (EOP) and\ndiscover 9 EOP instances, 7 of which can bypass all scanners. On the risky\nfunction surface, we discover 133 exploitable gadgets, achieving almost a 100%\nbypass rate. Even against the best-performing scanner, these gadgets maintain\nan 89% bypass rate. By systematically revealing the pickle-based model\npoisoning surface, we achieve practical and robust bypasses against real-world\nscanners. We responsibly disclose our findings to corresponding vendors,\nreceiving acknowledgments and a $6000 bug bounty."}
{"id": "2508.19819", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19819", "abs": "https://arxiv.org/abs/2508.19819", "authors": ["Viktor Valadi", "Mattias Åkesson", "Johan Östman", "Salman Toor", "Andreas Hellander"], "title": "From Research to Reality: Feasibility of Gradient Inversion Attacks in Federated Learning", "comment": "Under review at KDD 2026 (Research Track)", "summary": "Gradient inversion attacks have garnered attention for their ability to\ncompromise privacy in federated learning. However, many studies consider\nattacks with the model in inference mode, where training-time behaviors like\ndropout are disabled and batch normalization relies on fixed statistics. In\nthis work, we systematically analyze how architecture and training behavior\naffect vulnerability, including the first in-depth study of inference-mode\nclients, which we show dramatically simplifies inversion. To assess attack\nfeasibility under more realistic conditions, we turn to clients operating in\nstandard training mode. In this setting, we find that successful attacks are\nonly possible when several architectural conditions are met simultaneously:\nmodels must be shallow and wide, use skip connections, and, critically, employ\npre-activation normalization. We introduce two novel attacks against models in\ntraining-mode with varying attacker knowledge, achieving state-of-the-art\nperformance under realistic training conditions. We extend these efforts by\npresenting the first attack on a production-grade object-detection model. Here,\nto enable any visibly identifiable leakage, we revert to the lenient inference\nmode setting and make multiple architectural modifications to increase model\nvulnerability, with the extent of required changes highlighting the strong\ninherent robustness of such architectures. We conclude this work by offering\nthe first comprehensive mapping of settings, clarifying which combinations of\narchitectural choices and operational modes meaningfully impact privacy. Our\nanalysis provides actionable insight into when models are likely vulnerable,\nwhen they appear robust, and where subtle leakage may persist. Together, these\nfindings reframe how gradient inversion risk should be assessed in future\nresearch and deployment scenarios."}
{"id": "2508.19825", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19825", "abs": "https://arxiv.org/abs/2508.19825", "authors": ["Shaoor Munir", "Nurullah Demir", "Qian Li", "Konrad Kollnig", "Zubair Shafiq"], "title": "Every Keystroke You Make: A Tech-Law Measurement and Analysis of Event Listeners for Wiretapping", "comment": null, "summary": "The privacy community has a long track record of investigating emerging types\nof web tracking techniques. Recent work has focused on compliance of web\ntrackers with new privacy laws such as Europe's GDPR and California's CCPA.\nDespite the growing body of research documenting widespread lack of compliance\nwith new privacy laws, there is a lack of robust enforcement. Different from\nprior work, we conduct a tech-law analysis to map decades-old U.S. laws about\ninterception of electronic communications--so-called wiretapping--to web\ntracking. Bridging the tech-law gap for older wiretapping laws is important and\ntimely because, in cases where legal harm to privacy is proven, they can\nprovide statutory private right of action, are at the forefront of recent\nprivacy enforcement, and could ultimately lead to a meaningful change in the\nweb tracking landscape.\n  In this paper, we focus on a particularly invasive tracking technique: the\nuse of JavaScript event listeners by third-party trackers for real-time\nkeystroke interception on websites. We use an instrumented web browser to crawl\na sample of the top-million websites to investigate the use of event listeners\nthat aligns with the criteria for wiretapping, according to U.S. wiretapping\nlaw at the federal level and in California. We find evidence that 38.52%\nwebsites installed third-party event listeners to intercept keystrokes, and\nthat at least 3.18% websites transmitted intercepted information to a\nthird-party server, which aligns with the criteria for wiretapping. We further\nfind evidence that the intercepted information such as email addresses typed\ninto form fields are used for unsolicited email marketing. Beyond our work that\nmaps the intersection between technical measurement and U.S. wiretapping law,\nadditional future legal research is required to determine when the wiretapping\nobserved in our paper passes the threshold for illegality."}
{"id": "2508.19843", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19843", "abs": "https://arxiv.org/abs/2508.19843", "authors": ["Shuo Shao", "Yiming Li", "Yu He", "Hongwei Yao", "Wenyuan Yang", "Dacheng Tao", "Zhan Qin"], "title": "SoK: Large Language Model Copyright Auditing via Fingerprinting", "comment": null, "summary": "The broad capabilities and substantial resources required to train Large\nLanguage Models (LLMs) make them valuable intellectual property, yet they\nremain vulnerable to copyright infringement, such as unauthorized use and model\ntheft. LLM fingerprinting, a non-intrusive technique that extracts and compares\nthe distinctive features from LLMs to identify infringements, offers a\npromising solution to copyright auditing. However, its reliability remains\nuncertain due to the prevalence of diverse model modifications and the lack of\nstandardized evaluation. In this SoK, we present the first comprehensive study\nof LLM fingerprinting. We introduce a unified framework and formal taxonomy\nthat categorizes existing methods into white-box and black-box approaches,\nproviding a structured overview of the state of the art. We further propose\nLeaFBench, the first systematic benchmark for evaluating LLM fingerprinting\nunder realistic deployment scenarios. Built upon mainstream foundation models\nand comprising 149 distinct model instances, LeaFBench integrates 13\nrepresentative post-development techniques, spanning both parameter-altering\nmethods (e.g., fine-tuning, quantization) and parameter-independent mechanisms\n(e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal the\nstrengths and weaknesses of existing methods, thereby outlining future research\ndirections and critical open problems in this emerging field. The code is\navailable at https://github.com/shaoshuo-ss/LeaFBench."}
{"id": "2508.20051", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20051", "abs": "https://arxiv.org/abs/2508.20051", "authors": ["Prashanth Krishnamurthy", "Ramesh Karri", "Farshad Khorrami"], "title": "SCAMPER -- Synchrophasor Covert chAnnel for Malicious and Protective ERrands", "comment": "12 pages, 10 figures", "summary": "We note that constituent fields (notably the fraction-of-seconds timestamp\nfield) in the data payload structure of the synchrophasor communication\nprotocol (IEEE C37.118 standard) are overprovisioned relative to real-world\nusage and needs, lending themselves to abuse for embedding of covert channels.\nWe develop the SCAMPER (Synchrophasor Covert Channel for Malicious and\nProtective ERrands) framework to exploit these overprovisioned fields for\ncovert communication and show that SCAMPER can be applied for both malicious\n(attack) and protective (defense) purposes. Through modifications of the\ntimestamp field, we demonstrate that SCAMPER enables an attacker to accomplish\nsurreptitious communications between devices in the power system to trigger a\nvariety of malicious actions. These timestamp modifications can be performed\nwithout having any impact on the operation of the power system. However, having\nrecognized the potential for this covert channel, we show that SCAMPER can\ninstead be applied for defensive security purposes as an integrated\ncryptographic data integrity mechanism that can facilitate detection of false\ndata injection (FDI) attacks. We perform experimental studies of the proposed\nmethods on two Hardware-in-the-Loop (HIL) testbeds to demonstrate the\neffectiveness of the proposed SCAMPER framework for both malicious and\nprotective purposes."}
{"id": "2508.20083", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20083", "abs": "https://arxiv.org/abs/2508.20083", "authors": ["Yanbo Dai", "Zhenlan Ji", "Zongjie Li", "Kuan Li", "Shuai Wang"], "title": "Disabling Self-Correction in Retrieval-Augmented Generation via Stealthy Retriever Poisoning", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become a standard approach for\nimproving the reliability of large language models (LLMs). Prior work\ndemonstrates the vulnerability of RAG systems by misleading them into\ngenerating attacker-chosen outputs through poisoning the knowledge base.\nHowever, this paper uncovers that such attacks could be mitigated by the strong\n\\textit{self-correction ability (SCA)} of modern LLMs, which can reject false\ncontext once properly configured. This SCA poses a significant challenge for\nattackers aiming to manipulate RAG systems.\n  In contrast to previous poisoning methods, which primarily target the\nknowledge base, we introduce \\textsc{DisarmRAG}, a new poisoning paradigm that\ncompromises the retriever itself to suppress the SCA and enforce\nattacker-chosen outputs. This compromisation enables the attacker to\nstraightforwardly embed anti-SCA instructions into the context provided to the\ngenerator, thereby bypassing the SCA. To this end, we present a\ncontrastive-learning-based model editing technique that performs localized and\nstealthy edits, ensuring the retriever returns a malicious instruction only for\nspecific victim queries while preserving benign retrieval behavior. To further\nstrengthen the attack, we design an iterative co-optimization framework that\nautomatically discovers robust instructions capable of bypassing prompt-based\ndefenses. We extensively evaluate DisarmRAG across six LLMs and three QA\nbenchmarks. Our results show near-perfect retrieval of malicious instructions,\nwhich successfully suppress SCA and achieve attack success rates exceeding 90\\%\nunder diverse defensive prompts. Also, the edited retriever remains stealthy\nunder several detection methods, highlighting the urgent need for\nretriever-centric defenses."}
