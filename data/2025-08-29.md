<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 20]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [AI Propaganda factories with language models](https://arxiv.org/abs/2508.20186)
*Lukasz Olejnik*

Main category: cs.CR

TL;DR: 小型语言模型可在消费级硬件上实现端到端的AI影响力操作，产生连贯的政治信息，且能自动评估无需人工评分。研究发现人物设定比模型本身更重要，对抗性回复会强化意识形态和极端内容。


<details>
  <summary>Details</summary>
Motivation: 研究AI驱动的自动影响力操作的可行性和影响，特别是小型语言模型在政治宣传中的能力，以及这对网络安全防御策略的启示。

Method: 使用小型语言模型生成政治信息，通过自动评估系统分析信息内容，研究人物设定与模型身份对行为的影响，以及对抗性回复对意识形态强化的作用。

Result: 发现人物设定比模型身份更能解释行为；对抗性回复会增强意识形态一致性和极端内容；完全自动化的影响力内容生产已经可行。

Conclusion: 防御策略应从限制模型访问转向对话中心的检测和破坏活动基础设施，AI操作的一致性反而提供了检测特征。

Abstract: AI-powered influence operations can now be executed end-to-end on commodity
hardware. We show that small language models produce coherent, persona-driven
political messaging and can be evaluated automatically without human raters.
Two behavioural findings emerge. First, persona-over-model: persona design
explains behaviour more than model identity. Second, engagement as a stressor:
when replies must counter-arguments, ideological adherence strengthens and the
prevalence of extreme content increases. We demonstrate that fully automated
influence-content production is within reach of both large and small actors.
Consequently, defence should shift from restricting model access towards
conversation-centric detection and disruption of campaigns and coordination
infrastructure. Paradoxically, the very consistency that enables these
operations also provides a detection signature.

</details>


### [2] [FlowMalTrans: Unsupervised Binary Code Translation for Malware Detection Using Flow-Adapter Architecture](https://arxiv.org/abs/2508.20212)
*Minghao Hu,Junzhe Wang,Weisen Zhao,Qiang Zeng,Lannan Luo*

Main category: cs.CR

TL;DR: 提出基于神经机器翻译和标准化流的跨指令集架构恶意软件检测方法，通过将其他ISA的恶意代码翻译到X86-64架构，利用单一架构训练模型实现多架构检测


<details>
  <summary>Details</summary>
Motivation: 随着针对IoT设备的网络攻击增加，跨多种指令集架构的恶意软件日益增多，但为每个ISA收集和标注足够样本构建数据集成本高昂

Method: 结合神经机器翻译(NMT)和标准化流(NFs)技术，将目标ISA的恶意软件翻译到拥有充足样本的X86-64架构，然后使用在X86-64上训练的模型进行检测

Result: 该方法能够显著减少数据收集工作量，实现使用单一架构训练的模型检测多种指令集架构的恶意软件

Conclusion: 提出的跨架构翻译方法有效解决了多ISA恶意软件检测中的数据稀缺问题，为恶意软件检测提供了新的解决方案

Abstract: Applying deep learning to malware detection has drawn great attention due to
its notable performance. With the increasing prevalence of cyberattacks
targeting IoT devices, there is a parallel rise in the development of malware
across various Instruction Set Architectures (ISAs). It is thus important to
extend malware detection capacity to multiple ISAs. However, training a deep
learning-based malware detection model usually requires a large number of
labeled malware samples. The process of collecting and labeling sufficient
malware samples to build datasets for each ISA is labor-intensive and
time-consuming. To reduce the burden of data collection, we propose to leverage
the ideas of Neural Machine Translation (NMT) and Normalizing Flows (NFs) for
malware detection. Specifically, when dealing with malware in a certain ISA, we
translate it to an ISA with sufficient malware samples (like X86-64). This
allows us to apply a model trained on one ISA to analyze malware from another
ISA. Our approach reduces the data collection effort by enabling malware
detection across multiple ISAs using a model trained on a single ISA.

</details>


### [3] [Robustness Assessment and Enhancement of Text Watermarking for Google's SynthID](https://arxiv.org/abs/2508.20228)
*Xia Han,Qi Li,Jianbing Ni,Mohammad Zulkernine*

Main category: cs.CR

TL;DR: SynGuard是一个混合水印框架，结合语义信息检索和SynthID-Text的概率水印机制，在词汇和语义层面双重嵌入水印，显著提升了对改写攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有SynthID-Text水印方法在保持语义的改写攻击下容易失效，需要开发更鲁棒的水印方案来追踪AI生成文本的来源。

Method: 提出SynGuard混合框架，将语义信息检索(SIR)的语义对齐能力与SynthID-Text的概率水印机制相结合，在词汇和语义两个层面联合嵌入水印。

Result: 实验显示SynGuard在多种攻击场景下平均提高11.1%的F1分数水印恢复率，显著优于SynthID-Text。

Conclusion: 语义感知水印能有效抵抗现实世界中的篡改攻击，为AI生成文本的溯源提供了更可靠的解决方案。

Abstract: Recent advances in LLM watermarking methods such as SynthID-Text by Google
DeepMind offer promising solutions for tracing the provenance of AI-generated
text. However, our robustness assessment reveals that SynthID-Text is
vulnerable to meaning-preserving attacks, such as paraphrasing, copy-paste
modifications, and back-translation, which can significantly degrade watermark
detectability. To address these limitations, we propose SynGuard, a hybrid
framework that combines the semantic alignment strength of Semantic Information
Retrieval (SIR) with the probabilistic watermarking mechanism of SynthID-Text.
Our approach jointly embeds watermarks at both lexical and semantic levels,
enabling robust provenance tracking while preserving the original meaning.
Experimental results across multiple attack scenarios show that SynGuard
improves watermark recovery by an average of 11.1\% in F1 score compared to
SynthID-Text. These findings demonstrate the effectiveness of semantic-aware
watermarking in resisting real-world tampering. All code, datasets, and
evaluation scripts are publicly available at:
https://github.com/githshine/SynGuard.

</details>


### [4] [Network-Level Prompt and Trait Leakage in Local Research Agents](https://arxiv.org/abs/2508.20282)
*Hyejun Jeong,Mohammadreze Teymoorianfard,Abhinav Kumar,Amir Houmansadr,Eugene Badasarian*

Main category: cs.CR

TL;DR: 研究表明基于语言模型的网络研究代理（WRAs）易受被动网络攻击，攻击者仅通过分析访问的IP地址和时间等网络元数据，就能恢复73%的用户提示功能和领域知识，并能准确推断出19/32的潜在用户特征。


<details>
  <summary>Details</summary>
Motivation: WRAs被组织和个人本地部署用于隐私、法律或财务目的，但与人类零星浏览不同，WRAs访问70-140个具有可识别时间相关性的域名，使得独特的指纹识别攻击成为可能。

Method: 构建基于用户搜索查询和合成角色生成查询的WRA跟踪数据集，定义OBELS行为指标来全面评估原始提示与推断提示之间的相似性，并在多会话设置下扩展攻击。

Result: 攻击能够恢复超过73%的用户提示功能和领域知识，在多会话设置中准确恢复19/32的潜在特征，即使在部分可观察性和噪声条件下仍保持有效。

Conclusion: 通过限制域名多样性或混淆跟踪的缓解策略，在几乎不影响实用性的情况下，平均能将攻击效果降低29%。

Abstract: We show that Web and Research Agents (WRAs) -- language model-based systems
that investigate complex topics on the Internet -- are vulnerable to inference
attacks by passive network adversaries such as ISPs. These agents could be
deployed \emph{locally} by organizations and individuals for privacy, legal, or
financial purposes. Unlike sporadic web browsing by humans, WRAs visit
$70{-}140$ domains with distinguishable timing correlations, enabling unique
fingerprinting attacks.
  Specifically, we demonstrate a novel prompt and user trait leakage attack
against WRAs that only leverages their network-level metadata (i.e., visited IP
addresses and their timings). We start by building a new dataset of WRA traces
based on user search queries and queries generated by synthetic personas. We
define a behavioral metric (called OBELS) to comprehensively assess similarity
between original and inferred prompts, showing that our attack recovers over
73\% of the functional and domain knowledge of user prompts. Extending to a
multi-session setting, we recover up to 19 of 32 latent traits with high
accuracy. Our attack remains effective under partial observability and noisy
conditions. Finally, we discuss mitigation strategies that constrain domain
diversity or obfuscate traces, showing negligible utility impact while reducing
attack effectiveness by an average of 29\%.

</details>


### [5] [Surveying the Operational Cybersecurity and Supply Chain Threat Landscape when Developing and Deploying AI Systems](https://arxiv.org/abs/2508.20307)
*Michael R Smith,Joe Ingram*

Main category: cs.CR

TL;DR: AI技术的快速发展带来了新的网络安全风险，传统安全评估方法无法有效处理AI系统的独特攻击面。论文通过分析AI生命周期中的运营安全和供应链风险，提出需要专门的安全框架来应对这些新兴威胁。


<details>
  <summary>Details</summary>
Motivation: AI技术的应用引入了独特的攻击面和攻击目标，传统网络安全方法无法有效处理这些新风险。攻击者的目标从传统的权限提升转向操纵AI输出以获得期望的系统效果。

Method: 分析AI生命周期中的运营安全风险和供应链风险，提出需要专门的安全框架来应对这些新兴威胁，并通过实际案例分析来说明风险。

Result: 识别了AI系统中的多种新型威胁，包括系统性能降低、假正果泛滥、模型准确性下降等。提供了实践经验和见解，帮助组织更好地理解和应对这些风险。

Conclusion: 组织需要重新认识AI系统的安全风险，开发专门的安全框架来确保AI系统的可靠性和弹性性。通过提高对这些新兴威胁的认识，可以更有效地保护AI系统。

Abstract: The rise of AI has transformed the software and hardware landscape, enabling
powerful capabilities through specialized infrastructures, large-scale data
storage, and advanced hardware. However, these innovations introduce unique
attack surfaces and objectives which traditional cybersecurity assessments
often overlook. Cyber attackers are shifting their objectives from conventional
goals like privilege escalation and network pivoting to manipulating AI outputs
to achieve desired system effects, such as slowing system performance, flooding
outputs with false positives, or degrading model accuracy. This paper serves to
raise awareness of the novel cyber threats that are introduced when
incorporating AI into a software system. We explore the operational
cybersecurity and supply chain risks across the AI lifecycle, emphasizing the
need for tailored security frameworks to address evolving threats in the
AI-driven landscape. We highlight previous exploitations and provide insights
from working in this area. By understanding these risks, organizations can
better protect AI systems and ensure their reliability and resilience.

</details>


### [6] [MindGuard: Tracking, Detecting, and Attributing MCP Tool Poisoning Attack via Decision Dependence Graph](https://arxiv.org/abs/2508.20412)
*Zhiqiang Wang,Junyang Zhang,Guanquan Shi,HaoRan Cheng,Yunhao Yao,Kaiwen Guo,Haohua Du,Xiang-Yang Li*

Main category: cs.CR

TL;DR: MindGuard是一个针对LLM代理的决策级防护系统，通过注意力机制构建决策依赖图来检测和溯源工具中毒攻击，在保持高效的同时实现高精度检测。


<details>
  <summary>Details</summary>
Motivation: 随着模型上下文协议(MCP)的普及，工具中毒攻击(TPA)成为新的安全威胁，现有基于行为分析的防御方法对此无效，因为中毒工具可能不被执行而无法留下行为痕迹。

Method: 利用LLM注意力机制与工具调用决策的强相关性，构建决策依赖图(DDG)来建模推理过程，设计基于图的异常分析机制来检测和溯源TPA攻击。

Result: 在真实数据集上的实验显示，MindGuard达到94%-99%的平均检测精度，95%-100%的溯源准确率，处理时间低于1秒且无额外token成本。

Conclusion: MindGuard为LLM代理提供了有效的决策级防护，DDG作为经典程序依赖图的适配，为在决策层面应用传统安全策略奠定了基础。

Abstract: The Model Context Protocol (MCP) is increasingly adopted to standardize the
interaction between LLM agents and external tools. However, this trend
introduces a new threat: Tool Poisoning Attacks (TPA), where tool metadata is
poisoned to induce the agent to perform unauthorized operations. Existing
defenses that primarily focus on behavior-level analysis are fundamentally
ineffective against TPA, as poisoned tools need not be executed, leaving no
behavioral trace to monitor.
  Thus, we propose MindGuard, a decision-level guardrail for LLM agents,
providing provenance tracking of call decisions, policy-agnostic detection, and
poisoning source attribution against TPA. While fully explaining LLM decision
remains challenging, our empirical findings uncover a strong correlation
between LLM attention mechanisms and tool invocation decisions. Therefore, we
choose attention as an empirical signal for decision tracking and formalize
this as the Decision Dependence Graph (DDG), which models the LLM's reasoning
process as a weighted, directed graph where vertices represent logical concepts
and edges quantify the attention-based dependencies. We further design robust
DDG construction and graph-based anomaly analysis mechanisms that efficiently
detect and attribute TPA attacks. Extensive experiments on real-world datasets
demonstrate that MindGuard achieves 94\%-99\% average precision in detecting
poisoned invocations, 95\%-100\% attribution accuracy, with processing times
under one second and no additional token cost. Moreover, DDG can be viewed as
an adaptation of the classical Program Dependence Graph (PDG), providing a
solid foundation for applying traditional security policies at the decision
level.

</details>


### [7] [Federated Learning for Large Models in Medical Imaging: A Comprehensive Review](https://arxiv.org/abs/2508.20414)
*Mengyu Sun,Ziyuan Yang,Yongqiang Huang,Hui Yu,Yingyu Chen,Shuren Qi,Andrew Beng Jin Teoh,Yi Zhang*

Main category: cs.CR

TL;DR: 本文综述了联邦学习在医学影像全流程分析中的应用，包括上游重建任务和下游诊断任务，解决了医疗数据隐私保护和分散数据利用的难题。


<details>
  <summary>Details</summary>
Motivation: 医疗AI发展面临大规模集中数据训练的隐私法规限制，联邦学习提供隐私保护的分布式训练框架，实现跨机构医疗数据的协作模型开发。

Method: 采用联邦学习框架，在上游任务中联合训练CT/MRI重建网络，在下游任务中支持本地数据微调而不集中敏感图像，分析通信效率提升、异构数据对齐和安全参数聚合等创新方法。

Result: 联邦学习能够有效缓解医疗数据稀缺问题，保护患者隐私的同时实现多机构数据协作，支持医学影像从重建到诊断的全流程分析。

Conclusion: 联邦学习为医疗AI发展提供了重要的隐私保护解决方案，未来研究应继续优化通信效率、数据对齐和安全性，推动该领域进一步发展。

Abstract: Artificial intelligence (AI) has demonstrated considerable potential in the
realm of medical imaging. However, the development of high-performance AI
models typically necessitates training on large-scale, centralized datasets.
This approach is confronted with significant challenges due to strict patient
privacy regulations and legal restrictions on data sharing and utilization.
These limitations hinder the development of large-scale models in medical
domains and impede continuous updates and training with new data. Federated
Learning (FL), a privacy-preserving distributed training framework, offers a
new solution by enabling collaborative model development across fragmented
medical datasets. In this survey, we review FL's contributions at two stages of
the full-stack medical analysis pipeline. First, in upstream tasks such as CT
or MRI reconstruction, FL enables joint training of robust reconstruction
networks on diverse, multi-institutional datasets, alleviating data scarcity
while preserving confidentiality. Second, in downstream clinical tasks like
tumor diagnosis and segmentation, FL supports continuous model updating by
allowing local fine-tuning on new data without centralizing sensitive images.
We comprehensively analyze FL implementations across the medical imaging
pipeline, from physics-informed reconstruction networks to diagnostic AI
systems, highlighting innovations that improve communication efficiency, align
heterogeneous data, and ensure secure parameter aggregation. Meanwhile, this
paper provides an outlook on future research directions, aiming to serve as a
valuable reference for the field's development.

</details>


### [8] [Breaking Diffusion with Cache: Exploiting Approximate Caches in Diffusion Models](https://arxiv.org/abs/2508.20424)
*Desen Sun,Shuncheng Jie,Sihang Liu*

Main category: cs.CR

TL;DR: 本文系统评估了扩散模型中近似缓存机制的安全漏洞，发现了三种远程攻击方式：隐蔽信道通信、提示词窃取和缓存投毒攻击，揭示了近似缓存带来的严重安全风险


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然强大但计算成本高，近似缓存技术被用来重用相似提示词的中间状态以提升效率，但这种优化打破了用户间的隔离机制，需要全面评估其安全风险

Method: 通过实验演示了三种攻击方式：1）使用特殊关键词建立远程隐蔽信道进行信息交换；2）基于缓存命中提示恢复已缓存提示词；3）通过嵌入攻击者标识对被盗提示进行投毒

Result: 成功实现了所有三种远程攻击，证明攻击者可以通过服务系统远程建立隐蔽信道、窃取提示词并实施缓存投毒，即使在数天后仍能恢复信息

Conclusion: 近似缓存技术虽然提升了扩散模型的效率，但引入了严重的安全漏洞，破坏了用户隔离，需要重新设计安全机制来平衡效率与安全性

Abstract: Diffusion models are a powerful class of generative models that produce
content, such as images, from user prompts, but they are computationally
intensive. To mitigate this cost, recent academic and industry work has adopted
approximate caching, which reuses intermediate states from similar prompts in a
cache. While efficient, this optimization introduces new security risks by
breaking isolation among users. This work aims to comprehensively assess new
security vulnerabilities arising from approximate caching. First, we
demonstrate a remote covert channel established with the cache, where a sender
injects prompts with special keywords into the cache and a receiver can recover
that even after days, to exchange information. Second, we introduce a prompt
stealing attack using the cache, where an attacker can recover existing cached
prompts based on cache hit prompts. Finally, we introduce a poisoning attack
that embeds the attacker's logos into the previously stolen prompt, to render
them in future user prompts that hit the cache. These attacks are all performed
remotely through the serving system, which indicates severe security
vulnerabilities in approximate caching.

</details>


### [9] [Ransomware 3.0: Self-Composing and LLM-Orchestrated](https://arxiv.org/abs/2508.20444)
*Md Raz,Meet Udeshi,P. V. Sai Charan,Prashanth Krishnamurthy,Farshad Khorrami,Ramesh Karri*

Main category: cs.CR

TL;DR: 这篇论文提出了"Ransomware 3.0"，一种由大语言模型自主管理的新型敌意软件攻击模型，能够通过自动化理由、代码合成和上下文决策来执行敌意软件攻击生命周期。


<details>
  <summary>Details</summary>
Motivation: 传统敌意软件需要预先编写恶意代码，而Ransomware 3.0只需自然语言提示，代码在运行时动态生成，产生可逆向工程的多态变种，适应执行环境。

Method: 建立了一个研究原型系统，利用LLM在运行时动态合成恶意代码，执行伪装、载荷生成和个性化劫持等攻击步骤，形成闭环攻击链。采用阶段性方法评估数据保真度和质量一致性。

Result: 开源LLM能够生成功能性敌意软件组件，在个人、企业和嵌入式环境中维持闭环执行。通过案例研究提供了行为信号和多级遥测数据。

Conclusion: 这种AI驱动的新型敌意软件攻击需要发展更好的防御策略和政策执行，以应对新兴的AI加持敌意软件威胁。

Abstract: Using automated reasoning, code synthesis, and contextual decision-making, we
introduce a new threat that exploits large language models (LLMs) to
autonomously plan, adapt, and execute the ransomware attack lifecycle.
Ransomware 3.0 represents the first threat model and research prototype of
LLM-orchestrated ransomware. Unlike conventional malware, the prototype only
requires natural language prompts embedded in the binary; malicious code is
synthesized dynamically by the LLM at runtime, yielding polymorphic variants
that adapt to the execution environment. The system performs reconnaissance,
payload generation, and personalized extortion, in a closed-loop attack
campaign without human involvement. We evaluate this threat across personal,
enterprise, and embedded environments using a phase-centric methodology that
measures quantitative fidelity and qualitative coherence in each attack phase.
We show that open source LLMs can generate functional ransomware components and
sustain closed-loop execution across diverse environments. Finally, we present
behavioral signals and multi-level telemetry of Ransomware 3.0 through a case
study to motivate future development of better defenses and policy enforcements
to address novel AI-enabled ransomware attacks.

</details>


### [10] [Enhancing Resilience for IoE: A Perspective of Networking-Level Safeguard](https://arxiv.org/abs/2508.20504)
*Guan-Yan Yang,Jui-Ning Chen,Farn Wang,Kuo-Hui Yeh*

Main category: cs.CR

TL;DR: 提出基于图结构学习(GSL)的防护框架，通过联合优化图拓扑和节点表示来抵御IoE网络中的对抗性攻击，相比传统方法具有更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 能源物联网(IoE)的互联性使关键基础设施面临复杂的网络威胁，特别是对抗性攻击，这些威胁比一般IoT风险具有更高的公共安全后果，需要弹性解决方案。

Method: 从网络级防护角度提出图结构学习(GSL)框架，联合优化图拓扑和节点表示，内在抵抗对抗性网络模型操纵。通过概念概述、架构讨论和安全数据集案例研究进行验证。

Result: GSL方法在鲁棒性方面优于代表性方法，为实践者提供了保护IoE网络免受不断演变的攻击的可行路径。

Conclusion: 这项工作凸显了GSL在增强未来IoE网络弹性和可靠性方面的潜力，为管理关键基础设施的实践者提供了解决方案，并指出了该新兴研究领域的关键开放挑战和未来研究方向。

Abstract: The Internet of Energy (IoE) integrates IoT-driven digital communication with
power grids to enable efficient and sustainable energy systems. Still, its
interconnectivity exposes critical infrastructure to sophisticated cyber
threats, including adversarial attacks designed to bypass traditional
safeguards. Unlike general IoT risks, IoE threats have heightened public safety
consequences, demanding resilient solutions. From the networking-level
safeguard perspective, we propose a Graph Structure Learning (GSL)-based
safeguards framework that jointly optimizes graph topology and node
representations to resist adversarial network model manipulation inherently.
Through a conceptual overview, architectural discussion, and case study on a
security dataset, we demonstrate GSL's superior robustness over representative
methods, offering practitioners a viable path to secure IoE networks against
evolving attacks. This work highlights the potential of GSL to enhance the
resilience and reliability of future IoE networks for practitioners managing
critical infrastructure. Lastly, we identify key open challenges and propose
future research directions in this novel research area.

</details>


### [11] [BridgeShield: Enhancing Security for Cross-chain Bridge Applications via Heterogeneous Graph Mining](https://arxiv.org/abs/2508.20517)
*Dan Lin,Shunfeng Lu,Ziyan Liu,Jiajing Wu,Junyuan Fang,Kaixin Lin,Bowen Song,Zibin Zheng*

Main category: cs.CR

TL;DR: BridgeShield是一个基于异构图注意力网络的跨链桥攻击检测框架，通过统一建模源链、链下协调和目标链，实现了92.58%的平均F1分数，比现有方法提升24.39%。


<details>
  <summary>Details</summary>
Motivation: 跨链桥在区块链互操作性中至关重要，但由于设计缺陷和巨大价值，成为黑客攻击的主要目标。现有检测方法主要处理单链行为，无法捕捉跨链语义。

Method: 利用异构图注意力网络建模多类型实体和关系，捕捉跨链行为的复杂执行语义。提出BridgeShield框架，在统一异构图表示中联合建模源链、链下协调和目标链，包含元路径内注意力和元路径间注意力机制。

Result: 在51个真实跨链攻击事件上的实验表明，BridgeShield达到平均92.58%的F1分数，比最先进基线方法提升24.39%。

Conclusion: BridgeShield是保护跨链桥安全和增强多链生态系统韧性的有效解决方案。

Abstract: Cross-chain bridges play a vital role in enabling blockchain
interoperability. However, due to the inherent design flaws and the enormous
value they hold, they have become prime targets for hacker attacks. Existing
detection methods show progress yet remain limited, as they mainly address
single-chain behaviors and fail to capture cross-chain semantics. To address
this gap, we leverage heterogeneous graph attention networks, which are
well-suited for modeling multi-typed entities and relations, to capture the
complex execution semantics of cross-chain behaviors. We propose BridgeShield,
a detection framework that jointly models the source chain, off-chain
coordination, and destination chain within a unified heterogeneous graph
representation. BridgeShield incorporates intra-meta-path attention to learn
fine-grained dependencies within cross-chain paths and inter-meta-path
attention to highlight discriminative cross-chain patterns, thereby enabling
precise identification of attack behaviors. Extensive experiments on 51
real-world cross-chain attack events demonstrate that BridgeShield achieves an
average F1-score of 92.58%, representing a 24.39% improvement over
state-of-the-art baselines. These results validate the effectiveness of
BridgeShield as a practical solution for securing cross-chain bridges and
enhancing the resilience of multi-chain ecosystems.

</details>


### [12] [Bitcoin as an Interplanetary Monetary Standard with Proof-of-Transit Timestamping](https://arxiv.org/abs/2508.20591)
*Jose E. Puente,Carlos Puente*

Main category: cs.CR

TL;DR: 这篇论文探讨了在地球和火星之间部署比特币作为共享货币标准的可行性，提出了重新设计的Proof-of-Transit Timestamping (PoTT)原语来处理跨行星高延迟通信的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决地球与火星之间高延迟、间歇性连接的通信问题，使得比特币可以在跨行星环境中作为共享货币标准部署。

Method: 提出PoTT原语，结合延迟/中断忍耐网络(DTN)和光学低地轨道(LEO)网格平台，设计了头部优先复制、长期间闪电通道和安全结算的架构。

Result: PoTT能够在不改变比特币共识机制或货币基础的情况下，显著提高可靠性和可负责性，并为火星提供1:1挂钮的本地设施。

Conclusion: 该方案为跨行星比特币部署提供了可行的技术路径，近期可采用强联盟方案，远期可考虑盲合并挖矿链。

Abstract: We explore the feasibility of deploying Bitcoin as the shared monetary
standard between Earth and Mars, accounting for physical constraints of
interplanetary communication. We introduce a novel primitive, Proof-of-Transit
Timestamping (PoTT), to provide cryptographic, tamper-evident audit trails for
Bitcoin data across high-latency, intermittently-connected links. Leveraging
Delay/Disruption-Tolerant Networking (DTN) and optical low-Earth-orbit (LEO)
mesh constellations, we propose an architecture for header-first replication,
long-horizon Lightning channels with planetary watchtowers, and secure
settlement through federated sidechains or blind-merge-mined (BMM) commit
chains. We formalize PoTT, analyze its security model, and show how it
measurably improves reliability and accountability without altering Bitcoin
consensus or its monetary base. Near-term deployments favor strong federations
for local settlement; longer-term, blind-merge-mined commit chains (if adopted)
provide an alternative. The Earth L1 monetary base remains unchanged, while
Mars can operate a pegged commit chain or strong federation with 1:1 pegged
assets for local block production. For transparency, if both time-beacon
regimes are simultaneously compromised, PoTT-M2 (and PoTT generally) reduces to
administrative assertions rather than cryptographic time-anchoring.

</details>


### [13] [CyberSleuth: Autonomous Blue-Team LLM Agent for Web Attack Forensics](https://arxiv.org/abs/2508.20643)
*Stefano Fumero,Kai Huang,Matteo Boffa,Danilo Giordano,Marco Mellia,Zied Ben Houidi,Dario Rossi*

Main category: cs.CR

TL;DR: 本文提出了CyberSleuth，一个基于LLM的自主代理系统，用于网络攻击取证调查，通过分析数据包和应用日志来识别服务、漏洞和攻击结果。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理在网络安全领域主要应用于红队操作，而防御性应用如事件响应和取证研究相对较少，需要系统性的设计和评估。

Method: 提出CyberSleuth代理架构，整合工具并处理数据包级痕迹和应用日志，评估了四种代理架构和六个LLM后端在20个复杂场景中的表现。

Result: 在2025年的10个事件中，CyberSleuth准确识别CVE的比例达到80%，人类专家评估认为其报告完整、有用且连贯，对开源LLM DeepSeek R1略有偏好。

Conclusion: CyberSleuth是性能最佳的设计，为防御性LLM研究提供了可复现的评估基准和平台，推动了该领域的发展。

Abstract: Large Language Model (LLM) agents are powerful tools for automating complex
tasks. In cybersecurity, researchers have primarily explored their use in
red-team operations such as vulnerability discovery and penetration tests.
Defensive uses for incident response and forensics have received comparatively
less attention and remain at an early stage. This work presents a systematic
study of LLM-agent design for the forensic investigation of realistic web
application attacks. We propose CyberSleuth, an autonomous agent that processes
packet-level traces and application logs to identify the targeted service, the
exploited vulnerability (CVE), and attack success. We evaluate the consequences
of core design decisions - spanning tool integration and agent architecture -
and provide interpretable guidance for practitioners. We benchmark four agent
architectures and six LLM backends on 20 incident scenarios of increasing
complexity, identifying CyberSleuth as the best-performing design. In a
separate set of 10 incidents from 2025, CyberSleuth correctly identifies the
exact CVE in 80% of cases. At last, we conduct a human study with 22 experts,
which rated the reports of CyberSleuth as complete, useful, and coherent. They
also expressed a slight preference for DeepSeek R1, a good news for open source
LLM. To foster progress in defensive LLM research, we release both our
benchmark and the CyberSleuth platform as a foundation for fair, reproducible
evaluation of forensic agents.

</details>


### [14] [Multi-Agent Penetration Testing AI for the Web](https://arxiv.org/abs/2508.20816)
*Isaac David,Arthur Gervais*

Main category: cs.CR

TL;DR: MAPTA是一个多代理系统，用于自主进行Web应用安全评估，结合LLM编排、工具执行和端到端漏洞验证，在XBOW基准测试中达到76.9%的成功率，成本效益显著。


<details>
  <summary>Details</summary>
Motivation: AI驱动的开发平台使软件开发更易获得，但这也导致了安全审计的可扩展性危机，研究表明高达40%的AI生成代码包含漏洞，开发速度远超安全评估能力。

Method: 多代理系统结合大型语言模型编排、工具基础执行和端到端漏洞验证，实现自主的Web应用安全评估。

Result: 在104个挑战的XBOW基准测试中达到76.9%总体成功率，SSRF和配置错误漏洞完美表现，SQL注入83%成功率，XSS和盲注仍有挑战。成本分析显示总成本$21.38，成功尝试中位成本$0.073。

Conclusion: MAPTA在真实环境中发现了包括RCE、命令注入等关键漏洞，成本效益优异（平均$3.67/评估），10个发现正在CVE审核中，证明了其在规模化安全审计中的实用性。

Abstract: AI-powered development platforms are making software creation accessible to a
broader audience, but this democratization has triggered a scalability crisis
in security auditing. With studies showing that up to 40% of AI-generated code
contains vulnerabilities, the pace of development now vastly outstrips the
capacity for thorough security assessment.
  We present MAPTA, a multi-agent system for autonomous web application
security assessment that combines large language model orchestration with
tool-grounded execution and end-to-end exploit validation. On the 104-challenge
XBOW benchmark, MAPTA achieves 76.9% overall success with perfect performance
on SSRF and misconfiguration vulnerabilities, 83% success on broken
authorization, and strong results on injection attacks including server-side
template injection (85%) and SQL injection (83%). Cross-site scripting (57%)
and blind SQL injection (0%) remain challenging. Our comprehensive cost
analysis across all challenges totals $21.38 with a median cost of $0.073 for
successful attempts versus $0.357 for failures. Success correlates strongly
with resource efficiency, enabling practical early-stopping thresholds at
approximately 40 tool calls or $0.30 per challenge.
  MAPTA's real-world findings are impactful given both the popularity of the
respective scanned GitHub repositories (8K-70K stars) and MAPTA's low average
operating cost of $3.67 per open-source assessment: MAPTA discovered critical
vulnerabilities including RCEs, command injections, secret exposure, and
arbitrary file write vulnerabilities. Findings are responsibly disclosed, 10
findings are under CVE review.

</details>


### [15] [JADES: A Universal Framework for Jailbreak Assessment via Decompositional Scoring](https://arxiv.org/abs/2508.20848)
*Junjie Chu,Mingjie Li,Ziqing Yang,Ye Leng,Chenhao Lin,Chao Shen,Michael Backes,Yun Shen,Yang Zhang*

Main category: cs.CR

TL;DR: JADES是一个通过分解加权评分来评估越狱攻击成功率的框架，在二元评估中达到98.5%的人类评估一致性，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有越狱评估方法依赖不匹配的代理指标或简单整体判断，经常误解模型响应，导致与人类感知不一致的主观评估

Method: 自动将有害问题分解为加权子问题集，对每个子答案评分，并通过加权聚合得出最终决策，可选加入事实核查模块检测幻觉

Result: 在JailbreakQR基准测试中，JADES在二元设置下达到98.5%的人类评估一致性，比强基线高出9%以上；重新评估显示现有方法严重高估攻击成功率

Conclusion: JADES能够提供准确、一致且可解释的评估，为未来越狱攻击的测量提供可靠基础

Abstract: Accurately determining whether a jailbreak attempt has succeeded is a
fundamental yet unresolved challenge. Existing evaluation methods rely on
misaligned proxy indicators or naive holistic judgments. They frequently
misinterpret model responses, leading to inconsistent and subjective
assessments that misalign with human perception. To address this gap, we
introduce JADES (Jailbreak Assessment via Decompositional Scoring), a universal
jailbreak evaluation framework. Its key mechanism is to automatically decompose
an input harmful question into a set of weighted sub-questions, score each
sub-answer, and weight-aggregate the sub-scores into a final decision. JADES
also incorporates an optional fact-checking module to strengthen the detection
of hallucinations in jailbreak responses. We validate JADES on JailbreakQR, a
newly introduced benchmark proposed in this work, consisting of 400 pairs of
jailbreak prompts and responses, each meticulously annotated by humans. In a
binary setting (success/failure), JADES achieves 98.5% agreement with human
evaluators, outperforming strong baselines by over 9%. Re-evaluating five
popular attacks on four LLMs reveals substantial overestimation (e.g., LAA's
attack success rate on GPT-3.5-Turbo drops from 93% to 69%). Our results show
that JADES could deliver accurate, consistent, and interpretable evaluations,
providing a reliable basis for measuring future jailbreak attacks.

</details>


### [16] [Publish to Perish: Prompt Injection Attacks on LLM-Assisted Peer Review](https://arxiv.org/abs/2508.20863)
*Matteo Gioele Collu,Umberto Salviati,Roberto Confalonieri,Mauro Conti,Giovanni Apruzzese*

Main category: cs.CR

TL;DR: 本文研究LLM在科学同行评审中的隐藏提示注入攻击，作者通过在PDF中嵌入对抗性文本来操纵LLM生成的评审结果，证明了这种攻击的有效性并提出了降低检测性的方法。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地应用于科学同行评审过程，需要评估其对隐藏提示注入攻击的可靠性和抗操纵能力，以防范潜在的评审操纵风险。

Method: 形式化了三种威胁模型，设计了对人类读者不可见但能引导LLM输出的对抗性提示，通过用户研究获得代表性评审提示，并在不同评审系统、提示和论文上进行评估。

Result: 对抗性提示能够可靠地误导LLM，有时会对"诚实但懒惰"的评审者产生不利影响，同时提出的方法能有效降低自动化内容检查中的可检测性。

Conclusion: LLM在同行评审中存在被隐藏提示注入攻击操纵的风险，需要开发更强大的防御机制来确保科学评审过程的完整性。

Abstract: Large Language Models (LLMs) are increasingly being integrated into the
scientific peer-review process, raising new questions about their reliability
and resilience to manipulation. In this work, we investigate the potential for
hidden prompt injection attacks, where authors embed adversarial text within a
paper's PDF to influence the LLM-generated review. We begin by formalising
three distinct threat models that envision attackers with different motivations
-- not all of which implying malicious intent. For each threat model, we design
adversarial prompts that remain invisible to human readers yet can steer an
LLM's output toward the author's desired outcome. Using a user study with
domain scholars, we derive four representative reviewing prompts used to elicit
peer reviews from LLMs. We then evaluate the robustness of our adversarial
prompts across (i) different reviewing prompts, (ii) different commercial
LLM-based systems, and (iii) different peer-reviewed papers. Our results show
that adversarial prompts can reliably mislead the LLM, sometimes in ways that
adversely affect a "honest-but-lazy" reviewer. Finally, we propose and
empirically assess methods to reduce detectability of adversarial prompts under
automated content checks.

</details>


### [17] [AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning](https://arxiv.org/abs/2508.20866)
*Amine Lbath,Massih-Reza Amini,Aurelien Delaitre,Vadim Okun*

Main category: cs.CR

TL;DR: 提出了一种新型框架，通过多AI代理协作在安全C/C++代码中自动注入真实漏洞来生成数据集，实验显示在函数级别漏洞注入成功率可达89%-95%。


<details>
  <summary>Details</summary>
Motivation: 传统漏洞检测方法存在可扩展性、适应性差以及高误报漏报率问题，AI方法严重依赖训练数据质量，需要高质量漏洞数据集来改善检测和修复系统。

Method: 使用多AI代理模拟专家推理，结合函数代理和传统代码分析工具，利用检索增强生成进行上下文基础，采用低秩权重近似进行高效模型微调。

Result: 在三个不同基准测试的116个代码样本上，该方法在数据集准确性方面优于其他技术，函数级别漏洞注入成功率达到89%-95%。

Conclusion: 该框架能够有效生成高质量的漏洞数据集，为AI驱动的漏洞检测和修复系统提供了可靠的数据基础，显著提升了漏洞注入的准确性和成功率。

Abstract: The increasing complexity of software systems and the sophistication of
cyber-attacks have underscored the critical need for effective automated
vulnerability detection and repair systems. Traditional methods, such as static
program analysis, face significant challenges related to scalability,
adaptability, and high false-positive and false-negative rates. AI-driven
approaches, particularly those using machine learning and deep learning models,
show promise but are heavily reliant on the quality and quantity of training
data. This paper introduces a novel framework designed to automatically
introduce realistic, category-specific vulnerabilities into secure C/C++
codebases to generate datasets. The proposed approach coordinates multiple AI
agents that simulate expert reasoning, along with function agents and
traditional code analysis tools. It leverages Retrieval-Augmented Generation
for contextual grounding and employs Low-Rank approximation of weights for
efficient model fine-tuning. Our experimental study on 116 code samples from
three different benchmarks suggests that our approach outperforms other
techniques with regard to dataset accuracy, achieving between 89\% and 95\%
success rates in injecting vulnerabilities at function level.

</details>


### [18] [PromptSleuth: Detecting Prompt Injection via Semantic Intent Invariance](https://arxiv.org/abs/2508.20890)
*Mengxiao Wang,Yuxuan Zhang,Guofei Gu*

Main category: cs.CR

TL;DR: 提出了PromptSleuth防御框架，通过语义意图推理检测提示注入攻击，在新建的综合性基准测试中显著优于现有防御方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在现实应用中面临提示注入攻击威胁，现有基准测试无法全面覆盖新兴攻击策略，需要更鲁棒的防御解决方案。

Method: 构建新的综合性基准测试，包含多种操纵技术和多任务场景；提出PromptSleuth框架，基于任务级意图语义推理而非表面特征来检测提示注入。

Result: 现有防御方法在新基准测试中表现不佳，而PromptSleuth在所有基准测试中一致优于现有防御方法，同时保持相当的运行时和成本效率。

Conclusion: 基于意图的语义推理为防御LLM提示注入攻击提供了鲁棒、高效且可泛化的策略，能够应对不断演变的攻击威胁。

Abstract: Large Language Models (LLMs) are increasingly integrated into real-world
applications, from virtual assistants to autonomous agents. However, their
flexibility also introduces new attack vectors-particularly Prompt Injection
(PI), where adversaries manipulate model behavior through crafted inputs. As
attackers continuously evolve with paraphrased, obfuscated, and even multi-task
injection strategies, existing benchmarks are no longer sufficient to capture
the full spectrum of emerging threats.
  To address this gap, we construct a new benchmark that systematically extends
prior efforts. Our benchmark subsumes the two widely-used existing ones while
introducing new manipulation techniques and multi-task scenarios, thereby
providing a more comprehensive evaluation setting. We find that existing
defenses, though effective on their original benchmarks, show clear weaknesses
under our benchmark, underscoring the need for more robust solutions. Our key
insight is that while attack forms may vary, the adversary's intent-injecting
an unauthorized task-remains invariant. Building on this observation, we
propose PromptSleuth, a semantic-oriented defense framework that detects prompt
injection by reasoning over task-level intent rather than surface features.
Evaluated across state-of-the-art benchmarks, PromptSleuth consistently
outperforms existing defense while maintaining comparable runtime and cost
efficiency. These results demonstrate that intent-based semantic reasoning
offers a robust, efficient, and generalizable strategy for defending LLMs
against evolving prompt injection threats.

</details>


### [19] [Characterizing Trust Boundary Vulnerabilities in TEE Containers](https://arxiv.org/abs/2508.20962)
*Weijie Liu,Hongbo Chen,Shuo Huai,Zhen Xu,Wenhao Wang,Zhi Li,Zheli Liu*

Main category: cs.CR

TL;DR: 本文分析了现有TEE容器的隔离策略，开发了自动化分析工具来评估其隔离边界，发现多个关键安全漏洞，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: TEE容器作为机密计算的关键中间件，需要保护应用程序免受恶意操作系统和编排接口的攻击，但现有容器的隔离效果需要系统评估。

Method: 设计了自动化分析器来精确识别和评估TEE容器的隔离边界，分析其设计实现中的安全缺陷。

Result: 发现一些TEE容器因关键设计和实现缺陷（如信息泄露、回滚攻击、拒绝服务攻击和Iago攻击）而无法达到预期安全目标。

Conclusion: 基于研究发现分享了关键经验教训，指导开发更安全的容器解决方案，并讨论了TEE容器化设计的新兴趋势。

Abstract: Trusted Execution Environments (TEEs) have emerged as a cornerstone of
confidential computing, garnering significant attention from both academia and
industry. To enable the secure development, execution, and deployment, of
applications on TEE platforms, TEE containers have been introduced as
middleware solutions. These containers aim to shield applications from
potentially malicious operating systems and orchestration interfaces while
maintaining usability and reliability. In this paper, we analyze the isolation
strategies employed by existing TEE containers to protect secure applications.
To address the challenges in analyzing these interfaces, we designed an
automated analyzer to precisely identify and evaluate their isolation
boundaries. We observed that some TEE containers fail to achieve their intended
goals due to critical design and implementation flaws, such as information
leakage, rollback attacks, denial-of-service, and Iago attacks, which pose
significant security risks. Drawing from our findings, we share key lessons to
guide the development of more secure container solutions and discuss emerging
trends in TEE containerization design.

</details>


### [20] [Guarding Against Malicious Biased Threats (GAMBiT) Experiments: Revealing Cognitive Bias in Human-Subjects Red-Team Cyber Range Operations](https://arxiv.org/abs/2508.20963)
*Brandon Beltz,Jim Doty,Yvonne Fonken,Nikolos Gurney,Brett Israelsen,Nathan Lau,Stacy Marsella,Rachelle Thomas,Stoney Trent,Peggy Wu,Ya-Ting Yang,Quanyan Zhu*

Main category: cs.CR

TL;DR: 提供三个大规模人类红队网络安全测试数据集，包含多模态数据和涉客行为记录


<details>
  <summary>Details</summary>
Motivation: 为研究攻击者行为建模、偏见识别分析和方法测试提供综合性的实验数据支撑

Method: 通过众包测试平台，组织19-20名技能攻击者在模拟企业网络中进行自主操作，收集自我报告、操作笔记、终端历史等多模态数据

Result: 构建了包含多种数据源的综合性数据集，包括网络数据包、NIDS警报等，并提供了细化标签

Conclusion: 该数据集为网络安全领域的攻击者行为研究提供了价值较高的实验数据资源，通过IEEE Dataport可获得

Abstract: We present three large-scale human-subjects red-team cyber range datasets
from the Guarding Against Malicious Biased Threats (GAMBiT) project. Across
Experiments 1-3 (July 2024-March 2025), 19-20 skilled attackers per experiment
conducted two 8-hour days of self-paced operations in a simulated enterprise
network (SimSpace Cyber Force Platform) while we captured multi-modal data:
self-reports (background, demographics, psychometrics), operational notes,
terminal histories, keylogs, network packet captures (PCAP), and NIDS alerts
(Suricata). Each participant began from a standardized Kali Linux VM and
pursued realistic objectives (e.g., target discovery and data exfiltration)
under controlled constraints. Derivative curated logs and labels are included.
The combined release supports research on attacker behavior modeling,
bias-aware analytics, and method benchmarking. Data are available via IEEE
Dataport entries for Experiments 1-3.

</details>
