{"id": "2509.19485", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19485", "abs": "https://arxiv.org/abs/2509.19485", "authors": ["Hafijul Hoque Chowdhury", "Riad Ahmed Anonto", "Sourov Jajodia", "Suryadipta Majumdar", "Md. Shohrab Hossain"], "title": "Identifying and Addressing User-level Security Concerns in Smart Homes Using \"Smaller\" LLMs", "comment": "10 pages, accepted at PST 2025", "summary": "With the rapid growth of smart home IoT devices, users are increasingly\nexposed to various security risks, as evident from recent studies. While\nseeking answers to know more on those security concerns, users are mostly left\nwith their own discretion while going through various sources, such as online\nblogs and technical manuals, which may render higher complexity to regular\nusers trying to extract the necessary information. This requirement does not go\nalong with the common mindsets of smart home users and hence threatens the\nsecurity of smart homes furthermore. In this paper, we aim to identify and\naddress the major user-level security concerns in smart homes. Specifically, we\ndevelop a novel dataset of Q&A from public forums, capturing practical security\nchallenges faced by smart home users. We extract major security concerns in\nsmart homes from our dataset by leveraging the Latent Dirichlet Allocation\n(LDA). We fine-tune relatively \"smaller\" transformer models, such as T5 and\nFlan-T5, on this dataset to build a QA system tailored for smart home security.\nUnlike larger models like GPT and Gemini, which are powerful but often resource\nhungry and require data sharing, smaller models are more feasible for\ndeployment in resource-constrained or privacy-sensitive environments like smart\nhomes. The dataset is manually curated and supplemented with synthetic data to\nexplore its potential impact on model performance. This approach significantly\nimproves the system's ability to deliver accurate and relevant answers, helping\nusers address common security concerns with smart home IoT devices. Our\nexperiments on real-world user concerns show that our work improves the\nperformance of the base models."}
{"id": "2509.19568", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.19568", "abs": "https://arxiv.org/abs/2509.19568", "authors": ["Antoine Plin", "Lorenzo Casalino", "Thomas Rokicki", "Ruben Salvador"], "title": "Knock-Knock: Black-Box, Platform-Agnostic DRAM Address-Mapping Reverse Engineering", "comment": "Accepted in 2nd Microarchitecture Security Conference 2026 (uASC\n  '26), 17 pages, 8 figures, 3 tables, 1 algorithm, 1 appendix", "summary": "Modern Systems-on-Chip (SoCs) employ undocumented linear address-scrambling\nfunctions to obfuscate DRAM addressing, which complicates DRAM-aware\nperformance optimizations and hinders proactive security analysis of DRAM-based\nattacks; most notably, Rowhammer. Although previous work tackled the issue of\nreversing physical-to-DRAM mapping, existing heuristic-based\nreverse-engineering approaches are partial, costly, and impractical for\ncomprehensive recovery. This paper establishes a rigorous theoretical\nfoundation and provides efficient practical algorithms for black-box, complete\nphysical-to-DRAM address-mapping recovery.\n  We first formulate the reverse-engineering problem within a linear algebraic\nmodel over the finite field GF(2). We characterize the timing fingerprints of\nrow-buffer conflicts, proving a relationship between a bank addressing matrix\nand an empirically constructed matrix of physical addresses. Based on this\ncharacterization, we develop an efficient, noise-robust, and fully\nplatform-agnostic algorithm to recover the full bank-mask basis in polynomial\ntime, a significant improvement over the exponential search from previous\nworks. We further generalize our model to complex row mappings, introducing new\nhardware-based hypotheses that enable the automatic recovery of a row basis\ninstead of previous human-guided contributions.\n  Evaluations across embedded and server-class architectures confirm our\nmethod's effectiveness, successfully reconstructing known mappings and\nuncovering previously unknown scrambling functions. Our method provides a 99%\nrecall and accuracy on all tested platforms. Most notably, Knock-Knock runs in\nunder a few minutes, even on systems with more than 500GB of DRAM, showcasing\nthe scalability of our method. Our approach provides an automated, principled\npathway to accurate DRAM reverse engineering."}
{"id": "2509.19650", "categories": ["cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.19650", "abs": "https://arxiv.org/abs/2509.19650", "authors": ["Dehinde Molade", "Dave Ormrod", "Mamello Thinyane", "Nalin Arachchilage", "Jill Slay"], "title": "SoK: A Systematic Review of Malware Ontologies and Taxonomies and Implications for the Quantum Era", "comment": "40 pages, 9 figures, 5 tables", "summary": "The threat of quantum malware is real and a growing security concern that\nwill have catastrophic scientific and technological impacts, if not addressed\nearly. If weaponised or exploited especially by the wrong hands, malware will\nundermine highly sophisticated critical systems supported by next-generation\nquantum architectures, for example, in defence, communications, energy, and\nspace. This paper explores the fundamental nature and implications of quantum\nmalware to enable the future development of appropriate mitigations and\ndefences, thereby protecting critical infrastructure. By conducting a\nsystematic literature review (SLR) that draws on knowledge frameworks such as\nontologies and taxonomies to explore malware, this provides insights into how\nmalicious behaviours can be translated into attacks on quantum technologies,\nthereby providing a lens to analyse the severity of malware against quantum\ntechnologies. This study employs the European Competency Framework for Quantum\nTechnologies (CFQT) as a guide to map malware behaviour to several competency\nlayers, creating a foundation in this emerging field."}
{"id": "2509.19677", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.19677", "abs": "https://arxiv.org/abs/2509.19677", "authors": ["Michiharu Yamashita", "Thanh Tran", "Delvin Ce Zhang", "Dongwon Lee"], "title": "Unmasking Fake Careers: Detecting Machine-Generated Career Trajectories via Multi-layer Heterogeneous Graphs", "comment": "Accepted at EMNLP 2025 Main", "summary": "The rapid advancement of Large Language Models (LLMs) has enabled the\ngeneration of highly realistic synthetic data. We identify a new vulnerability,\nLLMs generating convincing career trajectories in fake resumes and explore\neffective detection methods. To address this challenge, we construct a dataset\nof machine-generated career trajectories using LLMs and various methods, and\ndemonstrate that conventional text-based detectors perform poorly on structured\ncareer data. We propose CareerScape, a novel heterogeneous, hierarchical\nmulti-layer graph framework that models career entities and their relations in\na unified global graph built from genuine resumes. Unlike conventional\nclassifiers that treat each instance independently, CareerScape employs a\nstructure-aware framework that augments user-specific subgraphs with trusted\nneighborhood information from a global graph, enabling the model to capture\nboth global structural patterns and local inconsistencies indicative of\nsynthetic career paths. Experimental results show that CareerScape outperforms\nstate-of-the-art baselines by 5.8-85.0% relatively, highlighting the importance\nof structure-aware detection for machine-generated content."}
{"id": "2509.19947", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19947", "abs": "https://arxiv.org/abs/2509.19947", "authors": ["Zhixiao Wu", "Yao Lu", "Jie Wen", "Hao Sun", "Qi Zhou", "Guangming Lu"], "title": "A Set of Generalized Components to Achieve Effective Poison-only Clean-label Backdoor Attacks with Collaborative Sample Selection and Triggers", "comment": "31 pages, 16 figures, accepted in Neurips 2025", "summary": "Poison-only Clean-label Backdoor Attacks aim to covertly inject\nattacker-desired behavior into DNNs by merely poisoning the dataset without\nchanging the labels. To effectively implant a backdoor, multiple\n\\textbf{triggers} are proposed for various attack requirements of Attack\nSuccess Rate (ASR) and stealthiness. Additionally, sample selection enhances\nclean-label backdoor attacks' ASR by meticulously selecting ``hard'' samples\ninstead of random samples to poison. Current methods 1) usually handle the\nsample selection and triggers in isolation, leading to severely limited\nimprovements on both ASR and stealthiness. Consequently, attacks exhibit\nunsatisfactory performance on evaluation metrics when converted to PCBAs via a\nmere stacking of methods. Therefore, we seek to explore the bidirectional\ncollaborative relations between the sample selection and triggers to address\nthe above dilemma. 2) Since the strong specificity within triggers, the simple\ncombination of sample selection and triggers fails to substantially enhance\nboth evaluation metrics, with generalization preserved among various attacks.\nTherefore, we seek to propose a set of components to significantly improve both\nstealthiness and ASR based on the commonalities of attacks. Specifically,\nComponent A ascertains two critical selection factors, and then makes them an\nappropriate combination based on the trigger scale to select more reasonable\n``hard'' samples for improving ASR. Component B is proposed to select samples\nwith similarities to relevant trigger implanted samples to promote\nstealthiness. Component C reassigns trigger poisoning intensity on RGB colors\nthrough distinct sensitivity of the human visual system to RGB for higher ASR,\nwith stealthiness ensured by sample selection, including Component B.\nFurthermore, all components can be strategically integrated into diverse PCBAs."}
{"id": "2509.20166", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20166", "abs": "https://arxiv.org/abs/2509.20166", "authors": ["Lauren Deason", "Adam Bali", "Ciprian Bejean", "Diana Bolocan", "James Crnkovich", "Ioana Croitoru", "Krishna Durai", "Chase Midler", "Calin Miron", "David Molnar", "Brad Moon", "Bruno Ostarcevic", "Alberto Peltea", "Matt Rosenberg", "Catalin Sandu", "Arthur Saputkin", "Sagar Shah", "Daniel Stan", "Ernest Szocs", "Shengye Wan", "Spencer Whitman", "Sven Krasser", "Joshua Saxe"], "title": "CyberSOCEval: Benchmarking LLMs Capabilities for Malware Analysis and Threat Intelligence Reasoning", "comment": null, "summary": "Today's cyber defenders are overwhelmed by a deluge of security alerts,\nthreat intelligence signals, and shifting business context, creating an urgent\nneed for AI systems to enhance operational security work. While Large Language\nModels (LLMs) have the potential to automate and scale Security Operations\nCenter (SOC) operations, existing evaluations do not fully assess the scenarios\nmost relevant to real-world defenders. This lack of informed evaluation impacts\nboth AI developers and those applying LLMs to SOC automation. Without clear\ninsight into LLM performance in real-world security scenarios, developers lack\na north star for development, and users cannot reliably select the most\neffective models. Meanwhile, malicious actors are using AI to scale cyber\nattacks, highlighting the need for open source benchmarks to drive adoption and\ncommunity-driven improvement among defenders and model developers. To address\nthis, we introduce CyberSOCEval, a new suite of open source benchmarks within\nCyberSecEval 4. CyberSOCEval includes benchmarks tailored to evaluate LLMs in\ntwo tasks: Malware Analysis and Threat Intelligence Reasoning--core defensive\ndomains with inadequate coverage in current benchmarks. Our evaluations show\nthat larger, more modern LLMs tend to perform better, confirming the training\nscaling laws paradigm. We also find that reasoning models leveraging test time\nscaling do not achieve the same boost as in coding and math, suggesting these\nmodels have not been trained to reason about cybersecurity analysis, and\npointing to a key opportunity for improvement. Finally, current LLMs are far\nfrom saturating our evaluations, showing that CyberSOCEval presents a\nsignificant challenge for AI developers to improve cyber defense capabilities."}
{"id": "2509.20190", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20190", "abs": "https://arxiv.org/abs/2509.20190", "authors": ["Tanmay Khule", "Stefan Marksteiner", "Jose Alguindigue", "Hannes Fuchs", "Sebastian Fischmeister", "Apurva Narayan"], "title": "STAF: Leveraging LLMs for Automated Attack Tree-Based Security Test Generation", "comment": "18 pages, 2 figures, accepted for 23rd escar Europe (Nov 05-06, 2025,\n  Frankfurt, Germany)", "summary": "In modern automotive development, security testing is critical for\nsafeguarding systems against increasingly advanced threats. Attack trees are\nwidely used to systematically represent potential attack vectors, but\ngenerating comprehensive test cases from these trees remains a labor-intensive,\nerror-prone task that has seen limited automation in the context of testing\nvehicular systems. This paper introduces STAF (Security Test Automation\nFramework), a novel approach to automating security test case generation.\nLeveraging Large Language Models (LLMs) and a four-step self-corrective\nRetrieval-Augmented Generation (RAG) framework, STAF automates the generation\nof executable security test cases from attack trees, providing an end-to-end\nsolution that encompasses the entire attack surface. We particularly show the\nelements and processes needed to provide an LLM to actually produce sensible\nand executable automotive security test suites, along with the integration with\nan automated testing framework. We further compare our tailored approach with\ngeneral purpose (vanilla) LLMs and the performance of different LLMs (namely\nGPT-4.1 and DeepSeek) using our approach. We also demonstrate the method of our\noperation step-by-step in a concrete case study. Our results show significant\nimprovements in efficiency, accuracy, scalability, and easy integration in any\nworkflow, marking a substantial advancement in automating automotive security\ntesting methodologies. Using TARAs as an input for verfication tests, we create\nsynergies by connecting two vital elements of a secure automotive development\nprocess."}
{"id": "2509.20277", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20277", "abs": "https://arxiv.org/abs/2509.20277", "authors": ["Xiaofan Li", "Xing Gao"], "title": "Investigating Security Implications of Automatically Generated Code on the Software Supply Chain", "comment": null, "summary": "In recent years, various software supply chain (SSC) attacks have posed\nsignificant risks to the global community. Severe consequences may arise if\ndevelopers integrate insecure code snippets that are vulnerable to SSC attacks\ninto their products. Particularly, code generation techniques, such as large\nlanguage models (LLMs), have been widely utilized in the developer community.\nHowever, LLMs are known to suffer from inherent issues when generating code,\nincluding fabrication, misinformation, and reliance on outdated training data,\nall of which can result in serious software supply chain threats. In this\npaper, we investigate the security threats to the SSC that arise from these\ninherent issues. We examine three categories of threats, including eleven\npotential SSC-related threats, related to external components in source code,\nand continuous integration configuration files. We find some threats in\nLLM-generated code could enable attackers to hijack software and workflows,\nwhile some others might cause potential hidden threats that compromise the\nsecurity of the software over time. To understand these security impacts and\nseverity, we design a tool, SSCGuard, to generate 439,138 prompts based on\nSSC-related questions collected online, and analyze the responses of four\npopular LLMs from GPT and Llama. Our results show that all identified\nSSC-related threats persistently exist. To mitigate these risks, we propose a\nnovel prompt-based defense mechanism, namely Chain-of-Confirmation, to reduce\nfabrication, and a middleware-based defense that informs users of various SSC\nthreats."}
{"id": "2509.20283", "categories": ["cs.CR", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.20283", "abs": "https://arxiv.org/abs/2509.20283", "authors": ["Önder Askin", "Tim Kutta", "Holger Dette"], "title": "Monitoring Violations of Differential Privacy over Time", "comment": null, "summary": "Auditing differential privacy has emerged as an important area of research\nthat supports the design of privacy-preserving mechanisms. Privacy audits help\nto obtain empirical estimates of the privacy parameter, to expose flawed\nimplementations of algorithms and to compare practical with theoretical privacy\nguarantees. In this work, we investigate an unexplored facet of privacy\nauditing: the sustained auditing of a mechanism that can go through changes\nduring its development or deployment. Monitoring the privacy of algorithms over\ntime comes with specific challenges. Running state-of-the-art (static) auditors\nrepeatedly requires excessive sampling efforts, while the reliability of such\nmethods deteriorates over time without proper adjustments. To overcome these\nobstacles, we present a new monitoring procedure that extracts information from\nthe entire deployment history of the algorithm. This allows us to reduce\nsampling efforts, while sustaining reliable outcomes of our auditor. We derive\nformal guarantees with regard to the soundness of our methods and evaluate\ntheir performance for important mechanisms from the literature. Our theoretical\nfindings and experiments demonstrate the efficacy of our approach."}
{"id": "2509.20324", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20324", "abs": "https://arxiv.org/abs/2509.20324", "authors": ["Atousa Arzanipour", "Rouzbeh Behnia", "Reza Ebrahimi", "Kaushik Dutta"], "title": "RAG Security and Privacy: Formalizing the Threat Model and Attack Surface", "comment": "Accepted at the 5th ICDM Workshop on September 20, 2025", "summary": "Retrieval-Augmented Generation (RAG) is an emerging approach in natural\nlanguage processing that combines large language models (LLMs) with external\ndocument retrieval to produce more accurate and grounded responses. While RAG\nhas shown strong potential in reducing hallucinations and improving factual\nconsistency, it also introduces new privacy and security challenges that differ\nfrom those faced by traditional LLMs. Existing research has demonstrated that\nLLMs can leak sensitive information through training data memorization or\nadversarial prompts, and RAG systems inherit many of these vulnerabilities. At\nthe same time, reliance of RAG on an external knowledge base opens new attack\nsurfaces, including the potential for leaking information about the presence or\ncontent of retrieved documents, or for injecting malicious content to\nmanipulate model behavior. Despite these risks, there is currently no formal\nframework that defines the threat landscape for RAG systems. In this paper, we\naddress a critical gap in the literature by proposing, to the best of our\nknowledge, the first formal threat model for retrieval-RAG systems. We\nintroduce a structured taxonomy of adversary types based on their access to\nmodel components and data, and we formally define key threat vectors such as\ndocument-level membership inference and data poisoning, which pose serious\nprivacy and integrity risks in real-world deployments. By establishing formal\ndefinitions and attack models, our work lays the foundation for a more rigorous\nand principled understanding of privacy and security in RAG systems."}
{"id": "2509.20356", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20356", "abs": "https://arxiv.org/abs/2509.20356", "authors": ["Mohamed E. Najd", "Ghada Almashaqbeh"], "title": "chainScale: Secure Functionality-oriented Scalability for Decentralized Resource Markets", "comment": null, "summary": "Decentralized resource markets are Web 3.0 applications that build\nopen-access platforms for trading digital resources among users without any\ncentral management. They promise cost reduction, transparency, and flexible\nservice provision. However, these markets usually have large workload that must\nbe processed in a timely manner, leading to serious scalability problems.\nDespite the large amount of work on blockchain scalability, existing solutions\nare ineffective as they do not account for these markets' work models and\ntraffic patterns.\n  We introduce chainScale, a secure hybrid sidechain-sharding solution that\naims to boost throughput of decentralized resource markets and reduce their\nlatency and storage footprint. At its core, chainScale leverages dependent\nsidechains and functionality-oriented workload splitting to parallelize traffic\nprocessing by having each market module assigned to a sidechain. Different from\nsharding, chainScale does not incur any cross-sidechain transactions that tend\nto be costly. chainScale introduces several techniques, including hierarchical\nworkload sharing that further sub-divides overloaded modules, and weighted\nminer assignment that assigns miners with vested interest in the system to\ncritical modules' sidechains. Furthermore, chainScale employs sidechain syncing\nto maintain the mainchain as the single truth of system state, and pruning to\ndiscard stale records. Beside analyzing security, we build a proof-of-concept\nimplementation for a distributed file storage market as a use case. Our\nexperiments show that, compared to a single sidechain-based prior solution,\nchainScale boosts throughput by 4x and reduces confirmation latency by 5x.\nAlso, they show that chainScale outperforms sharding by 2.5x in throughput and\n3.5x in latency."}
{"id": "2509.20362", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20362", "abs": "https://arxiv.org/abs/2509.20362", "authors": ["Shaoyuan Xie", "Mohamad Habib Fakih", "Junchi Lu", "Fayzah Alshammari", "Ningfei Wang", "Takami Sato", "Halima Bouzidi", "Mohammad Abdullah Al Faruque", "Qi Alfred Chen"], "title": "FlyTrap: Physical Distance-Pulling Attack Towards Camera-based Autonomous Target Tracking Systems", "comment": "An extended version of the paper accepted by NDSS 2026", "summary": "Autonomous Target Tracking (ATT) systems, especially ATT drones, are widely\nused in applications such as surveillance, border control, and law enforcement,\nwhile also being misused in stalking and destructive actions. Thus, the\nsecurity of ATT is highly critical for real-world applications. Under the\nscope, we present a new type of attack: distance-pulling attacks (DPA) and a\nsystematic study of it, which exploits vulnerabilities in ATT systems to\ndangerously reduce tracking distances, leading to drone capturing, increased\nsusceptibility to sensor attacks, or even physical collisions. To achieve these\ngoals, we present FlyTrap, a novel physical-world attack framework that employs\nan adversarial umbrella as a deployable and domain-specific attack vector.\nFlyTrap is specifically designed to meet key desired objectives in attacking\nATT drones: physical deployability, closed-loop effectiveness, and\nspatial-temporal consistency. Through novel progressive distance-pulling\nstrategy and controllable spatial-temporal consistency designs, FlyTrap\nmanipulates ATT drones in real-world setups to achieve significant system-level\nimpacts. Our evaluations include new datasets, metrics, and closed-loop\nexperiments on real-world white-box and even commercial ATT drones, including\nDJI and HoverAir. Results demonstrate FlyTrap's ability to reduce tracking\ndistances within the range to be captured, sensor attacked, or even directly\ncrashed, highlighting urgent security risks and practical implications for the\nsafe deployment of ATT systems."}
