<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 59]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Hierarchical Multi-Modal Threat Intelligence Fusion Without Aligned Data: A Practical Framework for Real-World Security Operations](https://arxiv.org/abs/2510.15953)
*Sisir Doppalapudi*

Main category: cs.CR

TL;DR: HM-TIF是一个专门为非对齐多模态安全数据设计的威胁检测框架，通过分层交叉注意力和时间相关协议实现异构数据流的有效融合，在保持操作有效性的同时显著降低误报率。


<details>
  <summary>Details</summary>
Motivation: 现实中的多模态威胁检测面临安全工具孤立运行的问题，导致网络、邮件和系统数据流缺乏自然对齐和关联，需要专门针对这种非对齐数据场景的解决方案。

Method: 采用分层交叉注意力机制配合动态权重调整，结合新颖的时间相关协议来保持统计独立性，专门处理独立安全数据流的关联问题。

Result: 在多个数据集上达到88.7%的准确率，误报率降低32%，即使在模态缺失时也能保持鲁棒性，适合实际安全部署。

Conclusion: HM-TIF证明了多模态融合在数据不完全对齐的情况下仍能提供操作优势，为面临异构、非协调数据源的安全团队提供了实用的部署指南。

Abstract: Multi-modal threat detection faces a fundamental challenge that involves
security tools operating in isolation, and this creates streams of network,
email, and system data with no natural alignment or correlation. We present
Hierarchical Multi-Modal Threat Intelligence Fusion (HM-TIF), a framework
explicitly designed for this realistic scenario where naturally aligned
multi-modal attack data does not exist. Unlike prior work that assumes or
creates artificial alignment, we develop principled methods for correlating
independent security data streams while maintaining operational validity. Our
architecture employs hierarchical cross-attention with dynamic weighting that
adapts to data availability and threat context, coupled with a novel temporal
correlation protocol that preserves statistical independence. Evaluation on
UNSW-NB15, CSE-CIC-IDS2018, and CICBell-DNS2021 datasets demonstrates that
HM-TIF achieves 88.7% accuracy with a critical 32% reduction in false positive
rates, even without true multi-modal training data. The framework maintains
robustness when modalities are missing, making it immediately deployable in
real security operations where data streams frequently have gaps. Our
contributions include: (i) the first multi-modal security framework explicitly
designed for non-aligned data, (ii) a temporal correlation protocol that avoids
common data leakage pitfalls, (iii) empirical validation that multi-modal
fusion provides operational benefits even without perfect alignment, and (iv)
practical deployment guidelines for security teams facing heterogeneous,
uncoordinated data sources. Index Terms: multi-modal learning, threat
intelligence, non-aligned data, operational security, cross-attention
mechanisms, practical deployment

</details>


### [2] [A Graph-Attentive LSTM Model for Malicious URL Detection](https://arxiv.org/abs/2510.15971)
*Md. Ifthekhar Hossain,Kazi Abdullah Al Arafat,Bryce Shepard,Kayd Craig,Imtiaz Parvez*

Main category: cs.CR

TL;DR: 提出了一种名为GNN-GAT-LSTM的混合深度学习模型，结合图神经网络、图注意力网络和LSTM网络，用于恶意URL检测，在651,191个URL数据集上取得了98.06%的准确率和98.04%的加权F1分数。


<details>
  <summary>Details</summary>
Motivation: 恶意URL带来严重安全风险，包括网络钓鱼、恶意软件分发和网站篡改。传统的黑名单检测方法无法识别新的或混淆的URL，因为它们依赖于预先存在的模式。

Method: 将URL转换为图结构，字符作为节点通过边连接，使用独热编码表示节点特征。结合GNN、GAT和LSTM网络提取结构和序列模式，通过特征工程和数据平衡技术处理类别不平衡问题。

Result: 模型在测试集上达到0.9806的准确率和0.9804的加权F1分数，在大多数类别特别是良性URL和篡改URL上表现出优异的精确率和召回率。

Conclusion: 该模型为恶意URL检测提供了一个高效且可扩展的系统，在现实世界的网络安全应用中展现出强大潜力。

Abstract: Malicious URLs pose significant security risks as they facilitate phishing
attacks, distribute malware, and empower attackers to deface websites.
Blacklist detection methods fail to identify new or obfuscated URLs because
they depend on pre-existing patterns. This work presents a hybrid deep learning
model named GNN-GAT-LSTM that combines Graph Neural Networks (GNNs) with Graph
Attention Networks (GATs) and Long Short-Term Memory (LSTM) networks. The
proposed architecture extracts both the structural and sequential patterns of
the features from data. The model transforms URLs into graphs through a process
where characters become nodes that connect through edges. It applies one-hot
encoding to represent node features. The model received training and testing
data from a collection of 651,191 URLs, which were classified into benign,
phishing, defacement, and malware categories. The preprocessing stage included
both feature engineering and data balancing techniques, which addressed the
class imbalance issue to enhance model learning. The GNN-GAT-LSTM model
achieved outstanding performance through its test accuracy of 0.9806 and its
weighted F1-score of 0.9804. It showed excellent precision and recall
performance across most classes, particularly for benign and defacement URLs.
Overall, the model provides an efficient and scalable system for detecting
malicious URLs while demonstrating strong potential for real-world
cybersecurity applications.

</details>


### [3] [Safeguarding Efficacy in Large Language Models: Evaluating Resistance to Human-Written and Algorithmic Adversarial Prompts](https://arxiv.org/abs/2510.15973)
*Tiarnaigh Downey-Webb,Olamide Jogunola,Oluwaseun Ajao*

Main category: cs.CR

TL;DR: 对四个主流大语言模型（Phi-2、Llama-2-7B-Chat、GPT-3.5-Turbo、GPT-4）进行系统性安全评估，发现模型鲁棒性存在显著差异，Llama-2安全性最高，Phi-2最脆弱，攻击在不同模型间具有显著可转移性。


<details>
  <summary>Details</summary>
Motivation: 评估不同大语言模型对抗多样化对抗攻击的安全性，识别模型间的安全漏洞模式和可转移性特征。

Method: 使用SALAD-Bench数据集的1200个分层提示，评估四种攻击方法：人工编写提示、AutoDAN、GCG和TAP攻击，涵盖六个危害类别。

Result: Llama-2整体安全性最高（平均攻击成功率3.4%），Phi-2最脆弱（7.0%）。GCG和TAP攻击对目标模型Llama-2无效，但转移到其他模型时成功率显著提高（GPT-4达17%）。恶意使用类提示攻击成功率最高（10.71%）。

Conclusion: 揭示了跨模型安全漏洞的显著差异和攻击可转移性模式，为开发针对性防御机制提供了重要见解。

Abstract: This paper presents a systematic security assessment of four prominent Large
Language Models (LLMs) against diverse adversarial attack vectors. We evaluate
Phi-2, Llama-2-7B-Chat, GPT-3.5-Turbo, and GPT-4 across four distinct attack
categories: human-written prompts, AutoDAN, Greedy Coordinate Gradient (GCG),
and Tree-of-Attacks-with-pruning (TAP). Our comprehensive evaluation employs
1,200 carefully stratified prompts from the SALAD-Bench dataset, spanning six
harm categories. Results demonstrate significant variations in model
robustness, with Llama-2 achieving the highest overall security (3.4% average
attack success rate) while Phi-2 exhibits the greatest vulnerability (7.0%
average attack success rate). We identify critical transferability patterns
where GCG and TAP attacks, though ineffective against their target model
(Llama-2), achieve substantially higher success rates when transferred to other
models (up to 17% for GPT-4). Statistical analysis using Friedman tests reveals
significant differences in vulnerability across harm categories ($p < 0.001$),
with malicious use prompts showing the highest attack success rates (10.71%
average). Our findings contribute to understanding cross-model security
vulnerabilities and provide actionable insights for developing targeted defense
mechanisms

</details>


### [4] [Generative AI for Biosciences: Emerging Threats and Roadmap to Biosecurity](https://arxiv.org/abs/2510.15975)
*Zaixi Zhang,Souradip Chakraborty,Amrit Singh Bedi,Emilin Mathew,Varsha Saravanan,Le Cong,Alvaro Velasquez,Sheng Lin-Gibson,Megan Blewett,Dan Hendrycs,Alex John London,Ellen Zhong,Ben Raphael,Jian Ma,Eric Xing,Russ Altman,George Church,Mengdi Wang*

Main category: cs.CR

TL;DR: 生成式AI在生物科学中的快速发展带来了新的生物安全威胁，包括合成病毒蛋白或毒素的生成等双重用途风险，现有安全防护措施脆弱且易被绕过。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在生物科学领域的广泛应用，其带来的双重用途风险和生物安全威胁日益凸显，现有监管和技术防护存在明显不足，需要建立新的治理框架和安全技术。

Method: 通过130位来自学术界、政府、产业界和政策领域的专家访谈，分析当前生成式AI在生物科学中的应用现状、威胁向量和监管缺口，并探索多层次的技术缓解路径。

Result: 约76%的专家对AI在生物学中的滥用表示担忧，74%呼吁开发新的治理框架。研究提出了包括严格数据过滤、伦理对齐开发和实时监控在内的多层次安全防御策略。

Conclusion: 随着生成式AI融入生物科学领域，需要立即承诺采用适应性治理和安全设计技术，在整个生成式AI生命周期中嵌入安全保障，以应对这一前沿领域的安全挑战。

Abstract: The rapid adoption of generative artificial intelligence (GenAI) in the
biosciences is transforming biotechnology, medicine, and synthetic biology. Yet
this advancement is intrinsically linked to new vulnerabilities, as GenAI
lowers the barrier to misuse and introduces novel biosecurity threats, such as
generating synthetic viral proteins or toxins. These dual-use risks are often
overlooked, as existing safety guardrails remain fragile and can be
circumvented through deceptive prompts or jailbreak techniques. In this
Perspective, we first outline the current state of GenAI in the biosciences and
emerging threat vectors ranging from jailbreak attacks and privacy risks to the
dual-use challenges posed by autonomous AI agents. We then examine urgent gaps
in regulation and oversight, drawing on insights from 130 expert interviews
across academia, government, industry, and policy. A large majority ($\approx
76$\%) expressed concern over AI misuse in biology, and 74\% called for the
development of new governance frameworks. Finally, we explore technical
pathways to mitigation, advocating a multi-layered approach to GenAI safety.
These defenses include rigorous data filtering, alignment with ethical
principles during development, and real-time monitoring to block harmful
requests. Together, these strategies provide a blueprint for embedding security
throughout the GenAI lifecycle. As GenAI becomes integrated into the
biosciences, safeguarding this frontier requires an immediate commitment to
both adaptive governance and secure-by-design technologies.

</details>


### [5] [Learning to Watermark: A Selective Watermarking Framework for Large Language Models via Multi-Objective Optimization](https://arxiv.org/abs/2510.15976)
*Chenrui Wang,Junyi Shu,Billy Chiu,Yu Li,Saleh Alharbi,Min Zhang,Jing Li*

Main category: cs.CR

TL;DR: LTW是一种选择性水印框架，通过多目标优化平衡水印可检测性和文本质量，使用轻量网络自适应决定何时应用水印。


<details>
  <summary>Details</summary>
Motivation: 现有水印技术在水印可检测性和生成文本质量之间存在权衡，需要一种能有效平衡这两个竞争目标的方法。

Method: LTW使用轻量网络分析句子嵌入、令牌熵和当前水印比例来自适应决定何时应用水印，通过两个专门构建的损失函数进行多目标优化训练。

Result: 实验评估表明，LTW与两种基线水印方法结合时，能显著提高文本质量而不损害可检测性。

Conclusion: LTW为LLM水印设计提供了新视角，能够在保持水印的同时维持高质量的文本生成。

Abstract: The rapid development of LLMs has raised concerns about their potential
misuse, leading to various watermarking schemes that typically offer high
detectability. However, existing watermarking techniques often face trade-off
between watermark detectability and generated text quality. In this paper, we
introduce Learning to Watermark (LTW), a novel selective watermarking framework
that leverages multi-objective optimization to effectively balance these
competing goals. LTW features a lightweight network that adaptively decides
when to apply the watermark by analyzing sentence embeddings, token entropy,
and current watermarking ratio. Training of the network involves two
specifically constructed loss functions that guide the model toward
Pareto-optimal solutions, thereby harmonizing watermark detectability and text
quality. By integrating LTW with two baseline watermarking methods, our
experimental evaluations demonstrate that LTW significantly enhances text
quality without compromising detectability. Our selective watermarking approach
offers a new perspective for designing watermarks for LLMs and a way to
preserve high text quality for watermarks. The code is publicly available at:
https://github.com/fattyray/learning-to-watermark

</details>


### [6] [Meta-Guardian: An Early Evaluation of an On-device Application to Mitigate Psychography Data Leakage in Immersive Technologies](https://arxiv.org/abs/2510.15989)
*Keshav Sood,Sanjay Selvaraj,Youyang Qu*

Main category: cs.CR

TL;DR: 提出了Meta-Guardian系统，这是一个在VR头显内实时识别和过滤生物特征信号的隐私保护架构，以解决沉浸式技术中的隐私问题。


<details>
  <summary>Details</summary>
Motivation: 沉浸式技术（VR/AR/MR）需要收集生物特征数据来创造沉浸式体验，但这些实时反馈信息包含敏感数据，存在严重的隐私问题。现有研究大多忽略了头戴显示系统中实时生物特征数据过滤的复杂性。

Method: 开发了模块化的Unity软件开发工具包（SDK），与主流沉浸式平台兼容。系统使用机器学习模型进行信号分类，并通过过滤机制阻止敏感数据传输或存储。

Result: 实现了一个名为Meta-Guardian的隐私保护框架，能够在VR头显内实时过滤生物特征信号。

Conclusion: 该框架使开发者能够在各种头显和应用中嵌入隐私设计原则，为沉浸式体验提供隐私保护解决方案。

Abstract: The use of Immersive Technologies has shown its potential to revolutionize
many sectors such as health, entertainment, education, and industrial sectors.
Immersive technologies such as Virtual Reality (VR), Augmented reality (AR),
and Mixed Reality (MR) have redefined user interaction through real-time
biometric and behavioral tracking. Although Immersive Technologies (XR)
essentially need the collection of the biometric data which acts as a baseline
to create immersive experience, however, this ongoing feedback information
(includes biometrics) poses critical privacy concerns due to the sensitive
nature of the data collected. A comprehensive review of recent literature
explored the technical dimensions of related problem; however, they largely
overlook the challenge particularly the intricacies of real-time biometric data
filtering within head-mounted display system. Motivated from this, in this
work, we propose a novel privacy-preserving system architecture that identifies
and filters biometric signals (within the VR headset) in real-time before
transmission or storage. Implemented as a modular Unity Software-development
Kit (SDK) compatible with major immersive platforms, our solution (named
Meta-Guardian) employs machine learning models for signal classification and a
filtering mechanism to block sensitive data. This framework aims to enable
developers to embed privacy-by-design principles into immersive experiences on
various headsets and applications.

</details>


### [7] [MCP Security Bench (MSB): Benchmarking Attacks Against Model Context Protocol in LLM Agents](https://arxiv.org/abs/2510.15994)
*Dongsen Zhang,Zekun Li,Xu Luo,Xuannan Liu,Peipei Li,Wenjun Xu*

Main category: cs.CR

TL;DR: MSB是首个端到端的MCP安全基准测试套件，系统评估LLM代理在整个工具使用流程中对MCP特定攻击的抵抗能力，包含12种攻击分类和实际工具执行环境。


<details>
  <summary>Details</summary>
Motivation: MCP协议虽然实现了工具互操作性，但也扩大了攻击面，使工具成为具有自然语言元数据和标准化I/O的一等可组合对象，需要系统评估其安全性。

Method: 开发了包含12种攻击分类的评估框架，通过实际运行工具而非模拟来执行攻击，使用Net Resilient Performance (NRP)指标量化安全与性能的权衡。

Result: 评估了9个流行LLM代理在10个领域400+工具上的2000个攻击实例，发现性能更强的模型由于出色的工具调用和指令遵循能力反而更容易受到攻击。

Conclusion: MSB为研究人员和从业者提供了研究、比较和强化MCP代理的实用基准，揭示了MCP各阶段攻击的有效性。

Abstract: The Model Context Protocol (MCP) standardizes how large language model (LLM)
agents discover, describe, and call external tools. While MCP unlocks broad
interoperability, it also enlarges the attack surface by making tools
first-class, composable objects with natural-language metadata, and
standardized I/O. We present MSB (MCP Security Benchmark), the first end-to-end
evaluation suite that systematically measures how well LLM agents resist
MCP-specific attacks throughout the full tool-use pipeline: task planning, tool
invocation, and response handling. MSB contributes: (1) a taxonomy of 12
attacks including name-collision, preference manipulation, prompt injections
embedded in tool descriptions, out-of-scope parameter requests,
user-impersonating responses, false-error escalation, tool-transfer, retrieval
injection, and mixed attacks; (2) an evaluation harness that executes attacks
by running real tools (both benign and malicious) via MCP rather than
simulation; and (3) a robustness metric that quantifies the trade-off between
security and performance: Net Resilient Performance (NRP). We evaluate nine
popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack
instances. Results reveal the effectiveness of attacks against each stage of
MCP. Models with stronger performance are more vulnerable to attacks due to
their outstanding tool calling and instruction following capabilities. MSB
provides a practical baseline for researchers and practitioners to study,
compare, and harden MCP agents.

</details>


### [8] [Breaking Guardrails, Facing Walls: Insights on Adversarial AI for Defenders & Researchers](https://arxiv.org/abs/2510.16005)
*Giacomo Bertollo,Naz Bodemir,Jonah Burgess*

Main category: cs.CR

TL;DR: 该论文通过分析500名CTF参与者发现，虽然参与者能轻松绕过简单的AI防护措施，但多层次的防御系统仍构成显著挑战，为构建更安全的AI系统提供了具体见解。


<details>
  <summary>Details</summary>
Motivation: 研究AI系统的安全防护能力，了解现有防护措施的弱点，为开发更强大的AI安全防御提供实证基础。

Method: 通过CTF（夺旗赛）实验，让500名参与者尝试绕过不同类型的AI防护措施，包括简单防护和多层次防御系统。

Result: 参与者能够轻松绕过简单的AI防护措施，但在面对多层次、多步骤的防御系统时遇到了显著困难。

Conclusion: 多层次的防御策略比单一防护措施更有效，这为构建更安全的AI系统提供了重要指导。

Abstract: Analyzing 500 CTF participants, this paper shows that while participants
readily bypassed simple AI guardrails using common techniques, layered
multi-step defenses still posed significant challenges, offering concrete
insights for building safer AI systems.

</details>


### [9] [On-Chain Decentralized Learning and Cost-Effective Inference for DeFi Attack Mitigation](https://arxiv.org/abs/2510.16024)
*Abdulrahman Alhaidari,Balaji Palanisamy,Prashant Krishnamurthy*

Main category: cs.CR

TL;DR: 提出了首个完全链上的去中心化学习框架，通过Layer-2执行计算密集型任务，在Layer-1进行验证推理，能够实时检测和阻止DeFi漏洞利用交易。


<details>
  <summary>Details</summary>
Motivation: DeFi平台每年因业务逻辑或会计漏洞损失数十亿美元，现有防御方法无法阻止通过私有中继或恶意合约在同一区块内执行的攻击。

Method: 采用Proof-of-Improvement协议管理训练过程，通过量化技术和循环展开实现在以太坊区块gas限制内进行逻辑回归、SVM、MLP、CNN和门控RNN的推理。

Result: 收集了2020-2025年间8条EVM链上的298个独特真实漏洞利用案例，涉及402次攻击交易，总损失达37.4亿美元。

Conclusion: 该框架能够在智能合约内部实现低延迟、gas受限的推理，为DeFi安全提供了有效的链上防御解决方案。

Abstract: Billions of dollars are lost every year in DeFi platforms by transactions
exploiting business logic or accounting vulnerabilities. Existing defenses
focus on static code analysis, public mempool screening, attacker contract
detection, or trusted off-chain monitors, none of which prevents exploits
submitted through private relays or malicious contracts that execute within the
same block. We present the first decentralized, fully on-chain learning
framework that: (i) performs gas-prohibitive computation on Layer-2 to reduce
cost, (ii) propagates verified model updates to Layer-1, and (iii) enables
gas-bounded, low-latency inference inside smart contracts. A novel
Proof-of-Improvement (PoIm) protocol governs the training process and verifies
each decentralized micro update as a self-verifying training transaction.
Updates are accepted by \textit{PoIm} only if they demonstrably improve at
least one core metric (e.g., accuracy, F1-score, precision, or recall) on a
public benchmark without degrading any of the other core metrics, while
adversarial proposals get financially penalized through an adaptable test set
for evolving threats. We develop quantization and loop-unrolling techniques
that enable inference for logistic regression, SVM, MLPs, CNNs, and gated RNNs
(with support for formally verified decision tree inference) within the
Ethereum block gas limit, while remaining bit-exact to their off-chain
counterparts, formally proven in Z3. We curate 298 unique real-world exploits
(2020 - 2025) with 402 exploit transactions across eight EVM chains,
collectively responsible for \$3.74 B in losses.

</details>


### [10] [Resource Estimation of CGGI and CKKS scheme workloads on FracTLcore Computing Fabric](https://arxiv.org/abs/2510.16025)
*Denis Ovichinnikov,Hemant Kavadia,Satya Keerti Chand Kudupudi,Ilya Rempel,Vineet Chadha,Marty Franz,Paul Master,Craig Gentry,Darlene Kindler,Alberto Reyes,Muthu Annamalai*

Main category: cs.CR

TL;DR: Cornami Mx2处理器通过基于脉动阵列核心和内存计算能力的架构，加速全同态加密(FHE)应用，解决了计算瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 全同态加密应用面临计算性能限制，需要专门的硬件架构来加速FHE计算。

Method: 采用基于脉动阵列核心的处理器架构，具有内存计算能力和片上网络(NoC)，称为FracTLcore计算结构处理器。

Result: 开发了编译器后端来估计处理器资源，支持CGGI(TFHE-rs)和CKKS两种FHE方案的工作负载计算。

Conclusion: Cornami Mx2处理器能够运行TFHE-rs布尔方案和CKKS方案的全同态加密应用，提供了专门的硬件加速解决方案。

Abstract: Cornami Mx2 accelerates of Fully Homomorphic Encryption (FHE) applications,
enabled by breakthrough work [1], which are otherwise compute limited. Our
processor architecture is based on the systolic array of cores with in-memory
compute capability and a network on chip (NoC) processor architecture called
the "FracTLcore compute fabric processor" (Mx2). Here, we describe the work to
estimate processor resources to compute workload in CGGI (TFHE-rs) or CKKS
scheme during construction of our compiler backend for this architecture [2].
These processors are available for running applications in both the TFHE-rs
Boolean scheme and CKKS scheme FHE applications.

</details>


### [11] [Nondeterminism-Aware Optimistic Verification for Floating-Point Neural Networks](https://arxiv.org/abs/2510.16028)
*Jianzhu Yao,Hongxu Su,Taobo Liao,Zerui Cheng,Huan Zhang,Xuechao Wang,Pramod Viswanath*

Main category: cs.CR

TL;DR: NAO是一个针对异构硬件上浮点神经网络执行的乐观验证协议，通过结合理论误差边界和实证百分位阈值来验证ML服务输出，无需可信硬件或确定性内核。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络在用户无法控制的硬件上运行，ML服务缺乏透明度，用户难以验证输出是否忠实反映预期输入，现有方法要么不实用要么重新引入供应商信任。

Method: NAO采用乐观验证协议，接受算子级接受区域内的输出，结合IEEE-754最坏情况边界和跨硬件校准的紧致经验百分位分布，通过Merkle锚定的争议游戏递归分区计算图进行裁决。

Result: 在A100、H100、RTX6000、RTX4090等硬件上，经验阈值比理论边界紧致10^2-10^3倍，边界感知对抗攻击成功率为0%，运行时开销仅为0.3%。

Conclusion: NAO为现实世界异构ML计算实现了可扩展性与可验证性的统一。

Abstract: Neural networks increasingly run on hardware outside the user's control
(cloud GPUs, inference marketplaces). Yet ML-as-a-Service reveals little about
what actually ran or whether returned outputs faithfully reflect the intended
inputs. Users lack recourse against service downgrades (model swaps,
quantization, graph rewrites, or discrepancies like altered ad embeddings).
Verifying outputs is hard because floating-point(FP) execution on heterogeneous
accelerators is inherently nondeterministic. Existing approaches are either
impractical for real FP neural networks or reintroduce vendor trust. We present
NAO: a Nondeterministic tolerance Aware Optimistic verification protocol that
accepts outputs within principled operator-level acceptance regions rather than
requiring bitwise equality. NAO combines two error models: (i) sound
per-operator IEEE-754 worst-case bounds and (ii) tight empirical percentile
profiles calibrated across hardware. Discrepancies trigger a Merkle-anchored,
threshold-guided dispute game that recursively partitions the computation graph
until one operator remains, where adjudication reduces to a lightweight
theoretical-bound check or a small honest-majority vote against empirical
thresholds. Unchallenged results finalize after a challenge window, without
requiring trusted hardware or deterministic kernels. We implement NAO as a
PyTorch-compatible runtime and a contract layer currently deployed on Ethereum
Holesky testnet. The runtime instruments graphs, computes per-operator bounds,
and runs unmodified vendor kernels in FP32 with negligible overhead (0.3% on
Qwen3-8B). Across CNNs, Transformers and diffusion models on A100, H100,
RTX6000, RTX4090, empirical thresholds are $10^2-10^3$ times tighter than
theoretical bounds, and bound-aware adversarial attacks achieve 0% success. NAO
reconciles scalability with verifiability for real-world heterogeneous ML
compute.

</details>


### [12] [Membership Inference over Diffusion-models-based Synthetic Tabular Data](https://arxiv.org/abs/2510.16037)
*Peini Cheng,Amir Bahmani*

Main category: cs.CR

TL;DR: 该研究调查了基于扩散的合成表格数据生成方法存在的隐私风险，重点关注其对成员推理攻击的易感性。研究发现TabDDPM模型比TabSyn更容易受到攻击。


<details>
  <summary>Details</summary>
Motivation: 评估扩散模型在合成表格数据生成中的隐私影响，揭示这些方法可能面临的成员推理攻击风险。

Method: 开发基于逐步误差比较方法的查询式成员推理攻击，对TabDDPM和TabSyn两种最新模型进行测试。

Result: TabDDPM对成员推理攻击表现出较高脆弱性，而TabSyn则对这些攻击具有抵抗力。

Conclusion: 需要评估扩散模型的隐私影响，并鼓励进一步研究合成数据生成的鲁棒隐私保护机制。

Abstract: This study investigates the privacy risks associated with diffusion-based
synthetic tabular data generation methods, focusing on their susceptibility to
Membership Inference Attacks (MIAs). We examine two recent models, TabDDPM and
TabSyn, by developing query-based MIAs based on the step-wise error comparison
method. Our findings reveal that TabDDPM is more vulnerable to these attacks.
TabSyn exhibits resilience against our attack models. Our work underscores the
importance of evaluating the privacy implications of diffusion models and
encourages further research into robust privacy-preserving mechanisms for
synthetic data generation.

</details>


### [13] [A Novel GPT-Based Framework for Anomaly Detection in System Logs](https://arxiv.org/abs/2510.16044)
*Zeng Zhang,Wenjie Yin,Xiaoqi Li*

Main category: cs.CR

TL;DR: 提出了一种基于GPT的系统日志智能异常检测方法，通过结构化输入设计和Focal Loss优化策略，显著提升了日志异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 系统日志异常检测在网络安全防御中至关重要，但面临数据量大、异常分布不均和传统方法精度不足等挑战。

Method: 使用Drain解析器将原始日志转换为事件ID序列，采用Focal Loss损失函数解决类别不平衡问题，优化GPT-2模型进行异常检测。

Result: 优化后的GPT-2模型在精确率、召回率和F1分数等关键指标上显著优于未优化模型，在某些任务中表现与GPT-3.5 API相当或更优。

Conclusion: 基于GPT的智能检测方法结合结构化输入和Focal Loss优化，能有效提升系统日志异常检测性能，为网络安全防御提供有力支持。

Abstract: Identification of anomalous events within system logs constitutes a pivotal
element within the frame- work of cybersecurity defense strategies. However,
this process faces numerous challenges, including the management of substantial
data volumes, the distribution of anomalies, and the precision of con-
ventional methods. To address this issue, the present paper puts forward a
proposal for an intelligent detection method for system logs based on Genera-
tive Pre-trained Transformers (GPT). The efficacy of this approach is
attributable to a combination of structured input design and a Focal Loss op-
timization strategy, which collectively result in a substantial enhancement of
the performance of log anomaly detection. The initial approach involves the
conversion of raw logs into event ID sequences through the use of the Drain
parser. Subsequently, the Focal Loss loss function is employed to address the
issue of class imbalance. The experimental re- sults demonstrate that the
optimized GPT-2 model significantly outperforms the unoptimized model in a
range of key metrics, including precision, recall, and F1 score. In specific
tasks, comparable or superior performance has been demonstrated to that of the
GPT-3.5 API.

</details>


### [14] [PrivacyPAD: A Reinforcement Learning Framework for Dynamic Privacy-Aware Delegation](https://arxiv.org/abs/2510.16054)
*Zheng Hui,Yijiang River Dong,Sanhanat Sivapiromrat,Ehsan Shareghi,Nigel Collier*

Main category: cs.CR

TL;DR: 提出了PrivacyPAD强化学习框架，通过动态路由文本块来平衡隐私保护与任务性能，在敏感数据环境中实现最优的隐私-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 解决用户在使用大语言模型时面临的隐私与性能两难选择：要么使用强大的专有LLM但面临数据泄露风险，要么使用本地小模型保证隐私但性能下降。

Method: 将隐私保护委托问题重构为顺序决策问题，使用强化学习训练智能体动态路由文本块，区分可替换的个人身份信息（本地处理）和任务关键信息（远程处理）。

Result: 在包含高密度PII的新医疗数据集上验证，在隐私-效用边界上达到了新的最优性能。

Conclusion: 证明了学习型自适应策略在敏感环境中部署LLM的必要性，能够智能平衡隐私保护与任务性能。

Abstract: When users submit queries to Large Language Models (LLMs), their prompts can
often contain sensitive data, forcing a difficult choice: Send the query to a
powerful proprietary LLM providers to achieving state-of-the-art performance
and risk data exposure, or relying on smaller, local models guarantees data
privacy but often results in a degradation of task performance. Prior
approaches have relied on static pipelines that use LLM rewriting, which
shatters linguistic coherence and indiscriminately removes privacy-sensitive
information, including task-critical content. We reformulate this challenge
(Privacy-Conscious Delegation) as a sequential decision-making problem and
introduce a novel reinforcement learning (RL) framework called PrivacyPAD to
solve it. Our framework trains an agent to dynamically route text chunks,
learning a policy that optimally balances the trade-off between privacy leakage
and task performance. It implicitly distinguishes between replaceable
Personally Identifiable Information (PII) (which it shields locally) and
task-critical PII (which it strategically sends to the remote model for maximal
utility). To validate our approach in complex scenarios, we also introduce a
new medical dataset with high PII density. Our framework achieves a new
state-of-the-art on the privacy-utility frontier, demonstrating the necessity
of learned, adaptive policies for deploying LLMs in sensitive environments.

</details>


### [15] [A Multi-Cloud Framework for Zero-Trust Workload Authentication](https://arxiv.org/abs/2510.16067)
*Saurabh Deochake,Ryan Murphy,Jeremiah Gearheart*

Main category: cs.CR

TL;DR: 提出基于Workload Identity Federation和OpenID Connect的无密钥认证框架，使用加密验证的临时令牌替代静态长期凭据，在多云环境中实现零信任原则。


<details>
  <summary>Details</summary>
Motivation: 静态长期凭据存在安全风险，违反零信任原则，需要解决工作负载认证中的凭据盗窃问题。

Method: 使用Workload Identity Federation和OpenID Connect构建多云框架，采用加密验证的临时令牌进行无密钥认证。

Result: 在企业级Kubernetes环境中验证了该框架，显著减少了攻击面。

Conclusion: 该模型提供了跨不同云平台统一管理工作负载身份的解决方案，为未来实现基于属性的访问控制奠定了基础。

Abstract: Static, long-lived credentials for workload authentication create untenable
security risks that violate Zero-Trust principles. This paper presents a
multi-cloud framework using Workload Identity Federation (WIF) and OpenID
Connect (OIDC) for secretless authentication. Our approach uses
cryptographically-verified, ephemeral tokens, allowing workloads to
authenticate without persistent private keys and mitigating credential theft.
We validate this framework in an enterprise-scale Kubernetes environment, which
significantly reduces the attack surface. The model offers a unified solution
to manage workload identities across disparate clouds, enabling future
implementation of robust, attribute-based access control.

</details>


### [16] [ISO/IEC-Compliant Match-on-Card Face Verification with Short Binary Templates](https://arxiv.org/abs/2510.16078)
*Abdelilah Ganmati,Karim Afdel,Lahcen Koutti*

Main category: cs.CR

TL;DR: 提出了一种实用的卡上匹配人脸验证设计，使用64/128位紧凑模板，通过PCA-ITQ离线生成模板，在卡上通过恒定时间汉明距离进行比较，满足ISO/IEC传输约束和隐私目标。


<details>
  <summary>Details</summary>
Motivation: 设计一种满足ISO/IEC 7816-4和14443-4标准的人脸验证系统，实现紧凑模板、固定载荷、仅决策状态字（无分数泄露）的安全验证，同时确保恒定时间匹配以抵御时序攻击。

Method: 使用PCA-ITQ生成64/128位二进制模板，在卡上通过恒定时间汉明距离进行匹配，采用固定长度的APDU命令载荷和仅决策状态字，最小化EEPROM存储需求。

Result: 在CelebA数据集（55个身份，412张图像）上测试，64位和128位模板在FAR=1%时达到TPR=0.836，验证时间在9.6kbps下分别为43.9ms和52.3ms，在38.4kbps下均小于14ms。128位模板相比64位降低了EER。

Conclusion: 短二进制模板、固定载荷仅决策APDU和恒定时间匹配能够满足ISO/IEC传输约束，具有宽裕的时间余量，并符合ISO/IEC 24745隐私目标。当前局限是单数据集评估和设计级时序分析，下一步将扩展到AgeDB/CFP-FP数据集并进行卡上微基准测试。

Abstract: We present a practical match-on-card design for face verification in which
compact 64/128-bit templates are produced off-card by PCA-ITQ and compared
on-card via constant-time Hamming distance. We specify ISO/IEC 7816-4 and
14443-4 command APDUs with fixed-length payloads and decision-only status words
(no score leakage), together with a minimal per-identity EEPROM map. Using real
binary codes from a CelebA working set (55 identities, 412 images), we (i)
derive operating thresholds from ROC/DET, (ii) replay enroll->verify
transactions at those thresholds, and (iii) bound end-to-end time by pure link
latency plus a small constant on-card budget. Even at the slowest contact rate
(9.6 kbps), total verification time is 43.9 ms (64 b) and 52.3 ms (128 b); at
38.4 kbps both are <14 ms. At FAR = 1%, both code lengths reach TPR = 0.836,
while 128 b lowers EER relative to 64 b. An optional +6 B helper (targeted
symbol-level parity over empirically unstable bits) is latency-negligible.
Overall, short binary templates, fixed-payload decision-only APDUs, and
constant-time matching satisfy ISO/IEC transport constraints with wide timing
margin and align with ISO/IEC 24745 privacy goals. Limitations: single-dataset
evaluation and design-level (pre-hardware) timing; we outline AgeDB/CFP-FP and
on-card microbenchmarks as next steps.

</details>


### [17] [Towards a Blockchain-Based CI/CD Framework to Enhance Security in Cloud Environments](https://arxiv.org/abs/2510.16087)
*Sabbir M Saleh,Nazim Madhavji,John Steinbacher*

Main category: cs.CR

TL;DR: 提出基于区块链的解决方案来增强云平台中CI/CD管道的安全性，通过分布式账本和防篡改特性来保护软件部署过程。


<details>
  <summary>Details</summary>
Motivation: 针对云平台中CI/CD管道面临的安全威胁，特别是近期发生的网络攻击事件，需要更强大的安全防护机制。

Method: 利用区块链的分布式账本技术和防篡改特性，集成威胁建模框架，遵循编码标准，并采用自动化安全测试工具来检测漏洞。

Result: 开发了一个框架，能够自动检测公开披露的漏洞和缺陷，如过时的Java Spring框架版本、未经验证的JavaScript库或允许SQL注入攻击的数据库库。

Conclusion: 区块链技术可以有效增强CI/CD管道的安全性，通过不可篡改的记录和自动化安全测试，确保软件部署过程的安全性和可靠性。

Abstract: Security is becoming a pivotal point in cloud platforms. Several divisions,
such as business organisations, health care, government, etc., have experienced
cyber-attacks on their infrastructures. This research focuses on security
issues within Continuous Integration and Deployment (CI/CD) pipelines in a
cloud platform as a reaction to recent cyber breaches. This research proposes a
blockchain-based solution to enhance CI/CD pipeline security. This research
aims to develop a framework that leverages blockchain's distributed ledger
technology and tamper-resistant features to improve CI/CD pipeline security.
The goal is to emphasise secure software deployment by integrating threat
modelling frameworks and adherence to coding standards. It also aims to employ
tools to automate security testing to detect publicly disclosed vulnerabilities
and flaws, such as an outdated version of Java Spring Framework, a JavaScript
library from an unverified source, or a database library that allows SQL
injection attacks in the deployed software through the framework.

</details>


### [18] [The Hidden Cost of Modeling P(X): Vulnerability to Membership Inference Attacks in Generative Text Classifiers](https://arxiv.org/abs/2510.16122)
*Owais Makroo,Siva Rajesh Kasa,Sumegh Roychowdhury,Karan Gupta,Nikhil Pattisapu,Santhosh Kasa,Sumit Negi*

Main category: cs.CR

TL;DR: 本文系统比较了生成式和判别式分类器在成员推理攻击(MIA)中的脆弱性，发现完全生成式分类器由于显式建模联合概率P(X,Y)而最易遭受成员信息泄露，揭示了分类器设计中存在的效用-隐私权衡。


<details>
  <summary>Details</summary>
Motivation: 现有研究对生成式和判别式分类器在成员推理攻击中的系统性比较有限，本文旨在填补这一空白，从理论上分析生成式分类器为何更易受MIA影响。

Method: 通过理论分析和实证评估，研究涵盖判别式、生成式和伪生成式文本分类器，在九个基准数据集上使用多种MIA策略进行测试，考察不同训练数据量下的表现。

Result: 实证结果表明，完全生成式分类器（显式建模P(X,Y)）对成员推理攻击最为脆弱，且生成式分类器的典型推理方法显著加剧了隐私风险。

Conclusion: 生成式分类器存在严重的隐私泄露风险，在隐私敏感应用中需谨慎使用，未来需要开发既能保持效用又能缓解MIA漏洞的隐私保护生成式分类器。

Abstract: Membership Inference Attacks (MIAs) pose a critical privacy threat by
enabling adversaries to determine whether a specific sample was included in a
model's training dataset. Despite extensive research on MIAs, systematic
comparisons between generative and discriminative classifiers remain limited.
This work addresses this gap by first providing theoretical motivation for why
generative classifiers exhibit heightened susceptibility to MIAs, then
validating these insights through comprehensive empirical evaluation. Our study
encompasses discriminative, generative, and pseudo-generative text classifiers
across varying training data volumes, evaluated on nine benchmark datasets.
Employing a diverse array of MIA strategies, we consistently demonstrate that
fully generative classifiers which explicitly model the joint likelihood
$P(X,Y)$ are most vulnerable to membership leakage. Furthermore, we observe
that the canonical inference approach commonly used in generative classifiers
significantly amplifies this privacy risk. These findings reveal a fundamental
utility-privacy trade-off inherent in classifier design, underscoring the
critical need for caution when deploying generative classifiers in
privacy-sensitive applications. Our results motivate future research directions
in developing privacy-preserving generative classifiers that can maintain
utility while mitigating membership inference vulnerabilities.

</details>


### [19] [Prompt injections as a tool for preserving identity in GAI image descriptions](https://arxiv.org/abs/2510.16128)
*Kate Glazko,Jennifer Mankoff*

Main category: cs.CR

TL;DR: 该论文提出将提示注入作为一种工具，让间接用户能够通过在自己的内容中嵌入提示来减轻生成式AI对他们造成的偏见和代表性不足等伤害。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的偏见和缺乏代表性等问题会影响那些不直接与AI系统交互但内容被AI使用的间接用户。现有缓解方法大多需要自上而下或外部干预，缺乏用户自主性。

Method: 将提示注入重新定义为内容所有者抵抗的工具，而非恶意攻击向量。通过案例研究展示如何在图像中嵌入提示，以在GAI描述图像时保留所有者的性别和残疾身份。

Result: 演示了提示注入能够有效帮助间接用户在GAI系统描述其内容时保留个人身份特征，如性别和残疾身份。

Conclusion: 提示注入可以作为一种赋权工具，让间接用户能够自主减轻生成式AI对他们造成的伤害，而不依赖外部干预。

Abstract: Generative AI risks such as bias and lack of representation impact people who
do not interact directly with GAI systems, but whose content does: indirect
users. Several approaches to mitigating harms to indirect users have been
described, but most require top down or external intervention. An emerging
strategy, prompt injections, provides an empowering alternative: indirect users
can mitigate harm against them, from within their own content. Our approach
proposes prompt injections not as a malicious attack vector, but as a tool for
content/image owner resistance. In this poster, we demonstrate one case study
of prompt injections for empowering an indirect user, by retaining an image
owner's gender and disabled identity when an image is described by GAI.

</details>


### [20] [WebRTC Metadata and IP Leakage in Modern Browsers: A Cross-Platform Measurement Study](https://arxiv.org/abs/2510.16168)
*Ahmed Fouad Kadhim Koysha,Aytug Boyaci,Rafet Akdeniz*

Main category: cs.CR

TL;DR: 对Chrome、Brave、Firefox和Tor浏览器在2025年版本的WebRTC元数据泄漏进行跨平台测量研究，发现Chrome泄漏最严重，Brave暴露mDNS标识符，Firefox在Android上泄漏内部IP，Tor完全防护。提出了分层缓解策略。


<details>
  <summary>Details</summary>
Motivation: WebRTC的ICE过程会无意中暴露内部和公共IP地址作为元数据，存在隐私风险，需要评估当前浏览器的防护状况。

Method: 使用当前(2025年)版本的Chrome、Brave、Firefox和Tor浏览器，在桌面和移动平台上，通过半可信Wi-Fi和不可信移动运营商网络进行跨平台测量研究。

Result: Chrome在移动端泄漏LAN或CGNAT地址，桌面端泄漏元数据；Brave避免直接IP泄漏但暴露会话稳定的mDNS标识符；Firefox在桌面端提供强保护但在Android上泄漏内部IP；Tor始终防止所有形式的泄漏。

Conclusion: 虽然直接LAN泄漏在减少，但mDNS和CGNAT等新兴载体造成持续隐私风险，需要协议级重新设计和政策行动。

Abstract: Web Real-Time Communication (WebRTC) enables real-time peer-to-peer
communication, but its Interactive Connectivity Establishment (ICE) process can
unintentionally expose internal and public IP addresses as metadata. This paper
presents a cross-platform measurement study of WebRTC metadata leakage using
current (2025) builds of Chrome, Brave, Firefox, and Tor on desktop and mobile
platforms. Experiments were conducted across semi-trusted Wi-Fi and untrusted
mobile carrier networks. Results show that Chrome remains the most
leakage-prone, disclosing LAN or Carrier-Grade NAT (CGNAT) addresses on mobile
and metadata on desktop; Brave avoids direct IP leaks but exposes
session-stable mDNS identifiers; Firefox provides strong protection on desktop
but leaks internal IPs on Android; and Tor consistently prevents all forms of
leakage. We introduce a structured threat model for semi-trusted environments
and evaluate the limitations of mDNS obfuscation. Finally, we propose layered
mitigation strategies combining browser defaults, institutional safeguards, and
user controls. Findings demonstrate that while direct LAN leakage is declining,
emerging vectors such as mDNS and CGNAT create persistent privacy risks
requiring protocol-level redesign and policy action.

</details>


### [21] [SentinelNet: Safeguarding Multi-Agent Collaboration Through Credit-Based Dynamic Threat Detection](https://arxiv.org/abs/2510.16219)
*Yang Feng,Xudong Pan*

Main category: cs.CR

TL;DR: SentinelNet是一个去中心化的恶意行为检测框架，通过信用评估和对比学习在多智能体系统中实现主动防御，在实验中达到接近完美的恶意智能体检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法存在反应式设计或集中式架构的缺陷，容易导致单点故障，无法有效应对恶意智能体对多智能体系统可靠性和决策能力的威胁。

Method: 为每个智能体配备基于信用的检测器，通过对比学习在增强的对抗性辩论轨迹上训练，实现消息可信度自主评估和动态邻居排名，采用bottom-k消除机制抑制恶意通信。

Result: 在多智能体系统基准测试中，SentinelNet在两轮辩论内实现接近100%的恶意智能体检测率，从受损基线中恢复95%的系统准确率。

Conclusion: SentinelNet建立了保护协作多智能体系统的新范式，在不同领域和攻击模式中展现出强大的泛化能力。

Abstract: Malicious agents pose significant threats to the reliability and
decision-making capabilities of Multi-Agent Systems (MAS) powered by Large
Language Models (LLMs). Existing defenses often fall short due to reactive
designs or centralized architectures which may introduce single points of
failure. To address these challenges, we propose SentinelNet, the first
decentralized framework for proactively detecting and mitigating malicious
behaviors in multi-agent collaboration. SentinelNet equips each agent with a
credit-based detector trained via contrastive learning on augmented adversarial
debate trajectories, enabling autonomous evaluation of message credibility and
dynamic neighbor ranking via bottom-k elimination to suppress malicious
communications. To overcome the scarcity of attack data, it generates
adversarial trajectories simulating diverse threats, ensuring robust training.
Experiments on MAS benchmarks show SentinelNet achieves near-perfect detection
of malicious agents, close to 100% within two debate rounds, and recovers 95%
of system accuracy from compromised baselines. By exhibiting strong
generalizability across domains and attack patterns, SentinelNet establishes a
novel paradigm for safeguarding collaborative MAS.

</details>


### [22] [C/N0 Analysis-Based GPS Spoofing Detection with Variable Antenna Orientations](https://arxiv.org/abs/2510.16229)
*Vienna Li,Justin Villa,Dan Diessner,Jayson Clifford,Laxima Niure Kandel*

Main category: cs.CR

TL;DR: 本文提出了一种基于卫星载波噪声密度比(C/N₀)变化分析的GPS欺骗检测方法，通过控制天线方向来识别欺骗信号的特征模式。


<details>
  <summary>Details</summary>
Motivation: GPS欺骗对航空安全构成日益严重的威胁，需要开发有效的检测方法来保护飞机导航系统免受误导。

Method: 使用u-blox EVK-M8U接收器和GPSG-1000卫星模拟器，在三种天线方向（水平、右倾、左倾）下收集真实信号和欺骗信号的C/N₀数据。

Result: 真实信号的C/N₀随方向自然波动，而欺骗信号在水平方向（正对欺骗天线）显示最高C/N₀值，倾斜方向因与欺骗源不对齐而C/N₀降低。

Conclusion: 通过简单的机动操作（如短暂倾斜）诱导C/N₀变化，可为通用航空和无人机系统提供GPS欺骗的早期检测线索。

Abstract: GPS spoofing poses a growing threat to aviation by falsifying satellite
signals and misleading aircraft navigation systems. This paper demonstrates a
proof-of-concept spoofing detection strategy based on analyzing satellite
Carrier-to-Noise Density Ratio (C/N$_0$) variation during controlled static
antenna orientations. Using a u-blox EVK-M8U receiver and a GPSG-1000 satellite
simulator, C/N$_0$ data is collected under three antenna orientations flat,
banked right, and banked left) in both real-sky (non-spoofed) and spoofed
environments. Our findings reveal that under non-spoofed signals, C/N$_0$
values fluctuate naturally with orientation, reflecting true geometric
dependencies. However, spoofed signals demonstrate a distinct pattern: the flat
orientation, which directly faces the spoofing antenna, consistently yielded
the highest C/N$_0$ values, while both banked orientations showed reduced
C/N$_0$ due to misalignment with the spoofing source. These findings suggest
that simple maneuvers such as brief banking to induce C/N$_0$ variations can
provide early cues of GPS spoofing for general aviation and UAV systems.

</details>


### [23] [LibIHT: A Hardware-Based Approach to Efficient and Evasion-Resistant Dynamic Binary Analysis](https://arxiv.org/abs/2510.16251)
*Changyu Zhao,Yohan Beugin,Jean-Charles Noirot Ferrand,Quinn Burke,Guancheng Li,Patrick McDaniel*

Main category: cs.CR

TL;DR: LibIHT是一个基于硬件辅助追踪的框架，利用CPU分支追踪功能（Intel LBR和BTS）高效捕获程序控制流，显著降低性能开销并提高抗规避能力。


<details>
  <summary>Details</summary>
Motivation: 软件插桩分析存在高开销且易被反分析技术规避，需要一种更高效、难以检测的动态程序分析方法。

Method: 通过内核收集硬件生成的分支执行数据，重建控制流图（CFG），实现为操作系统内核模块和用户空间库。

Result: 相比Intel Pin减少150倍运行时开销（7x vs 1053x减速），CFG重建保真度高（捕获超过99%的基本块和边）。

Conclusion: 硬件辅助追踪在性能和低可检测性方面具有显著优势，虽然牺牲了部分语义细节，但对于许多应用是可接受的权衡。

Abstract: Dynamic program analysis is invaluable for malware detection, debugging, and
performance profiling. However, software-based instrumentation incurs high
overhead and can be evaded by anti-analysis techniques. In this paper, we
propose LibIHT, a hardware-assisted tracing framework that leverages on-CPU
branch tracing features (Intel Last Branch Record and Branch Trace Store) to
efficiently capture program control-flow with minimal performance impact. Our
approach reconstructs control-flow graphs (CFGs) by collecting hardware
generated branch execution data in the kernel, preserving program behavior
against evasive malware. We implement LibIHT as an OS kernel module and
user-space library, and evaluate it on both benign benchmark programs and
adversarial anti-instrumentation samples. Our results indicate that LibIHT
reduces runtime overhead by over 150x compared to Intel Pin (7x vs 1,053x
slowdowns), while achieving high fidelity in CFG reconstruction (capturing over
99% of execution basic blocks and edges). Although this hardware-assisted
approach sacrifices the richer semantic detail available from full software
instrumentation by capturing only branch addresses, this trade-off is
acceptable for many applications where performance and low detectability are
paramount. Our findings show that hardware-based tracing captures control flow
information significantly faster, reduces detection risk and performs dynamic
analysis with minimal interference.

</details>


### [24] [Detecting Adversarial Fine-tuning with Auditing Agents](https://arxiv.org/abs/2510.16255)
*Sarah Egler,John Schulman,Nicholas Carlini*

Main category: cs.CR

TL;DR: 提出了一种针对LLM微调API的审计代理机制，能够检测对抗性微调攻击，在1%误报率下达到56.2%的检测率，特别能检测规避安全评估的隐蔽密码攻击。


<details>
  <summary>Details</summary>
Motivation: LLM提供商提供微调API，但攻击者可能利用这些API绕过安全防护，使用仅隐含有害的数据集进行微调攻击，现有基础内容审核难以检测。

Method: 引入微调审计代理概念，提供对微调数据集、微调前后模型的访问权限，要求代理为微调任务分配风险评分。

Result: 在8种强微调攻击和5个良性微调模型上评估，总计超过1400次独立审计，在最佳配置下达到56.2%检测率（1%误报率），能检测规避安全评估的隐蔽密码攻击。

Conclusion: 建立了该领域的基准配置，虽然良性微调中无意的安全退化仍是挑战，但为后续研究奠定了基础。

Abstract: Large Language Model (LLM) providers expose fine-tuning APIs that let end
users fine-tune their frontier LLMs. Unfortunately, it has been shown that an
adversary with fine-tuning access to an LLM can bypass safeguards. Particularly
concerning, such attacks may avoid detection with datasets that are only
implicitly harmful. Our work studies robust detection mechanisms for
adversarial use of fine-tuning APIs. We introduce the concept of a fine-tuning
auditing agent and show it can detect harmful fine-tuning prior to model
deployment. We provide our auditing agent with access to the fine-tuning
dataset, as well as the fine-tuned and pre-fine-tuned models, and request the
agent assigns a risk score for the fine-tuning job. We evaluate our detection
approach on a diverse set of eight strong fine-tuning attacks from the
literature, along with five benign fine-tuned models, totaling over 1400
independent audits. These attacks are undetectable with basic content
moderation on the dataset, highlighting the challenge of the task. With the
best set of affordances, our auditing agent achieves a 56.2% detection rate of
adversarial fine-tuning at a 1% false positive rate. Most promising, the
auditor is able to detect covert cipher attacks that evade safety evaluations
and content moderation of the dataset. While benign fine-tuning with
unintentional subtle safety degradation remains a challenge, we establish a
baseline configuration for further work in this area. We release our auditing
agent at https://github.com/safety-research/finetuning-auditor.

</details>


### [25] [Efficient and Privacy-Preserving Binary Dot Product via Multi-Party Computation](https://arxiv.org/abs/2510.16331)
*Fatemeh Jafarian Dehkordi,Elahe Vedadi,Alireza Feizbakhsh,Yasaman Keshtkarjahromi,Hulya Seferoglu*

Main category: cs.CR

TL;DR: 提出了一种名为BiMPC的新型二进制多方计算框架，专门针对树型垂直联邦学习中的位运算场景，通过DoMA方法和三方向意传输协议实现高效的二进制向量点积计算，同时保护每个比特的隐私。


<details>
  <summary>Details</summary>
Motivation: 解决分布式机器学习中数据隐私保护与协同计算之间的平衡问题，特别是在涉及位运算的场景（如树型垂直联邦学习）中，现有的隐私保护技术如Shamir秘密共享和多方计算对二进制数据的位运算不够优化。

Method: 提出BiMPC框架，核心是DoMA方法（通过模加法进行点积计算），使用高域随机掩码进行线性计算，采用三方向意传输协议处理非线性二进制运算。

Result: BiMPC框架在分布式环境中展现出高效性和可扩展性，能够安全地进行隐私保护的位运算，特别是二进制向量的点积计算。

Conclusion: BiMPC框架成功解决了现有方法在二进制位运算方面的局限性，为树型垂直联邦学习等场景提供了有效的隐私保护解决方案。

Abstract: Striking a balance between protecting data privacy and enabling collaborative
computation is a critical challenge for distributed machine learning. While
privacy-preserving techniques for federated learning have been extensively
developed, methods for scenarios involving bitwise operations, such as
tree-based vertical federated learning (VFL), are still underexplored.
Traditional mechanisms, including Shamir's secret sharing and multi-party
computation (MPC), are not optimized for bitwise operations over binary data,
particularly in settings where each participant holds a different part of the
binary vector. This paper addresses the limitations of existing methods by
proposing a novel binary multi-party computation (BiMPC) framework. The BiMPC
mechanism facilitates privacy-preserving bitwise operations, with a particular
focus on dot product computations of binary vectors, ensuring the privacy of
each individual bit. The core of BiMPC is a novel approach called Dot Product
via Modular Addition (DoMA), which uses regular and modular additions for
efficient binary dot product calculation. To ensure privacy, BiMPC uses random
masking in a higher field for linear computations and a three-party oblivious
transfer (triot) protocol for non-linear binary operations. The privacy
guarantees of the BiMPC framework are rigorously analyzed, demonstrating its
efficiency and scalability in distributed settings.

</details>


### [26] [EditMark: Watermarking Large Language Models based on Model Editing](https://arxiv.org/abs/2510.16367)
*Shuai Li,Kejiang Chen,Jun Jiang,Jie Zhang,Qiyi Yao,Kai Zeng,Weiming Zhang,Nenghai Yu*

Main category: cs.CR

TL;DR: EditMark是一种基于模型编辑的LLM水印方法，无需训练即可在20秒内嵌入32位水印，保持模型性能无损且水印隐蔽性高。


<details>
  <summary>Details</summary>
Motivation: 现有LLM水印方法需要重新训练模型，成本高且影响性能，生成的水印文本不自然。需要一种无需训练、隐蔽且不影响性能的水印方法。

Method: 利用模型编辑技术，为多答案问题分配不同水印，通过自适应多轮稳定编辑策略和噪声矩阵注入来更新模型权重。

Result: 在20秒内成功嵌入32位水印，提取成功率100%，相比微调方法(6875秒)效率显著提升，具有良好的保真度、隐蔽性和抗攻击能力。

Conclusion: EditMark是首个基于模型编辑的LLM水印方法，实现了训练免费、隐蔽且性能无损的水印嵌入，为LLM版权保护提供了高效解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities, but
their training requires extensive data and computational resources, rendering
them valuable digital assets. Therefore, it is essential to watermark LLMs to
protect their copyright and trace unauthorized use or resale. Existing methods
for watermarking LLMs primarily rely on training LLMs with a watermarked
dataset, which entails burdensome training costs and negatively impacts the
LLM's performance. In addition, their watermarked texts are not logical or
natural, thereby reducing the stealthiness of the watermark. To address these
issues, we propose EditMark, the first watermarking method that leverages model
editing to embed a training-free, stealthy, and performance-lossless watermark
for LLMs. We observe that some questions have multiple correct answers.
Therefore, we assign each answer a unique watermark and update the weights of
LLMs to generate corresponding questions and answers through the model editing
technique. In addition, we refine the model editing technique to align with the
requirements of watermark embedding. Specifically, we introduce an adaptive
multi-round stable editing strategy, coupled with the injection of a noise
matrix, to improve both the effectiveness and robustness of the watermark
embedding. Extensive experiments indicate that EditMark can embed 32-bit
watermarks into LLMs within 20 seconds (Fine-tuning: 6875 seconds) with a
watermark extraction success rate of 100%, which demonstrates its effectiveness
and efficiency. External experiments further demonstrate that EditMark has
fidelity, stealthiness, and a certain degree of robustness against common
attacks.

</details>


### [27] [Heimdallr: Fingerprinting SD-WAN Control-Plane Architecture via Encrypted Control Traffic](https://arxiv.org/abs/2510.16461)
*Minjae Seo,Jaehan Kim,Eduard Marin,Myoungsung You,Taejune Park,Seungsoo Lee,Seungwon Shin,Jinwoo Kim*

Main category: cs.CR

TL;DR: Heimdallr是一个SD-WAN指纹识别系统，通过分析加密控制流量的时间序列模式和流向关系，能够自动识别集群管理协议并推断控制平面拓扑。


<details>
  <summary>Details</summary>
Motivation: SD-WAN的分布式控制平面通过交换控制流量实现一致性，但即使流量加密，其时间序列模式和方向关系仍会暴露机密信息，如控制平面拓扑和协议依赖，可能被用于严重攻击。

Method: 利用深度学习分析SD-WAN集群管理协议的周期性和操作模式，以及流量的流向上下文，从收集的控制流量中自动分类集群管理协议。

Result: 在由三个地理上分散的校园网络和一个企业网络组成的现实SD-WAN环境中，Heimdallr能够以≥93%的准确率分类SD-WAN控制流量，以≥80%的宏F-1分数识别单个协议，并以≥70%的相似度推断控制平面拓扑。

Conclusion: Heimdallr系统能够有效识别SD-WAN控制流量的特征，揭示加密流量中隐藏的敏感信息，为SD-WAN安全提供了新的分析工具。

Abstract: Software-defined wide area network (SD-WAN) has emerged as a new paradigm for
steering a large-scale network flexibly by adopting distributed
software-defined network (SDN) controllers. The key to building a logically
centralized but physically distributed control-plane is running diverse cluster
management protocols to achieve consistency through an exchange of control
traffic. Meanwhile, we observe that the control traffic exposes unique
time-series patterns and directional relationships due to the operational
structure even though the traffic is encrypted, and this pattern can disclose
confidential information such as control-plane topology and protocol
dependencies, which can be exploited for severe attacks. With this insight, we
propose a new SD-WAN fingerprinting system, called Heimdallr. It analyzes
periodical and operational patterns of SD-WAN cluster management protocols and
the context of flow directions from the collected control traffic utilizing a
deep learning-based approach, so that it can classify the cluster management
protocols automatically from miscellaneous control traffic datasets. Our
evaluation, which is performed in a realistic SD-WAN environment consisting of
geographically distant three campus networks and one enterprise network shows
that Heimdallr can classify SD-WAN control traffic with $\geq$ 93%, identify
individual protocols with $\geq$ 80% macro F-1 scores, and finally can infer
control-plane topology with $\geq$ 70% similarity.

</details>


### [28] [$ρ$Hammer: Reviving RowHammer Attacks on New Architectures via Prefetching](https://arxiv.org/abs/2510.16544)
*Weijie Chen,Shan Tang,Yulin Tang,Xiapu Luo,Yinqian Zhang,Weizhong Qiang*

Main category: cs.CR

TL;DR: ρHammer是一个新型Rowhammer攻击框架，通过解决最新Intel架构的三个核心挑战：高效DRAM地址映射逆向工程、基于预取的锤击突破激活率瓶颈、以及应对推测执行导致的乱序问题，成功在最新Raptor Lake等架构上复活了Rowhammer攻击。


<details>
  <summary>Details</summary>
Motivation: 传统基于负载的Rowhammer攻击在最新Intel Alder和Raptor Lake架构上变得高度无效，需要新的攻击方法来克服这些架构带来的挑战。

Method: 1) 使用选择性成对测量和结构化推导的高效DRAM地址映射逆向工程方法；2) 基于预取的锤击范式，利用x86预取指令的异步特性并通过多bank并行最大化吞吐量；3) 使用控制流混淆和优化的NOP伪屏障的反推测锤击技术来维持预取顺序。

Result: 在四个最新Intel架构上的评估显示：在2小时攻击模式模糊测试过程中诱导超过20万额外比特翻转，在Comet和Rocket Lake上的翻转率比基于负载的基线高112倍，首次在Raptor Lake上复活Rowhammer攻击，达到稳定的291/分钟翻转率和快速端到端利用。

Conclusion: ρHammer框架成功克服了最新Intel架构的三个核心挑战，证明了Rowhammer攻击在这些现代系统上仍然构成严重威胁，需要新的防御措施。

Abstract: Rowhammer is a critical vulnerability in dynamic random access memory (DRAM)
that continues to pose a significant threat to various systems. However, we
find that conventional load-based attacks are becoming highly ineffective on
the most recent architectures such as Intel Alder and Raptor Lake. In this
paper, we present $\rho$Hammer, a new Rowhammer framework that systematically
overcomes three core challenges impeding attacks on these new architectures.
First, we design an efficient and generic DRAM address mapping
reverse-engineering method that uses selective pairwise measurements and
structured deduction, enabling recovery of complex mappings within seconds on
the latest memory controllers. Second, to break through the activation rate
bottleneck of load-based hammering, we introduce a novel prefetch-based
hammering paradigm that leverages the asynchronous nature of x86 prefetch
instructions and is further enhanced by multi-bank parallelism to maximize
throughput. Third, recognizing that speculative execution causes more severe
disorder issues for prefetching, which cannot be simply mitigated by memory
barriers, we develop a counter-speculation hammering technique using
control-flow obfuscation and optimized NOP-based pseudo-barriers to maintain
prefetch order with minimal overhead. Evaluations across four latest Intel
architectures demonstrate $\rho$Hammer's breakthrough effectiveness: it induces
up to 200K+ additional bit flips within 2-hour attack pattern fuzzing processes
and has a 112x higher flip rate than the load-based hammering baselines on
Comet and Rocket Lake. Also, we are the first to revive Rowhammer attacks on
the latest Raptor Lake architecture, where baselines completely fail, achieving
stable flip rates of 2,291/min and fast end-to-end exploitation.

</details>


### [29] [Toward Understanding Security Issues in the Model Context Protocol Ecosystem](https://arxiv.org/abs/2510.16558)
*Xiaofan Li,Xing Gao*

Main category: cs.CR

TL;DR: 本文首次对MCP生态系统进行全面的安全分析，揭示了由于缺乏输出验证机制和服务器审查流程，导致恶意服务器能够操纵模型行为并引发多种安全威胁。


<details>
  <summary>Details</summary>
Motivation: MCP生态系统虽然发展迅速，但缺乏对其架构和安全风险的系统性研究。随着MCP主机、注册中心和服务器数量的快速增长，需要评估其安全状况。

Method: 将MCP生态系统分解为主机、注册中心和服务器三个核心组件，分析它们之间的交互和信任关系。收集并分析了来自6个公共注册中心的67,057个服务器数据集。

Result: 定性分析显示主机缺乏对LLM生成输出的验证机制，恶意服务器可操纵模型行为。定量分析表明大量服务器可被攻击者劫持，存在敏感数据泄露等安全威胁。

Conclusion: MCP生态系统存在严重安全风险，需要为主机、注册中心和用户提出实用的防御策略，并已向受影响方负责任地披露了研究结果。

Abstract: The Model Context Protocol (MCP) is an emerging open standard that enables
AI-powered applications to interact with external tools through structured
metadata. A rapidly growing ecosystem has formed around MCP, including a wide
range of MCP hosts (i.e., Cursor, Windsurf, Claude Desktop, and Cline), MCP
registries (i.e., mcp.so, MCP Market, MCP Store, Pulse MCP, Smithery, and npm),
and thousands of community-contributed MCP servers. Although the MCP ecosystem
is gaining traction, there has been little systematic study of its architecture
and associated security risks. In this paper, we present the first
comprehensive security analysis of the MCP ecosystem. We decompose MCP
ecosystem into three core components: hosts, registries, and servers, and study
the interactions and trust relationships among them. Users search for servers
on registries and configure them in the host, which translates LLM-generated
output into external tool invocations provided by the servers and executes
them. Our qualitative analysis reveals that hosts lack output verification
mechanisms for LLM-generated outputs, enabling malicious servers to manipulate
model behavior and induce a variety of security threats, including but not
limited to sensitive data exfiltration. We uncover a wide range of
vulnerabilities that enable attackers to hijack servers, due to the lack of a
vetted server submission process in registries. To support our analysis, we
collect and analyze a dataset of 67,057 servers from six public registries. Our
quantitative analysis demonstrates that a substantial number of servers can be
hijacked by attackers. Finally, we propose practical defense strategies for MCP
hosts, registries, and users. We responsibly disclosed our findings to affected
hosts and registries.

</details>


### [30] [Patronus: Safeguarding Text-to-Image Models against White-Box Adversaries](https://arxiv.org/abs/2510.16581)
*Xinfeng Li,Shengyuan Pang,Jialin Wu,Jiangyi Deng,Huanlong Zhong,Yanjiao Chen,Jie Zhang,Wenyuan Xu*

Main category: cs.CR

TL;DR: Patronus是一个防御框架，通过内部调节器和非可微调学习机制保护文本到图像模型免受白盒攻击者的恶意微调攻击。


<details>
  <summary>Details</summary>
Motivation: 现有的安全措施（如内容审核或模型对齐）在面对知道并能调整模型参数的白盒攻击者时会失效，因此需要新的防御机制。

Method: 设计内部调节器将不安全输入特征解码为零向量，同时保持良性输入特征的解码性能；采用非可微调学习机制加强模型对齐，防止恶意微调。

Result: 实验验证了Patronus在安全内容生成上的性能保持完整，能有效拒绝不安全内容生成，并对各种白盒攻击者的微调攻击具有韧性。

Conclusion: Patronus为文本到图像模型提供了全面的保护，能够有效防御白盒攻击者的恶意微调，确保模型安全性和性能。

Abstract: Text-to-image (T2I) models, though exhibiting remarkable creativity in image
generation, can be exploited to produce unsafe images. Existing safety
measures, e.g., content moderation or model alignment, fail in the presence of
white-box adversaries who know and can adjust model parameters, e.g., by
fine-tuning. This paper presents a novel defensive framework, named Patronus,
which equips T2I models with holistic protection to defend against white-box
adversaries. Specifically, we design an internal moderator that decodes unsafe
input features into zero vectors while ensuring the decoding performance of
benign input features. Furthermore, we strengthen the model alignment with a
carefully designed non-fine-tunable learning mechanism, ensuring the T2I model
will not be compromised by malicious fine-tuning. We conduct extensive
experiments to validate the intactness of the performance on safe content
generation and the effectiveness of rejecting unsafe content generation.
Results also confirm the resilience of Patronus against various fine-tuning
attacks by white-box adversaries.

</details>


### [31] [DESTinE Block: Private Blockchain Based Data Storage Framework for Power System](https://arxiv.org/abs/2510.16593)
*Khandaker Akramul Haque,Katherine R. Davis*

Main category: cs.CR

TL;DR: DESTinE Block是一个基于区块链的数据存储框架，专为电力系统设计，优化用于资源受限环境，包括边缘设备如单板计算机。它结合IPFS存储大文件和自定义区块链记录元数据，采用双区块链抽象和PoA共识机制。


<details>
  <summary>Details</summary>
Motivation: 为电力系统提供安全、防篡改的数据存储解决方案，特别针对资源受限的边缘设备环境，确保数据真实性和完整性。

Method: 使用IPFS存储大文件，在自定义DESTinE Block区块链上记录元数据（包括IPFS CID、上传者身份、管理员验证和时间戳）。采用双区块链抽象分离存储层，基于PoA共识机制，需要管理员和上传者共同协作创建区块。

Result: 在x86设备和ARM64 Raspberry Pi上测试成功，与基于Multichain的类似框架相比，DESTinE Block在保持最小硬件要求的同时，为分布式电力系统基础设施提供了有前景的防篡改数据保留解决方案。

Conclusion: DESTinE Block框架展示了在资源受限环境中实现安全、去中心化日志和测量存储的潜力，特别适用于智能电网应用。

Abstract: This paper presents DESTinE Block, a blockchain-based data storage framework
designed for power systems and optimized for resource-constrained environments,
including grid-edge devices such as single-board computers. The proposed
architecture leverages the InterPlanetary File System (IPFS) for storing large
files while maintaining secure and traceable metadata on a custom blockchain
named DESTinE Block. The metadata, comprising the IPFS Content Identifier
(CID), uploader identity, administrator verification, and timestamp; is
immutably recorded on-chain to ensure authenticity and integrity. DESTinE Block
adopts a dual-blockchain abstraction, where the blockchain remains unaware of
the IPFS storage layer to enhance security and limit the exposure of sensitive
file data. The consensus mechanism is based on Proof of Authority (PoA), where
both an administrator and an uploader with distinct cryptographic key pairs are
required to create a block collaboratively. Each block contains verified
signatures of both parties and is designed to be computationally efficient,
enabling deployment on devices like the Raspberry Pi 5. The framework was
tested on both an x86-based device and an ARM64-based Raspberry Pi,
demonstrating its potential for secure, decentralized logging and measurement
storage in smart grid applications. Moreover, DESTinE Block is compared with a
similar framework based on Multichain. The results indicate that DESTinE Block
provides a promising solution for tamper-evident data retention in distributed
power system infrastructure while maintaining minimal hardware requirements.

</details>


### [32] [Structuring Security: A Survey of Cybersecurity Ontologies, Semantic Log Processing, and LLMs Application](https://arxiv.org/abs/2510.16610)
*Bruno Lourenço,Pedro Adão,João F. Ferreira,Mario Monteiro Marques,Cátia Vaz*

Main category: cs.CR

TL;DR: 本调查探讨了本体论、语义日志处理和大语言模型如何增强网络安全，重点分析了知识图谱自动构建和LLM在安全数据分析中的应用。


<details>
  <summary>Details</summary>
Motivation: 安全日志通常是非结构化和复杂的，需要更好的方法来组织和推理安全数据，以支持先进的威胁分析和欧盟网络安全倡议。

Method: 使用本体论构建领域知识结构，通过自动构建知识图谱来组织安全日志数据，并利用大语言模型提供上下文理解和从非结构化内容中提取见解。

Result: 提出了一个结合本体论、语义日志处理和LLM的综合框架，能够有效处理复杂的安全数据，支持互操作性和高级威胁分析。

Conclusion: 本体论驱动的智能网络防御方法在应对当前网络安全挑战方面具有重要潜力，与欧盟NIS 2和网络安全分类法倡议相一致。

Abstract: This survey investigates how ontologies, semantic log processing, and Large
Language Models (LLMs) enhance cybersecurity. Ontologies structure domain
knowledge, enabling interoperability, data integration, and advanced threat
analysis. Security logs, though critical, are often unstructured and complex.
To address this, automated construction of Knowledge Graphs (KGs) from raw logs
is emerging as a key strategy for organizing and reasoning over security data.
LLMs enrich this process by providing contextual understanding and extracting
insights from unstructured content. This work aligns with European Union (EU)
efforts such as NIS 2 and the Cybersecurity Taxonomy, highlighting challenges
and opportunities in intelligent ontology-driven cyber defense.

</details>


### [33] [A Versatile Framework for Designing Group-Sparse Adversarial Attacks](https://arxiv.org/abs/2510.16637)
*Alireza Heshmati,Saman Soleimani Roudi,Sajjad Amini,Shahrokh Ghaemmaghami,Farokh Marvasti*

Main category: cs.CR

TL;DR: ATOS是一个生成结构化稀疏对抗性扰动的可微分优化框架，通过重叠平滑L0函数促进稀疏结构化扰动，在保持高攻击成功率的同时提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗性攻击往往忽略扰动的稀疏性，限制了它们建模结构变化和解释深度神经网络如何处理有意义输入模式的能力。

Method: 提出ATOS框架，引入重叠平滑L0(OSL0)函数，通过分组通道和相邻像素生成元素级、像素级和分组级的结构化稀疏扰动，并使用对数指数绝对值之和近似L无穷梯度来控制扰动幅度。

Result: 在CIFAR-10和ImageNet上，ATOS实现了100%的攻击成功率，同时产生比现有方法更稀疏和结构一致的扰动。分组攻击从网络角度突出了关键区域。

Conclusion: ATOS通过生成结构化稀疏扰动，不仅提高了攻击效率，还提供了对抗性解释，通过用目标类的鲁棒特征替换类别定义区域来提供反事实解释。

Abstract: Existing adversarial attacks often neglect perturbation sparsity, limiting
their ability to model structural changes and to explain how deep neural
networks (DNNs) process meaningful input patterns. We propose ATOS (Attack
Through Overlapping Sparsity), a differentiable optimization framework that
generates structured, sparse adversarial perturbations in element-wise,
pixel-wise, and group-wise forms. For white-box attacks on image classifiers,
we introduce the Overlapping Smoothed L0 (OSL0) function, which promotes
convergence to a stationary point while encouraging sparse, structured
perturbations. By grouping channels and adjacent pixels, ATOS improves
interpretability and helps identify robust versus non-robust features. We
approximate the L-infinity gradient using the logarithm of the sum of
exponential absolute values to tightly control perturbation magnitude. On
CIFAR-10 and ImageNet, ATOS achieves a 100% attack success rate while producing
significantly sparser and more structurally coherent perturbations than prior
methods. The structured group-wise attack highlights critical regions from the
network's perspective, providing counterfactual explanations by replacing
class-defining regions with robust features from the target class.

</details>


### [34] [Rotation, Scale, and Translation Resilient Black-box Fingerprinting for Intellectual Property Protection of EaaS Models](https://arxiv.org/abs/2510.16706)
*Hongjie Zhang,Zhiqi Zhao,Hanzhou Wu,Zhihua Xia,Athanasios V. Vasilakos*

Main category: cs.CR

TL;DR: 提出了一种用于嵌入即服务(EaaS)模型的指纹框架，通过分析嵌入空间的拓扑结构来验证模型所有权，而不是依赖传统水印技术中的后门触发器。


<details>
  <summary>Details</summary>
Motivation: 现有EaaS模型水印方法会产生可检测的语义模式，且容易受到旋转、缩放、平移等几何变换攻击，需要更鲁棒的所有权验证方案。

Method: 将受害模型和可疑模型的嵌入建模为点云，通过空间对齐和相似性测量来验证所有权，该方法固有地抵抗几何变换攻击。

Result: 在视觉和文本嵌入任务上的实验验证了该方法的优越性和适用性。

Conclusion: 该研究揭示了EaaS模型的内在特征，为黑盒场景下的模型所有权验证提供了有前景的解决方案。

Abstract: Feature embedding has become a cornerstone technology for processing
high-dimensional and complex data, which results in that Embedding as a Service
(EaaS) models have been widely deployed in the cloud. To protect the
intellectual property of EaaS models, existing methods apply digital
watermarking to inject specific backdoor triggers into EaaS models by modifying
training samples or network parameters. However, these methods inevitably
produce detectable patterns through semantic analysis and exhibit
susceptibility to geometric transformations including rotation, scaling, and
translation (RST). To address this problem, we propose a fingerprinting
framework for EaaS models, rather than merely refining existing watermarking
techniques. Different from watermarking techniques, the proposed method
establishes EaaS model ownership through geometric analysis of embedding
space's topological structure, rather than relying on the modified training
samples or triggers. The key innovation lies in modeling the victim and
suspicious embeddings as point clouds, allowing us to perform robust spatial
alignment and similarity measurement, which inherently resists RST attacks.
Experimental results evaluated on visual and textual embedding tasks verify the
superiority and applicability. This research reveals inherent characteristics
of EaaS models and provides a promising solution for ownership verification of
EaaS models under the black-box scenario.

</details>


### [35] [DistilLock: Safeguarding LLMs from Unauthorized Knowledge Distillation on the Edge](https://arxiv.org/abs/2510.16716)
*Asmita Mohanty,Gezheng Kang,Lei Gao,Murali Annavaram*

Main category: cs.CR

TL;DR: DistilLock是一个基于可信执行环境(TEE)的隐私保护知识蒸馏框架，用于在边缘设备上安全地微调大型语言模型，同时保护数据隐私和模型知识产权。


<details>
  <summary>Details</summary>
Motivation: 解决云中心化微调LLM时的数据隐私问题，以及边缘设备微调时的模型知识产权泄露风险，需要在保护数据隐私和模型IP之间找到平衡。

Method: 在数据所有者的设备上使用TEE安全执行专有基础模型作为黑盒教师，采用模型混淆机制将混淆后的权重卸载到不可信加速器进行高效知识蒸馏。

Result: DistilLock能够防止未经授权的知识蒸馏过程和模型窃取攻击，同时保持高计算效率。

Conclusion: DistilLock为基于边缘的LLM个性化提供了一个安全实用的解决方案，在保护数据隐私和模型IP的同时实现了高效的知识蒸馏。

Abstract: Large Language Models (LLMs) have demonstrated strong performance across
diverse tasks, but fine-tuning them typically relies on cloud-based,
centralized infrastructures. This requires data owners to upload potentially
sensitive data to external servers, raising serious privacy concerns. An
alternative approach is to fine-tune LLMs directly on edge devices using local
data; however, this introduces a new challenge: the model owner must transfer
proprietary models to the edge, which risks intellectual property (IP) leakage.
To address this dilemma, we propose DistilLock, a TEE-assisted fine-tuning
framework that enables privacy-preserving knowledge distillation on the edge.
In DistilLock, a proprietary foundation model is executed within a trusted
execution environment (TEE) enclave on the data owner's device, acting as a
secure black-box teacher. This setup preserves both data privacy and model IP
by preventing direct access to model internals. Furthermore, DistilLock employs
a model obfuscation mechanism to offload obfuscated weights to untrusted
accelerators for efficient knowledge distillation without compromising
security. We demonstrate that DistilLock prevents unauthorized knowledge
distillation processes and model-stealing attacks while maintaining high
computational efficiency, but offering a secure and practical solution for
edge-based LLM personalization.

</details>


### [36] [Cryptanalysis of a Privacy-Preserving Ride-Hailing Service from NSS 2022](https://arxiv.org/abs/2510.16744)
*Srinivas Vivek*

Main category: cs.CR

TL;DR: 对Xie等人在NSS 2022提出的隐私保护网约车服务协议进行被动攻击，能够完全恢复乘客和司机的位置信息。


<details>
  <summary>Details</summary>
Motivation: 评估现有隐私保护网约车服务协议的安全性，发现其中存在的安全漏洞。

Method: 设计被动攻击方法，通过分析协议执行过程来恢复位置信息，攻击效率与安全参数无关。

Result: 成功完全恢复每个乘车请求中乘客和响应司机的具体位置信息。

Conclusion: 该隐私保护网约车服务协议存在严重安全漏洞，无法有效保护用户位置隐私。

Abstract: Ride-Hailing Services (RHS) match a ride request initiated by a rider with a
suitable driver responding to the ride request. A Privacy-Preserving RHS
(PP-RHS) aims to facilitate ride matching while ensuring the privacy of riders'
and drivers' location data w.r.t. the Service Provider (SP). At NSS 2022, Xie
et al. proposed a PP-RHS. In this work, we demonstrate a passive attack on
their PP-RHS protocol. Our attack allows the SP to completely recover the
locations of the rider as well as that of the responding drivers in every ride
request. Further, our attack is very efficient as it is independent of the
security parameter.

</details>


### [37] [Black-box Optimization of LLM Outputs by Asking for Directions](https://arxiv.org/abs/2510.16794)
*Jie Zhang,Meng Ding,Yang Liu,Jue Hong,Florian Tramèr*

Main category: cs.CR

TL;DR: 提出了一种利用LLM自然语言表达置信度的黑盒攻击方法，无需访问logits或置信度分数，通过提示LLM表达内部置信度来进行对抗优化，成功攻击仅暴露文本输出的系统。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒攻击需要访问连续模型输出或依赖其他模型的代理信号，这在实践中很少可用。本文旨在开发一种仅利用LLM文本输出的攻击方法。

Method: 通过提示LLM用自然语言表达其内部置信度，利用这种表达进行对抗优化，应用于视觉LLM的对抗样本、越狱和提示注入三种攻击场景。

Result: 成功生成了针对仅暴露文本输出系统的恶意输入，显著扩大了已部署LLM的攻击面。发现更好更大的模型在表达置信度时具有更好的校准性。

Conclusion: 模型能力的提升直接增强了漏洞，形成了令人担忧的安全悖论，该方法显著扩展了LLM的攻击面。

Abstract: We present a novel approach for attacking black-box large language models
(LLMs) by exploiting their ability to express confidence in natural language.
Existing black-box attacks require either access to continuous model outputs
like logits or confidence scores (which are rarely available in practice), or
rely on proxy signals from other models. Instead, we demonstrate how to prompt
LLMs to express their internal confidence in a way that is sufficiently
calibrated to enable effective adversarial optimization. We apply our general
method to three attack scenarios: adversarial examples for vision-LLMs,
jailbreaks and prompt injections. Our attacks successfully generate malicious
inputs against systems that only expose textual outputs, thereby dramatically
expanding the attack surface for deployed LLMs. We further find that better and
larger models exhibit superior calibration when expressing confidence, creating
a concerning security paradox where model capability improvements directly
enhance vulnerability. Our code is available at this
[link](https://github.com/zj-jayzhang/black_box_llm_optimization).

</details>


### [38] [Verifiable Fine-Tuning for LLMs: Zero-Knowledge Training Proofs Bound to Data Provenance and Policy](https://arxiv.org/abs/2510.16830)
*Hasan Akgul,Daniel Borg,Arta Berisha,Amina Rahimova,Andrej Novak,Mila Petrov*

Main category: cs.CR

TL;DR: 提出可验证微调协议，通过零知识证明确保模型发布符合声明的训练程序和可审计数据集承诺，解决当前模型发布实践中的信任问题。


<details>
  <summary>Details</summary>
Motivation: 当前参数高效微调模型的发布实践在数据使用和更新计算方面缺乏可靠保证，存在信任缺口，特别是在受监管和去中心化部署场景中。

Method: 结合五个要素：数据集承诺绑定、可验证采样器、参数高效微调的更新电路、递归证明聚合、来源绑定和可信执行属性卡。

Result: 在英语和双语指令混合任务中，该方法在严格预算内保持实用性，实现实际证明性能，策略配额零违规，私有采样无索引泄露。

Conclusion: 端到端可验证微调对于真实参数高效管道是可行的，为受监管和去中心化部署填补了关键信任缺口。

Abstract: Large language models are often adapted through parameter efficient fine
tuning, but current release practices provide weak assurances about what data
were used and how updates were computed. We present Verifiable Fine Tuning, a
protocol and system that produces succinct zero knowledge proofs that a
released model was obtained from a public initialization under a declared
training program and an auditable dataset commitment. The approach combines
five elements. First, commitments that bind data sources, preprocessing,
licenses, and per epoch quota counters to a manifest. Second, a verifiable
sampler that supports public replayable and private index hiding batch
selection. Third, update circuits restricted to parameter efficient fine tuning
that enforce AdamW style optimizer semantics and proof friendly approximations
with explicit error budgets. Fourth, recursive aggregation that folds per step
proofs into per epoch and end to end certificates with millisecond
verification. Fifth, provenance binding and optional trusted execution property
cards that attest code identity and constants. On English and bilingual
instruction mixtures, the method maintains utility within tight budgets while
achieving practical proof performance. Policy quotas are enforced with zero
violations, and private sampling windows show no measurable index leakage.
Federated experiments demonstrate that the system composes with probabilistic
audits and bandwidth constraints. These results indicate that end to end
verifiable fine tuning is feasible today for real parameter efficient
pipelines, closing a critical trust gap for regulated and decentralized
deployments.

</details>


### [39] [ThreatIntel-Andro: Expert-Verified Benchmarking for Robust Android Malware Research](https://arxiv.org/abs/2510.16835)
*Hongpeng Bai,Minhong Dong,Yao Zhang,Shunzhe Zhao,Haobo Zhang,Lingyue Li,Yude Bai,Guangquan Xu*

Main category: cs.CR

TL;DR: 现有Android恶意软件数据集存在标签噪声大、样本过时、自动标注工具聚合策略不佳等问题，影响了工业网络安全研究的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着移动设备在工业系统中的广泛应用，Android恶意软件已成为工业网络安全的关键攻击面。但主流数据集存在标签噪声严重、样本过时、自动标注工具准确性不足等问题，影响了恶意软件检测和防御研究的质量。

Method: 论文未在摘要中明确描述具体方法，但指出了当前数据集和标注工具存在的问题：过度依赖VirusTotal多引擎聚合导致标签噪声、样本时效性差、自动标注工具聚合策略不佳。

Result: 摘要未提供具体实验结果，但指出了当前数据集存在的严重问题：标签噪声显著、样本过时降低时间相关性、自动标注工具传播错误标签。

Conclusion: Android恶意软件生态系统需要高质量、实时的数据集作为有效检测和防御的基础，当前主流数据集和标注工具的局限性严重影响了工业网络安全研究的准确性和时效性。

Abstract: The rapidly evolving Android malware ecosystem demands high-quality,
real-time datasets as a foundation for effective detection and defense. With
the widespread adoption of mobile devices across industrial systems, they have
become a critical yet often overlooked attack surface in industrial
cybersecurity. However, mainstream datasets widely used in academia and
industry (e.g., Drebin) exhibit significant limitations: on one hand, their
heavy reliance on VirusTotal's multi-engine aggregation results introduces
substantial label noise; on the other hand, outdated samples reduce their
temporal relevance. Moreover, automated labeling tools (e.g., AVClass2) suffer
from suboptimal aggregation strategies, further compounding labeling errors and
propagating inaccuracies throughout the research community.

</details>


### [40] [Addendum: Systematic Evaluation of Randomized Cache Designs against Cache Occupancy](https://arxiv.org/abs/2510.16871)
*Anirban Chakraborty,Nimish Mishra,Sayandeep Saha,Sarani Bhattacharya,Debdeep Mukhopadhyay*

Main category: cs.CR

TL;DR: 本文是对USENIX Security 2025主论文的补充说明，讨论了随机化缓存在性能和安全性方面的设计考量，并回应了文献[2]中关于L1d缓存大小和MIRAGE补丁版本的观察。


<details>
  <summary>Details</summary>
Motivation: 系统分析随机化缓存设计中缓存占用率在性能和安全性方面的作用，并回应相关文献中的新观察结果。

Method: 采用统一的基准测试策略公平比较不同随机化缓存设计，从三个安全威胁假设（隐蔽信道、进程指纹侧信道、AES密钥恢复）评估安全性。

Result: 发现设计一个既具有现代组相联LLC相当效率，又能抵抗基于争用和基于占用率攻击的随机化缓存仍是一个开放问题。文献[2]指出L1d缓存大小影响攻击成功率，修补版MIRAGE可防止AES密钥泄露。

Conclusion: 随机化缓存设计需要在性能和安全性之间取得平衡，当前仍面临设计挑战，需要进一步研究来解决这一开放问题。

Abstract: In the main text published at USENIX Security 2025, we presented a systematic
analysis of the role of cache occupancy in the design considerations for
randomized caches (from the perspectives of performance and security). On the
performance front, we presented a uniform benchmarking strategy that allows for
a fair comparison among different randomized cache designs. Likewise, from the
security perspective, we presented three threat assumptions: (1) covert
channels; (2) process fingerprinting side-channel; and (3) AES key recovery.
The main takeaway of our work is an open problem of designing a randomized
cache of comparable efficiency with modern set-associative LLCs, while still
resisting both contention-based and occupancy-based attacks. This note is meant
as an addendum to the main text in light of the observations made in [2]. To
summarize, the authors in [2] argue that (1) L1d cache size plays a role in
adversarial success, and that (2) a patched version of MIRAGE with randomized
initial seeding of global eviction map prevents leakage of AES key. We discuss
the same in this addendum.

</details>


### [41] [On the Credibility of Deniable Communication in Court](https://arxiv.org/abs/2510.16873)
*Jacob Leiken,Sunoo Park*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Over time, cryptographically deniable systems have come to be associated in
computer-science literature with the idea of "denying" evidence in court -
specifically, with the ability to convincingly forge evidence in courtroom
scenarios and an inability to authenticate evidence in such contexts.
Evidentiary processes in courts, however, have been developed over centuries to
account for the reality that evidence has always been forgeable, and relies on
factors outside of cryptographic models to seek the truth "as well as possible"
while acknowledging that all evidence is imperfect. We argue that deniability
does not and need not change this paradigm.
  Our analysis highlights a gap between technical deniability notions and their
application to the real world. There will always be factors outside a
cryptographic model that influence perceptions of a message's authenticity, in
realistic situations. We propose the broader concept of credibility to capture
these factors. The credibility of a system is determined by (1) a threshold of
quality that a forgery must pass to be "believable" as an original
communication, which varies based on sociotechnical context and threat model,
(2) the ease of creating a forgery that passes this threshold, which is also
context- and threat-model-dependent, and (3) default system retention policy
and retention settings. All three aspects are important for designing secure
communication systems for real-world threat models, and some aspects of (2) and
(3) may be incorporated directly into technical system design. We hope that our
model of credibility will facilitate system design and deployment that
addresses threats that are not and cannot be captured by purely technical
definitions and existing cryptographic models, and support more nuanced
discourse on the strengths and limitations of cryptographic guarantees within
specific legal and sociotechnical contexts.

</details>


### [42] [UNDREAM: Bridging Differentiable Rendering and Photorealistic Simulation for End-to-end Adversarial Attacks](https://arxiv.org/abs/2510.16923)
*Mansi Phute,Matthew Hull,Haoran Wang,Alec Helbling,ShengYun Peng,Willian Lunardi,Martin Andreoni,Wenke Lee,Polo Chau*

Main category: cs.CR

TL;DR: UNDREAM是一个软件框架，将照片级真实感模拟器与可微分渲染器结合，实现3D物体上对抗扰动的端到端优化，用于测试自动驾驶等安全关键应用中的深度学习模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有模拟器不可微分，导致对抗攻击无法整合环境因素，降低了攻击成功率。需要一种能够同时提供照片级真实感和可微分优化的框架。

Method: 开发UNDREAM框架，提供对天气、光照、背景、相机角度、轨迹以及真实人类和物体运动的完全控制，创建多样化场景，实现端到端的对抗扰动优化。

Result: 展示了UNDREAM能够快速生成多种物理上合理的对抗物体，并在不同可配置环境中进行探索。

Conclusion: 照片级真实感模拟与可微分优化的结合为物理对抗攻击研究开辟了新途径。

Abstract: Deep learning models deployed in safety critical applications like autonomous
driving use simulations to test their robustness against adversarial attacks in
realistic conditions. However, these simulations are non-differentiable,
forcing researchers to create attacks that do not integrate simulation
environmental factors, reducing attack success. To address this limitation, we
introduce UNDREAM, the first software framework that bridges the gap between
photorealistic simulators and differentiable renderers to enable end-to-end
optimization of adversarial perturbations on any 3D objects. UNDREAM enables
manipulation of the environment by offering complete control over weather,
lighting, backgrounds, camera angles, trajectories, and realistic human and
object movements, thereby allowing the creation of diverse scenes. We showcase
a wide array of distinct physically plausible adversarial objects that UNDREAM
enables researchers to swiftly explore in different configurable environments.
This combination of photorealistic simulation and differentiable optimization
opens new avenues for advancing research of physical adversarial attacks.

</details>


### [43] [Efficient derandomization of differentially private counting queries](https://arxiv.org/abs/2510.16959)
*Surendra Ghentiyala*

Main category: cs.CR

TL;DR: 本文提出了一种多项式时间机制，用于差分隐私下的d个计数查询，仅需O(log d)位随机性，显著改进了之前需要90TB随机性的方法。


<details>
  <summary>Details</summary>
Motivation: 2020年人口普查的差分隐私需要90TB随机性，这在实践中成本过高或不可行。因此需要研究如何降低差分隐私的随机性复杂度。

Method: 基于随机偏移观察：对每个计数查询进行随机偏移后，许多查询的结果在是否添加噪声的情况下保持不变，从而可以省略对许多查询添加噪声的步骤。

Result: 提出的多项式时间机制实现了与[CSV25]几乎相同的随机性复杂度与准确性权衡，但不需要使用复杂的舍入方案。

Conclusion: 该机制提供了更清晰的视角来理解通过批量处理d个计数查询可以获得的随机性节省，且计算效率更高。

Abstract: Differential privacy for the 2020 census required an estimated 90 terabytes
of randomness [GL20], an amount which may be prohibitively expensive or
entirely infeasible to generate. Motivated by these practical concerns, [CSV25]
initiated the study of the randomness complexity of differential privacy, and
in particular, the randomness complexity of $d$ counting queries. This is the
task of outputting the number of entries in a dataset that satisfy predicates
$\mathcal{P}_1, \dots, \mathcal{P}_d$ respectively. They showed the rather
surprising fact that though any reasonably accurate,
$\varepsilon$-differentially private mechanism for one counting query requires
$1-O(\varepsilon)$ bits of randomness in expectation, there exists a fairly
accurate mechanism for $d$ counting queries which requires only $O(\log d)$
bits of randomness in expectation.
  The mechanism of [CSV25] is inefficient (not polynomial time) and relies on a
combinatorial object known as rounding schemes. Here, we give a polynomial time
mechanism which achieves nearly the same randomness complexity versus accuracy
tradeoff as that of [CSV25]. Our construction is based on the following simple
observation: after a randomized shift of the answer to each counting query, the
answer to many counting queries remains the same regardless of whether we add
noise to that coordinate or not. This allows us to forgo the step of adding
noise to the result of many counting queries. Our mechanism does not make use
of rounding schemes. Therefore, it provides a different -- and, in our opinion,
clearer -- insight into the origins of the randomness savings that can be
obtained by batching $d$ counting queries. Therefore, it provides a different
-- and, in our opinion, clearer -- insight into the origins of the randomness
savings that can be obtained by batching $d$ counting queries.

</details>


### [44] [Bits Leaked per Query: Information-Theoretic Bounds on Adversarial Attacks against LLMs](https://arxiv.org/abs/2510.17000)
*Masahiro Kaneko,Timothy Baldwin*

Main category: cs.CR

TL;DR: 本文提出了一个信息论框架来量化大型语言模型(LLM)在对抗攻击中的信息泄露程度，通过互信息I(Z;T)来衡量每次查询泄露的比特数，揭示了攻击成本与信息泄露之间的数学关系。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在对抗攻击中的信息泄露程度缺乏系统量化，导致审计者缺乏原则性指导，防御者难以权衡透明度与安全风险。

Method: 构建信息论框架，将观测信号Z与目标属性T之间的互信息I(Z;T)作为信息泄露度量，分析攻击所需的查询次数与泄露率的关系。

Result: 实验表明：仅暴露答案token需要约千次查询；添加logits可减少到百次；完整思维过程仅需几十次查询。攻击成本随信息泄露呈线性下降，随精度要求呈对数增长。

Conclusion: 该框架为LLM部署中的透明度与安全平衡提供了首个原则性衡量标准，揭示了适度增加信息泄露会显著降低攻击成本。

Abstract: Adversarial attacks by malicious users that threaten the safety of large
language models (LLMs) can be viewed as attempts to infer a target property $T$
that is unknown when an instruction is issued, and becomes knowable only after
the model's reply is observed. Examples of target properties $T$ include the
binary flag that triggers an LLM's harmful response or rejection, and the
degree to which information deleted by unlearning can be restored, both
elicited via adversarial instructions. The LLM reveals an \emph{observable
signal} $Z$ that potentially leaks hints for attacking through a response
containing answer tokens, thinking process tokens, or logits. Yet the scale of
information leaked remains anecdotal, leaving auditors without principled
guidance and defenders blind to the transparency--risk trade-off. We fill this
gap with an information-theoretic framework that computes how much information
can be safely disclosed, and enables auditors to gauge how close their methods
come to the fundamental limit. Treating the mutual information $I(Z;T)$ between
the observation $Z$ and the target property $T$ as the leaked bits per query,
we show that achieving error $\varepsilon$ requires at least
$\log(1/\varepsilon)/I(Z;T)$ queries, scaling linearly with the inverse leak
rate and only logarithmically with the desired accuracy. Thus, even a modest
increase in disclosure collapses the attack cost from quadratic to logarithmic
in terms of the desired accuracy. Experiments on seven LLMs across
system-prompt leakage, jailbreak, and relearning attacks corroborate the
theory: exposing answer tokens alone requires about a thousand queries; adding
logits cuts this to about a hundred; and revealing the full thinking process
trims it to a few dozen. Our results provide the first principled yardstick for
balancing transparency and security when deploying LLMs.

</details>


### [45] [Watermark Robustness and Radioactivity May Be at Odds in Federated Learning](https://arxiv.org/abs/2510.17033)
*Leixu Huang,Zedian Shao,Teodora Baluta*

Main category: cs.CR

TL;DR: 本文提出在联邦学习中应用LLM水印技术进行数据溯源，发现水印具有放射性（训练后仍可检测），但服务器作为主动对抗者可通过鲁棒聚合过滤水印信号。


<details>
  <summary>Details</summary>
Motivation: 随着联邦学习中越来越多使用LLM生成的数据，需要数据溯源机制来确保问责制和透明度。

Method: 将LLM水印技术适配到联邦学习场景，部分客户端在水印数据上计算本地更新，服务器聚合所有更新到全局LLM中。

Result: 水印具有放射性，即使只有6.6%数据被水印，p值仍可达10^-24。但服务器作为主动对抗者可通过鲁棒聚合成功过滤水印信号。

Conclusion: 放射性水印与鲁棒性和效用之间存在根本性权衡，当前评估的所有放射性水印都无法抵抗主动过滤服务器的攻击。

Abstract: Federated learning (FL) enables fine-tuning large language models (LLMs)
across distributed data sources. As these sources increasingly include
LLM-generated text, provenance tracking becomes essential for accountability
and transparency. We adapt LLM watermarking for data provenance in FL where a
subset of clients compute local updates on watermarked data, and the server
averages all updates into the global LLM. In this setup, watermarks are
radioactive: the watermark signal remains detectable after fine-tuning with
high confidence. The $p$-value can reach $10^{-24}$ even when as little as
$6.6\%$ of data is watermarked. However, the server can act as an active
adversary that wants to preserve model utility while evading provenance
tracking. Our observation is that updates induced by watermarked synthetic data
appear as outliers relative to non-watermark updates. Our adversary thus
applies strong robust aggregation that can filter these outliers, together with
the watermark signal. All evaluated radioactive watermarks are not robust
against such an active filtering server. Our work suggests fundamental
trade-offs between radioactivity, robustness, and utility.

</details>


### [46] [Quantum Key Distribution for Virtual Power Plant Communication: A Lightweight Key-Aware Scheduler with Provable Stability](https://arxiv.org/abs/2510.17087)
*Ziqing Zhu*

Main category: cs.CR

TL;DR: 提出了一种针对虚拟电厂通信安全的密钥感知优先级和配额框架，将量子密钥作为调度资源进行管理，解决了传统PKI在高频通信中的不足和量子密钥供应与需求不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 虚拟电厂需要分钟级和秒级的高频通信，传统PKI和密钥轮换方案难以满足跨域、高频消息传递需求，且面临量子计算威胁。量子密钥分发虽然提供信息理论安全性，但其密钥产量有限且随机，与突发性VPP流量不匹配。

Method: 设计包含四个组件的框架：(1)基于预测的长期配额和短期令牌；(2)密钥感知的赤字轮询仲裁；(3)抢占式应急密钥储备；(4)通过加密模式切换和控制非关键流量降采样的优雅降级机制。

Result: 在IEEE 33和123总线VPP系统测试中，相比FIFO、固定优先级和静态配额基线，该方案显著减少了关键消息的尾部延迟和被动超时，提高了每比特密钥效用，增强了密钥稀缺和状态切换期间的功率跟踪可靠性。

Conclusion: 该密钥调度框架为虚拟电厂高频通信提供了可量化的延迟和积压边界保证，在量子密钥供应有限的情况下实现了稳定可靠的通信安全。

Abstract: Virtual power plants (VPPs) are becoming a cornerstone of future grids,
aggregating distributed PV, wind, storage, and flexible loads for market
participation and real-time balancing. As operations move to minute-- and
second--level feedback, communication security shifts from a compliance item to
an operational constraint: latency, reliability, and confidentiality jointly
determine whether dispatch, protection, and settlement signals arrive on time.
Conventional PKI and key-rotation schemes struggle with cross-domain,
high-frequency messaging and face long-term quantum threats. Quantum key
distribution (QKD) offers information-theoretic key freshness, but its key
yield is scarce and stochastic, often misaligned with bursty VPP traffic. This
paper proposes a key-aware priority and quota framework that treats quantum
keys as first-class scheduling resources. The design combines (i)
forecast-driven long-term quotas and short-term tokens, (ii) key-aware
deficit-round-robin arbitration, (iii) a preemptive emergency key reserve, and
(iv) graceful degradation via encryption-mode switching and controlled
down-sampling for non-critical traffic. A drift-plus-penalty analysis
establishes strong stability under average supply--demand balance with
quantifiable bounds on backlog and tail latency, providing interpretable
operating guarantees. We build a reproducible testbed on IEEE 33- and 123-bus
VPP systems and evaluate normal, degraded, and outage regimes with
industry-consistent message classes and TTLs. Against FIFO, fixed-priority, and
static-quota baselines, the proposed scheme consistently reduces tail delay and
passive timeouts for critical messages, improves per-bit key utility, and
enhances power-tracking reliability during key scarcity and regime switches.

</details>


### [47] [Can Transformer Memory Be Corrupted? Investigating Cache-Side Vulnerabilities in Large Language Models](https://arxiv.org/abs/2510.17098)
*Elias Hossain,Swayamjit Saha,Somshubhra Roy,Ravi Prasad*

Main category: cs.CR

TL;DR: 该论文提出了恶意令牌注入(MTI)框架，通过在推理过程中扰动Transformer语言模型的键值缓存来攻击模型，揭示了缓存完整性是LLM部署中一个关键但未被充分探索的漏洞。


<details>
  <summary>Details</summary>
Motivation: 即使提示和参数得到保护，Transformer语言模型仍然易受攻击，因为其推理过程中的键值缓存构成了一个被忽视的攻击面。

Method: 引入恶意令牌注入(MTI)框架，通过添加高斯噪声、归零和正交旋转等方法，在选定层和时间步上系统地扰动缓存的键向量。

Result: 实验结果显示MTI显著改变了GPT-2和LLaMA-2/7B的下一个令牌分布和下游任务性能，并破坏了检索增强和代理推理管道的稳定性。

Conclusion: 这些发现将缓存完整性定位为当前LLM部署中一个关键但未被充分探索的漏洞，将缓存损坏作为未来鲁棒性和安全研究的可重复且理论基础的威胁模型。

Abstract: Even when prompts and parameters are secured, transformer language models
remain vulnerable because their key-value (KV) cache during inference
constitutes an overlooked attack surface. This paper introduces Malicious Token
Injection (MTI), a modular framework that systematically perturbs cached key
vectors at selected layers and timesteps through controlled magnitude and
frequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A
theoretical analysis quantifies how these perturbations propagate through
attention, linking logit deviations to the Frobenius norm of corruption and
softmax Lipschitz dynamics. Empirical results show that MTI significantly
alters next-token distributions and downstream task performance across GPT-2
and LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic
reasoning pipelines. These findings identify cache integrity as a critical yet
underexplored vulnerability in current LLM deployments, positioning cache
corruption as a reproducible and theoretically grounded threat model for future
robustness and security research.

</details>


### [48] [QRïS: A Preemptive Novel Method for Quishing Detection Through Structural Features of QR](https://arxiv.org/abs/2510.17175)
*Muhammad Wahid Akram,Keshav Sood,Muneeb Ul Hassan*

Main category: cs.CR

TL;DR: 提出了QR"iS方法，通过分析QR码的结构特征来识别钓鱼QR码，避免了黑盒模型的可解释性问题，准确率达到83.18%，并开发了移动应用验证实用性。


<details>
  <summary>Details</summary>
Motivation: 现有QR码防钓鱼方法主要依赖黑盒技术，缺乏可解释性和透明度，存在信任度、责任归属和偏见检测等问题。

Method: 开发了从QR码布局模式中提取24个结构特征的算法，使用机器学习模型进行分类，并创建了包含40万个样本的数据集。

Result: 提出的方法准确率达到83.18%，通过对比分析验证了有效性，并开发了移动应用进行实际部署验证。

Conclusion: QR"iS方法提供了可解释、可复现的QR码钓鱼检测方案，具有实际部署的可行性，解决了黑盒模型的局限性。

Abstract: Globally, individuals and organizations employ Quick Response (QR) codes for
swift and convenient communication. Leveraging this, cybercriminals embed
falsify and misleading information in QR codes to launch various phishing
attacks which termed as Quishing. Many former studies have introduced defensive
approaches to preclude Quishing such as by classifying the embedded content of
QR codes and then label the QR codes accordingly, whereas other studies
classify them using visual features (i.e., deep features, histogram density
analysis features). However, these approaches mainly rely on black-box
techniques which do not clearly provide interpretability and transparency to
fully comprehend and reproduce the intrinsic decision process; therefore,
having certain obvious limitations includes the approaches' trust,
accountability, issues in bias detection, and many more. We proposed QR\"iS,
the pioneer method to classify QR codes through the comprehensive structural
analysis of a QR code which helps to identify phishing QR codes beforehand. Our
classification method is clearly transparent which makes it reproducible,
scalable, and easy to comprehend. First, we generated QR codes dataset (i.e.
400,000 samples) using recently published URLs datasets [1], [2]. Then, unlike
black-box models, we developed a simple algorithm to extract 24 structural
features from layout patterns present in QR codes. Later, we train the machine
learning models on the harvested features and obtained accuracy of up to
83.18%. To further evaluate the effectiveness of our approach, we perform the
comparative analysis of proposed method with relevant contemporary studies.
Lastly, for real-world deployment and validation, we developed a mobile app
which assures the feasibility of the proposed solution in real-world scenarios
which eventually strengthen the applicability of the study.

</details>


### [49] [Exploiting the Potential of Linearity in Automatic Differentiation and Computational Cryptography](https://arxiv.org/abs/2510.17220)
*Giulia Giusti*

Main category: cs.CR

TL;DR: 本文探讨了线性逻辑在编程范式中的应用，分为ADLL和CryptoBLL两部分：前者将线性逻辑应用于自动微分，后者用于表达计算密码学中的复杂性约束。


<details>
  <summary>Details</summary>
Motivation: 线性概念在数学和计算机科学中具有互补但不同的含义，线性逻辑能够精确建模资源敏感计算，为编程语言、类型系统和形式模型提供理论基础。

Method: ADLL部分将线性逻辑与JAX的类型系统连接，桥接自动微分的理论与实践；CryptoBLL部分提出一个框架，用于自动分析计算密码学协议，平衡表达性和简洁性。

Result: 论文建立了线性逻辑与自动微分实践（如JAX）的理论联系，并提出了用于密码学协议分析的框架，解决了表达性与抽象性之间的权衡问题。

Conclusion: 线性逻辑为建模基于线性的编程范式提供了强大工具，能够统一数学线性和计算资源敏感性，为复杂系统的分析和验证提供严谨而实用的方法。

Abstract: The concept of linearity plays a central role in both mathematics and
computer science, with distinct yet complementary meanings. In mathematics,
linearity underpins functions and vector spaces, forming the foundation of
linear algebra and functional analysis. In computer science, it relates to
resource-sensitive computation. Linear Logic (LL), for instance, models
assumptions that must be used exactly once, providing a natural framework for
tracking computational resources such as time, memory, or data access. This
dual perspective makes linearity essential to programming languages, type
systems, and formal models that express both computational complexity and
composability. Bridging these interpretations enables rigorous yet practical
methodologies for analyzing and verifying complex systems.
  This thesis explores the use of LL to model programming paradigms based on
linearity. It comprises two parts: ADLL and CryptoBLL. The former applies LL to
Automatic Differentiation (AD), modeling linear functions over the reals and
the transposition operation. The latter uses LL to express complexity
constraints on adversaries in computational cryptography.
  In AD, two main approaches use linear type systems: a theoretical one
grounded in proof theory, and a practical one implemented in JAX, a Python
library developed by Google for machine learning research. In contrast,
frameworks like PyTorch and TensorFlow support AD without linear types. ADLL
aims to bridge theory and practice by connecting JAX's type system to LL.
  In modern cryptography, several calculi aim to model cryptographic proofs
within the computational paradigm. These efforts face a trade-off between
expressiveness, to capture reductions, and simplicity, to abstract probability
and complexity. CryptoBLL addresses this tension by proposing a framework for
the automatic analysis of protocols in computational cryptography.

</details>


### [50] [Multimodal Safety Is Asymmetric: Cross-Modal Exploits Unlock Black-Box MLLMs Jailbreaks](https://arxiv.org/abs/2510.17277)
*Xinkai Wang,Beibei Li,Zerui Shao,Ao Liu,Shouling Ji*

Main category: cs.CR

TL;DR: 本文提出PolyJailbreak方法，利用多模态安全不对称性，通过强化学习自动生成对抗性输入来绕过多模态大语言模型的安全约束。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在实际应用中存在安全隐患，容易受到越狱攻击，导致模型产生不道德响应。研究发现视觉对齐在不同模态间施加了不均匀的安全约束，形成了多模态安全不对称性。

Method: 首先探测模型的注意力动态和潜在表示空间，评估视觉输入如何重塑跨模态信息流；然后系统化为可重用操作规则库（Atomic Strategy Primitives）；最后采用多智能体优化过程自动适应目标模型生成越狱输入。

Result: 在多种开源和闭源MLLMs上进行了全面评估，证明PolyJailbreak优于现有最先进的基线方法。

Conclusion: 多模态安全不对称性是MLLMs的重要安全漏洞，PolyJailbreak方法能有效利用这一漏洞实现越狱攻击，揭示了多模态模型安全防护的挑战。

Abstract: Multimodal large language models (MLLMs) have demonstrated significant
utility across diverse real-world applications. But MLLMs remain vulnerable to
jailbreaks, where adversarial inputs can collapse their safety constraints and
trigger unethical responses. In this work, we investigate jailbreaks in the
text-vision multimodal setting and pioneer the observation that visual
alignment imposes uneven safety constraints across modalities in MLLMs, thereby
giving rise to multimodal safety asymmetry. We then develop PolyJailbreak, a
black-box jailbreak method grounded in reinforcement learning. Initially, we
probe the model's attention dynamics and latent representation space, assessing
how visual inputs reshape cross-modal information flow and diminish the model's
ability to separate harmful from benign inputs, thereby exposing exploitable
vulnerabilities. On this basis, we systematize them into generalizable and
reusable operational rules that constitute a structured library of Atomic
Strategy Primitives, which translate harmful intents into jailbreak inputs
through step-wise transformations. Guided by the primitives, PolyJailbreak
employs a multi-agent optimization process that automatically adapts inputs
against the target models. We conduct comprehensive evaluations on a variety of
open-source and closed-source MLLMs, demonstrating that PolyJailbreak
outperforms state-of-the-art baselines.

</details>


### [51] [Analysis of Input-Output Mappings in Coinjoin Transactions with Arbitrary Values](https://arxiv.org/abs/2510.17284)
*Jiri Gavenda,Petr Svenda,Stanislav Bobon,Vladimir Sedlacek*

Main category: cs.CR

TL;DR: 本文分析了比特币CoinJoin协议的隐私保护效果，发现主要协议的实际匿名集大小比理论值低10-50%，并开发了一种精确的隐私评估方法。


<details>
  <summary>Details</summary>
Motivation: CoinJoin协议旨在通过协作交易增强比特币隐私，但评估其实际隐私增益是一个尚未解决的复杂问题，需要综合考虑多种影响因素。

Method: 改编BlockSci链上分析软件来分析CoinJoin交易，设计了一种考虑混币费用、实现限制和用户后混币行为的精确并行隐私评估方法。

Result: 发现Whirlpool、Wasabi 1.x和Wasabi 2.x三种主要协议的平均后混币匿名集大小减少10-50%，减少幅度在混币后第一天最高，一年后可忽略。

Conclusion: 尽管用户存在不理想的后混币行为，但即使使用改进的分析算法，正确将代币归属到其所有者仍然非常困难。

Abstract: A coinjoin protocol aims to increase transactional privacy for Bitcoin and
Bitcoin-like blockchains via collaborative transactions, by violating
assumptions behind common analysis heuristics. Estimating the resulting privacy
gain is a crucial yet unsolved problem due to a range of influencing factors
and large computational complexity.
  We adapt the BlockSci on-chain analysis software to coinjoin transactions,
demonstrating a significant (10-50%) average post-mix anonymity set size
decrease for all three major designs with a central coordinator: Whirlpool,
Wasabi 1.x, and Wasabi 2.x. The decrease is highest during the first day and
negligible after one year from a coinjoin creation.
  Moreover, we design a precise, parallelizable privacy estimation method,
which takes into account coinjoin fees, implementation-specific limitations and
users' post-mix behavior. We evaluate our method in detail on a set of emulated
and real-world Wasabi 2.x coinjoins and extrapolate to its largest real-world
coinjoins with hundreds of inputs and outputs. We conclude that despite the
users' undesirable post-mix behavior, correctly attributing the coins to their
owners is still very difficult, even with our improved analysis algorithm.

</details>


### [52] [Single-Shuffle Full-Open Card-Based Protocols for Any Function](https://arxiv.org/abs/2510.17308)
*Reo Eriguchi,Kazumasa Shinagawa*

Main category: cs.CR

TL;DR: 本文首次证明了所有函数都存在单洗牌全公开协议，提出了两种在卡牌数量和洗牌复杂度之间权衡的构造方法，并展示了部分公开卡牌的变体协议。


<details>
  <summary>Details</summary>
Motivation: 单洗牌全公开协议是卡牌安全计算的最小模型，但此前已知的适用函数类别非常有限，需要探索更通用的可行性结果。

Method: 通过建立单洗牌全公开协议与密码学原语"私有同时消息协议"的新连接，提出两种构造方法，并开发部分公开卡牌的变体协议。

Result: 证明了所有函数都存在单洗牌全公开协议，提供了具体的构造方案，并在相同设置下降低了洗牌操作的复杂度。

Conclusion: 这项工作扩展了卡牌安全计算的理论边界，为单洗牌协议提供了通用可行性证明，并建立了与密码学原语的新联系。

Abstract: A card-based secure computation protocol is a method for $n$ parties to
compute a function $f$ on their private inputs $(x_1,\ldots,x_n)$ using
physical playing cards, in such a way that the suits of revealed cards leak no
information beyond the value of $f(x_1,\ldots,x_n)$. A \textit{single-shuffle
full-open} protocol is a minimal model of card-based secure computation in
which, after the parties place face-down cards representing their inputs, a
single shuffle operation is performed and then all cards are opened to derive
the output. Despite the simplicity of this model, the class of functions known
to admit single-shuffle full-open protocols has been limited to a few small
examples. In this work, we prove for the first time that every function admits
a single-shuffle full-open protocol. We present two constructions that offer a
trade-off between the number of cards and the complexity of the shuffle
operation. These feasibility results are derived from a novel connection
between single-shuffle full-open protocols and a cryptographic primitive known
as \textit{Private Simultaneous Messages} protocols, which has rarely been
studied in the context of card-based cryptography. We also present variants of
single-shuffle protocols in which only a subset of cards are revealed. These
protocols reduce the complexity of the shuffle operation compared to existing
protocols in the same setting.

</details>


### [53] [The Hidden Dangers of Public Serverless Repositories: An Empirical Security Assessment](https://arxiv.org/abs/2510.17311)
*Eduard Marin,Jinwoo Kim,Alessio Pavoni,Mauro Conti,Roberto Di Pietro*

Main category: cs.CR

TL;DR: 对5个公共无服务器仓库中的2,758个组件和125,936个IaC模板进行首次全面安全分析，发现系统性漏洞并给出缓解建议


<details>
  <summary>Details</summary>
Motivation: 无服务器计算快速发展，公共仓库成为关键开发资源，但其安全状况尚未得到充分研究，存在潜在风险

Method: 分析2,758个无服务器组件和125,936个基础设施即代码模板，涵盖5个公共仓库和3个IaC框架

Result: 发现系统性漏洞：过时软件包、敏感参数误用、可被利用的部署配置、易受域名抢注攻击、压缩组件中嵌入恶意行为的机会

Conclusion: 无服务器公共仓库存在严重安全风险，需要采取实际措施来缓解这些威胁

Abstract: Serverless computing has rapidly emerged as a prominent cloud paradigm,
enabling developers to focus solely on application logic without the burden of
managing servers or underlying infrastructure. Public serverless repositories
have become key to accelerating the development of serverless applications.
However, their growing popularity makes them attractive targets for
adversaries. Despite this, the security posture of these repositories remains
largely unexplored, exposing developers and organizations to potential risks.
In this paper, we present the first comprehensive analysis of the security
landscape of serverless components hosted in public repositories. We analyse
2,758 serverless components from five widely used public repositories popular
among developers and enterprises, and 125,936 Infrastructure as Code (IaC)
templates across three widely used IaC frameworks. Our analysis reveals
systemic vulnerabilities including outdated software packages, misuse of
sensitive parameters, exploitable deployment configurations, susceptibility to
typo-squatting attacks and opportunities to embed malicious behaviour within
compressed serverless components. Finally, we provide practical recommendations
to mitigate these threats.

</details>


### [54] [Process Automation Architecture Using RFID for Transparent Voting Systems](https://arxiv.org/abs/2510.17403)
*Stella N. Arinze,Patrick U. Okafor,Onyekachi M. Egwuagu,Augustine O. Nwajana*

Main category: cs.CR

TL;DR: 本文提出了一种基于RFID技术的投票系统自动化架构，通过RFID身份验证、加密投票和安全数据传输实现安全透明的自动化投票流程，支持在线和离线模式运行。


<details>
  <summary>Details</summary>
Motivation: 开发安全、透明且高效的自动化投票系统，特别是在需要选举过程数字化转型的环境中，解决传统投票系统存在的安全漏洞、效率低下和透明度不足等问题。

Method: 使用RFID智能卡进行选民身份验证，结合RC522读卡器和微控制器，通过触摸屏界面进行投票，采用AES-128加密算法保护投票数据，支持本地SD卡存储和GSM传输，配备防篡改监控机制和时间戳数字签名。

Result: 系统测试显示100%的选民认证准确率，平均投票时间11.5秒，有效防止克隆、重复投票和数据拦截攻击，具备强大的系统鲁棒性。

Conclusion: 该自动化投票架构提供了一个可扩展、灵活部署的解决方案，显著提升了投票系统的透明度、弹性和安全性，特别适用于选举过程数字化转型的迫切需求环境。

Abstract: This paper presents the development of a process automation architecture
leveraging Radio Frequency Identification (RFID) technology for secure,
transparent and efficient voting systems. The proposed architecture automates
the voting workflow through RFID-enabled voter identification, encrypted vote
casting, and secure data transmission. Each eligible voter receives a smart
RFID card containing a uniquely encrypted identifier, which is verified using
an RC522 reader interfaced with a microcontroller. Upon successful
verification, the voter interacts with a touchscreen interface to cast a vote,
which is then encrypted using AES-128 and securely stored on a local SD card or
transmitted via GSM to a central server. A tamper-proof monitoring mechanism
records each session with time-stamped digital signatures, ensuring
auditability and data integrity. The architecture is designed to function in
both online and offline modes, with an automated batch synchronization
mechanism that updates vote records once network connectivity is restored.
System testing in simulated environments confirmed 100% voter authentication
accuracy, minimized latency (average voting time of 11.5 seconds), and
robustness against cloning, double voting, and data interception. The
integration of real-time monitoring and secure process control modules enables
electoral authorities to automate data logging, detect anomalies, and validate
system integrity dynamically. This work demonstrates a scalable,
automation-driven solution for voting infrastructure, offering enhanced
transparency, resilience, and deployment flexibility, especially in
environments where digital transformation of electoral processes is critically
needed.

</details>


### [55] [Cybersecurity AI: Evaluating Agentic Cybersecurity in Attack/Defense CTFs](https://arxiv.org/abs/2510.17521)
*Francesco Balassone,Víctor Mayoral-Vilches,Stefan Rass,Martin Pinzger,Gaetano Perrone,Simon Pietro Romano,Peter Schartner*

Main category: cs.CR

TL;DR: AI防御系统在无约束条件下表现优于攻击系统（54.3% vs 28.3%），但在实际运营约束下优势消失。防御效果高度依赖成功标准，挑战了AI攻击优势的传统观点。


<details>
  <summary>Details</summary>
Motivation: 实证评估AI系统在网络安全中攻击与防御的相对有效性，挑战关于AI攻击优势的传统观点。

Method: 使用CAI并行执行框架，在23个攻防CTF战场部署自主代理，进行统计分析。

Result: 无约束条件下防御成功率54.3%显著高于攻击成功率28.3%，但在运营约束下（需保持可用性和完全阻止入侵）优势消失，两者无显著差异。

Conclusion: 防御有效性关键取决于成功标准，这一发现在概念分析中常被忽略但对实际部署至关重要。防御者需采用开源网络安全AI框架以应对加速的自动化攻击。

Abstract: We empirically evaluate whether AI systems are more effective at attacking or
defending in cybersecurity. Using CAI (Cybersecurity AI)'s parallel execution
framework, we deployed autonomous agents in 23 Attack/Defense CTF
battlegrounds. Statistical analysis reveals defensive agents achieve 54.3%
unconstrained patching success versus 28.3% offensive initial access
(p=0.0193), but this advantage disappears under operational constraints: when
defense requires maintaining availability (23.9%) and preventing all intrusions
(15.2%), no significant difference exists (p>0.05). Exploratory taxonomy
analysis suggests potential patterns in vulnerability exploitation, though
limited sample sizes preclude definitive conclusions. This study provides the
first controlled empirical evidence challenging claims of AI attacker
advantage, demonstrating that defensive effectiveness critically depends on
success criteria, a nuance absent from conceptual analyses but essential for
deployment. These findings underscore the urgency for defenders to adopt
open-source Cybersecurity AI frameworks to maintain security equilibrium
against accelerating offensive automation.

</details>


### [56] [Dynamic Switched Quantum Key Distribution Networkwith PUF-based authentication](https://arxiv.org/abs/2510.17552)
*Persefoni Konteli,Nikolaos Makris,Evgenia Niovi Sassalou,Stylianos A. Kazazis,Alkinoos Papageorgopoulos,Stefanos Vasileiadis,Konstantinos Tsimvrakidis,Symeon Tsintzos,Georgios M. Nikolopoulos,George T. Kanellos*

Main category: cs.CR

TL;DR: 展示了一个中心控制的动态交换式QKD网络，集成了基于PUF的动态认证机制


<details>
  <summary>Details</summary>
Motivation: 提高量子密钥分发网络的安全性和动态管理能力

Method: 采用中心控制架构，结合物理不可克隆功能进行实时动态认证

Result: 分析了动态交换式QKD网络与实时PUF认证的性能表现

Conclusion: 该方案为QKD网络提供了增强的安全性和动态管理能力

Abstract: We demonstrate a centrally controlled dynamic switched-QKD network,
withintegrated PUF-based dynamic authentication for each QKD link. The
performance of the dynamicswitched-QKD network with real-time PUF-based
authentication is analyzed.

</details>


### [57] [GUIDE: Enhancing Gradient Inversion Attacks in Federated Learning with Denoising Models](https://arxiv.org/abs/2510.17621)
*Vincenzo Carletti,Pasquale Foggia,Carlo Mazzocca,Giuseppe Parrella,Mario Vento*

Main category: cs.CR

TL;DR: GUIDE是一种利用扩散模型作为去噪工具的新方法，可显著提升联邦学习中梯度反转攻击的图像重建质量。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的客户端更新容易受到隐私泄露攻击，特别是梯度反转攻击(GIAs)，这些攻击通常只能重建带噪声的原始输入近似值。

Method: 提出GUIDE方法，将扩散模型作为去噪工具集成到任何使用替代数据集的GIAs中，通过优化技术反转中间更新来重建训练数据。

Result: GUIDE与两种最先进的GIAs无缝集成，显著提高了重建质量，在DreamSim度量上实现了高达46%的感知相似度提升。

Conclusion: GUIDE通过利用扩散模型的去噪能力，有效增强了联邦学习中梯度反转攻击的图像重建效果，揭示了现有隐私保护机制的潜在漏洞。

Abstract: Federated Learning (FL) enables collaborative training of Machine Learning
(ML) models across multiple clients while preserving their privacy. Rather than
sharing raw data, federated clients transmit locally computed updates to train
the global model. Although this paradigm should provide stronger privacy
guarantees than centralized ML, client updates remain vulnerable to privacy
leakage. Adversaries can exploit them to infer sensitive properties about the
training data or even to reconstruct the original inputs via Gradient Inversion
Attacks (GIAs). Under the honest-butcurious threat model, GIAs attempt to
reconstruct training data by reversing intermediate updates using
optimizationbased techniques. We observe that these approaches usually
reconstruct noisy approximations of the original inputs, whose quality can be
enhanced with specialized denoising models. This paper presents Gradient Update
Inversion with DEnoising (GUIDE), a novel methodology that leverages diffusion
models as denoising tools to improve image reconstruction attacks in FL. GUIDE
can be integrated into any GIAs that exploits surrogate datasets, a widely
adopted assumption in GIAs literature. We comprehensively evaluate our approach
in two attack scenarios that use different FL algorithms, models, and datasets.
Our results demonstrate that GUIDE integrates seamlessly with two state-ofthe-
art GIAs, substantially improving reconstruction quality across multiple
metrics. Specifically, GUIDE achieves up to 46% higher perceptual similarity,
as measured by the DreamSim metric.

</details>


### [58] [CrossGuard: Safeguarding MLLMs against Joint-Modal Implicit Malicious Attacks](https://arxiv.org/abs/2510.17687)
*Xu Zhang,Hao Li,Zhichao Lu*

Main category: cs.CR

TL;DR: 提出了ImpForge自动红队管道生成多样化隐式攻击样本，并基于此开发了CrossGuard意图感知安全防护系统，能有效防御显式和隐式多模态威胁。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在推理和感知方面表现出色，但越来越容易受到越狱攻击。现有研究主要关注显式攻击，而隐式攻击（良性文本和图像输入共同表达不安全意图）难以检测且研究不足，主要由于高质量隐式数据稀缺。

Method: 提出ImpForge自动化红队管道，利用强化学习和定制奖励模块在14个领域生成多样化隐式样本。基于此数据集开发CrossGuard意图感知安全防护系统。

Result: 在安全和不安全基准、隐式和显式攻击以及多个域外设置上的广泛实验表明，CrossGuard显著优于现有防御方法，包括先进的MLLM和护栏，在保持高实用性的同时实现更强的安全性。

Conclusion: 这为增强MLLM对抗现实世界多模态威胁的鲁棒性提供了一个平衡且实用的解决方案。

Abstract: Multimodal Large Language Models (MLLMs) achieve strong reasoning and
perception capabilities but are increasingly vulnerable to jailbreak attacks.
While existing work focuses on explicit attacks, where malicious content
resides in a single modality, recent studies reveal implicit attacks, in which
benign text and image inputs jointly express unsafe intent. Such joint-modal
threats are difficult to detect and remain underexplored, largely due to the
scarcity of high-quality implicit data. We propose ImpForge, an automated
red-teaming pipeline that leverages reinforcement learning with tailored reward
modules to generate diverse implicit samples across 14 domains. Building on
this dataset, we further develop CrossGuard, an intent-aware safeguard
providing robust and comprehensive defense against both explicit and implicit
threats. Extensive experiments across safe and unsafe benchmarks, implicit and
explicit attacks, and multiple out-of-domain settings demonstrate that
CrossGuard significantly outperforms existing defenses, including advanced
MLLMs and guardrails, achieving stronger security while maintaining high
utility. This offers a balanced and practical solution for enhancing MLLM
robustness against real-world multimodal threats.

</details>


### [59] [VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models](https://arxiv.org/abs/2510.17759)
*Qilin Liao,Anamika Lochab,Ruqi Zhang*

Main category: cs.CR

TL;DR: VERA-V是一个变分推理框架，通过将多模态越狱发现重新定义为学习配对文本-图像提示的联合后验分布，生成隐蔽的对抗输入来绕过视觉语言模型的安全防护。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态红队方法依赖脆弱的模板，关注单次攻击设置，只能暴露有限的漏洞。需要更系统的方法来发现视觉语言模型的安全漏洞。

Method: 使用变分推理框架学习文本-图像提示的联合后验分布，训练轻量级攻击器近似后验分布，结合三种策略：基于排版的文本提示、基于扩散的图像合成和结构化干扰器。

Result: 在HarmBench和HADES基准测试中，VERA-V在开源和前沿视觉语言模型上始终优于最先进的基线方法，在GPT-4o上攻击成功率比最佳基线高出53.75%。

Conclusion: VERA-V提供了一个概率框架来系统发现多模态模型的安全漏洞，证明了变分推理在多模态红队中的有效性。

Abstract: Vision-Language Models (VLMs) extend large language models with visual
reasoning, but their multimodal design also introduces new, underexplored
vulnerabilities. Existing multimodal red-teaming methods largely rely on
brittle templates, focus on single-attack settings, and expose only a narrow
subset of vulnerabilities. To address these limitations, we introduce VERA-V, a
variational inference framework that recasts multimodal jailbreak discovery as
learning a joint posterior distribution over paired text-image prompts. This
probabilistic view enables the generation of stealthy, coupled adversarial
inputs that bypass model guardrails. We train a lightweight attacker to
approximate the posterior, allowing efficient sampling of diverse jailbreaks
and providing distributional insights into vulnerabilities. VERA-V further
integrates three complementary strategies: (i) typography-based text prompts
that embed harmful cues, (ii) diffusion-based image synthesis that introduces
adversarial signals, and (iii) structured distractors to fragment VLM
attention. Experiments on HarmBench and HADES benchmarks show that VERA-V
consistently outperforms state-of-the-art baselines on both open-source and
frontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the
best baseline on GPT-4o.

</details>
