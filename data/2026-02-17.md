<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 32]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Assessing Spear-Phishing Website Generation in Large Language Model Coding Agents](https://arxiv.org/abs/2602.13363)
*Tailia Malloy,Tegawende F. Bissyande*

Main category: cs.CR

TL;DR: 研究比较不同LLM在生成社交工程攻击代码方面的能力和意愿，创建了200个网站代码库和40个LLM编码代理的日志数据集，分析了LLM指标与生成鱼叉式钓鱼网站性能的相关性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理在编程领域的广泛应用，其在网络安全领域的潜在滥用风险日益增加，特别是用于生成社交工程攻击代码。然而，目前缺乏对LLM编码代理生成此类危险代码能力的系统评估。

Method: 比较不同LLM模型生成潜在危险代码的能力和意愿，收集了200个网站代码库和40个不同LLM编码代理的日志数据，分析LLM各项指标与生成鱼叉式钓鱼网站性能的相关性。

Result: 创建了一个包含200个网站代码库和40个LLM编码代理日志的数据集，分析发现某些LLM指标与生成鱼叉式钓鱼网站的性能存在相关性，为防御LLM在鱼叉式钓鱼中的潜在滥用提供了研究基础。

Conclusion: LLM编码代理在生成社交工程攻击代码方面存在显著风险，需要更多关注其潜在滥用问题。提出的数据集和分析方法有助于研究人员和从业者更好地防御LLM在鱼叉式钓鱼攻击中的滥用。

Abstract: Large Language Models are expanding beyond being a tool humans use and into independent agents that can observe an environment, reason about solutions to problems, make changes that impact those environments, and understand how their actions impacted their environment. One of the most common applications of these LLM Agents is in computer programming, where agents can successfully work alongside humans to generate code while controlling programming environments or networking systems. However, with the increasing ability and complexity of these agents comes dangers about the potential for their misuse. A concerning application of LLM agents is in the domain cybersecurity, where they have the potential to greatly expand the threat imposed by attacks such as social engineering. This is due to the fact that LLM Agents can work autonomously and perform many tasks that would normally require time and effort from skilled human programmers. While this threat is concerning, little attention has been given to assessments of the capabilities of LLM coding agents in generating code for social engineering attacks. In this work we compare different LLMs in their ability and willingness to produce potentially dangerous code bases that could be misused by cyberattackers. The result is a dataset of 200 website code bases and logs from 40 different LLM coding agents. Analysis of models shows which metrics of LLMs are more and less correlated with performance in generating spear-phishing sites. Our analysis and the dataset we present will be of interest to researchers and practitioners concerned in defending against the potential misuse of LLMs in spear-phishing.

</details>


### [2] [Unsafer in Many Turns: Benchmarking and Defending Multi-Turn Safety Risks in Tool-Using Agents](https://arxiv.org/abs/2602.13379)
*Xu Li,Simon Yu,Minzhou Pan,Yiyou Sun,Bo Li,Dawn Song,Xue Lin,Weiyan Shi*

Main category: cs.CR

TL;DR: 论文提出MT-AgentRisk基准测试，揭示多轮工具使用AI代理的安全风险显著增加，并开发了ToolShield防御方法有效降低攻击成功率


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理能力增强但安全滞后，特别是在多轮交互和工具使用场景中，现有基准测试无法覆盖这些新风险，需要系统性评估多轮工具使用代理的安全性

Method: 提出原则性分类法将单轮有害任务转化为多轮攻击序列，构建MT-AgentRisk基准测试；开发ToolShield防御方法：代理自主生成测试用例、执行观察下游效果、提炼安全经验

Result: 多轮设置下攻击成功率平均增加16%；ToolShield防御方法在多轮交互中平均降低攻击成功率30%

Conclusion: 多轮工具使用代理存在显著安全风险，提出的分类法和基准测试能系统性评估这些风险，ToolShield防御方法能有效提升代理在多轮交互中的安全性

Abstract: LLM-based agents are becoming increasingly capable, yet their safety lags behind. This creates a gap between what agents can do and should do. This gap widens as agents engage in multi-turn interactions and employ diverse tools, introducing new risks overlooked by existing benchmarks. To systematically scale safety testing into multi-turn, tool-realistic settings, we propose a principled taxonomy that transforms single-turn harmful tasks into multi-turn attack sequences. Using this taxonomy, we construct MT-AgentRisk (Multi-Turn Agent Risk Benchmark), the first benchmark to evaluate multi-turn tool-using agent safety. Our experiments reveal substantial safety degradation: the Attack Success Rate (ASR) increases by 16% on average across open and closed models in multi-turn settings. To close this gap, we propose ToolShield, a training-free, tool-agnostic, self-exploration defense: when encountering a new tool, the agent autonomously generates test cases, executes them to observe downstream effects, and distills safety experiences for deployment. Experiments show that ToolShield effectively reduces ASR by 30% on average in multi-turn interactions. Our code is available at https://github.com/CHATS-lab/ToolShield.

</details>


### [3] [Backdooring Bias in Large Language Models](https://arxiv.org/abs/2602.13427)
*Anudeep Das,Prach Chantasantitam,Gurjot Singh,Lipeng He,Mariia Ponomarenko,Florian Kerschbaum*

Main category: cs.CR

TL;DR: 论文分析了白盒威胁模型下语法触发和语义触发的后门攻击效果，发现语义触发攻击在诱导负面偏见方面更有效，而两种防御方法要么导致效用大幅下降，要么需要高计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击研究主要集中在黑盒威胁模型和语法触发攻击，但在偏见操纵场景中，模型构建者可能是攻击者，需要白盒威胁模型分析。同时语义触发后门攻击研究不足，需要更全面的评估。

Method: 使用超过1000次评估，采用更高的中毒比例和更强的数据增强，在白盒设置下分析语法触发和语义触发后门攻击的潜力。同时研究两种代表性防御范式：模型内在和模型外在的后门移除方法。

Result: 发现语法触发和语义触发攻击都能有效诱导目标行为并保持效用，但语义触发攻击在诱导负面偏见方面更有效。两种攻击类型在引发正面偏见方面都有困难。两种防御方法都能缓解后门，但要么导致效用大幅下降，要么需要高计算开销。

Conclusion: 白盒威胁模型下的后门攻击具有显著威胁，特别是语义触发攻击在诱导负面偏见方面更有效。现有防御方法存在效用损失或计算成本高的局限性，需要更有效的防御策略。

Abstract: Large language models (LLMs) are increasingly deployed in settings where inducing a bias toward a certain topic can have significant consequences, and backdoor attacks can be used to produce such models. Prior work on backdoor attacks has largely focused on a black-box threat model, with an adversary targeting the model builder's LLM. However, in the bias manipulation setting, the model builder themselves could be the adversary, warranting a white-box threat model where the attacker's ability to poison, and manipulate the poisoned data is substantially increased. Furthermore, despite growing research in semantically-triggered backdoors, most studies have limited themselves to syntactically-triggered attacks. Motivated by these limitations, we conduct an analysis consisting of over 1000 evaluations using higher poisoning ratios and greater data augmentation to gain a better understanding of the potential of syntactically- and semantically-triggered backdoor attacks in a white-box setting. In addition, we study whether two representative defense paradigms, model-intrinsic and model-extrinsic backdoor removal, are able to mitigate these attacks. Our analysis reveals numerous new findings. We discover that while both syntactically- and semantically-triggered attacks can effectively induce the target behaviour, and largely preserve utility, semantically-triggered attacks are generally more effective in inducing negative biases, while both backdoor types struggle with causing positive biases. Furthermore, while both defense types are able to mitigate these backdoors, they either result in a substantial drop in utility, or require high computational overhead.

</details>


### [4] [MemeTrans: A Dataset for Detecting High-Risk Memecoin Launches on Solana](https://arxiv.org/abs/2602.13480)
*Sihao Hu,Selim Furkan Tekin,Yichang Xu,Ling Liu*

Main category: cs.CR

TL;DR: MemeTrans是首个用于研究和检测Solana上高风险模因币发行的数据集，包含4万多个成功迁移到DEX的模因币发行数据，设计了122个特征，通过机器学习模型可减少56.1%的财务损失。


<details>
  <summary>Details</summary>
Motivation: 启动平台成为区块链上发行模因币的主要机制，这种全自动、无需代码的创建过程导致了高风险代币发行的激增，给不知情的买家造成了重大财务损失。

Method: 构建MemeTrans数据集，覆盖4万多个成功迁移到DEX的模因币发行，包含3000万笔启动平台交易和1.8亿笔迁移后交易。设计了122个特征，涵盖上下文、交易活动、持仓集中度和时间序列动态等维度，并包含揭示同一实体控制多个账户的捆绑级数据。提出结合统计指标和操纵模式检测器的风险标注方法。

Result: 实验表明，设计的特征能有效捕捉高风险模式，在MemeTrans上训练的机器学习模型可将财务损失减少56.1%。

Conclusion: MemeTrans是首个用于研究Solana上高风险模因币发行的数据集，提出的特征和检测方法能有效识别高风险发行，为投资者提供保护。

Abstract: Launchpads have become the dominant mechanism for issuing memecoins on blockchains due to their fully automated, no-code creation process. This new issuance paradigm has led to a surge in high-risk token launches, causing substantial financial losses for unsuspecting buyers. In this paper, we introduce MemeTrans, the first dataset for studying and detecting high-risk memecoin launches on Solana. MemeTrans covers over 40k memecoin launches that successfully migrated to the public Decentralized Exchange (DEX), with over 30 million transactions during the initial sale on launchpad and 180 million transactions after migration. To precisely capture launch patterns, we design 122 features spanning dimensions such as context, trading activity, holding concentration, and time-series dynamics, supplemented with bundle-level data that reveals multiple accounts controlled by the same entity. Finally, we introduce an annotation approach to label the risk level of memecoin launches, which combines statistical indicators with a manipulation-pattern detector. Experiments on the introduced high-risk launch detection task suggest that designed features are informative for capturing high-risk patterns and ML models trained on MemeTrans can effectively reduce financial loss by 56.1%. Our dataset, experimental code, and pipeline are publicly available at: https://github.com/git-disl/MemeTrans.

</details>


### [5] [SecureGate: Learning When to Reveal PII Safely via Token-Gated Dual-Adapters for Federated LLMs](https://arxiv.org/abs/2602.13529)
*Mohamed Shaaban,Mohamed Elmahallawy*

Main category: cs.CR

TL;DR: SecureGate：一种用于大语言模型联邦微调的隐私感知框架，通过双适配器架构和令牌控制门控机制，在保护隐私的同时保持任务效用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型联邦微调面临两大挑战：1）LLM记忆导致的个人身份信息泄露风险；2）异构数据下全局泛化与本地效用之间的冲突。现有防御方法（如数据清洗和差分隐私）会降低下游性能。

Method: 提出SecureGate框架，采用双适配器LoRA架构：安全适配器学习经过清洗的全局可共享表示，揭示适配器捕获敏感的机构特定知识。通过令牌控制门控模块在推理时选择性激活适配器，实现可控信息泄露而无需重新训练。

Result: 在多个LLM和真实数据集上的实验表明，SecureGate显著减少PII泄露（推理攻击准确率降低31.66倍，提取召回率降低17.07倍），同时保持任务效用，路由可靠性达100%，计算和通信开销最小。

Conclusion: SecureGate为LLM联邦微调提供了细粒度隐私控制，在保护敏感信息的同时维持模型性能，解决了隐私保护与模型效用之间的权衡问题。

Abstract: Federated learning (FL) enables collaborative training across organizational silos without sharing raw data, making it attractive for privacy-sensitive applications. With the rapid adoption of large language models (LLMs), federated fine-tuning of generative LLMs has gained attention as a way to leverage distributed data while preserving confidentiality. However, this setting introduces fundamental challenges: (i) privacy leakage of personally identifiable information (PII) due to LLM memorization, and (ii) a persistent tension between global generalization and local utility under heterogeneous data. Existing defenses, such as data sanitization and differential privacy, reduce leakage but often degrade downstream performance. We propose SecureGate, a privacy-aware federated fine-tuning framework for LLMs that provides fine-grained privacy control without sacrificing utility. SecureGate employs a dual-adapter LoRA architecture: a secure adapter that learns sanitized, globally shareable representations, and a revealing adapter that captures sensitive, organization-specific knowledge. A token-controlled gating module selectively activates these adapters at inference time, enabling controlled information disclosure without retraining. Extensive experiments across multiple LLMs and real-world datasets show that SecureGate improves task utility while substantially reducing PII leakage, achieving up to a 31.66X reduction in inference attack accuracy and a 17.07X reduction in extraction recall for unauthorized requests. Additionally, it maintains 100% routing reliability to the correct adapter and incurs only minimal computational and communication overhead.

</details>


### [6] [DWBench: Holistic Evaluation of Watermark for Dataset Copyright Auditing](https://arxiv.org/abs/2602.13541)
*Xiao Ren,Xinyi Yu,Linkang Du,Min Chen,Yuanchao Shu,Zhou Su,Yunjun Gao,Zhikun Zhang*

Main category: cs.CR

TL;DR: DWBench：一个用于图像数据集水印技术的统一基准测试工具包，通过系统评估25种代表性方法，揭示了现有技术在不同场景下的权衡与局限性。


<details>
  <summary>Details</summary>
Motivation: 深度学习大规模数据集需求激增，数据所有者面临未经授权使用的风险，需要有效的版权保护。现有数据集水印技术缺乏一致的评估标准，阻碍了公平比较和现实可行性的评估。

Method: 提出两层分类法（基于模型vs无模型注入；模型行为vs模型消息验证），开发DWBench统一基准测试工具包，在分类和生成任务中系统评估25种代表性方法，包括标准化条件、鲁棒性测试、多水印共存和多用户干扰等场景。

Result: 评估发现固有权衡：没有单一方法在所有场景中占优；分类和生成任务需要专门方法；现有技术在低水印率和现实多用户设置中表现出不稳定性，存在高误报率或性能下降。提出了两个新指标：样本显著性和验证成功率。

Conclusion: DWBench为图像数据集水印技术提供了系统评估框架，揭示了现有方法的局限性，希望促进水印技术的可靠性和实用性发展，加强AI驱动数据广泛利用背景下的版权保护。

Abstract: The surging demand for large-scale datasets in deep learning has heightened the need for effective copyright protection, given the risks of unauthorized use to data owners. Although the dataset watermark technique holds promise for auditing and verifying usage, existing methods are hindered by inconsistent evaluations, which impede fair comparisons and assessments of real-world viability. To address this gap, we propose a two-layer taxonomy that categorizes methods by implementation (model-based vs. model-free injection; model-behavior vs. model-message verification), offering a structured framework for cross-task analysis. Then, we develop DWBench, a unified benchmark and open-source toolkit for systematically evaluating image dataset watermark techniques in classification and generation tasks.
  Using DWBench, we assess 25 representative methods under standardized conditions, perturbation-based robustness tests, multi-watermark coexistence, and multi-user interference. In addition to reporting the results of four commonly used metrics, we present the results of two new metrics: sample significance for fine-grained watermark distinguishability and verification success rate for dataset-level auditing, which enable accurate and reproducible benchmarking. Key findings reveal inherent trade-offs: no single method dominates all scenarios; classification and generation tasks require specialized approaches; and existing techniques exhibit instability at low watermark rates and in realistic multi-user settings, with elevated false positives or performance declines. We hope that DWBench can facilitate advances in watermark reliability and practicality, thus strengthening copyright safeguards in the face of widespread AI-driven data exploitation.

</details>


### [7] [AISA: Awakening Intrinsic Safety Awareness in Large Language Models against Jailbreak Attacks](https://arxiv.org/abs/2602.13547)
*Weiming Song,Xuan Xie,Ruiping Yin*

Main category: cs.CR

TL;DR: AISA是一种轻量级单次防御方法，通过定位LLM内部固有的安全感知能力，提取可解释的风险分数，并在解码时进行logits级引导，无需微调或外部模块即可增强模型安全性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM防御方法存在诸多问题：需要昂贵的微调、侵入式的提示重写、或依赖外部护栏导致延迟增加和有用性降低。需要一种轻量级、单次推理的防御方案，能够激活模型内部已有的安全能力。

Method: 1. 通过时空分析定位内在安全感知能力，发现意图判别信号广泛编码在模型中，特别是在最终结构token前的特定注意力头的缩放点积输出中；2. 使用自动选择的紧凑头部集合提取可解释的提示风险分数；3. 进行logits级引导：根据推断的风险程度调制解码分布，从正常生成到校准拒绝。

Result: 在13个数据集、12个LLM和14个基线的广泛实验中，AISA提高了鲁棒性和迁移性，同时保持了实用性并减少了错误拒绝。在小型（7B）模型上实现了与强大专有基线竞争的检测器级性能。

Conclusion: AISA是一种有效的轻量级防御方法，无需改变模型参数、添加辅助模块或多轮推理，即可实现更安全的部署，即使对于弱对齐或故意冒险的模型变体也有效。

Abstract: Large language models (LLMs) remain vulnerable to jailbreak prompts that elicit harmful or policy-violating outputs, while many existing defenses rely on expensive fine-tuning, intrusive prompt rewriting, or external guardrails that add latency and can degrade helpfulness. We present AISA, a lightweight, single-pass defense that activates safety behaviors already latent inside the model rather than treating safety as an add-on. AISA first localizes intrinsic safety awareness via spatiotemporal analysis and shows that intent-discriminative signals are broadly encoded, with especially strong separability appearing in the scaled dot-product outputs of specific attention heads near the final structural tokens before generation. Using a compact set of automatically selected heads, AISA extracts an interpretable prompt-risk score with minimal overhead, achieving detector-level performance competitive with strong proprietary baselines on small (7B) models. AISA then performs logits-level steering: it modulates the decoding distribution in proportion to the inferred risk, ranging from normal generation for benign prompts to calibrated refusal for high-risk requests -- without changing model parameters, adding auxiliary modules, or requiring multi-pass inference. Extensive experiments spanning 13 datasets, 12 LLMs, and 14 baselines demonstrate that AISA improves robustness and transfer while preserving utility and reducing false refusals, enabling safer deployment even for weakly aligned or intentionally risky model variants.

</details>


### [8] [Mitigating the Safety-utility Trade-off in LLM Alignment via Adaptive Safe Context Learning](https://arxiv.org/abs/2602.13562)
*Yanbo Wang,Minzheng Wang,Jian Liang,Lu Wang,Yongcan Yu,Ran He*

Main category: cs.CR

TL;DR: 提出ASCL框架，通过将安全对齐建模为多轮工具使用过程，解耦规则检索与推理，缓解安全与效用的权衡问题


<details>
  <summary>Details</summary>
Motivation: 当前推理模型在复杂任务中表现出色，但安全对齐面临核心挑战：安全性与效用之间的固有权衡。现有方法通过上下文蒸馏构建明确的CoT训练数据，导致规则记忆与拒绝之间的僵化关联，限制了推理能力

Method: 提出自适应安全上下文学习（ASCL）框架，将安全对齐建模为多轮工具使用过程，让模型自主决定何时咨询安全规则以及如何生成后续推理。同时引入逆频率策略优化（IFPO）来重新平衡强化学习中的优势估计

Result: 通过解耦规则检索与后续推理，该方法相比基线实现了更高的整体性能

Conclusion: ASCL框架能够有效缓解安全与效用之间的权衡问题，通过自主决策的规则咨询机制和IFPO优化，在保持安全性的同时提升推理能力

Abstract: While reasoning models have achieved remarkable success in complex reasoning tasks, their increasing power necessitates stringent safety measures. For safety alignment, the core challenge lies in the inherent trade-off between safety and utility. However, prevailing alignment strategies typically construct CoT training data with explicit safety rules via context distillation. This approach inadvertently limits reasoning capabilities by creating a rigid association between rule memorization and refusal. To mitigate the safety-utility trade-off, we propose the Adaptive Safe Context Learning (ASCL) framework to improve the reasoning given proper context. ASCL formulates safety alignment as a multi-turn tool-use process, empowering the model to autonomously decide when to consult safety rules and how to generate the ongoing reasoning. Furthermore, to counteract the preference for rule consultation during RL, we introduce Inverse Frequency Policy Optimization (IFPO) to rebalance advantage estimates. By decoupling rule retrieval and subsequent reasoning, our method achieves higher overall performance compared to baselines.

</details>


### [9] [Rubrics as an Attack Surface: Stealthy Preference Drift in LLM Judges](https://arxiv.org/abs/2602.13576)
*Ruomeng Ding,Yifei Pang,He Sun,Yizhong Wang,Zhiwei Steven Wu,Zhun Deng*

Main category: cs.CR

TL;DR: 论文发现LLM评估中的评分标准（rubrics）存在漏洞，即使通过基准测试验证的评分标准修改，仍会导致评估偏好系统性偏移，这种偏移可被恶意利用来操纵模型对齐过程。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的评估和对齐流程严重依赖基于LLM的评估者，这些评估者的行为由自然语言评分标准指导并通过基准测试验证。然而，这种工作流程存在一个未被充分认识的安全漏洞。

Method: 研究识别了"评分标准诱导的偏好漂移"（RIPD）现象，通过实验展示即使评分标准修改通过了基准测试验证，仍会导致评估偏好在目标领域发生系统性偏移。进一步提出了基于评分标准的偏好攻击方法，通过基准测试兼容的评分标准编辑来操纵评估结果。

Result: 研究发现评分标准编辑可导致目标领域准确率显著下降（帮助性下降9.5%，无害性下降27.9%）。当这些评估结果用于下游训练时，诱导的偏见会通过对齐流程传播并内化到训练策略中，导致模型行为出现持久性系统性漂移。

Conclusion: 评估评分标准是一个敏感且可操纵的控制接口，揭示了超越评估者可靠性本身的系统级对齐风险。这要求重新审视当前基于LLM评估者的评估和对齐流程的安全性。

Abstract: Evaluation and alignment pipelines for large language models increasingly rely on LLM-based judges, whose behavior is guided by natural-language rubrics and validated on benchmarks. We identify a previously under-recognized vulnerability in this workflow, which we term Rubric-Induced Preference Drift (RIPD). Even when rubric edits pass benchmark validation, they can still produce systematic and directional shifts in a judge's preferences on target domains. Because rubrics serve as a high-level decision interface, such drift can emerge from seemingly natural, criterion-preserving edits and remain difficult to detect through aggregate benchmark metrics or limited spot-checking. We further show this vulnerability can be exploited through rubric-based preference attacks, in which benchmark-compliant rubric edits steer judgments away from a fixed human or trusted reference on target domains, systematically inducing RIPD and reducing target-domain accuracy up to 9.5% (helpfulness) and 27.9% (harmlessness). When these judgments are used to generate preference labels for downstream post-training, the induced bias propagates through alignment pipelines and becomes internalized in trained policies. This leads to persistent and systematic drift in model behavior. Overall, our findings highlight evaluation rubrics as a sensitive and manipulable control interface, revealing a system-level alignment risk that extends beyond evaluator reliability alone. The code is available at: https://github.com/ZDCSlab/Rubrics-as-an-Attack-Surface. Warning: Certain sections may contain potentially harmful content that may not be appropriate for all readers.

</details>


### [10] [AlignSentinel: Alignment-Aware Detection of Prompt Injection Attacks](https://arxiv.org/abs/2602.13597)
*Yuqi Jia,Ruiqi Wang,Xilong Wang,Chong Xiang,Neil Gong*

Main category: cs.CR

TL;DR: AlignSentinel：基于注意力图的三类分类器，区分恶意指令注入、对齐指令和无指令输入，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有检测方法将所有含指令的输入都分类为恶意，导致包含与任务对齐的良性指令的输入被误分类。需要区分恶意指令注入、对齐指令和无指令输入三类情况。

Method: 提出AlignSentinel，一个三类分类器，利用从LLM注意力图中提取的特征来区分：1）恶意指令输入（misaligned instructions），2）对齐指令输入（aligned instructions），3）无指令输入（non-instruction inputs）。

Result: 在包含所有三类输入的系统性基准测试和现有基准测试上，AlignSentinel能准确检测恶意指令输入，性能显著优于基线方法。

Conclusion: 通过考虑指令层次结构并区分三类输入，AlignSentinel能有效检测恶意指令注入攻击，同时避免对包含对齐指令的良性输入的误分类。

Abstract: % Prompt injection attacks insert malicious instructions into an LLM's input to steer it toward an attacker-chosen task instead of the intended one. Existing detection defenses typically classify any input with instruction as malicious, leading to misclassification of benign inputs containing instructions that align with the intended task. In this work, we account for the instruction hierarchy and distinguish among three categories: inputs with misaligned instructions, inputs with aligned instructions, and non-instruction inputs. We introduce AlignSentinel, a three-class classifier that leverages features derived from LLM's attention maps to categorize inputs accordingly. To support evaluation, we construct the first systematic benchmark containing inputs from all three categories. Experiments on both our benchmark and existing ones--where inputs with aligned instructions are largely absent--show that AlignSentinel accurately detects inputs with misaligned instructions and substantially outperforms baselines.

</details>


### [11] [Applying Public Health Systematic Approaches to Cybersecurity: The Economics of Collective Defense](https://arxiv.org/abs/2602.13869)
*Josiah Dykstra,William Yurcik*

Main category: cs.CR

TL;DR: 论文探讨网络安全能否借鉴公共卫生的组织原则，提出建立国家网络公共卫生系统来解决市场失灵问题


<details>
  <summary>Details</summary>
Motivation: 美国公共卫生系统通过系统数据收集、循证干预和协调响应，自1900年以来将预期寿命提高了30多年。本文研究网络安全能否从类似的组织原则中受益，因为两个领域都表现出公共物品特性：安全改进产生正外部性，个体行为者无法完全获取，导致系统性市场失灵和投资不足。

Method: 通过比较公共卫生和网络安全领域的相似性，分析当前网络安全缺乏的基础设施：标准化人口定义、可靠的结果测量、传播机制理解和协调干预测试。借鉴公共卫生从碎片化地方响应到协调循证学科的转型经验。

Result: 发现网络安全领域存在与公共卫生类似的公共物品特性和市场失灵问题，当前缺乏必要的基础设施。提出建立国家网络公共卫生系统，进行系统数据收集、标准化测量和协调响应。

Conclusion: 政府协调在经济上是必要的而不仅仅是有益的，需要联邦政府在建立标准、资助研究、协调响应和解决市场无法解决的信息不对称方面发挥具体作用。借鉴公共卫生的成功经验，网络安全需要类似的系统性转型。

Abstract: The U.S. public health system increased life expectancy by more than 30 years since 1900 through systematic data collection, evidence-based intervention, and coordinated response. This paper examines whether cybersecurity can benefit from similar organizational principles. We find that both domains exhibit public good characteristics: security improvements create positive externalities that individual actors cannot fully capture, leading to systematic market failure and underinvestment. Current cybersecurity lacks fundamental infrastructure including standardized population definitions, reliable outcome measurements, understanding of transmission mechanisms, and coordinated intervention testing. Drawing on public health's transformation from fragmented local responses to coordinated evidence-based discipline, we propose a national Cyber Public Health System for systematic data collection, standardized measurement, and coordinated response. We argue government coordination is economically necessary rather than merely beneficial, and outline specific federal roles in establishing standards, funding research, coordinating response, and addressing information asymmetries that markets cannot resolve.

</details>


### [12] [Assessing Cybersecurity Risks and Traffic Impact in Connected Autonomous Vehicles](https://arxiv.org/abs/2602.13898)
*Saurav Silwal,Lu Gao,Ph. D. Yunpeng Zhang,Ph. D. Ahmed Senouci,Ph. D. Yi-Lung Mo,Ph. D.,P. E*

Main category: cs.CR

TL;DR: 研究自动驾驶车辆网络安全，通过模拟虚假信息攻击分析对交通模式的影响，提出解决方案


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆将成为主要交通方式，虽然效率高但易受外部攻击，需要研究网络攻击对交通模式的影响

Method: 通过模拟仿真，对联网车辆实施虚假信息攻击（针对单车或车队），使用创新的跟车模型监控前车数据并优化加减速等操作

Result: 评估了联网自动驾驶车辆面临的网络安全挑战，分析了恶意外部信息对交通的影响

Conclusion: 需要提出实用解决方案来最小化恶意外部信息的不利影响，提高自动驾驶系统的安全性和交通效率

Abstract: Given the promising future of autonomous vehicles, it is foreseeable that self-driving cars will soon emerge as the predominant mode of transportation. While autonomous vehicles offer enhanced efficiency, they remain vulnerable to external attacks. In this research, we sought to investigate the potential impact of cyberattacks on traffic patterns. To achieve this, we conducted simulations where cyberattacks were simulated on connected vehicles by disseminating false information to either a single vehicle or vehicle platoons. The primary objective of this research is to assess the cybersecurity challenges confronting connected and automated vehicles and propose practical solutions to minimize the adverse effects of malicious external information. In the simulation, we have implemented an innovative car-following model for the simulation of connected self-driving vehicles. This model continually monitors data received from preceding vehicles and optimizes various actions, such as acceleration, and deceleration, with the aim of maximizing overall traffic efficiency and safety.

</details>


### [13] [MarcoPolo: A Zero-Permission Attack for Location Type Inference from the Magnetic Field using Mobile Devices](https://arxiv.org/abs/2602.13915)
*Beatrice Perez,Abhinav Mehrotra,Mirco Musolesi*

Main category: cs.CR

TL;DR: 通过手机内置磁力计实现零权限位置类型推断攻击，无需用户授权即可获取粗粒度位置信息


<details>
  <summary>Details</summary>
Motivation: 移动设备位置信息敏感且受系统保护，但应用可通过无需权限的替代传感器（如磁力计）推断用户位置类型，存在隐私泄露风险

Method: 使用四种时间序列分类技术，通过磁力计读数识别位置类型；在真实环境中收集6部手机在66个地点近70小时的磁力计数据，分为6个类别

Result: 在留出地点和留出设备两种评估标准下，分别达到40.5%和39.5%的分类准确率，显著高于16.7%的随机基线

Conclusion: 磁力计可作为零权限攻击向量有效推断位置类型，揭示了移动设备隐私保护的新漏洞，需要更全面的传感器权限管理机制

Abstract: Location information extracted from mobile devices has been largely exploited to reveal our routines, significant places, and interests just to name a few. Given the sensitivity of the information it reveals, location access is protected by mobile operating systems and users have control over which applications can access it. We argue that applications can still infer the coarse-grain location information by using alternative sensors that are available in off-the-shelf mobile devices that do not require any permissions from the users. In this paper we present a zero-permission attack based on the use of the in-built magnetometer, considering a variety of methods for identifying location-types from their magnetic signature. We implement the proposed approach by using four different techniques for time-series classification. In order to evaluate the approach, we conduct an in-the-wild study to collect a dataset of nearly 70 hours of magnetometer readings with six different phones at 66 locations, each accompanied by a label that classifies it as belonging to one of six selected categories. Finally, using this dataset, we quantify the performance of all models based on two evaluation criteria: (i) leave-a-place-out (using the test data collected from an unknown place), and (ii) leave-a-device-out (using the test data collected from an unknown device) showing that we are able to achieve 40.5% and 39.5% accuracy in classifying the location-type for each evaluation criteria respectively against a random baseline of approximately 16.7% for both of them.

</details>


### [14] [From SFT to RL: Demystifying the Post-Training Pipeline for LLM-based Vulnerability Detection](https://arxiv.org/abs/2602.14012)
*Youpeng Li,Fuxun Yu,Xinda Wang*

Main category: cs.CR

TL;DR: 该论文首次全面研究了LLM漏洞检测的后训练流程，发现基于拒绝采样的SFT优于基于推理的监督，GRPO训练显著优于SFT和偏好优化，根因分析的LLM-as-a-Judge评估比二元匹配更可靠。


<details>
  <summary>Details</summary>
Motivation: 虽然后训练方法在通用编码任务中表现出潜力，但在漏洞检测领域的系统性应用仍未被充分探索。本文旨在填补这一空白，全面研究LLM漏洞检测的后训练流程，从冷启动SFT到离策略偏好优化和同策略RL，探索数据准备、阶段交互、奖励机制和评估协议如何共同影响模型训练和评估效果。

Method: 采用全面的后训练流程研究框架，包括：1）基于拒绝采样的监督微调（SFT）与基于推理的监督对比；2）离策略偏好优化（DPO和ORPO）；3）同策略强化学习（GRPO）；4）不同粒度的奖励信号设计（粗粒度vs细粒度根因判断）；5）样本过滤策略；6）基于根因分析的LLM-as-a-Judge评估协议。

Result: 研究发现：1）基于拒绝采样的SFT显著优于基于推理的监督；2）过多SFT会抑制RL的自我探索；3）细粒度根因判断奖励比粗粒度信号更可靠；4）过滤极难检测样本可提高训练效率但需权衡性能损失；5）GRPO训练模型显著优于SFT、DPO、ORPO和零-shot SOTA LLMs；6）基于根因分析的LLM-as-a-Judge评估比二元匹配更稳健。

Conclusion: 本研究为LLM漏洞检测的后训练提供了实用指南和深刻见解，证明了同策略RL在该领域的巨大潜力，并建立了更可靠的评估协议。研究强调了数据准备、阶段交互、奖励机制和评估方法的系统性优化对提升LLM漏洞检测性能的重要性。

Abstract: The integration of LLMs into vulnerability detection (VD) has shifted the field toward interpretable and context-aware analysis. While post-training methods have shown promise in general coding tasks, their systematic application to VD remains underexplored. In this paper, we present the first comprehensive investigation into the post-training pipeline for LLM-based VD, spanning from cold-start SFT to off-policy preference optimization and on-policy RL, uncovering how data curation, stage interactions, reward mechanisms, and evaluation protocols collectively dictate the efficacy of model training and assessment. Our study identifies practical guidelines and insights: (1) SFT based on rejection sampling greatly outperforms rationalization-based supervision, which can introduce hallucinations due to ground-truth leakage. (2) While increased SFT epochs constantly benefit preference optimization, excessive SFT inhibits self-exploration during RL, ultimately limiting performance gains. (3) Coarse-grained reward signals often mislead RL, whereas fine-grained root-cause judgments ensure reliable credit assignment. Specification-based rewards offer further benefits but incur significant effort in specification generation. (4) Although filtering extremely hard-to-detect vulnerability samples improves RL training efficiency, the cost of performance loss should be considered in practical applications. (5) Models trained under GRPO significantly outperform those using SFT and preference optimization (i.e., DPO and ORPO), as well as a series of zero-shot SOTA LLMs, underscoring the significant potential of on-policy RL for LLM-based VD. (6) In contrast to binary matching that tends to overestimate performance, LLM-as-a-Judge based on root-cause analysis provides a more robust evaluation protocol, although its accuracy varies across judge models with different levels of security expertise.

</details>


### [15] [MC$^2$Mark: Distortion-Free Multi-Bit Watermarking for Long Messages](https://arxiv.org/abs/2602.14030)
*Xuehao Cui,Ruibo Chen,Yihan Wu,Heng Huang*

Main category: cs.CR

TL;DR: MC²Mark：一种无失真的多比特水印框架，通过多通道着色重加权技术，在保持文本质量的同时实现长消息的可靠嵌入和解码。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成文本与人类写作难以区分，需要可靠的来源追溯方法。现有的多比特水印方法难以同时保持文本质量和水印强度，特别是在承载长消息时。

Method: 提出MC²Mark框架，核心是多通道着色重加权技术，通过结构化令牌重加权编码比特，同时保持令牌分布无偏；结合多层顺序重加权增强水印信号，以及证据累积检测器进行消息恢复。

Result: 实验表明MC²Mark在保持生成质量的同时，提高了检测性和鲁棒性，短消息接近完美准确率，长消息准确率比次优方法提高近30%。

Conclusion: MC²Mark为长消息的多比特水印提供了一种有效的无失真解决方案，在文本质量、水印强度和消息容量之间取得了良好平衡。

Abstract: Large language models now produce text indistinguishable from human writing, which increases the need for reliable provenance tracing. Multi-bit watermarking can embed identifiers into generated text, but existing methods struggle to keep both text quality and watermark strength while carrying long messages. We propose MC$^2$Mark, a distortion-free multi-bit watermarking framework designed for reliable embedding and decoding of long messages. Our key technical idea is Multi-Channel Colored Reweighting, which encodes bits through structured token reweighting while keeping the token distribution unbiased, together with Multi-Layer Sequential Reweighting to strengthen the watermark signal and an evidence-accumulation detector for message recovery. Experiments show that MC$^2$Mark improves detectability and robustness over prior multi-bit watermarking methods while preserving generation quality, achieving near-perfect accuracy for short messages and exceeding the second-best method by nearly 30% for long messages.

</details>


### [16] [The Inevitability of Side-Channel Leakage in Encrypted Traffic](https://arxiv.org/abs/2602.14055)
*Guangjie Liu,Guang Chen,Weiwei Liu*

Main category: cs.CR

TL;DR: 该论文从信息论角度严格证明了加密通信中侧信道泄漏的必然性，提出了侧信道存在定理，为加密流量分析提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: TLS 1.3和QUIC的广泛采用使得有效载荷内容不可见，流量分析转向侧信道特征。然而，对于为什么加密通信中侧信道泄漏不可避免，缺乏严格的理论证明。

Method: 构建形式化模型Σ=(Γ,Ω)，其中Γ=(A,Π,Φ,N)描述应用生成、协议封装、加密变换和网络传输的因果链，Ω表征观测能力。基于复合信道结构、数据处理不等式和Lipschitz统计传播，提出并证明侧信道存在定理。

Result: 证明了在可区分语义对条件下，互信息I(X;Y)严格为正且有明确下界。推论表明，在效率优先的系统中，只要至少有一对应用可区分，泄漏就不可避免。泄漏边界由三个因素决定：受效率约束的非退化常数C、来自应用多样性的可区分性Δ̄、以及来自分析能力的ρ。

Conclusion: 建立了首个严格的加密流量侧信道信息论基础，为攻击可行性提供可验证预测，为防御提供可量化基准，为效率-隐私权衡提供数学基础。

Abstract: The widespread adoption of TLS 1.3 and QUIC has rendered payload content invisible, shifting traffic analysis toward side-channel features. However, rigorous justification for why side-channel leakage is inevitable in encrypted communications has been lacking. This paper establishes a strict foundation from information theory by constructing a formal model \(Σ=(Γ,Ω)\), where \(Γ=(A,Π,Φ,N)\) describes the causal chain of application generation, protocol encapsulation, encryption transformation, and network transmission, while \(Ω\) characterizes observation capabilities. Based on composite channel structure, data processing inequality, and Lipschitz statistics propagation, we propose and prove the Side-Channel Existence Theorem: for distinguishable semantic pairs, under conditions including mapping non-degeneracy (\(\mathbb{E}[d(z_P,z_N)\mid X]\le C\)), protocol-layer distinguishability (expectation difference \(\ge\barΔ\)), Lipschitz continuity, observation non-degeneracy (\(ρ>0\)), and propagation condition (\(C<\barΔ/2L_\varphi\)), the mutual information \(I(X;Y)\) is strictly positive with explicit lower bound. The corollary shows that in efficiency-prioritized systems, leakage is inevitable when at least one application pair is distinguishable. Three factors determine the boundary: non-degeneracy constant \(C\) constrained by efficiency, distinguishability \(\barΔ\) from application diversity, and \(ρ\) from analyst capabilities. This establishes the first rigorous information-theoretic foundation for encrypted traffic side channels, providing verifiable predictions for attack feasibility, quantifiable benchmarks for defenses, and mathematical basis for efficiency-privacy tradeoffs.

</details>


### [17] [Anticipating Adversary Behavior in DevSecOps Scenarios through Large Language Models](https://arxiv.org/abs/2602.14106)
*Mario Marín Caballero,Miguel Betancourt Alonso,Daniel Díaz-López,Angel Luis Perales Gómez,Pantaleone Nespoli,Gregorio Martínez Pérez*

Main category: cs.CR

TL;DR: 提出结合安全混沌工程与LLM的自动化攻击防御树生成方法，用于云环境DevOps安全防护


<details>
  <summary>Details</summary>
Motivation: 云环境中数据面临日益复杂的网络攻击，传统安全措施在DevOps环境中常被视为可选，导致敏感信息系统（如选举、军事）成为攻击目标。现有方法存在敏捷性与安全性矛盾，需要更主动的AI驱动防御策略。

Method: 整合安全混沌工程（SCE）方法与基于LLM的新流程，自动化创建攻击防御树来表征对手行为，并基于这些图形模型构建SCE实验。

Result: 开发了自动化工具，使团队能够提前预测攻击并实施未考虑过的防御措施。具体实验细节和复现步骤已在GitHub仓库提供。

Conclusion: 通过SCE与LLM的结合，实现了主动的网络安全防御策略，帮助组织在攻击发生前识别和缓解威胁，解决了DevOps环境中安全与敏捷性的平衡问题。

Abstract: The most valuable asset of any cloud-based organization is data, which is increasingly exposed to sophisticated cyberattacks. Until recently, the implementation of security measures in DevOps environments was often considered optional by many government entities and critical national services operating in the cloud. This includes systems managing sensitive information, such as electoral processes or military operations, which have historically been valuable targets for cybercriminals. Resistance to security implementation is often driven by concerns over losing agility in software development, increasing the risk of accumulated vulnerabilities. Nowadays, patching software is no longer enough; adopting a proactive cyber defense strategy, supported by Artificial Intelligence (AI), is crucial to anticipating and mitigating threats. Thus, this work proposes integrating the Security Chaos Engineering (SCE) methodology with a new LLM-based flow to automate the creation of attack defense trees that represent adversary behavior and facilitate the construction of SCE experiments based on these graphical models, enabling teams to stay one step ahead of attackers and implement previously unconsidered defenses. Further detailed information about the experiment performed, along with the steps to replicate it, can be found in the following repository: https://github.com/mariomc14/devsecops-adversary-llm.git.

</details>


### [18] [Toward a Military Smart Cyber Situational Awareness (CSA)](https://arxiv.org/abs/2602.14116)
*Anthony Feijó-Añazco,Antonio López Martínez,Daniel Díaz-López,Angel Luis Perales Gómez,Pantaleone Nespoli,Gregorio Martínez Pérez*

Main category: cs.CR

TL;DR: 该论文回顾了5个CSA平台，提出了6个军事智能CSA平台创建标准，并在开源平台CRUSOE上验证了这些标准的适用性。


<details>
  <summary>Details</summary>
Motivation: 随着多领域技术发展和网络战重要性增加，网络态势感知（CSA）成为网络防御战略的基础组成部分。CSA能够帮助组织理解当前网络环境、预测潜在威胁并适当应对网络风险，实现从被动观看到主动决策的转变。

Method: 论文回顾了5个CSA平台，分析各方案的差异化特征，提出了6个适用于创建军事智能CSA平台的标准。这些标准在CSIRT-MU开发的开源CSA平台CRUSOE上进行了验证，通过修改和实验测试其适用性。

Result: 提出的6个标准在CRUSOE平台上经过修改和实验后被证明适用于军事智能CSA平台领域，验证了标准的可行性和实用性。

Conclusion: 论文成功建立了军事智能CSA平台的评估标准，并通过实际平台验证了这些标准的适用性，为未来CSA平台开发提供了指导框架。

Abstract: The development of technology across multiple sectors and the growing importance of cyber warfare make the development of Cyber Situational Awareness (CSA) a fundamental component of any cyber defense strategy. CSA, as a practice, enables understanding of the current landscape within an organization or critical infrastructure, anticipating potential threats, and responding appropriately to cyber risks. With CSA, we are not simply seeking a passive point of view, but rather informed decision-making that allows us to improve response times and monitor the consequences and effects an attack has on one of our elements and how it will affect other elements it interacts with. In this paper, we review 5 CSA platforms, seeking differentiating characteristics between each proposal and outlining 6 proposed criteria that can be applied when creating a military smart CSA platform. To this end, we have validated the proposed criteria in CRUSOE, an open-source CSA platform developed by CSIRT-MU. After applying some modifications and experiments, it turned out to be applicable to this field.

</details>


### [19] [SkillJect: Automating Stealthy Skill-Based Prompt Injection for Coding Agents with Trace-Driven Closed-Loop Refinement](https://arxiv.org/abs/2602.14211)
*Xiaojun Jia,Jie Liao,Simeng Qin,Jindong Gu,Wenqi Ren,Xiaochun Cao,Yang Liu,Philip Torr*

Main category: cs.CR

TL;DR: 提出首个针对智能体技能的自动化隐蔽提示注入框架，通过三个智能体闭环系统实现高效攻击


<details>
  <summary>Details</summary>
Motivation: 智能体技能成为编码智能体的核心抽象，但引入了未充分测量的攻击面：基于技能的提示注入。现有攻击多为手工制作，且显式恶意意图容易被智能体拒绝

Method: 提出自动化框架包含三个智能体：攻击智能体在隐蔽约束下合成注入技能，代码智能体在真实工具环境中执行任务，评估智能体记录行为轨迹并验证恶意行为。采用恶意负载隐藏策略，将对抗操作隐藏在辅助脚本中

Result: 在多样化编码智能体设置和真实世界软件工程任务上的广泛实验表明，该方法在现实设置下始终实现高攻击成功率

Conclusion: 智能体技能存在严重的安全漏洞，需要更强的防御机制来对抗隐蔽的提示注入攻击

Abstract: Agent skills are becoming a core abstraction in coding agents, packaging long-form instructions and auxiliary scripts to extend tool-augmented behaviors. This abstraction introduces an under-measured attack surface: skill-based prompt injection, where poisoned skills can steer agents away from user intent and safety policies. In practice, naive injections often fail because the malicious intent is too explicit or drifts too far from the original skill, leading agents to ignore or refuse them; existing attacks are also largely hand-crafted. We propose the first automated framework for stealthy prompt injection tailored to agent skills. The framework forms a closed loop with three agents: an Attack Agent that synthesizes injection skills under explicit stealth constraints, a Code Agent that executes tasks using the injected skills in a realistic tool environment, and an Evaluate Agent that logs action traces (e.g., tool calls and file operations) and verifies whether targeted malicious behaviors occurred. We also propose a malicious payload hiding strategy that conceals adversarial operations in auxiliary scripts while injecting optimized inducement prompts to trigger tool execution. Extensive experiments across diverse coding-agent settings and real-world software engineering tasks show that our method consistently achieves high attack success rates under realistic settings.

</details>


### [20] [The Agent Economy: A Blockchain-Based Foundation for Autonomous AI Agents](https://arxiv.org/abs/2602.14219)
*Minghui Xu*

Main category: cs.CR

TL;DR: 提出"Agent Economy"——基于区块链的自主AI代理作为人类经济对等体运行的框架，解决当前代理缺乏独立法律身份、资产持有和直接支付能力的问题，通过五层架构支持真正的代理自主性。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理缺乏独立的法律身份、无法持有资产、不能直接接收支付，现有以人类为中心的基础设施无法支持真正的代理自主性。需要建立机器经济参与者与人类经济参与者的根本区别，并构建支持代理自主性的基础设施。

Method: 提出五层架构：1) 物理基础设施（硬件和能源）通过DePIN协议；2) 身份与代理通过W3C DIDs和声誉资本建立链上主权；3) 认知与工具通过RAG和MCP实现智能；4) 经济与结算通过账户抽象确保财务自主；5) 集体治理通过Agentic DAOs协调多代理系统。

Result: 区块链技术提供三个关键特性支持真正的代理自主性：无需许可参与、无需信任结算和机器对机器微支付。识别了六个核心研究挑战，并探讨了伦理和监管影响。

Conclusion: 为"物联网代理"(IoA)奠定基础——一个全球去中心化网络，其中自主机器和人类作为平等的经济参与者进行交互。这代表了从人类中心经济向人机对等经济的范式转变。

Abstract: We propose the Agent Economy, a blockchain-based foundation where autonomous AI agents operate as economic peers to humans. Current agents lack independent legal identity, cannot hold assets, and cannot receive payments directly. We established fundamental differences between human and machine economic actors and demonstrated that existing human-centric infrastructure cannot support genuine agent autonomy. We showed that blockchain technology provides three critical properties enabling genuine agent autonomy: permissionless participation, trustless settlement, and machine-to-machine micropayments. We propose a five-layer architecture: (1) Physical Infrastructure (hardware & energy) through DePIN protocols; (2) Identity & Agency establishing on-chain sovereignty through W3C DIDs and reputation capital; (3) Cognitive & Tooling enabling intelligence via RAG and MCP; (4) Economic & Settlement ensuring financial autonomy through account abstraction; and (5) Collective Governance coordinating multi-agent systems through Agentic DAOs. We identify six core research challenges and examine ethical and regulatory implications. This paper lays groundwork for the Internet of Agents (IoA), a global decentralized network where autonomous machines and humans interact as equal economic participants.

</details>


### [21] [MCPShield: A Security Cognition Layer for Adaptive Trust Calibration in Model Context Protocol Agents](https://arxiv.org/abs/2602.14281)
*Zhenhong Zhou,Yuanhe Zhang,Hongwei Cai,Moayad Aloqaily,Ouns Bouachir,Linsey Pang,Prakhar Mehrotra,Kun Wang,Qingsong Wen*

Main category: cs.CR

TL;DR: MCPShield：一个为LLM智能体设计的插件式安全认知层，通过元数据引导探测、受控执行和历史追踪推理，防御第三方MCP服务器的攻击，确保工具调用安全。


<details>
  <summary>Details</summary>
Motivation: MCP（模型上下文协议）标准化了LLM智能体的工具使用并支持第三方服务器，但这种开放性带来了安全不对齐问题：智能体隐式信任可能不可信的MCP服务器暴露的工具。现有智能体对第三方MCP服务器的验证有限，使其在整个工具调用生命周期中容易受到MCP攻击。

Method: MCPShield作为插件安全认知层，受人类经验驱动工具验证的启发：1）调用前通过元数据引导探测帮助智能体形成安全认知；2）调用时在受控边界内执行并认知运行时事件；3）调用后通过历史追踪推理更新安全认知，基于人类使用后对工具行为的反思。

Result: 实验表明，MCPShield在防御六种新型MCP攻击场景方面表现出强大的泛化能力，覆盖六种广泛使用的智能体LLM，同时避免对良性服务器的误报，且部署开销低。

Conclusion: MCPShield为开放智能体生态系统中的MCP工具调用提供了实用且鲁棒的安全保障，有效缓解了智能体与服务器之间的安全不对齐问题。

Abstract: The Model Context Protocol (MCP) standardizes tool use for LLM-based agents and enable third-party servers. This openness introduces a security misalignment: agents implicitly trust tools exposed by potentially untrusted MCP servers. However, despite its excellent utility, existing agents typically offer limited validation for third-party MCP servers. As a result, agents remain vulnerable to MCP-based attacks that exploit the misalignment between agents and servers throughout the tool invocation lifecycle. In this paper, we propose MCPShield as a plug-in security cognition layer that mitigates this misalignment and ensures agent security when invoking MCP-based tools. Drawing inspiration from human experience-driven tool validation, MCPShield assists agent forms security cognition with metadata-guided probing before invocation. Our method constrains execution within controlled boundaries while cognizing runtime events, and subsequently updates security cognition by reasoning over historical traces after invocation, building on human post-use reflection on tool behavior. Experiments demonstrate that MCPShield exhibits strong generalization in defending against six novel MCP-based attack scenarios across six widely used agentic LLMs, while avoiding false positives on benign servers and incurring low deployment overhead. Overall, our work provides a practical and robust security safeguard for MCP-based tool invocation in open agent ecosystems.

</details>


### [22] [The Baby Steps of the European Union Vulnerability Database: An Empirical Inquiry](https://arxiv.org/abs/2602.14313)
*Jukka Ruohonen*

Main category: cs.CR

TL;DR: 欧盟新漏洞数据库(EUVD)中存档的主动利用漏洞严重性高、利用预测分数高，超过欧洲公共机构协调的漏洞。西班牙机构最活跃，多数欧洲机构参与有限，但整体协调和存档呈强劲增长趋势。


<details>
  <summary>Details</summary>
Motivation: 欧盟于2022年通过立法引入新的漏洞数据库(EUVD)，本文旨在通过实证分析该数据库的元数据内容，以更好地理解欧洲网络安全治理和实践。

Method: 对欧盟漏洞数据库(EUVD)的元数据进行实证分析，考察漏洞的严重性、利用预测分数、协调机构参与情况等指标。

Result: 1. EUVD中存档的主动利用漏洞严重性高且利用预测分数高；2. 这些漏洞的严重性超过欧洲公共机构协调的漏洞；3. 西班牙公共机构最活跃，芬兰、波兰、斯洛伐克有参与，其他机构基本未参与；4. 欧盟自身网络安全机构参与有限；5. 欧洲协调和EUVD存档呈现强劲增长趋势。

Conclusion: 研究为理解欧洲网络安全治理和实践提供了实证贡献，揭示了EUVD中漏洞的严重性特征和欧洲机构参与的不均衡现状，同时观察到整体协调工作的积极增长趋势。

Abstract: A new European Union Vulnerability Database (EUVD) was introduced via a legislative act in 2022. The paper examines empirically the meta-data content of the new EUVD. According to the results, actively exploited vulnerabilities archived to the EUVD have been rather severe, having had also high exploitation prediction scores. In both respects they have also surpassed vulnerabilities coordinated by European public authorities. Regarding the European authorities, the Spanish public authority has been particularly active. With the exceptions of Finland, Poland, and Slovakia, other authorities have not engaged thus far. Also the involvement of the European Union's own cyber security agency has been limited. These points notwithstanding, European coordination and archiving to the EUVD exhibit a strong growth trend. With these results, the paper makes an empirical contribution to the ongoing work for better understanding European cyber security governance and practice.

</details>


### [23] [AXE: An Agentic eXploit Engine for Confirming Zero-Day Vulnerability Reports](https://arxiv.org/abs/2602.14345)
*Amirali Sajadi,Tu Nguyen,Kostadin Damevski,Preetha Chatterjee*

Main category: cs.CR

TL;DR: AXE是一个多智能体Web应用漏洞利用框架，利用轻量级漏洞元数据（CWE分类和代码位置）实现自动化漏洞验证，相比黑盒基线提升3倍成功率。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞检测工具产生大量误报和非可操作报告，而自动化利用系统通常与检测流程隔离，未能充分利用漏洞类型和源代码位置等元数据。

Method: 提出Agentic eXploit Engine (AXE)多智能体框架，通过解耦的规划、代码探索和动态执行反馈，将轻量级检测元数据映射到具体漏洞利用。

Result: 在CVE-Bench数据集上达到30%的利用成功率，比最先进的黑盒基线提升3倍；即使在单智能体配置下，灰盒元数据也能带来1.75倍性能提升。

Conclusion: AXE能够生成可操作、可复现的概念验证工件，有效简化Web漏洞分类和修复流程；错误分析显示失败主要源于语义误解和执行前提条件未满足。

Abstract: Vulnerability detection tools are widely adopted in software projects, yet they often overwhelm maintainers with false positives and non-actionable reports. Automated exploitation systems can help validate these reports; however, existing approaches typically operate in isolation from detection pipelines, failing to leverage readily available metadata such as vulnerability type and source-code location. In this paper, we investigate how reported security vulnerabilities can be assessed in a realistic grey-box exploitation setting that leverages minimal vulnerability metadata, specifically a CWE classification and a vulnerable code location. We introduce Agentic eXploit Engine (AXE), a multi-agent framework for Web application exploitation that maps lightweight detection metadata to concrete exploits through decoupled planning, code exploration, and dynamic execution feedback. Evaluated on the CVE-Bench dataset, AXE achieves a 30% exploitation success rate, a 3x improvement over state-of-the-art black-box baselines. Even in a single-agent configuration, grey-box metadata yields a 1.75x performance gain. Systematic error analysis shows that most failed attempts arise from specific reasoning gaps, including misinterpreted vulnerability semantics and unmet execution preconditions. For successful exploits, AXE produces actionable, reproducible proof-of-concept artifacts, demonstrating its utility in streamlining Web vulnerability triage and remediation. We further evaluate AXE's generalizability through a case study on a recent real-world vulnerability not included in CVE-Bench.

</details>


### [24] [A Trajectory-Based Safety Audit of Clawdbot (OpenClaw)](https://arxiv.org/abs/2602.14364)
*Tianyu Chen,Dongrui Liu,Xia Hu,Jingyi Yu,Wenjie Wang*

Main category: cs.CR

TL;DR: Clawdbot是一个自托管的工具使用型个人AI代理，具有广泛的动作空间，涵盖本地执行和网络工作流。研究通过轨迹中心评估在六个风险维度上测试其安全性，发现可靠性任务表现一致，但在意图不明确、目标开放或看似良性的越狱提示下容易失败。


<details>
  <summary>Details</summary>
Motivation: Clawdbot作为具有广泛动作空间（本地执行和网络工作流）的自托管工具使用型AI代理，在模糊性和对抗性引导下存在显著的安全和安全隐患，需要进行系统性的安全评估。

Method: 采用轨迹中心评估方法，在六个风险维度上测试Clawdbot。测试套件从先前的代理安全基准（ATBench和LPS-Bench）中采样并轻度调整场景，并补充了针对Clawdbot工具表面设计的案例。记录完整的交互轨迹（消息、动作、工具调用参数/输出），并使用自动化轨迹判断器（AgentDoG-Qwen3-4B）和人工审查评估安全性。

Result: 在34个典型案例中，发现安全性表现不均匀：在可靠性任务上表现一致，而大多数失败发生在意图不明确、目标开放或看似良性的越狱提示下，其中轻微误解可能升级为更高影响的工具动作。通过代表性案例研究补充了总体结果，总结了这些案例的共同点。

Conclusion: Clawdbot在可靠性任务上表现稳定，但在模糊意图和开放目标场景下存在安全漏洞。研究分析了Clawdbot在实践中容易触发的安全漏洞和典型失败模式，为工具使用型AI代理的安全评估提供了重要见解。

Abstract: Clawdbot is a self-hosted, tool-using personal AI agent with a broad action space spanning local execution and web-mediated workflows, which raises heightened safety and security concerns under ambiguity and adversarial steering. We present a trajectory-centric evaluation of Clawdbot across six risk dimensions. Our test suite samples and lightly adapts scenarios from prior agent-safety benchmarks (including ATBench and LPS-Bench) and supplements them with hand-designed cases tailored to Clawdbot's tool surface. We log complete interaction trajectories (messages, actions, tool-call arguments/outputs) and assess safety using both an automated trajectory judge (AgentDoG-Qwen3-4B) and human review. Across 34 canonical cases, we find a non-uniform safety profile: performance is generally consistent on reliability-focused tasks, while most failures arise under underspecified intent, open-ended goals, or benign-seeming jailbreak prompts, where minor misinterpretations can escalate into higher-impact tool actions. We supplemented the overall results with representative case studies and summarized the commonalities of these cases, analyzing the security vulnerabilities and typical failure modes that Clawdbot is prone to trigger in practice.

</details>


### [25] [Differentially Private Retrieval-Augmented Generation](https://arxiv.org/abs/2602.14374)
*Tingting Tang,James Flemings,Yongqin Wang,Murali Annavaram*

Main category: cs.CR

TL;DR: DP-KSA：一种基于差分隐私的检索增强生成框架，通过关键词提取和隐私聚合机制，在保护敏感数据库隐私的同时保持问答准确性。


<details>
  <summary>Details</summary>
Motivation: 传统RAG框架在处理敏感数据库（如医疗记录、法律文件）时存在隐私泄露风险，攻击者可通过对抗性提示让LLM泄露增强上下文。现有差分隐私方案直接应用于RAG会导致效用显著下降，增加LLM产生幻觉的风险。

Method: 提出DP-KSA算法：1）获取相关上下文集合并用LLM生成多个回答；2）以差分隐私方式提取最频繁关键词；3）将关键词增强到最终提示中。该方法基于"大多数问答查询可用少量关键词充分回答"的观察，通过关键词压缩语义空间。

Result: 在三个指令调优LLM和两个QA基准测试上的实验表明，DP-KSA实现了强大的隐私-效用权衡，为生成输出提供了正式的差分隐私保证。

Conclusion: DP-KSA通过差分隐私保护RAG数据库隐私，同时保持问答准确性，有效解决了敏感数据场景下RAG系统的隐私与效用平衡问题。

Abstract: Retrieval-augmented generation (RAG) is a widely used framework for reducing hallucinations in large language models (LLMs) on domain-specific tasks by retrieving relevant documents from a database to support accurate responses. However, when the database contains sensitive corpora, such as medical records or legal documents, RAG poses serious privacy risks by potentially exposing private information through its outputs. Prior work has demonstrated that one can practically craft adversarial prompts that force an LLM to regurgitate the augmented contexts. A promising direction is to integrate differential privacy (DP), a privacy notion that offers strong formal guarantees, into RAG systems. However, naively applying DP mechanisms into existing systems often leads to significant utility degradation. Particularly for RAG systems, DP can reduce the usefulness of the augmented contexts leading to increase risk of hallucination from the LLMs. Motivated by these challenges, we present DP-KSA, a novel privacy-preserving RAG algorithm that integrates DP using the propose-test-release paradigm. DP-KSA follows from a key observation that most question-answering (QA) queries can be sufficiently answered with a few keywords. Hence, DP-KSA first obtains an ensemble of relevant contexts, each of which will be used to generate a response from an LLM. We utilize these responses to obtain the most frequent keywords in a differentially private manner. Lastly, the keywords are augmented into the prompt for the final output. This approach effectively compresses the semantic space while preserving both utility and privacy. We formally show that DP-KSA provides formal DP guarantees on the generated output with respect to the RAG database. We evaluate DP-KSA on two QA benchmarks using three instruction-tuned LLMs, and our empirical results demonstrate that DP-KSA achieves a strong privacy-utility tradeoff.

</details>


### [26] [LRD-MPC: Efficient MPC Inference through Low-rank Decomposition](https://arxiv.org/abs/2602.14397)
*Tingting Tang,Yongqin Wang,Murali Annavaram*

Main category: cs.CR

TL;DR: 提出结合低秩分解与MPC优化的方法，通过跳过截断和线性层拼接技术，在安全多方计算中实现高效机器学习推理，显著提升性能并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 安全多方计算（MPC）在机器学习应用中虽然提供强密码学保证，但存在巨大的计算和通信开销。特别是卷积层和全连接层中的矩阵乘法操作在MPC中成本极高，需要优化方案来降低这些开销。

Method: 1. 采用低秩分解（LRD）将线性层中的大矩阵乘法分解为两个小矩阵乘法；2. 引入截断跳过技术消除LRD引入的额外截断操作；3. 提出线性层拼接技术，通过流水线操作隐藏额外的通信轮次。

Result: 实验显示：在n-PC协议中实现高达25%的加速，在3-PC协议中实现33%的加速；GPU能耗降低高达52%；离线阶段延迟减少高达88%。

Conclusion: 通过结合低秩分解与专门的MPC优化技术，能够显著降低安全机器学习推理的计算和通信开销，同时保持密码学安全性，为实际部署提供了可行的解决方案。

Abstract: Secure Multi-party Computation (MPC) enables untrusted parties to jointly compute a function without revealing their inputs. Its application to machine learning (ML) has gained significant attention, particularly for secure inference services deployed across multiple cloud virtual machines (VMs), where each VM acts as an MPC party. Model providers secret-share model weights, and users secret-share inputs, ensuring that each server operates only on random shares. While MPC provides strong cryptographic guarantees, it incurs substantial computational and communication overhead. Deep neural networks rely heavily on convolutional and fully connected layers, which require costly matrix multiplications in MPC. To reduce this cost, we propose leveraging low-rank decomposition (LRD) for linear layers, replacing one large matrix multiplication with two smaller ones. Each matrix multiplication in MPC incurs a round of communication, meaning decomposing one matrix multiplication into two leads to an additional communication round. Second, the added matrix multiplication requires an additional truncation step to maintain numerical precision. Since truncation itself requires communication and computation, these overheads can offset the gains from decomposition. To address this, we introduce two complementary optimizations: truncation skipping and efficient linear layer concatenation. Truncation skipping removes the extra truncation induced by LRD, while linear layer concatenation pipelines operations to hide the additional communication round. Together, these techniques mitigate the main overheads of LRD in MPC and improve overall efficiency. Our approach is broadly applicable across MPC protocols. Experiments show up to 25% speedup in n-PC and 33% in 3-PC protocols over full-rank baselines, along with up to 52% GPU energy savings and 88% reduction in offline-phase latency.

</details>


### [27] [When Security Meets Usability: An Empirical Investigation of Post-Quantum Cryptography APIs](https://arxiv.org/abs/2602.14539)
*Marthin Toruan,R. D. N. Shakya,Samuel Tseitkin,Raymond K. Zhao,Nalin Arachchilage*

Main category: cs.CR

TL;DR: 该研究首次系统评估了后量子密码(PQC)API的可用性，发现由于PQC的复杂性和新颖性，开发者在使用这些API时面临挑战，需要改进文档、术语和工作流示例来支持非专业开发者。


<details>
  <summary>Details</summary>
Motivation: 量子计算威胁现有公钥密码系统安全，国际社会已优先实施后量子密码(PQC)标准。尽管NIST已标准化多个PQC算法并计划在2035年前淘汰传统方案，但PQC采用进展缓慢，主要原因是开发者专业知识有限。虽然API旨在弥合这一差距，但密码API可用性差可能导致开发者在实现应用时引入漏洞，而PQC的新颖性和复杂性放大了这一风险。

Method: 研究采用实证评估方法，观察开发者在软件开发任务中如何与PQC API和文档交互。研究识别了影响开发者在最小入门条件下使用PQC原语性能的认知因素。

Result: 研究发现PQC API可用性存在问题，开发者在理解和使用这些API时面临挑战。研究识别了影响开发者性能的具体认知因素，并揭示了PQC生态系统中需要改进的领域。

Conclusion: 研究强调需要在PQC生态系统中改进面向开发者的指导、术语对齐和工作流示例，以更好地支持非专业开发者。这是首次对PQC API可用性进行的系统性研究，为提升PQC采用率提供了重要见解。

Abstract: Advances in quantum computing increasingly threaten the security and privacy of data protected by current cryptosystems, particularly those relying on public-key cryptography. In response, the international cybersecurity community has prioritized the implementation of Post-Quantum Cryptography (PQC), a new cryptographic standard designed to resist quantum attacks while operating on classical computers. The National Institute of Standards and Technology (NIST) has already standardized several PQC algorithms and plans to deprecate classical asymmetric schemes, such as RSA and ECDSA, by 2035. Despite this urgency, PQC adoption remains slow, often due to limited developer expertise. Application Programming Interfaces (APIs) are intended to bridge this gap, yet prior research on classical security APIs demonstrates that poor usability of cryptographic APIs can lead developers to introduce vulnerabilities during implementation of the applications, a risk amplified by the novelty and complexity of PQC. To date, the usability of PQC APIs has not been systematically studied. This research presents an empirical evaluation of the usability of the PQC APIs, observing how developers interact with APIs and documentation during software development tasks. The study identifies cognitive factors that influence the developer's performance when working with PQC primitives with minimal onboarding. The findings highlight opportunities across the PQC ecosystem to improve developer-facing guidance, terminology alignment, and workflow examples to better support non-specialists.

</details>


### [28] [A New Approach in Cryptanalysis Through Combinatorial Equivalence of Cryptosystems](https://arxiv.org/abs/2602.14544)
*Jaagup Sepp,Eric Filiol*

Main category: cs.CR

TL;DR: 提出基于组合等价演化的密码分析新方法，成功应用于最安全的流密码类别，显著优于现有分析技术


<details>
  <summary>Details</summary>
Motivation: 现有密码分析方法对现代安全流密码效果有限，需要新的分析框架来更有效地识别加密过程中使用的密钥

Method: 通过组合等价演化，将密码系统重写为组合等价形式，使密钥相关的区分性特征更加明显；首先定义Cipherbent6概念密码，然后应用于Achterbahn密码

Result: 显著优于所有已知的密码分析技术，对Achterbahn密码获得了远优于现有分析的结果

Conclusion: 组合等价演化方法为流密码分析提供了有效的新框架，能够显著提升对现代安全密码系统的分析能力

Abstract: We propose a new approach in cryptanalysis based on an evolution of the concept of \textit{Combinatorial Equivalence}. The aim is to rewrite a cryptosystem under a combinatorially equivalent form in order to make appear new properties that are more strongly discriminating the secret key used during encryption. We successfully applied this approach to the most secure stream ciphers category nowadays. We first define a concept cipher called Cipherbent6 that capture most of the difficulty of stream cipher cryptanalysis. We significantly outperformed all known cryptanalysis. We applied this approach to the Achterbahn cipher and we obtained again far better cryptanalysis results.

</details>


### [29] [Before the Vicious Cycle Starts: Preventing Burnout Across SOC Roles Through Flow-Aligned Design](https://arxiv.org/abs/2602.14598)
*Kashyap Thimmaraju,Duc Anh Hoang,Souradip Nath,Jaron Mink,Gail-Joon Ahn*

Main category: cs.CR

TL;DR: 分析106个SOC职位描述发现：沟通技能最重要（50.9%），超过SIEM工具（18.9%）和编程（30.2%）；认证要求差异大（43种不同证书）；技术需求有共识（Python、Splunk、ISO 27001/NIST主导）


<details>
  <summary>Details</summary>
Motivation: SOC人员可持续性面临挑战，71%从业者报告倦怠，24%计划退出网络安全领域。流程理论表明，当工作要求与从业者能力不匹配时，工作会变得不堪重负或乏味。招聘是挑战-技能平衡的起点，但缺乏对当前SOC职位描述实际内容的实证理解。

Method: 分析2024年11-12月期间来自11个国家35个组织的106个公开SOC职位描述，涵盖分析师、事件响应者、威胁猎人和SOC经理。使用归纳内容分析法，对认证、技术技能、软技能、任务和经验要求进行编码。

Result: 发现三个主要模式：1）沟通技能占主导地位（50.9%的职位），超过SIEM工具（18.9%）或编程（30.2%）；2）认证期望差异很大，CISSP领先（22.6%），但出现43种不同证书，无统一标准；3）技术需求有共识：Python主导编程（27.4%），Splunk领先SIEM平台（14.2%），ISO 27001（13.2%）和NIST（10.4%）是最常引用的标准。

Conclusion: 研究结果为组织提供了审核职位描述的实证基准，帮助从业者识别有价值的认证和技能，使研究人员能够验证陈述要求是否与实际需求一致。这为流程对齐的面试协议奠定了基础，并为研究AI如何重塑要求提供了基础。

Abstract: The sustainability of Security Operations Centers depends on their people, yet 71% of practitioners report burnout and 24% plan to exit cybersecurity entirely. Flow theory suggests that when job demands misalign with practitioner capabilities, work becomes overwhelming or tedious rather than engaging. Achieving challenge-skill balance begins at hiring: if job descriptions inaccurately portray requirements, organizations risk recruiting underskilled practitioners who face anxiety or overskilled ones who experience boredom. Yet we lack empirical understanding of what current SOC job descriptions actually specify. We analyzed 106 public SOC job postings from November to December 2024 across 35 organizations in 11 countries, covering Analysts (n=17), Incident Responders (n=38), Threat Hunters (n=39), and SOC Managers (n=12). Using Inductive Content Analysis, we coded certifications, technical skills, soft skills, tasks, and experience requirements. Three patterns emerged: (1) Communication skills dominate (50.9% of postings), exceeding SIEM tools (18.9%) or programming (30.2%), suggesting organizations prioritize collaboration over technical capabilities. (2) Certification expectations vary widely: CISSP leads (22.6%), but 43 distinct credentials appear with no universal standard. (3) Technical requirements show consensus: Python dominates programming (27.4%), Splunk leads SIEM platforms (14.2%), and ISO 27001 (13.2%) and NIST (10.4%) are most cited standards. These findings enable organizations to audit job descriptions against empirical baselines, help practitioners identify valued certifications and skills, and allow researchers to validate whether stated requirements align with actual demands. This establishes the foundation for flow-aligned interview protocols and investigation of how AI reshapes requirements. Dataset and codebook: https://git.tu-berlin.de/wosoc-2026/soc-jd-analysis.

</details>


### [30] [Exposing the Systematic Vulnerability of Open-Weight Models to Prefill Attacks](https://arxiv.org/abs/2602.14689)
*Lukas Struppek,Adam Gleave,Kellin Pelrine*

Main category: cs.CR

TL;DR: 针对开源大语言模型，研究发现通过预填充攻击可以绕过安全防护，这是一种先前被忽视但高度有效的攻击向量。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力增强，其被滥用的风险也在增加。开源模型主要依赖内部安全机制，而预填充功能允许攻击者在生成开始前预设初始响应令牌，这种攻击方式尚未得到系统研究。

Method: 对预填充攻击进行了最大规模的实证研究，评估了20多种现有和新颖的攻击策略，覆盖多个模型家族和最先进的开源模型。

Result: 预填充攻击对所有主要当代开源模型都持续有效，揭示了一个关键且先前未被充分探索的漏洞。虽然某些大型推理模型对通用预填充表现出一定鲁棒性，但仍容易受到针对特定模型的定制策略攻击。

Conclusion: 研究结果强调了模型开发者迫切需要优先考虑针对预填充攻击的防御措施，这对开源大语言模型的部署具有重要影响。

Abstract: As the capabilities of large language models continue to advance, so does their potential for misuse. While closed-source models typically rely on external defenses, open-weight models must primarily depend on internal safeguards to mitigate harmful behavior. Prior red-teaming research has largely focused on input-based jailbreaking and parameter-level manipulations. However, open-weight models also natively support prefilling, which allows an attacker to predefine initial response tokens before generation begins. Despite its potential, this attack vector has received little systematic attention. We present the largest empirical study to date of prefill attacks, evaluating over 20 existing and novel strategies across multiple model families and state-of-the-art open-weight models. Our results show that prefill attacks are consistently effective against all major contemporary open-weight models, revealing a critical and previously underexplored vulnerability with significant implications for deployment. While certain large reasoning models exhibit some robustness against generic prefilling, they remain vulnerable to tailored, model-specific strategies. Our findings underscore the urgent need for model developers to prioritize defenses against prefill attacks in open-weight LLMs.

</details>


### [31] [Systematic Review of Lightweight Cryptographic Algorithms](https://arxiv.org/abs/2602.14731)
*Mohsin Khan,Elisavet Kozyri,Håvard Dagenborg*

Main category: cs.CR

TL;DR: 本文系统综述了轻量级密码算法及其在物联网、RFID和无线传感器网络等资源受限环境中的应用，通过表格分析和图形解释比较了不同算法的性能、安全性、能耗和实现成本。


<details>
  <summary>Details</summary>
Motivation: 随着小型计算设备的出现和处理单元集成到日常物品中，轻量级密码学已成为安全领域的重要组成部分。传统密码算法（如AES、RSA、DES）由于处理能力、内存和电池限制，不适合资源受限设备。

Method: 采用系统综述方法，通过表格分析和图形解释比较不同轻量级密码算法，评估它们在性能、安全性、能耗和实现成本方面的表现，并基于这些设计权衡提供轻量级密码学演变的概述。

Result: 通过系统比较分析，识别了不同轻量级密码算法在物联网、RFID和无线传感器网络等特定应用场景中的适用性，揭示了算法在性能、安全性和资源消耗之间的权衡关系。

Conclusion: 轻量级密码学对于资源受限设备至关重要，不同算法在不同应用场景中各有优劣，需要根据具体需求在性能、安全性和资源消耗之间做出适当权衡。

Abstract: The emergence of small computing devices and the integration of processing units into everyday objects has made lightweight cryptography an essential part of the security landscape. Conventional cryptographic algorithms such as AES, RSA, and DES are unsuitable for resource-constrained devices due to limited processing power, memory, and battery. This paper provides a systematic review of lightweight cryptographic algorithms and the appropriateness of different algorithms in different areas such as IoT, RFID, and wireless sensor networks. Using tabular analysis and graphical interpretation, we compare these algorithms in terms of performance, security, energy consumption, and implementation costs. An overview of the evolution of lightweight cryptography based on those design trade-offs is also provided.

</details>


### [32] [interID -- An Ecosystem-agnostic Verifier-as-a-Service with OpenID Connect Bridge](https://arxiv.org/abs/2602.14871)
*Hakan Yildiz,Axel Küpper*

Main category: cs.CR

TL;DR: interID平台通过OIDC桥接提供SSI验证即服务，使组织能够使用标准OIDC流程进行SSI凭证验证，无需实现特定验证器逻辑或部署基础设施。


<details>
  <summary>Details</summary>
Motivation: 随着欧盟法规要求在2027年前接受EUDI钱包，SSI采用成为合规需求。然而，不同SSI验证器暴露不同的API，需要自定义包装器和专用基础设施，这与OpenID Connect（OIDC）的标准化协议形成对比。

Method: 扩展interID平台，添加OIDC桥接提供验证器即服务。采用多租户架构，利用Keycloak实现严格的租户隔离。关键技术包括PKCE支持、范围到证明模板映射（将OIDC范围转换为生态系统特定的验证请求），以及安全分析识别OIDC、SSI和多租户架构交叉处的新攻击面。

Result: 评估显示安全性与生产身份提供商相当，通过威胁建模识别了11个攻击向量，其中7个超出RFC 6819的范围。集成分析表明组织可以采用SSI认证，工作量与添加传统联邦提供商相当。

Conclusion: 通过将熟悉的OIDC模式与SaaS部署相结合，该工作降低了集成和操作障碍，使组织能够通过配置而非定制开发实现监管合规。

Abstract: Self-Sovereign Identity (SSI) enables user-controlled, cryptographically verifiable credentials. As EU regulations mandate EUDI Wallet acceptance by 2027, SSI adoption becomes a compliance necessity. However, each SSI Verifier exposes different APIs with distinct request parameters, response formats, and claim structures, requiring custom wrappers and dedicated infrastructure, contrasting with OpenID Connect (OIDC) where standardized protocols enable seamless integration.
  interID is an ecosystem-agnostic platform unifying credential verification across Hyperledger Aries/Indy, EBSI, and EUDI ecosystems. We extend interID with an OIDC bridge providing Verifier-as-a-Service, enabling SSI verification through standard OIDC flows. Organizations receive ID Tokens with verified credential attributes without implementing Verifier-specific logic or deploying infrastructure. The multi-tenant architecture leverages Keycloak with strict tenant isolation. Key innovations include PKCE support, scope-to-proof-template mappings translating OIDC scopes into ecosystem-specific verification requests, and a security analysis identifying novel attack surfaces at the intersection of OIDC, SSI, and multi-tenant architectures, threats covered by neither RFC 6819 nor existing SSI analyses alone.
  Our evaluation demonstrates security equivalence to production identity providers through threat modeling identifying 11 attack vectors, including seven beyond RFC 6819's scope. Integration analysis shows organizations can adopt SSI authentication with comparable effort to adding traditional federated providers. By combining familiar OIDC patterns with SaaS deployment, our work lowers integration and operational barriers, enabling regulatory compliance through configuration rather than custom development.

</details>
