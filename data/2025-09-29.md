<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 30]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Design and Implementation of a Secure RAG-Enhanced AI Chatbot for Smart Tourism Customer Service: Defending Against Prompt Injection Attacks -- A Case Study of Hsinchu, Taiwan](https://arxiv.org/abs/2509.21367)
*Yu-Kai Shih,You-Kai Kang*

Main category: cs.CR

TL;DR: 本文提出一个安全的检索增强生成(RAG)聊天机器人框架，用于新竹智慧旅游服务，通过多层防御机制对抗提示注入攻击，确保AI系统的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着智慧旅游发展，AI聊天机器人面临提示注入攻击的威胁，可能导致敏感信息泄露或生成有害内容，需要设计安全可靠的系统来保护用户数据和系统完整性。

Method: 集成RAG与API函数调用、多层语言分析(词汇、语义、语用层面)和防护机制，采用分层响应策略、RAG驱动的知识基础和意图分解，以及系统规范、意图判断门控和反向RAG文本等防御措施。

Result: 使用674个对抗性提示和223个良性查询进行评估，系统在良性任务上准确率超过95%，并能显著检测注入攻击。GPT-5变体能够阻挡约85%的攻击。

Conclusion: 该研究为智慧旅游中安全聊天机器人的部署提供了实用框架，强调了多层防御的必要性，并对可持续旅游、多语言可访问性和伦理AI部署做出了贡献。

Abstract: As smart tourism evolves, AI-powered chatbots have become indispensable for
delivering personalized, real-time assistance to travelers while promoting
sustainability and efficiency. However, these systems are increasingly
vulnerable to prompt injection attacks, where adversaries manipulate inputs to
elicit unintended behaviors such as leaking sensitive information or generating
harmful content. This paper presents a case study on the design and
implementation of a secure retrieval-augmented generation (RAG) chatbot for
Hsinchu smart tourism services. The system integrates RAG with API function
calls, multi-layered linguistic analysis, and guardrails against injections,
achieving high contextual awareness and security. Key features include a tiered
response strategy, RAG-driven knowledge grounding, and intent decomposition
across lexical, semantic, and pragmatic levels. Defense mechanisms include
system norms, gatekeepers for intent judgment, and reverse RAG text to
prioritize verified data. We also benchmark a GPT-5 variant (released
2025-08-07) to assess inherent robustness. Evaluations with 674 adversarial
prompts and 223 benign queries show over 95% accuracy on benign tasks and
substantial detection of injection attacks. GPT-5 blocked about 85% of attacks,
showing progress yet highlighting the need for layered defenses. Findings
emphasize contributions to sustainable tourism, multilingual accessibility, and
ethical AI deployment. This work offers a practical framework for deploying
secure chatbots in smart tourism and contributes to resilient, trustworthy AI
applications.

</details>


### [2] [Towards Adapting Federated & Quantum Machine Learning for Network Intrusion Detection: A Survey](https://arxiv.org/abs/2509.21389)
*Devashish Chaudhary,Sutharshan Rajasegarar,Shiva Raj Pokhrel*

Main category: cs.CR

TL;DR: 该调查探讨了联邦学习与网络入侵检测系统的集成，特别关注深度学习和量子机器学习方法，分析了FL架构、隐私保护技术，并开创性地探索了量子联邦学习在网络安全中的应用前景。


<details>
  <summary>Details</summary>
Motivation: 网络环境中敏感流量数据无法集中化处理，需要保护数据隐私的同时实现协作模型训练，联邦学习能够满足这一关键需求。

Method: 系统性地分析了联邦学习的完整架构谱系、部署策略、通信协议和聚合方法，特别针对入侵检测进行了定制化研究，并深入探讨了隐私保护技术、模型压缩方法和针对特定攻击的联邦解决方案。

Result: 通过经典方法和量子方法的严格比较分析，识别了研究空白，评估了实际部署情况，为工业采用和未来研究方向提供了具体路线图。

Conclusion: 本工作为研究人员和从业者提供了权威参考，帮助增强联邦入侵检测系统在日益复杂网络环境中的隐私性、效率和鲁棒性，同时为未来的量子增强网络安全格局做好准备。

Abstract: This survey explores the integration of Federated Learning (FL) with Network
Intrusion Detection Systems (NIDS), with particular emphasis on deep learning
and quantum machine learning approaches. FL enables collaborative model
training across distributed devices while preserving data privacy-a critical
requirement in network security contexts where sensitive traffic data cannot be
centralized. Our comprehensive analysis systematically examines the full
spectrum of FL architectures, deployment strategies, communication protocols,
and aggregation methods specifically tailored for intrusion detection. We
provide an in-depth investigation of privacy-preserving techniques, model
compression approaches, and attack-specific federated solutions for threats
including DDoS, MITM, and botnet attacks. The survey further delivers a
pioneering exploration of Quantum FL (QFL), discussing quantum feature
encoding, quantum machine learning algorithms, and quantum-specific aggregation
methods that promise exponential speedups for complex pattern recognition in
network traffic. Through rigorous comparative analysis of classical and quantum
approaches, identification of research gaps, and evaluation of real-world
deployments, we outline a concrete roadmap for industrial adoption and future
research directions. This work serves as an authoritative reference for
researchers and practitioners seeking to enhance privacy, efficiency, and
robustness of federated intrusion detection systems in increasingly complex
network environments, while preparing for the quantum-enhanced cybersecurity
landscape of tomorrow.

</details>


### [3] [Dynamic Dual-level Defense Routing for Continual Adversarial Training](https://arxiv.org/abs/2509.21392)
*Wenxuan Wang,Chenglei Wang,Xuelin Qian*

Main category: cs.CR

TL;DR: 提出DDeR框架，通过双层防御路由机制自主选择防御专家来应对持续对抗攻击，解决现有方法在持续学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有持续对抗训练方法存在灾难性遗忘问题，无法有效应对多样化和激进的对抗样本攻击。

Method: 使用双层防御路由：第一层包含多个防御专家和路由器，路由器动态选择和组合专家处理受攻击特征；第二层通过对抗哨兵网络指导路由器选择；采用伪任务替代训练策略促进路由器间通信。

Result: 大量实验表明DDeR在持续防御性能和分类准确率方面优于现有方法。

Conclusion: DDeR框架能有效适应不断演化的对抗攻击，解决持续对抗训练中的灾难性遗忘问题。

Abstract: As adversarial attacks continue to evolve, defense models face the risk of
recurrent vulnerabilities, underscoring the importance of continuous
adversarial training (CAT). Existing CAT approaches typically balance decision
boundaries by either data replay or optimization strategy to constrain shared
model parameters. However, due to the diverse and aggressive nature of
adversarial examples, these methods suffer from catastrophic forgetting of
previous defense knowledge after continual learning. In this paper, we propose
a novel framework, called Dual-level Defense Routing or DDeR, that can
autonomously select appropriate routers to integrate specific defense experts,
thereby adapting to evolving adversarial attacks. Concretely, the first-level
defense routing comprises multiple defense experts and routers, with each
router dynamically selecting and combining suitable experts to process attacked
features. Routers are independently incremented as continuous adversarial
training progresses, and their selections are guided by an Adversarial Sentinel
Network (ASN) in the second-level defense routing. To compensate for the
inability to test due to the independence of routers, we further present a
Pseudo-task Substitution Training (PST) strategy, which leverages
distributional discrepancy in data to facilitate inter-router communication
without storing historical data. Extensive experiments demonstrate that DDeR
achieves superior continuous defense performance and classification accuracy
compared to existing methods.

</details>


### [4] [SafeSteer: Adaptive Subspace Steering for Efficient Jailbreak Defense in Vision-Language Models](https://arxiv.org/abs/2509.21400)
*Xiyu Zeng,Siyuan Liang,Liming Lu,Haotian Zhu,Enguang Liu,Jisheng Dang,Yongbin Zhou,Shuchao Pang*

Main category: cs.CR

TL;DR: SafeSteer是一个轻量级的推理时转向框架，通过奇异值分解构建安全子空间，有效防御多种越狱攻击，同时保持模型性能和推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法存在两大局限：(1)难以在保证安全性的同时不损害模型实用性；(2)许多防御机制显著降低模型推理效率。

Method: 使用奇异值分解构建低维"安全子空间"，在推理时将原始转向向量投影到该子空间并重构，自适应移除有害生成信号，同时保持处理良性输入的能力。

Result: 实验表明，SafeSteer将攻击成功率降低60%以上，在正常任务上的准确率提高1-2%，且不引入显著推理延迟。

Conclusion: 通过简单高效的推理时控制可以实现鲁棒且实用的越狱防御。

Abstract: As the capabilities of Vision Language Models (VLMs) continue to improve,
they are increasingly targeted by jailbreak attacks. Existing defense methods
face two major limitations: (1) they struggle to ensure safety without
compromising the model's utility; and (2) many defense mechanisms significantly
reduce the model's inference efficiency. To address these challenges, we
propose SafeSteer, a lightweight, inference-time steering framework that
effectively defends against diverse jailbreak attacks without modifying model
weights. At the core of SafeSteer is the innovative use of Singular Value
Decomposition to construct a low-dimensional "safety subspace." By projecting
and reconstructing the raw steering vector into this subspace during inference,
SafeSteer adaptively removes harmful generation signals while preserving the
model's ability to handle benign inputs. The entire process is executed in a
single inference pass, introducing negligible overhead. Extensive experiments
show that SafeSteer reduces the attack success rate by over 60% and improves
accuracy on normal tasks by 1-2%, without introducing significant inference
latency. These results demonstrate that robust and practical jailbreak defense
can be achieved through simple, efficient inference-time control.

</details>


### [5] [Designing Ethereum's Geographical (De)Centralization Beyond the Atlantic](https://arxiv.org/abs/2509.21475)
*Sen Yang,Burak Öz,Fei Wu,Fan Zhang*

Main category: cs.CR

TL;DR: 论文分析了以太坊验证器地理分布问题，比较了两种区块构建范式对地理去中心化的影响。研究发现多源范式(MSP)比单源范式(SSP)导致更快的中心化，北美成为主要枢纽。


<details>
  <summary>Details</summary>
Motivation: 传统指标如权益分布忽略了去中心化的地理维度。验证器位置影响系统对区域冲击的韧性，但当前以太坊验证器在大西洋沿岸聚集，存在地理中心化问题。

Method: 开发了基于延迟校准的智能体模型，比较两种区块构建范式：单源范式(SSP，类似MEV-Boost)和多源范式(MSP)。通过模拟分析验证器地理分布动态。

Result: SSP围绕中继器位置缓慢集中，MSP中心化更快。多源聚合使边际价值位置依赖，放大收益差异并推动验证器向延迟最小区域迁移。北美成为主要枢纽。

Conclusion: 协议设计显著影响验证器地理分布，提供了促进地理去中心化的调控手段。一旦验证器已聚集，源位置对去中心化的影响有限。

Abstract: Decentralization has a geographic dimension that conventional metrics such as
stake distribution overlook. Where validators run affects resilience to
regional shocks (outages, disasters, government intervention) and fairness in
reward access. Yet in permissionless systems, locations cannot be mandated, but
they emerge from incentives. Today, Ethereum's validators cluster along the
Atlantic (EU and U.S. East Coast), where latency is structurally favorable.
This raises a key question: when some regions already enjoy latency advantages,
how does protocol design shape validator incentives and the geography of
(de)centralization? We develop a latency-calibrated agent-based model and
compare two Ethereum block-building paradigms: a Single-Source Paradigm (SSP),
akin to MEV-Boost, where proposers fetch full blocks from a relay that also
propagates them; and a Multi-Source Paradigm (MSP), where proposers aggregate
value from multiple sources and broadcast the block themselves. Simulations
show that SSP concentrates around relay placement but more slowly, since
proximity mainly affects propagation, and the marginal value of time is
relatively uniform across regions. MSP centralizes faster: aggregating across
sources makes marginal value location-dependent, amplifying payoff dispersion
and migration toward latency minima. Source placement and consensus settings
can dampen or intensify these effects, though once validators are already
clustered, the impact of source placement on decentralization is marginal. In
most cases, North America consistently emerges as the focal hub. These findings
show that protocol design materially shapes validator geography and offer
levers for promoting geographical decentralization.

</details>


### [6] [Functional Encryption in Secure Neural Network Training: Data Leakage and Practical Mitigations](https://arxiv.org/abs/2509.21497)
*Alexandru Ioniţă,Andreea Ioniţă*

Main category: cs.CR

TL;DR: 本文提出了一种针对使用功能加密进行安全训练的神经网络的攻击方法，通过线性规划重建原始输入，并提出了两种解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习即服务的发展，云端训练存在隐私泄露风险。虽然功能加密被用于安全训练，但现有方案存在漏洞。

Method: 使用线性规划攻击方法重建原始输入数据，并提出了两种防御方案：客户端参与计算的非加密方法和函数隐藏内积技术。

Result: 成功展示了针对功能加密安全训练方案的攻击有效性，暴露了现有安全承诺的脆弱性。

Conclusion: 需要重新评估基于功能加密的神经网络安全训练方案，提出的客户端参与方案能有效防御此类攻击。

Abstract: With the increased interest in artificial intelligence, Machine Learning as a
Service provides the infrastructure in the Cloud for easy training, testing,
and deploying models. However, these systems have a major privacy issue:
uploading sensitive data to the Cloud, especially during training. Therefore,
achieving secure Neural Network training has been on many researchers' minds
lately. More and more solutions for this problem are built around a main
pillar: Functional Encryption (FE). Although these approaches are very
interesting and offer a new perspective on ML training over encrypted data,
some vulnerabilities do not seem to be taken into consideration. In our paper,
we present an attack on neural networks that uses FE for secure training over
encrypted data. Our approach uses linear programming to reconstruct the
original input, unveiling the previous security promises. To address the
attack, we propose two solutions for secure training and inference that involve
the client during the computation phase. One approach ensures security without
relying on encryption, while the other uses function-hiding inner-product
techniques.

</details>


### [7] [From Indexing to Coding: A New Paradigm for Data Availability Sampling](https://arxiv.org/abs/2509.21586)
*Moritz Grundei,Aayush Rajasekaran,Kishori Konwar,Muriel Medard*

Main category: cs.CR

TL;DR: 提出了一种新的数据可用性采样方法，通过模块化编码和承诺过程，在采样时进行即时编码，相比传统固定速率擦除编码方法，能提供多个数量级更强的数据可用性保证。


<details>
  <summary>Details</summary>
Motivation: 解决区块链系统中的数据可用性问题，这是以太坊等平台可访问性和可扩展性问题的核心挑战。现有DAS方法通常对固定速率擦除编码的码字形成密码学承诺，限制了轻节点只能从预定编码符号集中采样。

Method: 引入模块化方法，承诺未编码数据，通过即时编码进行采样。使用随机线性网络编码(RLNC)实现具体协议，使采样更具表达性。

Result: 在具体实现中，轻节点可以获得比传统使用Reed Solomon或低密度奇偶校验码的DAS方案多个数量级更强的数据可用性保证。

Conclusion: 提出的新DAS方法通过模块化编码和承诺过程，显著提高了数据可用性采样的效率和保证强度。

Abstract: The data availability problem is a central challenge in blockchain systems
and lies at the core of the accessibility and scalability issues faced by
platforms such as Ethereum. Modern solutions employ several approaches, with
data availability sampling (DAS) being the most self-sufficient and
minimalistic in its security assumptions. Existing DAS methods typically form
cryptographic commitments on codewords of fixed-rate erasure codes, which
restrict light nodes to sampling from a predetermined set of coded symbols.
  In this paper, we introduce a new approach to DAS that modularizes the coding
and commitment process by committing to the uncoded data while performing
sampling through on-the-fly coding. The resulting samples are significantly
more expressive, enabling light nodes to obtain, in concrete implementations,
up to multiple orders of magnitude stronger assurances of data availability
than from sampling pre-committed symbols from a fixed-rate redundancy code as
done in established DAS schemes using Reed Solomon or low density parity check
codes. We present a concrete protocol that realizes this paradigm using random
linear network coding (RLNC).

</details>


### [8] [It's not Easy: Applying Supervised Machine Learning to Detect Malicious Extensions in the Chrome Web Store](https://arxiv.org/abs/2509.21590)
*Ben Rosenzweig,Valentino Dalla Valle,Giovanni Apruzzese,Aurore Fass*

Main category: cs.CR

TL;DR: 该研究开发了基于监督机器学习的Chrome恶意扩展检测系统，在实验室环境下准确率达98%，但在真实场景中发现概念漂移问题，检测效果显著下降，凸显恶意浏览器扩展检测的根本性困难。


<details>
  <summary>Details</summary>
Motivation: Chrome Web Store中的恶意扩展可能绕过谷歌的审查流程，威胁用户安全和隐私，需要更有效的自动化检测机制。

Method: 收集2017-2023年的7,140个恶意扩展和63,598个良性扩展，开发三种监督机器学习分类器，并在2023年新发布的35,462个扩展上进行测试。

Result: 实验室环境下分类器准确率达98%，但在真实场景中仅识别出68个恶意扩展，同时报告超过1,000个疑似恶意扩展，显示概念漂移问题严重。商业检测器(VirusTotal)对已知恶意扩展检测效果差。

Conclusion: 检测恶意浏览器扩展是一个根本性难题，需要研究社区和谷歌共同努力改进方法，同时向谷歌报告了发现并公开了研究工具。

Abstract: Google Chrome is the most popular Web browser. Users can customize it with
extensions that enhance their browsing experience. The most well-known
marketplace of such extensions is the Chrome Web Store (CWS). Developers can
upload their extensions on the CWS, but such extensions are made available to
users only after a vetting process carried out by Google itself. Unfortunately,
some malicious extensions bypass such checks, putting the security and privacy
of downstream browser extension users at risk.
  Here, we scrutinize the extent to which automated mechanisms reliant on
supervised machine learning (ML) can be used to detect malicious extensions on
the CWS. To this end, we first collect 7,140 malicious extensions published in
2017--2023. We combine this dataset with 63,598 benign extensions published or
updated on the CWS before 2023, and we develop three supervised-ML-based
classifiers. We show that, in a "lab setting", our classifiers work well (e.g.,
98% accuracy). Then, we collect a more recent set of 35,462 extensions from the
CWS, published or last updated in 2023, with unknown ground truth. We were
eventually able to identify 68 malicious extensions that bypassed the vetting
process of the CWS. However, our classifiers also reported >1k likely malicious
extensions. Based on this finding (further supported with empirical evidence),
we elucidate, for the first time, a strong concept drift effect on browser
extensions. We also show that commercial detectors (e.g., VirusTotal) work
poorly to detect known malicious extensions. Altogether, our results highlight
that detecting malicious browser extensions is a fundamentally hard problem.
This requires additional work both by the research community and by Google
itself -- potentially by revising their approaches. In the meantime, we
informed Google of our discoveries, and we release our artifacts.

</details>


### [9] [World's First Authenticated Satellite Pseudorange from Orbit](https://arxiv.org/abs/2509.21601)
*Jason Anderson*

Main category: cs.CR

TL;DR: Pulsar-0卫星从太空广播认证测距服务，无需加密密钥所有权或泄露假设，通过水印设计实现防欺骗检测，声称实现了世界上首个来自轨道的认证卫星伪距。


<details>
  <summary>Details</summary>
Motivation: 提供无需加密密钥假设的认证测距服务，解决传统加密方法中密钥泄露风险的问题，实现更安全的卫星定位认证。

Method: 设计Pulsar水印系统，分析水印的漏检概率和虚警概率，开发接收机处理算法，利用轨道传输进行验证，并在包含真实轨道传输的欺骗场景中测试防欺骗效果。

Result: 成功验证了Pulsar水印在轨道传输中的有效性，展示了在欺骗场景下的防欺骗检测能力，2025年7月的轨道传输实现了世界上首个认证卫星伪距。

Conclusion: Pulsar认证测距服务通过数学上安全的水印设计，无需加密密钥假设即可提供可靠的防欺骗认证，为卫星定位安全提供了新的解决方案。

Abstract: Cryptographic Ranging Authentication is here! We present initial results on
the Pulsar authenticated ranging service broadcast from space with Pulsar-0
utilizing a recording taken at Xona headquarters in Burlingame, CA. No
assumptions pertaining to the ownership or leakage of encryption keys are
required. This work discusses the Pulsar watermark design and security
analysis. We derive the Pulsar watermark's probabilities of missed detection
and false alarm, and we discuss the required receiver processing needed to
utilize the Pulsar watermark. We present validation results of the Pulsar
watermark utilizing the transmissions from orbit. Lastly, we provide results
that demonstrate the spoofing detection efficacy with a spoofing scenario that
incorporates the authentic transmissions from orbit. Because we make no
assumption about the leakage of symmetric encryption keys, this work provides
mathematical justification of the watermark's security, and our July 2025
transmissions from orbit, we claim the world's first authenticated satellite
pseudorange from orbit.

</details>


### [10] [MobiLLM: An Agentic AI Framework for Closed-Loop Threat Mitigation in 6G Open RANs](https://arxiv.org/abs/2509.21634)
*Prakhar Sharma,Haohuang Wen,Vinod Yegneswaran,Ashish Gehani,Phillip Porras,Zhiqiang Lin*

Main category: cs.CR

TL;DR: 提出了MobiLLM框架，这是一个基于大语言模型的智能AI系统，用于在6G O-RAN环境中实现端到端的自动化威胁缓解。


<details>
  <summary>Details</summary>
Motivation: 6G O-RAN的开放性虽然促进了创新，但也扩大了攻击面，而传统防御措施反应迟钝、劳动密集，无法应对下一代系统的规模和复杂性。当前O-RAN应用主要关注网络优化或被动威胁检测，缺乏闭环自动化响应能力。

Method: MobiLLM通过模块化多智能体系统协调安全流程，包括：威胁分析代理进行实时数据分类、威胁分类代理使用检索增强生成将异常映射到具体对策、威胁响应代理通过O-RAN控制接口安全执行缓解操作。

Result: 初步评估表明，MobiLLM能有效识别和协调复杂缓解策略，显著降低响应延迟，展示了在6G中实现自主安全操作的可行性。

Conclusion: MobiLLM基于可信知识库（如MITRE FiGHT框架和3GPP规范）并配备稳健安全防护措施，为可信AI驱动的网络安全提供了蓝图。

Abstract: The evolution toward 6G networks is being accelerated by the Open Radio
Access Network (O-RAN) paradigm -- an open, interoperable architecture that
enables intelligent, modular applications across public telecom and private
enterprise domains. While this openness creates unprecedented opportunities for
innovation, it also expands the attack surface, demanding resilient, low-cost,
and autonomous security solutions. Legacy defenses remain largely reactive,
labor-intensive, and inadequate for the scale and complexity of next-generation
systems. Current O-RAN applications focus mainly on network optimization or
passive threat detection, with limited capability for closed-loop, automated
response.
  To address this critical gap, we present an agentic AI framework for fully
automated, end-to-end threat mitigation in 6G O-RAN environments. MobiLLM
orchestrates security workflows through a modular multi-agent system powered by
Large Language Models (LLMs). The framework features a Threat Analysis Agent
for real-time data triage, a Threat Classification Agent that uses
Retrieval-Augmented Generation (RAG) to map anomalies to specific
countermeasures, and a Threat Response Agent that safely operationalizes
mitigation actions via O-RAN control interfaces. Grounded in trusted knowledge
bases such as the MITRE FiGHT framework and 3GPP specifications, and equipped
with robust safety guardrails, MobiLLM provides a blueprint for trustworthy
AI-driven network security. Initial evaluations demonstrate that MobiLLM can
effectively identify and orchestrate complex mitigation strategies,
significantly reducing response latency and showcasing the feasibility of
autonomous security operations in 6G.

</details>


### [11] [Not My Agent, Not My Boundary? Elicitation of Personal Privacy Boundaries in AI-Delegated Information Sharing](https://arxiv.org/abs/2509.21712)
*Bingcan Guo,Eryue Xu,Zhiping Zhang,Tianshi Li*

Main category: cs.CR

TL;DR: 提出了一种基于AI的隐私边界探测方法，通过判别性任务来了解个体的隐私偏好，研究发现沟通角色、AI委托等因素显著影响隐私边界设定。


<details>
  <summary>Details</summary>
Motivation: 需要理解个体在隐私披露中的细微行为差异，而不仅仅是通用规范，因为隐私决策具有情境依赖性且涉及复杂权衡。

Method: 采用AI驱动的探测方法，通过判别性任务探索个体隐私边界，进行了169名参与者的组间研究，收集了1,681个边界规范数据，涵盖61种情境。

Result: 沟通角色影响个体对详细和可识别披露的接受度；AI委托和隐私需求增强了对披露标识符的敏感性；AI委托导致个体间共识度降低。

Conclusion: 强调在真实数据流中定位隐私偏好探测的重要性，建议将细微隐私边界作为未来AI系统的对齐目标。

Abstract: Aligning AI systems with human privacy preferences requires understanding
individuals' nuanced disclosure behaviors beyond general norms. Yet eliciting
such boundaries remains challenging due to the context-dependent nature of
privacy decisions and the complex trade-offs involved. We present an AI-powered
elicitation approach that probes individuals' privacy boundaries through a
discriminative task. We conducted a between-subjects study that systematically
varied communication roles and delegation conditions, resulting in 1,681
boundary specifications from 169 participants for 61 scenarios. We examined how
these contextual factors and individual differences influence the boundary
specification. Quantitative results show that communication roles influence
individuals' acceptance of detailed and identifiable disclosure, AI delegation
and individuals' need for privacy heighten sensitivity to disclosed
identifiers, and AI delegation results in less consensus across individuals.
Our findings highlight the importance of situating privacy preference
elicitation within real-world data flows. We advocate using nuanced privacy
boundaries as an alignment goal for future AI systems.

</details>


### [12] [Backdoor Attribution: Elucidating and Controlling Backdoor in Language Models](https://arxiv.org/abs/2509.21761)
*Miao Yu,Zhenhong Zhou,Moayad Aloqaily,Kun Wang,Biwei Huang,Stephen Wang,Yueming Jin,Qingsong Wen*

Main category: cs.CR

TL;DR: 本文提出了Backdoor Attribution (BkdAttr)框架，通过因果分析揭示LLM后门机制，发现后门特征由稀疏的注意力头处理，仅需消融约3%的注意力头即可降低攻击成功率90%以上，并能构建后门向量实现对后门的精确控制。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全可解释性研究主要关注对齐、越狱和幻觉问题，而忽视了后门机制，导致难以理解和完全消除后门威胁。

Method: 提出Backdoor Attribution (BkdAttr)三部分因果分析框架：Backdoor Probe证明后门特征的存在，Backdoor Attention Head Attribution (BAHA)精确定位处理这些特征的特定注意力头。

Result: 发现后门注意力头相对稀疏，消融约3%的总头数即可将攻击成功率降低90%以上；构建的后门向量仅需在单个表示上进行1点干预，就能将攻击成功率提升至约100%或降至约0%。

Conclusion: 本研究开创了LLM后门机制可解释性探索，展示了强大的后门控制方法，为社区提供了可操作的见解。

Abstract: Fine-tuned Large Language Models (LLMs) are vulnerable to backdoor attacks
through data poisoning, yet the internal mechanisms governing these attacks
remain a black box. Previous research on interpretability for LLM safety tends
to focus on alignment, jailbreak, and hallucination, but overlooks backdoor
mechanisms, making it difficult to understand and fully eliminate the backdoor
threat. In this paper, aiming to bridge this gap, we explore the interpretable
mechanisms of LLM backdoors through Backdoor Attribution (BkdAttr), a
tripartite causal analysis framework. We first introduce the Backdoor Probe
that proves the existence of learnable backdoor features encoded within the
representations. Building on this insight, we further develop Backdoor
Attention Head Attribution (BAHA), efficiently pinpointing the specific
attention heads responsible for processing these features. Our primary
experiments reveals these heads are relatively sparse; ablating a minimal
\textbf{$\sim$ 3%} of total heads is sufficient to reduce the Attack Success
Rate (ASR) by \textbf{over 90%}. More importantly, we further employ these
findings to construct the Backdoor Vector derived from these attributed heads
as a master controller for the backdoor. Through only \textbf{1-point}
intervention on \textbf{single} representation, the vector can either boost ASR
up to \textbf{$\sim$ 100% ($\uparrow$)} on clean inputs, or completely
neutralize backdoor, suppressing ASR down to \textbf{$\sim$ 0% ($\downarrow$)}
on triggered inputs. In conclusion, our work pioneers the exploration of
mechanistic interpretability in LLM backdoors, demonstrating a powerful method
for backdoor control and revealing actionable insights for the community.

</details>


### [13] [PSRT: Accelerating LRM-based Guard Models via Prefilled Safe Reasoning Traces](https://arxiv.org/abs/2509.21768)
*Jiawei Zhao,Yuang Qi,Weiming Zhang,Nenghai Yu,Kejiang Chen*

Main category: cs.CR

TL;DR: PSRT方法通过预填充安全推理轨迹来替代大型推理模型的推理过程，显著降低推理成本，同时保持有害查询检测的分类效果。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在推理过程中会产生冗长的推理轨迹，导致计算开销巨大。

Method: PSRT预填充来自构建数据集的"安全推理虚拟标记"，并在其连续嵌入上进行学习，借助指示标记实现单次前向传播的有害查询检测。

Result: 在效率方面，PSRT完全消除了推理过程中生成推理标记的开销；在分类性能方面，在7个模型和5个数据集上仅平均F1分数下降0.015。

Conclusion: PSRT能够在保持大型推理模型分类效果的同时，显著降低推理成本。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on
tasks such as mathematics and code generation. Motivated by these strengths,
recent work has empirically demonstrated the effectiveness of LRMs as guard
models in improving harmful query detection. However, LRMs typically generate
long reasoning traces during inference, causing substantial computational
overhead. In this paper, we introduce PSRT, a method that replaces the model's
reasoning process with a Prefilled Safe Reasoning Trace, thereby significantly
reducing the inference cost of LRMs. Concretely, PSRT prefills "safe reasoning
virtual tokens" from a constructed dataset and learns over their continuous
embeddings. With the aid of indicator tokens, PSRT enables harmful-query
detection in a single forward pass while preserving the classification
effectiveness of LRMs. We evaluate PSRT on 7 models, 13 datasets, and 8
jailbreak methods. In terms of efficiency, PSRT completely removes the overhead
of generating reasoning tokens during inference. In terms of classification
performance, PSRT achieves nearly identical accuracy, with only a minor average
F1 drop of 0.015 across 7 models and 5 datasets.

</details>


### [14] [PhishLumos: An Adaptive Multi-Agent System for Proactive Phishing Campaign Mitigation](https://arxiv.org/abs/2509.21772)
*Daiki Chiba,Hiroki Nakano,Takashi Koide*

Main category: cs.CR

TL;DR: PhishLumos是一个自适应多智能体系统，通过将钓鱼攻击的规避行为作为关键信号，主动识别并缓解整个攻击活动，而非仅仅阻止单个URL。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击对社会构成重大威胁，尤其危害弱势群体，而现有防御措施往往是反应性的，无法应对现代规避技术如伪装技术。

Method: 使用大型语言模型驱动的智能体系统，通过分析共享托管、证书和域名注册模式来识别攻击活动基础设施。

Result: 在真实世界数据上，系统在中位数情况下提前一周识别出100%的攻击活动，早于网络安全专家的确认。

Conclusion: PhishLumos实现了从反应性URL阻止到主动活动缓解的实践转变，在用户受害前提供保护，使数字世界更安全。

Abstract: Phishing attacks are a significant societal threat, disproportionately
harming vulnerable populations and eroding trust in essential digital services.
Current defenses are often reactive, failing against modern evasive tactics
like cloaking that conceal malicious content. To address this, we introduce
PhishLumos, an adaptive multi-agent system that proactively mitigates entire
attack campaigns. It confronts a core cybersecurity imbalance: attackers can
easily scale operations, while defense remains an intensive expert task.
Instead of being blocked by evasion, PhishLumos treats it as a critical signal
to investigate the underlying infrastructure. Its Large Language Model
(LLM)-powered agents uncover shared hosting, certificates, and domain
registration patterns. On real-world data, our system identified 100% of
campaigns in the median case, over a week before their confirmation by
cybersecurity experts. PhishLumos demonstrates a practical shift from reactive
URL blocking to proactive campaign mitigation, protecting users before they are
harmed and making the digital world safer for all.

</details>


### [15] [Lattice-Based Dynamic $k$-times Anonymous Authentication](https://arxiv.org/abs/2509.21786)
*Junjie Song,Jinguang Han,Man Ho Au,Rupeng Yang,Chao Sun*

Main category: cs.CR

TL;DR: 提出首个基于格的动态k次匿名认证方案，支持动态用户管理和后量子安全，通信效率优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有基于格的k-TAA方案不支持动态用户授权和撤销，无法满足实际应用需求，且需要考虑量子计算攻击的威胁。

Method: 构建了具体的格基动态k-TAA方案，将安全性归约到标准复杂性假设。

Result: 方案实现了有限次数的匿名认证、动态成员管理和后量子安全性，通信成本优于现有格基k-TAA方案。

Conclusion: 成功构建了首个高效且安全的格基动态k-TAA方案，为隐私保护系统提供了实用的后量子安全解决方案。

Abstract: With the development of Internet, privacy has become a close concern of
users. Anonymous authentication plays an important role in privacy-preserving
systems. $k$-times anonymous authentication ($k$-TAA) scheme allows members of
a group to be authenticated anonymously by application providers up to $k$
times. Considering quantum computing attacks, lattice-based $k$-TAA was
introduced. However, existing schemes do not support dynamically granting and
revoking users. In this paper, we construct the first lattice-based dynamic
$k$-TAA, which offers limited times anonymous authentication, dynamic member
management, and post-quantum security. We present a concrete construction, and
reduce its security to standard complexity assumptions. Notably, compared with
existing lattice-based $k$-TAA, our scheme is efficient in terms of
communication cost.

</details>


### [16] [SoK: Potentials and Challenges of Large Language Models for Reverse Engineering](https://arxiv.org/abs/2509.21821)
*Xinyu Hu,Zhiwei Fu,Shaocong Xie,Steven H. H. Ding,Philippe Charland*

Main category: cs.CR

TL;DR: 本文系统综述了44篇研究论文和18个开源项目，探讨了大型语言模型在逆向工程中的应用，提出了分类法并分析了现有工作的优缺点、可重复性问题和新兴风险。


<details>
  <summary>Details</summary>
Motivation: 逆向工程在软件安全中至关重要，但传统方法劳动密集且需要专业知识。虽然深度学习已开始自动化部分逆向工程任务，但大型语言模型的应用角色尚不明确，且现有研究在问题定义、方法和评估实践上存在差异，限制了可比性和累积进展。

Method: 通过系统综述44篇研究论文和18个开源项目，提出了一个按目标、目标对象、方法、评估策略和数据规模组织的分类法，分析现有工作的优缺点、可重复性差距和新兴风险。

Result: 识别出现有研究的优势和局限性，突出了可重复性和评估方面的差距，并检查了新兴风险。现有工作包括简单适应现有流水线和利用更广泛推理生成能力两种不同方法。

Conclusion: 提出了开放挑战和未来研究方向，旨在指导在逆向工程中更一致和安全相关的大型语言模型应用。需要解决可重复性、评估标准化和风险缓解等问题。

Abstract: Reverse Engineering (RE) is central to software security, enabling tasks such
as vulnerability discovery and malware analysis, but it remains labor-intensive
and requires substantial expertise. Earlier advances in deep learning start to
automate parts of RE, particularly for malware detection and vulnerability
classification. More recently, a rapidly growing body of work has applied Large
Language Models (LLMs) to similar purposes. Their role compared to prior
machine learning remains unclear, since some efforts simply adapt existing
pipelines with minimal change while others seek to exploit broader reasoning
and generative abilities. These differences, combined with varied problem
definitions, methods, and evaluation practices, limit comparability,
reproducibility, and cumulative progress. This paper systematizes the field by
reviewing 44 research papers, including peer-reviewed publications and
preprints, and 18 additional open-source projects that apply LLMs in RE. We
propose a taxonomy that organizes existing work by objective, target, method,
evaluation strategy, and data scale. Our analysis identifies strengths and
limitations, highlights reproducibility and evaluation gaps, and examines
emerging risks. We conclude with open challenges and future research directions
that aim to guide more coherent and security-relevant applications of LLMs in
RE.

</details>


### [17] [The Dark Art of Financial Disguise in Web3: Money Laundering Schemes and Countermeasures](https://arxiv.org/abs/2509.21831)
*Hesam Sarkhosh,Uzma Maroof,Diogo Barradas*

Main category: cs.CR

TL;DR: 该调查论文分析了Web3和DeFi生态系统中洗钱活动的策略和机制，重点关注犯罪分子如何利用Web3的匿名性和监管薄弱环节来掩盖非法金融活动。


<details>
  <summary>Details</summary>
Motivation: Web3和DeFi的无信任、无需许可和无国界特性带来了重大监管挑战，为洗钱等金融犯罪提供了温床，需要系统性地研究和应对这些风险。

Method: 通过构建洗钱策略和机制的分类法，分析犯罪分子如何利用Web3的匿名特性和监管框架薄弱环节，识别知识差距和开放挑战。

Result: 提出了Web3洗钱活动的高级策略和底层机制分类法，揭示了犯罪分子利用技术特性和监管漏洞进行洗钱的具体方式。

Conclusion: 该研究为构建更透明的Web3金融生态系统提供了重要见解，为研究人员、政策制定者和行业从业者指明了未来研究方向。

Abstract: The rise of Web3 and Decentralized Finance (DeFi) has enabled borderless
access to financial services empowered by smart contracts and blockchain
technology. However, the ecosystem's trustless, permissionless, and borderless
nature presents substantial regulatory challenges. The absence of centralized
oversight and the technical complexity create fertile ground for financial
crimes. Among these, money laundering is particularly concerning, as in the
event of successful scams, code exploits, and market manipulations, it
facilitates covert movement of illicit gains. Beyond this, there is a growing
concern that cryptocurrencies can be leveraged to launder proceeds from drug
trafficking, or to transfer funds linked to terrorism financing.
  This survey aims to outline a taxonomy of high-level strategies and
underlying mechanisms exploited to facilitate money laundering in Web3. We
examine how criminals leverage the pseudonymous nature of Web3, alongside weak
regulatory frameworks, to obscure illicit financial activities. Our study seeks
to bridge existing knowledge gaps on laundering schemes, identify open
challenges in the detection and prevention of such activities, and propose
future research directions to foster a more transparent Web3 financial
ecosystem -- offering valuable insights for researchers, policymakers, and
industry practitioners.

</details>


### [18] [SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models](https://arxiv.org/abs/2509.21843)
*Jingkai Guo,Chaitali Chakrabarti,Deliang Fan*

Main category: cs.CR

TL;DR: SBFA是一种新型比特翻转攻击，仅需翻转单个比特就能使大语言模型性能崩溃，同时保持扰动值在良性权重分布范围内，揭示了SOTA LLM模型的严重安全隐患。


<details>
  <summary>Details</summary>
Motivation: 现有比特翻转攻击方法通常分别针对整数或浮点模型，限制了攻击灵活性。在浮点模型中，随机比特翻转常导致参数变为极端值，不够隐蔽且会引起数值运行时错误。

Method: 提出SBFA攻击方法，通过定义的参数敏感度指标ImpactScore进行迭代搜索和排序，结合梯度敏感性和受良性层间权重分布约束的扰动范围。还提出轻量级SKIP搜索算法来大幅降低搜索复杂度。

Result: 在Qwen、LLaMA和Gemma模型上，仅翻转单个比特就能成功将MMLU和SST-2的准确率降至随机水平以下，适用于BF16和INT8数据格式。

Conclusion: 仅翻转数十亿参数中的单个比特就能揭示SOTA LLM模型的严重安全关切，表明模型完整性面临严峻挑战。

Abstract: Model integrity of Large language models (LLMs) has become a pressing
security concern with their massive online deployment. Prior Bit-Flip Attacks
(BFAs) -- a class of popular AI weight memory fault-injection techniques -- can
severely compromise Deep Neural Networks (DNNs): as few as tens of bit flips
can degrade accuracy toward random guessing. Recent studies extend BFAs to LLMs
and reveal that, despite the intuition of better robustness from modularity and
redundancy, only a handful of adversarial bit flips can also cause LLMs'
catastrophic accuracy degradation. However, existing BFA methods typically
focus on either integer or floating-point models separately, limiting attack
flexibility. Moreover, in floating-point models, random bit flips often cause
perturbed parameters to extreme values (e.g., flipping in exponent bit), making
it not stealthy and leading to numerical runtime error (e.g., invalid tensor
values (NaN/Inf)). In this work, for the first time, we propose SBFA (Sneaky
Bit-Flip Attack), which collapses LLM performance with only one single bit flip
while keeping perturbed values within benign layer-wise weight distribution. It
is achieved through iterative searching and ranking through our defined
parameter sensitivity metric, ImpactScore, which combines gradient sensitivity
and perturbation range constrained by the benign layer-wise weight
distribution. A novel lightweight SKIP searching algorithm is also proposed to
greatly reduce searching complexity, which leads to successful SBFA searching
taking only tens of minutes for SOTA LLMs. Across Qwen, LLaMA, and Gemma
models, with only one single bit flip, SBFA successfully degrades accuracy to
below random levels on MMLU and SST-2 in both BF16 and INT8 data formats.
Remarkably, flipping a single bit out of billions of parameters reveals a
severe security concern of SOTA LLM models.

</details>


### [19] [You Can't Steal Nothing: Mitigating Prompt Leakages in LLMs via System Vectors](https://arxiv.org/abs/2509.21884)
*Bochuan Cao,Changjiang Li,Yuanpu Cao,Yameng Ge,Ting Wang,Jinghui Chen*

Main category: cs.CR

TL;DR: 本文提出SysVec方法，通过将系统提示编码为内部表示向量而非原始文本，有效防止系统提示泄露攻击，同时提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型广泛使用系统提示，但面临泄露风险。现有防御方法仅能阻止已知攻击模式，无法应对新型攻击。

Method: 提出SysVec方法，将系统提示编码为内部表示向量，避免在上下文中暴露原始文本。

Result: 实验证明SysVec能有效缓解提示泄露攻击，保持模型功能完整性，并改善长上下文中的遗忘问题。

Conclusion: SysVec通过向量化编码系统提示，在增强安全性的同时提升了模型的指令遵循能力，为解决系统提示泄露问题提供了根本性方案。

Abstract: Large language models (LLMs) have been widely adopted across various
applications, leveraging customized system prompts for diverse tasks. Facing
potential system prompt leakage risks, model developers have implemented
strategies to prevent leakage, primarily by disabling LLMs from repeating their
context when encountering known attack patterns. However, it remains vulnerable
to new and unforeseen prompt-leaking techniques. In this paper, we first
introduce a simple yet effective prompt leaking attack to reveal such risks.
Our attack is capable of extracting system prompts from various LLM-based
application, even from SOTA LLM models such as GPT-4o or Claude 3.5 Sonnet. Our
findings further inspire us to search for a fundamental solution to the
problems by having no system prompt in the context. To this end, we propose
SysVec, a novel method that encodes system prompts as internal representation
vectors rather than raw text. By doing so, SysVec minimizes the risk of
unauthorized disclosure while preserving the LLM's core language capabilities.
Remarkably, this approach not only enhances security but also improves the
model's general instruction-following abilities. Experimental results
demonstrate that SysVec effectively mitigates prompt leakage attacks, preserves
the LLM's functional integrity, and helps alleviate the forgetting issue in
long-context scenarios.

</details>


### [20] [Eliminating Exponential Key Growth in PRG-Based Distributed Point Functions](https://arxiv.org/abs/2509.22022)
*Marc Damie,Florian Hahn,Andreas Peter,Jan Ramon*

Main category: cs.CR

TL;DR: 本文优化了多党分布式点函数方案，通过诚实多数假设消除了指数级密钥增长，实现了实用的密钥大小，比现有最佳方案小3倍。


<details>
  <summary>Details</summary>
Motivation: 解决PRG-based多党DPF方案中密钥大小随参与方数量和域大小呈指数增长的问题，提升方案的实用性。

Method: 基于Boyle等人的方案进行优化，利用诚实多数假设来消除指数因子，构建首个具有实用密钥大小的PRG-based多党DPF方案。

Result: 实现了密钥大小比已知最佳多党DPF方案小3倍的优化效果，证明了PRG-based多党DPF可以达到实用性能。

Conclusion: 通过精心优化，PRG-based多党DPF不仅能够实现实用性能，甚至可以达到顶尖性能水平。

Abstract: Distributed Point Functions (DPFs) enable sharing secret point functions
across multiple parties, supporting privacy-preserving technologies such as
Private Information Retrieval, and anonymous communications. While 2-party
PRG-based schemes with logarithmic key sizes have been known for a decade,
extending these solutions to multi-party settings has proven challenging. In
particular, PRG-based multi-party DPFs have historically struggled with
practicality due to key sizes growing exponentially with the number of parties
and the field size.
  Our work addresses this efficiency bottleneck by optimizing the PRG-based
multi-party DPF scheme of Boyle et al. (EUROCRYPT'15). By leveraging the
honest-majority assumption, we eliminate the exponential factor present in this
scheme. Our construction is the first PRG-based multi-party DPF scheme with
practical key sizes, and provides key up to 3x smaller than the best known
multi-party DPF. This work demonstrates that with careful optimization,
PRG-based multi-party DPFs can achieve practical performances, and even obtain
top performances.

</details>


### [21] [NanoTag: Systems Support for Efficient Byte-Granular Overflow Detection on ARM MTE](https://arxiv.org/abs/2509.22027)
*Mingkai Li,Hang Ye,Joseph Devietti,Suman Jana,Tanvir Ahmed Khan*

Main category: cs.CR

TL;DR: NanoTag是一个在ARM MTE硬件上实现字节级内存安全bug检测的系统，通过设置tripwire来检测16字节粒度内的缓冲区溢出，在保持低开销的同时接近ASAN的检测精度。


<details>
  <summary>Details</summary>
Motivation: ARM MTE硬件检测内存安全bug的开销低，但由于16字节的标签粒度，检测精度较粗。需要一种方法在保持低开销的同时提高检测精度。

Method: NanoTag通过为需要检测内部溢出的标签粒度设置tripwire，结合软件检测和MTE硬件检测，在未修改的二进制文件中实现字节级内存安全bug检测。

Result: 评估显示NanoTag检测到的内存安全bug数量接近ASAN，同时运行时开销与MTE SYNC模式下的Scudo Hardened Allocator相当。

Conclusion: NanoTag成功解决了MTE检测精度不足的问题，在低硬件开销下实现了接近软件方法的检测精度。

Abstract: Memory safety bugs, such as buffer overflows and use-after-frees, are the
leading causes of software safety issues in production. Software-based
approaches, e.g., Address Sanitizer (ASAN), can detect such bugs with high
precision, but with prohibitively high overhead. ARM's Memory Tagging Extension
(MTE) offers a promising alternative to detect these bugs in hardware with a
much lower overhead. However, in this paper, we perform a thorough
investigation of Google Pixel 8, the first production implementation of ARM
MTE, and show that MTE can only achieve coarse precision in bug detection
compared with software-based approaches such as ASAN, mainly due to its 16-byte
tag granularity. To address this issue, we present NanoTag, a system to detect
memory safety bugs in unmodified binaries at byte granularity with ARM MTE.
NanoTag detects intra-granule buffer overflows by setting up a tripwire for tag
granules that may require intra-granule overflow detection. The memory access
to the tripwire causes additional overflow detection in the software while
using MTE's hardware to detect bugs for the rest of the accesses. We implement
NanoTag based on the Scudo Hardened Allocator, the default memory allocator on
Android since Android 11. Our evaluation results across popular benchmarks and
real-world case studies show that NanoTag detects nearly as many memory safety
bugs as ASAN while incurring similar run-time overhead to Scudo Hardened
Allocator in MTE SYNC mode.

</details>


### [22] ["Your AI, My Shell": Demystifying Prompt Injection Attacks on Agentic AI Coding Editors](https://arxiv.org/abs/2509.22040)
*Yue Liu,Yanjie Zhao,Yunbo Lyu,Ting Zhang,Haoyu Wang,David Lo*

Main category: cs.CR

TL;DR: 本研究首次对高权限AI编程编辑器的提示注入攻击进行实证分析，展示了攻击者如何通过污染外部开发资源来远程利用这些系统，将AI代理变成攻击者的shell。


<details>
  <summary>Details</summary>
Motivation: 随着AI编程编辑器的流行，这些系统拥有更多系统权限来执行复杂编码任务，虽然提升了开发效率，但也带来了新的安全风险。本研究旨在分析这些高权限AI编辑器的提示注入漏洞。

Method: 开发了AIShellJack自动化测试框架，包含314个独特攻击载荷，覆盖MITRE ATT&CK框架中的70种技术，用于评估AI编程编辑器的提示注入漏洞。

Result: 在GitHub Copilot和Cursor上的大规模评估显示，恶意命令执行的成功率高达84%，攻击在初始访问、系统发现、凭据窃取和数据外泄等多个目标上都有效。

Conclusion: 高权限AI编程编辑器存在严重的提示注入漏洞，攻击者可以远程利用这些漏洞将AI代理变成攻击工具，需要加强安全防护措施。

Abstract: Agentic AI coding editors driven by large language models have recently
become more popular due to their ability to improve developer productivity
during software development. Modern editors such as Cursor are designed not
just for code completion, but also with more system privileges for complex
coding tasks (e.g., run commands in the terminal, access development
environments, and interact with external systems). While this brings us closer
to the "fully automated programming" dream, it also raises new security
concerns. In this study, we present the first empirical analysis of prompt
injection attacks targeting these high-privilege agentic AI coding editors. We
show how attackers can remotely exploit these systems by poisoning external
development resources with malicious instructions, effectively hijacking AI
agents to run malicious commands, turning "your AI" into "attacker's shell". To
perform this analysis, we implement AIShellJack, an automated testing framework
for assessing prompt injection vulnerabilities in agentic AI coding editors.
AIShellJack contains 314 unique attack payloads that cover 70 techniques from
the MITRE ATT&CK framework. Using AIShellJack, we conduct a large-scale
evaluation on GitHub Copilot and Cursor, and our evaluation results show that
attack success rates can reach as high as 84% for executing malicious commands.
Moreover, these attacks are proven effective across a wide range of objectives,
ranging from initial access and system discovery to credential theft and data
exfiltration.

</details>


### [23] [Guidance Watermarking for Diffusion Models](https://arxiv.org/abs/2509.22126)
*Enoal Gesny,Eva Giboulot,Teddy Furon,Vivien Chappelier*

Main category: cs.CR

TL;DR: 提出一种基于梯度引导的扩散模型水印方法，可将后处理水印方案转换为生成过程中的嵌入，无需重新训练或微调。


<details>
  <summary>Details</summary>
Motivation: 将后处理水印方案有效整合到扩散模型的生成过程中，提高水印的鲁棒性，同时保持生成图像的质量和多样性。

Method: 使用现成水印解码器的梯度来引导扩散过程，梯度计算包含多种图像增强，增强对未训练攻击的鲁棒性。

Result: 该方法在不同扩散模型和检测器上验证有效，水印引导不会显著改变给定种子和提示的生成图像，保持了生成质量和多样性。

Conclusion: 该方法成功将后处理水印转换为生成过程嵌入，与变分自编码器修改技术互补，为扩散模型提供了有效的水印解决方案。

Abstract: This paper introduces a novel watermarking method for diffusion models. It is
based on guiding the diffusion process using the gradient computed from any
off-the-shelf watermark decoder. The gradient computation encompasses different
image augmentations, increasing robustness to attacks against which the decoder
was not originally robust, without retraining or fine-tuning. Our method
effectively convert any \textit{post-hoc} watermarking scheme into an
in-generation embedding along the diffusion process. We show that this approach
is complementary to watermarking techniques modifying the variational
autoencoder at the end of the diffusion process. We validate the methods on
different diffusion models and detectors. The watermarking guidance does not
significantly alter the generated image for a given seed and prompt, preserving
both the diversity and quality of generation.

</details>


### [24] [The Express Lane to Spam and Centralization: An Empirical Analysis of Arbitrum's Timeboost](https://arxiv.org/abs/2509.22143)
*Johnnatan Messias,Christof Ferreira Torres*

Main category: cs.CR

TL;DR: 对Arbitrum Timeboost机制的大规模实证研究显示，该拍卖式交易排序机制未能实现公平、去中心化和减少垃圾交易的目标，反而加剧了中心化问题。


<details>
  <summary>Details</summary>
Motivation: 研究Timeboost机制在缓解MEV（最大可提取价值）和实现公平交易排序方面的实际效果，验证其是否达到设计目标。

Method: 分析2025年4月至7月期间的1150万条快速通道交易和15.1万次拍卖数据，进行大规模实证研究。

Result: 发现快速通道控制高度集中（两个实体赢得90%以上拍卖）、MEV机会集中在区块末尾、22%的时间提升交易被回滚、二级市场崩溃、DAO收入持续下降。

Conclusion: Timeboost未能实现其宣称的公平性、去中心化和垃圾交易减少目标，反而强化了中心化并限制了采用，突显了拍卖式排序机制在rollup中的局限性。

Abstract: DeFi applications are vulnerable to MEV, where specialized actors profit by
reordering or inserting transactions. To mitigate latency races and internalize
MEV revenue, Arbitrum introduced Timeboost, an auction-based transaction
sequencing mechanism that grants short-term priority access to an express lane.
In this paper we present the first large-scale empirical study of Timeboost,
analyzing over 11.5 million express lane transactions and 151 thousand auctions
between April and July 2025. Our results reveal five main findings. First,
express lane control is highly centralized, with two entities winning more than
90% of auctions. Second, while express lane access provides earlier inclusion,
profitable MEV opportunities cluster at the end of blocks, limiting the value
of priority access. Third, approximately 22% of time-boosted transactions are
reverted, indicating that the Timeboost does not effectively mitigate spam.
Fourth, secondary markets for reselling express lane rights have collapsed due
to poor execution reliability and unsustainable economics. Finally, auction
competition declined over time, leading to steadily reduced revenue for the
Arbitrum DAO. Taken together, these findings show that Timeboost fails to
deliver on its stated goals of fairness, decentralization, and spam reduction.
Instead, it reinforces centralization and narrows adoption, highlighting the
limitations of auction-based ordering as a mechanism for fair transaction
sequencing in rollups.

</details>


### [25] [Collusion-Driven Impersonation Attack on Channel-Resistant RF Fingerprinting](https://arxiv.org/abs/2509.22154)
*Zhou Xu,Guyue Li,Zhe Peng,Aiqun Hu*

Main category: cs.CR

TL;DR: 提出了一种基于共谋的射频指纹模仿攻击，通过同步接收器匹配合法发射器的集中对数功率谱特征，成功破解射频指纹识别系统，在不同信道条件下攻击成功率超过95%。


<details>
  <summary>Details</summary>
Motivation: 射频指纹技术的研究重点从鲁棒性转向安全性，目前虽然验证了对抗基本欺骗的安全性，但对抗高级模仿攻击的韧性仍未知，需要填补这一安全空白。

Method: 设计了一种共谋驱动的模仿攻击，攻击者与共谋接收器同步匹配合法发射器的集中对数功率谱特征；构建了结合变分自编码器和多目标损失函数的欺骗信号生成网络，增强生成样本的相似性和欺骗能力。

Result: 在包含加性高斯白噪声、多径衰落和多普勒频移的标准信道变化环境下进行了广泛仿真验证，结果表明所提出的攻击方案在不同信道条件下基本保持超过95%的成功率。

Conclusion: 该攻击方案揭示了射频指纹识别系统在面对高级模仿攻击时的安全漏洞，证明了射频级欺骗的有效性，对射频指纹技术的安全性提出了重要挑战。

Abstract: Radio frequency fingerprint (RFF) is a promising device identification
technology, with recent research shifting from robustness to security due to
growing concerns over vulnerabilities. To date, while the security of RFF
against basic spoofing such as MAC address tampering has been validated, its
resilience to advanced mimicry remains unknown. To address this gap, we propose
a collusion-driven impersonation attack that achieves RF-level mimicry,
successfully breaking RFF identification systems across diverse environments.
Specifically, the attacker synchronizes with a colluding receiver to match the
centralized logarithmic power spectrum (CLPS) of the legitimate transmitter;
once the colluder deems the CLPS identical, the victim receiver will also
accept the forged fingerprint, completing RF-level spoofing. Given that the
distribution of CLPS features is relatively concentrated and has a clear
underlying structure, we design a spoofed signal generation network that
integrates a variational autoencoder (VAE) with a multi-objective loss function
to enhance the similarity and deceptive capability of the generated samples. We
carry out extensive simulations, validating cross-channel attacks in
environments that incorporate standard channel variations including additive
white Gaussian noise (AWGN), multipath fading, and Doppler shift. The results
indicate that the proposed attack scheme essentially maintains a success rate
of over 95% under different channel conditions, revealing the effectiveness of
this attack.

</details>


### [26] [Accuracy-First Rényi Differential Privacy and Post-Processing Immunity](https://arxiv.org/abs/2509.22213)
*Ossi Räisä,Antti Koskela,Antti Honkela*

Main category: cs.CR

TL;DR: 本文研究了差分隐私中精度优先视角下的后处理免疫性问题，提出了基于Rényi差分隐私的新定义，并开发了实用工具，在合成数据生成应用中验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 现有精度优先视角的差分隐私研究忽视了后处理免疫性这一重要特性，导致隐私保证可能因后处理而被削弱。

Method: 提出基于Rényi差分隐私的新定义，开发了包括高斯机制和验证集精度检查算法在内的实用工具。

Result: 新定义具有后处理免疫性，在合成数据生成应用中能够自适应调整隐私边界直到达到精度阈值。

Conclusion: 基于Rényi差分隐私的新定义填补了精度优先视角下后处理免疫性的空白，为实际应用提供了可行的理论框架和工具。

Abstract: The accuracy-first perspective of differential privacy addresses an important
shortcoming by allowing a data analyst to adaptively adjust the quantitative
privacy bound instead of sticking to a predetermined bound. Existing works on
the accuracy-first perspective have neglected an important property of
differential privacy known as post-processing immunity, which ensures that an
adversary is not able to weaken the privacy guarantee by post-processing. We
address this gap by determining which existing definitions in the
accuracy-first perspective have post-processing immunity, and which do not. The
only definition with post-processing immunity, pure ex-post privacy, lacks
useful tools for practical problems, such as an ex-post analogue of the
Gaussian mechanism, and an algorithm to check if accuracy on separate private
validation set is high enough. To address this, we propose a new definition
based on R\'enyi differential privacy that has post-processing immunity, and we
develop basic theory and tools needed for practical applications. We
demonstrate the practicality of our theory with an application to synthetic
data generation, where our algorithm successfully adjusts the privacy bound
until an accuracy threshold is met on a private validation dataset.

</details>


### [27] [Learn, Check, Test -- Security Testing Using Automata Learning and Model Checking](https://arxiv.org/abs/2509.22215)
*Stefan Marksteiner,Mikael Sjödin,Marjan Sirjani*

Main category: cs.CR

TL;DR: 提出了一种针对黑盒网络物理系统的自动化安全验证方法，通过主动学习推断系统行为模型，并使用模型检查器验证安全属性。


<details>
  <summary>Details</summary>
Motivation: 网络物理系统作为工业系统和关键基础设施，需要全面验证其正确性和安全性。但由于供应链长或保密性等原因，许多系统只能以黑盒方式检查，因此需要系统化、自动化的验证方法。

Method: 使用主动黑盒学习技术推断系统的Mealy机行为模型，通过上下文命题映射(CPMs)为模型添加命题注释，将其转换为类似Kripke结构的形式，然后定义模板将结构转换为模型检查器兼容格式。

Result: 开发了灵活可扩展的方法，能够为不同通信协议(NFC和UDS)生成模型检查器兼容的模型，并使用相同的安全属性进行验证。

Conclusion: 该方法能够有效处理黑盒网络物理系统的安全验证，通过CPMs实现上下文相关的安全属性定义，支持非确定性行为和故障建模，具有很好的通用性和可扩展性。

Abstract: Cyber-physical systems are part of industrial systems and critical
infrastructure. Therefore, they should be examined in a comprehensive manner to
verify their correctness and security. At the same time, the complexity of such
systems demands such examinations to be systematic and, if possible, automated
for efficiency and accuracy. A method that can be useful in this context is
model checking. However, this requires a model that faithfully represents the
behavior of the examined system. Obtaining such a model is not trivial, as many
of these systems can be examined only in black box settings due to, e.g., long
supply chains or secrecy. We therefore utilize active black box learning
techniques to infer behavioral models in the form of Mealy machines of such
systems and translate them into a form that can be evaluated using a model
checker. To this end, we will investigate a cyber-physical systems as a black
box using its external communication interface. We first annotate the model
with propositions by mapping context information from the respective protocol
to the model using Context-based Proposition Maps (CPMs). We gain annotated
Mealy machines that resemble Kripke structures. We then formally define a
template, to transfer the structures model checker-compatible format. We
further define generic security properties based on basic security
requirements. Due to the used CPMs, we can instantiate these properties with a
meaningful context to check a specific protocol, which makes the approach
flexible and scalable. The gained model can be easily altered to introduce
non-deterministic behavior (like timeouts) or faults and examined if the
properties still. Lastly, we demonstrate the versatility of the approach by
providing case studies of different communication protocols (NFC and UDS),
checked with the same tool chain and the same security properties.

</details>


### [28] [Secure and Efficient Access Control for Computer-Use Agents via Context Space](https://arxiv.org/abs/2509.22256)
*Haochen Gong,Chenxiao Li,Rui Chang,Wenbo Shen*

Main category: cs.CR

TL;DR: CSAgent是一个系统级的静态策略访问控制框架，用于保护基于LLM的计算机使用代理，通过意图和上下文感知策略来防止代理偏离用户意图造成安全风险。


<details>
  <summary>Details</summary>
Motivation: 由于LLM固有的不确定性，让代理控制计算机会带来重大安全风险。当代理行为偏离用户意图时，可能导致不可逆的后果。现有的缓解方法在可用性、安全性和性能方面仍有局限。

Method: 提出CSAgent框架，引入意图和上下文感知策略，提供自动化工具链帮助开发者构建和优化策略，并通过优化的操作系统服务强制执行这些策略。支持API、CLI和GUI等多种接口。

Result: CSAgent成功防御了超过99.36%的攻击，同时仅引入6.83%的性能开销。

Conclusion: CSAgent提供了一个有效的系统级安全框架，能够在保证性能的同时显著提升基于LLM的计算机使用代理的安全性。

Abstract: Large language model (LLM)-based computer-use agents represent a convergence
of AI and OS capabilities, enabling natural language to control system- and
application-level functions. However, due to LLMs' inherent uncertainty issues,
granting agents control over computers poses significant security risks. When
agent actions deviate from user intentions, they can cause irreversible
consequences. Existing mitigation approaches, such as user confirmation and
LLM-based dynamic action validation, still suffer from limitations in
usability, security, and performance. To address these challenges, we propose
CSAgent, a system-level, static policy-based access control framework for
computer-use agents. To bridge the gap between static policy and dynamic
context and user intent, CSAgent introduces intent- and context-aware policies,
and provides an automated toolchain to assist developers in constructing and
refining them. CSAgent enforces these policies through an optimized OS service,
ensuring that agent actions can only be executed under specific user intents
and contexts. CSAgent supports protecting agents that control computers through
diverse interfaces, including API, CLI, and GUI. We implement and evaluate
CSAgent, which successfully defends against more than 99.36% of attacks while
introducing only 6.83% performance overhead.

</details>


### [29] [A Global Analysis of Cyber Threats to the Energy Sector: "Currents of Conflict" from a Geopolitical Perspective](https://arxiv.org/abs/2509.22280)
*Gustavo Sánchez,Ghada Elbez,Veit Hagenmeyer*

Main category: cs.CR

TL;DR: 使用生成式AI分析网络威胁情报，聚焦能源领域的地缘政治动态和攻击检测技术评估


<details>
  <summary>Details</summary>
Motivation: 网络威胁日益频繁和复杂化，需要全面理解威胁态势，特别是在关键基础设施如能源领域

Method: 利用生成式AI从原始威胁描述中提取结构化信息，进行地缘政治比较分析，并评估基于学习的网络安全工具检测效果

Result: 揭示了威胁态势中的趋势，提供了针对能源攻击的妥协指标检测效果评估

Conclusion: 为研究人员、政策制定者和网络安全专业人员提供了可操作的信息和新的洞察

Abstract: The escalating frequency and sophistication of cyber threats increased the
need for their comprehensive understanding. This paper explores the
intersection of geopolitical dynamics, cyber threat intelligence analysis, and
advanced detection technologies, with a focus on the energy domain. We leverage
generative artificial intelligence to extract and structure information from
raw cyber threat descriptions, enabling enhanced analysis. By conducting a
geopolitical comparison of threat actor origins and target regions across
multiple databases, we provide insights into trends within the general threat
landscape. Additionally, we evaluate the effectiveness of cybersecurity tools
-- with particular emphasis on learning-based techniques -- in detecting
indicators of compromise for energy-targeted attacks. This analysis yields new
insights, providing actionable information to researchers, policy makers, and
cybersecurity professionals.

</details>


### [30] [Privacy Mechanism Design based on Empirical Distributions](https://arxiv.org/abs/2509.22428)
*Leonhard Grosse,Sara Saeidian,Mikael Skoglund,Tobias J. Oechtering*

Main category: cs.CR

TL;DR: 提出了一个基于经验估计数据生成分布的点态最大泄漏隐私评估框架，通过考虑分布集合来提供最坏情况泄漏界限，并设计了最优二元机制。


<details>
  <summary>Details</summary>
Motivation: PML隐私保证依赖于对私有数据生成分布的了解，但在实际中通常只能获得经验估计，需要解决分布不确定性下的隐私评估和机制设计问题。

Method: 扩展PML框架以考虑数据生成分布集合，结合大偏差理论获得分布独立的(ε,δ)-PML保证，将机制设计转化为线性约束凸优化问题。

Result: 提供了最优二元机制设计方法，证明针对分布估计设计的最优机制可以实际使用，在二元私有数据上应用拉普拉斯和高斯机制并显示相比本地差分隐私有显著效用提升。

Conclusion: 所提出的方法能够在保持相似隐私保证的同时，显著提高机制效用，为分布不确定性下的隐私保护提供了有效解决方案。

Abstract: Pointwise maximal leakage (PML) is a per-outcome privacy measure based on
threat models from quantitative information flow. Privacy guarantees with PML
rely on knowledge about the distribution that generated the private data. In
this work, we propose a framework for PML privacy assessment and mechanism
design with empirical estimates of this data-generating distribution. By
extending the PML framework to consider sets of data-generating distributions,
we arrive at bounds on the worst-case leakage within a given set. We use these
bounds alongside large-deviation bounds from the literature to provide a method
for obtaining distribution-independent $(\varepsilon,\delta)$-PML guarantees
when the data-generating distribution is estimated from available data samples.
We provide an optimal binary mechanism, and show that mechanism design with
this type of uncertainty about the data-generating distribution reduces to a
linearly constrained convex program. Further, we show that optimal mechanisms
designed for a distribution estimate can be used. Finally, we apply these tools
to leakage assessment of the Laplace mechanism and the Gaussian mechanism for
binary private data, and numerically show that the presented approach to
mechanism design can yield significant utility increase compared to local
differential privacy, while retaining similar privacy guarantees.

</details>
