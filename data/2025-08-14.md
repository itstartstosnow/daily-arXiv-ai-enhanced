<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 12]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models: A Unified and Accurate Approach](https://arxiv.org/abs/2508.09201)
*Shuang Liang,Zhihao Xu,Jialing Tao,Hui Xue,Xiting Wang*

Main category: cs.CR

TL;DR: 论文提出了一种名为LoD的无监督框架，通过异常检测来识别大型视觉语言模型（LVLM）的越狱攻击，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管现有方法尝试利用内部多模态信息检测越狱攻击，但依赖启发式规则导致性能不佳，因此需要一种基于原则性目标的方法。

Method: LoD框架包含两个核心组件：多模态安全概念激活向量（MSCAV）和安全模式自动编码器，通过建模安全输入的分布并检测异常重构误差来识别攻击。

Result: 在三个LVLM和五个基准测试中，LoD的平均AUROC达到0.9951，最小AUROC比最强基线提升38.89%。

Conclusion: LoD是一种高效且无需攻击标签的越狱攻击检测方法，显著优于现有技术。

Abstract: Despite extensive alignment efforts, Large Vision-Language Models (LVLMs)
remain vulnerable to jailbreak attacks, posing serious safety risks. Although
recent detection works have shifted to internal representations due to their
rich cross-modal information, most methods rely on heuristic rules rather than
principled objectives, resulting in suboptimal performance. To address these
limitations, we propose Learning to Detect (LoD), a novel unsupervised
framework that formulates jailbreak detection as anomaly detection. LoD
introduces two key components: Multi-modal Safety Concept Activation Vectors
(MSCAV), which capture layer-wise safety-related representations across
modalities, and the Safety Pattern Auto-Encoder, which models the distribution
of MSCAV derived from safe inputs and detects anomalies via reconstruction
errors. By training the auto-encoder (AE) solely on safe samples without attack
labels, LoD naturally identifies jailbreak inputs as distributional anomalies,
enabling accurate and unified detection of jailbreak attacks. Comprehensive
experiments on three different LVLMs and five benchmarks demonstrate that LoD
achieves state-of-the-art performance, with an average AUROC of 0.9951 and an
improvement of up to 38.89% in the minimum AUROC over the strongest baselines.

</details>


### [2] [VeriPHY: Physical Layer Signal Authentication for Wireless Communication in 5G Environments](https://arxiv.org/abs/2508.09213)
*Clifton Paul Robinson,Salvatore D'Oro,Tommaso Melodia*

Main category: cs.CR

TL;DR: VeriPHY是一种基于深度学习的物理层认证方案，用于5G网络，通过隐写术在无线I/Q传输中嵌入签名，实现高精度设备认证。


<details>
  <summary>Details</summary>
Motivation: 传统加密方法在无线网络中效率较低，物理层认证（PLA）利用通信介质的固有特性提供高效认证。深度学习的发展使PLA更准确可靠。

Method: VeriPHY通过高斯混合模型生成伪随机签名，并将其嵌入5G网络的I/Q传输中，利用深度神经网络进行用户识别和认证。

Result: VeriPHY的认证精度在93%到100%之间，误报率低，推理时间为28毫秒（签名每20毫秒更新）。

Conclusion: VeriPHY在5G网络中实现了高效、隐蔽且高精度的设备认证，同时支持隐蔽模式生成签名。

Abstract: Physical layer authentication (PLA) uses inherent characteristics of the
communication medium to provide secure and efficient authentication in wireless
networks, bypassing the need for traditional cryptographic methods. With
advancements in deep learning, PLA has become a widely adopted technique for
its accuracy and reliability. In this paper, we introduce VeriPHY, a novel deep
learning-based PLA solution for 5G networks, which enables unique device
identification by embedding signatures within wireless I/Q transmissions using
steganography. VeriPHY continuously generates pseudo-random signatures by
sampling from Gaussian Mixture Models whose distribution is carefully varied to
ensure signature uniqueness and stealthiness over time, and then embeds the
newly generated signatures over I/Q samples transmitted by users to the 5G gNB.
Utilizing deep neural networks, VeriPHY identifies and authenticates users
based on these embedded signatures. VeriPHY achieves high precision,
identifying unique signatures between 93% and 100% with low false positive
rates and an inference time of 28 ms when signatures are updated every 20 ms.
Additionally, we also demonstrate a stealth generation mode where signatures
are generated in a way that makes them virtually indistinguishable from
unaltered 5G signals while maintaining over 93% detection accuracy.

</details>


### [3] [Can AI Keep a Secret? Contextual Integrity Verification: A Provable Security Architecture for LLMs](https://arxiv.org/abs/2508.09288)
*Aayush Gupta*

Main category: cs.CR

TL;DR: 提出了一种名为CIV的安全架构，通过加密签名和信任源格栅防止LLMs的提示注入攻击，效果显著且无需微调。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）易受提示注入攻击，现有防护措施常被绕过，需更可靠的解决方案。

Method: 采用上下文完整性验证（CIV），通过加密签名标签和信任源格栅在推理时强制执行安全策略。

Result: 在基准测试中，CIV实现了0%攻击成功率，同时保持93.1%的令牌相似性，且不影响模型性能。

Conclusion: CIV是一种轻量级、无需微调的安全补丁，适用于多种LLMs，支持可复现研究。

Abstract: Large language models (LLMs) remain acutely vulnerable to prompt injection
and related jailbreak attacks; heuristic guardrails (rules, filters, LLM
judges) are routinely bypassed. We present Contextual Integrity Verification
(CIV), an inference-time security architecture that attaches cryptographically
signed provenance labels to every token and enforces a source-trust lattice
inside the transformer via a pre-softmax hard attention mask (with optional
FFN/residual gating). CIV provides deterministic, per-token non-interference
guarantees on frozen models: lower-trust tokens cannot influence higher-trust
representations. On benchmarks derived from recent taxonomies of
prompt-injection vectors (Elite-Attack + SoK-246), CIV attains 0% attack
success rate under the stated threat model while preserving 93.1% token-level
similarity and showing no degradation in model perplexity on benign tasks; we
note a latency overhead attributable to a non-optimized data path. Because CIV
is a lightweight patch -- no fine-tuning required -- we demonstrate drop-in
protection for Llama-3-8B and Mistral-7B. We release a reference
implementation, an automated certification harness, and the Elite-Attack corpus
to support reproducible research.

</details>


### [4] [Security Analysis of ChatGPT: Threats and Privacy Risks](https://arxiv.org/abs/2508.09426)
*Yushan Xiang,Zhongwen Li,Xiaoqi Li*

Main category: cs.CR

TL;DR: 本文研究了ChatGPT的安全性和隐私风险，分析了其漏洞及成因，并探讨了伦理争议，同时模拟攻击与防御场景。


<details>
  <summary>Details</summary>
Motivation: 随着ChatGPT应用扩展，其安全威胁和隐私风险日益突出，需系统研究以应对挑战。

Method: 从安全威胁和隐私风险两方面分析漏洞及成因，模拟攻击与防御测试场景。

Result: 揭示了ChatGPT的潜在漏洞及伦理争议，验证了其在安全检测和工具生成中的可行性。

Conclusion: 需进一步研究以提升ChatGPT的安全性和隐私保护能力。

Abstract: As artificial intelligence technology continues to advance, chatbots are
becoming increasingly powerful. Among them, ChatGPT, launched by OpenAI, has
garnered widespread attention globally due to its powerful natural language
processing capabilities based on the GPT model, which enables it to engage in
natural conversations with users, understand various forms of linguistic
expressions, and generate useful information and suggestions. However, as its
application scope expands, user demand grows, and malicious attacks related to
it become increasingly frequent, the security threats and privacy risks faced
by ChatGPT are gradually coming to the forefront. In this paper, the security
of ChatGPT is mainly studied from two aspects, security threats and privacy
risks. The article systematically analyzes various types of vulnerabilities
involved in the above two types of problems and their causes. Briefly, we
discuss the controversies that ChatGPT may cause at the ethical and moral
levels. In addition, this paper reproduces several network attack and defense
test scenarios by simulating the attacker's perspective and methodology.
Simultaneously, it explores the feasibility of using ChatGPT for security
vulnerability detection and security tool generation from the defender's
perspective.

</details>


### [5] [Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference](https://arxiv.org/abs/2508.09442)
*Zhifan Luo,Shuo Shao,Su Zhang,Lijing Zhou,Yuke Hu,Chenxu Zhao,Zhihao Liu,Zhan Qin*

Main category: cs.CR

TL;DR: KV缓存存储中间注意力计算（Key和Value对）以加速大型语言模型（LLM）推理，但存在隐私风险。本文首次全面分析这些漏洞，提出三种攻击方法，并提出防御机制KV-Cloak，有效阻止攻击且不影响模型性能。


<details>
  <summary>Details</summary>
Motivation: KV缓存虽提升LLM推理效率，但未充分研究的隐私风险可能导致敏感用户输入泄露。

Method: 设计三种攻击方法（直接反转攻击、碰撞攻击和语义注入攻击），并提出防御机制KV-Cloak，采用可逆矩阵混淆和算子融合技术。

Result: KV-Cloak成功阻止所有攻击，将重建质量降至随机噪声水平，且几乎不影响模型准确性和性能。

Conclusion: KV-Cloak为LLM部署提供了一种轻量、高效且安全的解决方案，显著降低隐私风险。

Abstract: The Key-Value (KV) cache, which stores intermediate attention computations
(Key and Value pairs) to avoid redundant calculations, is a fundamental
mechanism for accelerating Large Language Model (LLM) inference. However, this
efficiency optimization introduces significant yet underexplored privacy risks.
This paper provides the first comprehensive analysis of these vulnerabilities,
demonstrating that an attacker can reconstruct sensitive user inputs directly
from the KV-cache. We design and implement three distinct attack vectors: a
direct Inversion Attack, a more broadly applicable and potent Collision Attack,
and a semantic-based Injection Attack. These methods demonstrate the
practicality and severity of KV-cache privacy leakage issues. To mitigate this,
we propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.
KV-Cloak uses a reversible matrix-based obfuscation scheme, combined with
operator fusion, to secure the KV-cache. Our extensive experiments show that
KV-Cloak effectively thwarts all proposed attacks, reducing reconstruction
quality to random noise. Crucially, it achieves this robust security with
virtually no degradation in model accuracy and minimal performance overhead,
offering a practical solution for trustworthy LLM deployment.

</details>


### [6] [Demystifying the Role of Rule-based Detection in AI Systems for Windows Malware Detection](https://arxiv.org/abs/2508.09652)
*Andrea Ponte,Luca Demetrio,Luca Oneto,Ivan Tesfai Ogbu,Battista Biggio,Fabio Roli*

Main category: cs.CR

TL;DR: 论文探讨了在恶意软件检测中，将基于签名的检测与机器学习结合训练对模型性能的影响，发现能提升对抗性样本和时序数据漂移的鲁棒性，但也带来固定的假阳性下限。


<details>
  <summary>Details</summary>
Motivation: 现有恶意软件检测系统通常孤立开发签名检测和机器学习组件，未能充分利用数据简化机会和增强对抗性样本防御。

Method: 比较两种训练方式：一种使用完整数据集，另一种仅用未被签名检测标记的样本训练机器学习组件。

Result: 结果显示，后者能提升对抗性样本和时序数据漂移的鲁棒性，但会因规则选择不佳导致固定的假阳性下限。

Conclusion: 未来研究可结合动态分析以进一步提升系统鲁棒性。

Abstract: Malware detection increasingly relies on AI systems that integrate
signature-based detection with machine learning. However, these components are
typically developed and combined in isolation, missing opportunities to reduce
data complexity and strengthen defenses against adversarial EXEmples, carefully
crafted programs designed to evade detection. Hence, in this work we
investigate the influence that signature-based detection exerts on model
training, when they are included inside the training pipeline. Specifically, we
compare models trained on a comprehensive dataset with an AI system whose
machine learning component is trained solely on samples not already flagged by
signatures. Our results demonstrate improved robustness to both adversarial
EXEmples and temporal data drift, although this comes at the cost of a fixed
lower bound on false positives, driven by suboptimal rule selection. We
conclude by discussing these limitations and outlining how future research
could extend AI-based malware detection to include dynamic analysis, thereby
further enhancing system resilience.

</details>


### [7] [Social-Sensor Identity Cloning Detection Using Weakly Supervised Deep Forest and Cryptographic Authentication](https://arxiv.org/abs/2508.09665)
*Ahmed Alharbi,Hai Dong,Xun Yi*

Main category: cs.CR

TL;DR: 提出了一种检测社交传感器云中身份克隆的新方法，结合相似身份检测和加密认证协议，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在检测重复账户和大规模真实数据集评估方面表现不佳，亟需改进。

Method: 1) 使用弱监督深度森林模型检测相似身份；2) 设计加密认证协议验证身份来源。

Result: 在大规模真实数据集上验证了方法的可行性和优越性能。

Conclusion: 该方法在身份克隆检测中表现优异，解决了现有方法的不足。

Abstract: Recent years have witnessed a rising trend in social-sensor cloud identity
cloning incidents. However, existing approaches suffer from unsatisfactory
performance, a lack of solutions for detecting duplicated accounts, and a lack
of large-scale evaluations on real-world datasets. We introduce a novel method
for detecting identity cloning in social-sensor cloud service providers. Our
proposed technique consists of two primary components: 1) a similar identity
detection method and 2) a cryptography-based authentication protocol.
Initially, we developed a weakly supervised deep forest model to identify
similar identities using non-privacy-sensitive user profile features provided
by the service. Subsequently, we designed a cryptography-based authentication
protocol to verify whether similar identities were generated by the same
provider. Our extensive experiments on a large real-world dataset demonstrate
the feasibility and superior performance of our technique compared to current
state-of-the-art identity clone detection methods.

</details>


### [8] [Succinct Oblivious Tensor Evaluation and Applications: Adaptively-Secure Laconic Function Evaluation and Trapdoor Hashing for All Circuits](https://arxiv.org/abs/2508.09673)
*Damiano Abram,Giulio Malavolta,Lawrence Roy*

Main category: cs.CR

TL;DR: 提出了一种简洁的OT（OTE）概念，用于计算两个向量的张量积的秘密共享，消息和CRS大小与向量维度无关。基于LWE问题构造了OTE，并展示了其在多种密码学原语中的应用。


<details>
  <summary>Details</summary>
Motivation: 解决现有技术中计算张量积秘密共享时通信和CRS大小依赖向量维度的问题，同时提升安全性。

Method: 基于LWE问题构造OTE，引入自适应格编码技术，支持多种密码学原语。

Result: 实现了自适应安全的简洁函数评估、陷门哈希函数、同态秘密共享等，通信效率最优。

Conclusion: OTE是一种高效且安全的工具，扩展了基于LWE的密码学应用范围。

Abstract: We propose the notion of succinct oblivious tensor evaluation (OTE), where
two parties compute an additive secret sharing of a tensor product of two
vectors $\mathbf{x} \otimes \mathbf{y}$, exchanging two simultaneous messages.
Crucially, the size of both messages and of the CRS is independent of the
dimension of $\mathbf{x}$.
  We present a construction of OTE with optimal complexity from the standard
learning with errors (LWE) problem. Then we show how this new technical tool
enables a host of cryptographic primitives, all with security reducible to LWE,
such as:
  * Adaptively secure laconic function evaluation for depth-$D$ functions
$f:\{0, 1\}^m\rightarrow\{0, 1\}^\ell$ with communication $m+\ell+D\cdot
\mathrm{poly}(\lambda)$.
  * A trapdoor hash function for all functions.
  * An (optimally) succinct homomorphic secret sharing for all functions.
  * A rate-$1/2$ laconic oblivious transfer for batch messages, which is best
possible.
  In particular, we obtain the first laconic function evaluation scheme that is
adaptively secure from the standard LWE assumption, improving upon Quach, Wee,
and Wichs (FOCS 2018).
  As a key technical ingredient, we introduce a new notion of \emph{adaptive
lattice encodings}, which may be of independent interest.

</details>


### [9] [Enhance the machine learning algorithm performance in phishing detection with keyword features](https://arxiv.org/abs/2508.09765)
*Zijiang Yang*

Main category: cs.CR

TL;DR: 论文提出了一种结合关键词特征与传统特征的新方法，用于提升钓鱼URL检测的机器学习算法效果，平均减少分类错误30%，最高准确率达99.68%。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击日益增多，导致用户敏感信息泄露和财务损失，早期检测钓鱼URL至关重要。现有机器学习算法需进一步优化。

Method: 提出一种新方法，将关键词特征与传统特征结合，应用于多种传统机器学习算法。

Result: 实验显示该方法有效，平均减少分类错误30%，小数据集效果更显著，最高准确率达99.68%。

Conclusion: 该方法显著提升钓鱼URL检测效果，且不依赖第三方服务，适用于不同规模数据集。

Abstract: Recently, we can observe a significant increase of the phishing attacks in
the Internet. In a typical phishing attack, the attacker sets up a malicious
website that looks similar to the legitimate website in order to obtain the
end-users' information. This may cause the leakage of the sensitive information
and the financial loss for the end-users. To avoid such attacks, the early
detection of these websites' URLs is vital and necessary. Previous researchers
have proposed many machine learning algorithms to distinguish the phishing URLs
from the legitimate ones. In this paper, we would like to enhance these machine
learning algorithms from the perspective of feature selection. We propose a
novel method to incorporate the keyword features with the traditional features.
This method is applied on multiple traditional machine learning algorithms and
the experimental results have shown this method is useful and effective. On
average, this method can reduce the classification error by 30% for the large
dataset. Moreover, its enhancement is more significant for the small dataset.
In addition, this method extracts the information from the URL and does not
rely on the additional information provided by the third-part service. The best
result for the machine learning algorithm using our proposed method has
achieved the accuracy of 99.68%.

</details>


### [10] [Perfect message authentication codes are robust to small deviations from uniform key distributions](https://arxiv.org/abs/2508.09783)
*Boris Ryabko*

Main category: cs.CR

TL;DR: 研究了密钥概率分布偏离均匀分布对信息论强（完美）消息认证码的影响，发现安全性降低与统计距离之间的简单关系。


<details>
  <summary>Details</summary>
Motivation: 探讨密钥分布不均匀对完美消息认证码安全性的影响。

Method: 分析统计距离与安全性降低之间的关系。

Result: 发现完美消息认证码对小偏离均匀分布具有鲁棒性。

Conclusion: 完美消息认证码对密钥分布的小偏离具有鲁棒性。

Abstract: We investigate the impact of (possible) deviations of the probability
distribution of key values from a uniform distribution for the
information-theoretic strong, or perfect, message authentication code. We found
a simple expression for the decrease in security as a function of the
statistical distance between the real key probability distribution and the
uniform one. In a sense, a perfect message authentication code is robust to
small deviations from a uniform key distribution.

</details>


### [11] [Explainable Ensemble Learning for Graph-Based Malware Detection](https://arxiv.org/abs/2508.09801)
*Hossein Shokouhinejad,Roozbeh Razavi-Far,Griffin Higgins,Ali A Ghorbani*

Main category: cs.CR

TL;DR: 提出了一种基于图神经网络的堆叠集成框架，用于恶意软件检测和解释，通过动态提取控制流图并利用多样化的GNN基学习器提升分类性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现代计算环境需要准确、可解释且能抵抗规避技术的恶意软件检测模型，而单模型方法在泛化和可解释性上存在不足。

Method: 动态提取PE文件中的控制流图，采用两步嵌入策略编码基本块，使用多样化的GNN基学习器捕获互补特征，并通过注意力机制的多层感知机聚合预测结果。

Result: 实验表明，该框架提高了分类性能，并提供了对恶意软件行为的可解释分析。

Conclusion: 提出的堆叠集成框架在恶意软件检测中表现出色，同时增强了模型的可解释性。

Abstract: Malware detection in modern computing environments demands models that are
not only accurate but also interpretable and robust to evasive techniques.
Graph neural networks (GNNs) have shown promise in this domain by modeling rich
structural dependencies in graph-based program representations such as control
flow graphs (CFGs). However, single-model approaches may suffer from limited
generalization and lack interpretability, especially in high-stakes security
applications. In this paper, we propose a novel stacking ensemble framework for
graph-based malware detection and explanation. Our method dynamically extracts
CFGs from portable executable (PE) files and encodes their basic blocks through
a two-step embedding strategy. A set of diverse GNN base learners, each with a
distinct message-passing mechanism, is used to capture complementary behavioral
features. Their prediction outputs are aggregated by a meta-learner implemented
as an attention-based multilayer perceptron, which both classifies malware
instances and quantifies the contribution of each base model. To enhance
explainability, we introduce an ensemble-aware post-hoc explanation technique
that leverages edge-level importance scores generated by a GNN explainer and
fuses them using the learned attention weights. This produces interpretable,
model-agnostic explanations aligned with the final ensemble decision.
Experimental results demonstrate that our framework improves classification
performance while providing insightful interpretations of malware behavior.

</details>


### [12] [On the Consistency and Performance of the Iterative Bayesian Update](https://arxiv.org/abs/2508.09980)
*Ehab ElSalamouny,Catuscia Palamidessi*

Main category: cs.CR

TL;DR: 本文证明了迭代贝叶斯更新（IBU）作为最大似然估计器的性质，并验证了其一致性，同时通过实验展示了IBU在多种隐私保护机制下的优越性能。


<details>
  <summary>Details</summary>
Motivation: 在保护用户隐私的同时估计敏感属性的分布是重要的社会、科学和商业需求，但现有方法中IBU的一致性未被正确证明。

Method: 利用IBU作为最大似然估计器的性质，证明其一致性，并通过实验比较IBU与其他方法在多种隐私保护机制下的表现。

Result: IBU在几何、拉普拉斯和指数机制下显著优于其他方法，而在k-RR和RAPPOR机制下表现相当。此外，IBU还可扩展到无限字母表的情况。

Conclusion: IBU是一种有效的隐私保护分布估计方法，具有一致性和广泛的适用性。

Abstract: For many social, scientific, and commercial purposes, it is often important
to estimate the distribution of the users' data regarding a sensitive
attribute, e.g., their ages, locations, etc. To allow this estimation while
protecting the users' privacy, every user applies a local privacy protection
mechanism that releases a noisy (sanitized) version of their original datum to
the data collector; then the original distribution is estimated using one of
the known methods, such as the matrix inversion (INV), RAPPOR's estimator, and
the iterative Bayesian update (IBU). Unlike the other estimators, the
consistency of IBU, i.e., the convergence of its estimate to the real
distribution as the amount of noisy data grows, has been either ignored or
incorrectly proved in the literature. In this article, we use the fact that IBU
is a maximum likelihood estimator to prove that IBU is consistent. We also
show, through experiments on real datasets, that IBU significantly outperforms
the other methods when the users' data are sanitized by geometric, Laplace, and
exponential mechanisms, whereas it is comparable to the other methods in the
case of the k-RR and RAPPOR mechanisms. Finally, we consider the case when the
alphabet of the sensitive data is infinite, and we show a technique that allows
IBU to operate in this case too.

</details>
