{"id": "2508.18439", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18439", "abs": "https://arxiv.org/abs/2508.18439", "authors": ["Anders Mølmen Høst", "Pierre Lison", "Leon Moonen"], "title": "A Systematic Approach to Predict the Impact of Cybersecurity Vulnerabilities Using LLMs", "comment": null, "summary": "Vulnerability databases, such as the National Vulnerability Database (NVD),\noffer detailed descriptions of Common Vulnerabilities and Exposures (CVEs), but\noften lack information on their real-world impact, such as the tactics,\ntechniques, and procedures (TTPs) that adversaries may use to exploit the\nvulnerability. However, manually linking CVEs to their corresponding TTPs is a\nchallenging and time-consuming task, and the high volume of new vulnerabilities\npublished annually makes automated support desirable.\n  This paper introduces TRIAGE, a two-pronged automated approach that uses\nLarge Language Models (LLMs) to map CVEs to relevant techniques from the ATT&CK\nknowledge base. We first prompt an LLM with instructions based on MITRE's CVE\nMapping Methodology to predict an initial list of techniques. This list is then\ncombined with the results from a second LLM-based module that uses in-context\nlearning to map a CVE to relevant techniques. This hybrid approach\nstrategically combines rule-based reasoning with data-driven inference. Our\nevaluation reveals that in-context learning outperforms the individual mapping\nmethods, and the hybrid approach improves recall of exploitation techniques. We\nalso find that GPT-4o-mini performs better than Llama3.3-70B on this task.\nOverall, our results show that LLMs can be used to automatically predict the\nimpact of cybersecurity vulnerabilities and TRIAGE makes the process of mapping\nCVEs to ATT&CK more efficient.\n  Keywords: vulnerability impact, CVE, ATT&CK techniques, large language\nmodels, automated mapping."}
{"id": "2508.18453", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.18453", "abs": "https://arxiv.org/abs/2508.18453", "authors": ["Yaser Baseri", "Abdelhakim Senhaji Hafid", "Dimitrios Makrakis", "Hamidreza Fereidouni"], "title": "Privacy-Preserving Federated Learning Framework for Risk-Based Adaptive Authentication", "comment": null, "summary": "Balancing robust security with strong privacy guarantees is critical for\nRisk-Based Adaptive Authentication (RBA), particularly in decentralized\nsettings. Federated Learning (FL) offers a promising solution by enabling\ncollaborative risk assessment without centralizing user data. However, existing\nFL approaches struggle with Non-Independent and Identically Distributed\n(Non-IID) user features, resulting in biased, unstable, and poorly generalized\nglobal models. This paper introduces FL-RBA2, a novel Federated Learning\nframework for Risk-Based Adaptive Authentication that addresses Non-IID\nchallenges through a mathematically grounded similarity transformation. By\nconverting heterogeneous user features (including behavioral, biometric,\ncontextual, interaction-based, and knowledge-based modalities) into IID\nsimilarity vectors, FL-RBA2 supports unbiased aggregation and personalized risk\nmodeling across distributed clients. The framework mitigates cold-start\nlimitations via clustering-based risk labeling, incorporates Differential\nPrivacy (DP) to safeguard sensitive information, and employs Message\nAuthentication Codes (MACs) to ensure model integrity and authenticity.\nFederated updates are securely aggregated into a global model, achieving strong\nbalance between user privacy, scalability, and adaptive authentication\nrobustness. Rigorous game-based security proofs in the Random Oracle Model\nformally establish privacy, correctness, and adaptive security guarantees.\nExtensive experiments on keystroke, mouse, and contextual datasets validate\nFL-RBA2's effectiveness in high-risk user detection and its resilience to model\ninversion and inference attacks, even under strong DP constraints."}
{"id": "2508.18485", "categories": ["cs.CR", "cs.DS", "cs.SE", "E.3; F.2.1"], "pdf": "https://arxiv.org/pdf/2508.18485", "abs": "https://arxiv.org/abs/2508.18485", "authors": ["Peter T. Breuer"], "title": "An 8- and 12-bit block AES cipher", "comment": "This \"research note\" of mine from 2013 has been requested so often\n  from me over the years, along with requests for a way to cite it properly,\n  that I think it's appropriate to put it out on the web in a citeable archive.\n  Arxiv, step up", "summary": "Because it is so unusual, or hard to find, or expository, a truly tiny 8- or\n12-bit block AES (Rijndael) cipher is documented here, along with Java source\ncode."}
{"id": "2508.18488", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18488", "abs": "https://arxiv.org/abs/2508.18488", "authors": ["Martin Lochner", "Keegan Keplinger"], "title": "Collaborative Intelligence: Topic Modelling of Large Language Model use in Live Cybersecurity Operations", "comment": null, "summary": "Objective: This work describes the topic modelling of Security Operations\nCentre (SOC) use of a large language model (LLM), during live security\noperations. The goal is to better understand how these specialists voluntarily\nuse this tool.\n  Background: Human-automation teams have been extensively studied, but\ntransformer-based language models have sparked a new wave of collaboration. SOC\npersonnel at a major cybersecurity provider used an LLM to support live\nsecurity operations. This study examines how these specialists incorporated the\nLLM into their work.\n  Method: Our data set is the result of 10 months of SOC operators accessing\nGPT-4 over an internally deployed HTTP-based chat application. We performed two\ntopic modelling exercises, first using the established BERTopic model\n(Grootendorst, 2022), and second, using a novel topic modeling workflow.\n  Results: Both the BERTopic analysis and novel modelling approach revealed\nthat SOC operators primarily used the LLM to facilitate their understanding of\ncomplex text strings. Variations on this use-case accounted for ~40% of SOC LLM\nusage.\n  Conclusion: SOC operators are required to rapidly interpret complex commands\nand similar information. Their natural tendency to leverage LLMs to support\nthis activity indicates that their workflow can be supported and augmented by\ndesigning collaborative LLM tools for use in the SOC.\n  Application: This work can aid in creating next-generation tools for Security\nOperations Centres. By understanding common use-cases, we can develop workflows\nsupporting SOC task flow. One example is a right-click context menu for\nexecuting a command line analysis LLM call directly in the SOC environment."}
{"id": "2508.18649", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18649", "abs": "https://arxiv.org/abs/2508.18649", "authors": ["Nanxi Li", "Zhengyue Zhao", "Chaowei Xiao"], "title": "PRISM: Robust VLM Alignment with Principled Reasoning for Integrated Safety in Multimodality", "comment": null, "summary": "Safeguarding vision-language models (VLMs) is a critical challenge, as\nexisting methods often suffer from over-defense, which harms utility, or rely\non shallow alignment, failing to detect complex threats that require deep\nreasoning. To this end, we introduce PRISM (Principled Reasoning for Integrated\nSafety in Multimodality), a system2-like framework that aligns VLMs by\nembedding a structured, safety-aware reasoning process. Our framework consists\nof two key components: PRISM-CoT, a dataset that teaches safety-aware\nchain-of-thought reasoning, and PRISM-DPO, generated via Monte Carlo Tree\nSearch (MCTS) to further refine this reasoning through Direct Preference\nOptimization to help obtain a delicate safety boundary. Comprehensive\nevaluations demonstrate PRISM's effectiveness, achieving remarkably low attack\nsuccess rates including 0.15% on JailbreakV-28K for Qwen2-VL and 90%\nimprovement over the previous best method on VLBreak for LLaVA-1.5. PRISM also\nexhibits strong robustness against adaptive attacks, significantly increasing\ncomputational costs for adversaries, and generalizes effectively to\nout-of-distribution challenges, reducing attack success rates to just 8.70% on\nthe challenging multi-image MIS benchmark. Remarkably, this robust defense is\nachieved while preserving, and in some cases enhancing, model utility. To\npromote reproducibility, we have made our code, data, and model weights\navailable at https://github.com/SaFoLab-WISC/PRISM."}
{"id": "2508.18652", "categories": ["cs.CR", "cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.18652", "abs": "https://arxiv.org/abs/2508.18652", "authors": ["Runpeng Geng", "Yanting Wang", "Ying Chen", "Jinyuan Jia"], "title": "UniC-RAG: Universal Knowledge Corruption Attacks to Retrieval-Augmented Generation", "comment": "21 pages, 4 figures", "summary": "Retrieval-augmented generation (RAG) systems are widely deployed in\nreal-world applications in diverse domains such as finance, healthcare, and\ncybersecurity. However, many studies showed that they are vulnerable to\nknowledge corruption attacks, where an attacker can inject adversarial texts\ninto the knowledge database of a RAG system to induce the LLM to generate\nattacker-desired outputs. Existing studies mainly focus on attacking specific\nqueries or queries with similar topics (or keywords). In this work, we propose\nUniC-RAG, a universal knowledge corruption attack against RAG systems. Unlike\nprior work, UniC-RAG jointly optimizes a small number of adversarial texts that\ncan simultaneously attack a large number of user queries with diverse topics\nand domains, enabling an attacker to achieve various malicious objectives, such\nas directing users to malicious websites, triggering harmful command execution,\nor launching denial-of-service attacks. We formulate UniC-RAG as an\noptimization problem and further design an effective solution to solve it,\nincluding a balanced similarity-based clustering method to enhance the attack's\neffectiveness. Our extensive evaluations demonstrate that UniC-RAG is highly\neffective and significantly outperforms baselines. For instance, UniC-RAG could\nachieve over 90% attack success rate by injecting 100 adversarial texts into a\nknowledge database with millions of texts to simultaneously attack a large set\nof user queries (e.g., 2,000). Additionally, we evaluate existing defenses and\nshow that they are insufficient to defend against UniC-RAG, highlighting the\nneed for new defense mechanisms in RAG systems."}
{"id": "2508.18684", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.18684", "abs": "https://arxiv.org/abs/2508.18684", "authors": ["Shaswata Mitra", "Azim Bazarov", "Martin Duclos", "Sudip Mittal", "Aritran Piplai", "Md Rayhanur Rahman", "Edward Zieglar", "Shahram Rahimi"], "title": "FALCON: Autonomous Cyber Threat Intelligence Mining with LLMs for IDS Rule Generation", "comment": "11 pages, 5 figures, 4 tables", "summary": "Signature-based Intrusion Detection Systems (IDS) detect malicious activities\nby matching network or host activity against predefined rules. These rules are\nderived from extensive Cyber Threat Intelligence (CTI), which includes attack\nsignatures and behavioral patterns obtained through automated tools and manual\nthreat analysis, such as sandboxing. The CTI is then transformed into\nactionable rules for the IDS engine, enabling real-time detection and\nprevention. However, the constant evolution of cyber threats necessitates\nfrequent rule updates, which delay deployment time and weaken overall security\nreadiness. Recent advancements in agentic systems powered by Large Language\nModels (LLMs) offer the potential for autonomous IDS rule generation with\ninternal evaluation. We introduce FALCON, an autonomous agentic framework that\ngenerates deployable IDS rules from CTI data in real-time and evaluates them\nusing built-in multi-phased validators. To demonstrate versatility, we target\nboth network (Snort) and host-based (YARA) mediums and construct a\ncomprehensive dataset of IDS rules with their corresponding CTIs. Our\nevaluations indicate FALCON excels in automatic rule generation, with an\naverage of 95% accuracy validated by qualitative evaluation with 84%\ninter-rater agreement among multiple cybersecurity analysts across all metrics.\nThese results underscore the feasibility and effectiveness of LLM-driven data\nmining for real-time cyber threat mitigation."}
{"id": "2508.18750", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.18750", "abs": "https://arxiv.org/abs/2508.18750", "authors": ["Zeng Zhang", "Xiaoqi Li"], "title": "Immutable Digital Recognition via Blockchain", "comment": null, "summary": "The process integrates the decentralised management and centralised operation\nmodels, aligning them with the national policy directives. The developed\nsolution enables the full utilisation of blockchain technology's advantages\nwhile also fostering community participation. Consequently, it establishes a\nsecure, legal, reliable, and dynamic electronic certification system."}
{"id": "2508.18805", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18805", "abs": "https://arxiv.org/abs/2508.18805", "authors": ["Rui Zhang", "Zihan Wang", "Tianli Yang", "Hongwei Li", "Wenbo Jiang", "Qingchuan Zhao", "Yang Liu", "Guowen Xu"], "title": "Hidden Tail: Adversarial Image Causing Stealthy Resource Consumption in Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) are increasingly deployed in real-world\napplications, but their high inference cost makes them vulnerable to resource\nconsumption attacks. Prior attacks attempt to extend VLM output sequences by\noptimizing adversarial images, thereby increasing inference costs. However,\nthese extended outputs often introduce irrelevant abnormal content,\ncompromising attack stealthiness. This trade-off between effectiveness and\nstealthiness poses a major limitation for existing attacks. To address this\nchallenge, we propose \\textit{Hidden Tail}, a stealthy resource consumption\nattack that crafts prompt-agnostic adversarial images, inducing VLMs to\ngenerate maximum-length outputs by appending special tokens invisible to users.\nOur method employs a composite loss function that balances semantic\npreservation, repetitive special token induction, and suppression of the\nend-of-sequence (EOS) token, optimized via a dynamic weighting strategy.\nExtensive experiments show that \\textit{Hidden Tail} outperforms existing\nattacks, increasing output length by up to 19.2$\\times$ and reaching the\nmaximum token limit, while preserving attack stealthiness. These results\nhighlight the urgent need to improve the robustness of VLMs against\nefficiency-oriented adversarial threats. Our code is available at\nhttps://github.com/zhangrui4041/Hidden_Tail."}
{"id": "2508.18832", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.18832", "abs": "https://arxiv.org/abs/2508.18832", "authors": ["Sara Saeidian", "Ata Yavuzyılmaz", "Leonhard Grosse", "Georg Schuppe", "Tobias J. Oechtering"], "title": "A Tight Context-aware Privacy Bound for Histogram Publication", "comment": "Submitted to IEEE Signal Processing Letters", "summary": "We analyze the privacy guarantees of the Laplace mechanism releasing the\nhistogram of a dataset through the lens of pointwise maximal leakage (PML).\nWhile differential privacy is commonly used to quantify the privacy loss, it is\na context-free definition that does not depend on the data distribution. In\ncontrast, PML enables a more refined analysis by incorporating assumptions\nabout the data distribution. We show that when the probability of each\nhistogram bin is bounded away from zero, stronger privacy protection can be\nachieved for a fixed level of noise. Our results demonstrate the advantage of\ncontext-aware privacy measures and show that incorporating assumptions about\nthe data can improve privacy-utility tradeoffs."}
{"id": "2508.18942", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.18942", "abs": "https://arxiv.org/abs/2508.18942", "authors": ["Ahmed Mounsf Rafik Bendada", "Yacine Ghamri-Doudane"], "title": "EnerSwap: Large-Scale, Privacy-First Automated Market Maker for V2G Energy Trading", "comment": "11 pages, 7 figures, 1 table, 1 algorithm, Paper accepted in 27th\n  MSWiM Conference", "summary": "With the rapid growth of Electric Vehicle (EV) technology, EVs are destined\nto shape the future of transportation. The large number of EVs facilitates the\ndevelopment of the emerging vehicle-to-grid (V2G) technology, which realizes\nbidirectional energy exchanges between EVs and the power grid. This has led to\nthe setting up of electricity markets that are usually confined to a small\ngeographical location, often with a small number of participants. Usually,\nthese markets are manipulated by intermediaries responsible for collecting bids\nfrom prosumers, determining the market-clearing price, incorporating grid\nconstraints, and accounting for network losses. While centralized models can be\nhighly efficient, they grant excessive power to the intermediary by allowing\nthem to gain exclusive access to prosumers \\textquotesingle price preferences.\nThis opens the door to potential market manipulation and raises significant\nprivacy concerns for users, such as the location of energy providers. This lack\nof protection exposes users to potential risks, as untrustworthy servers and\nmalicious adversaries can exploit this information to infer trading activities\nand real identities. This work proposes a secure, decentralized exchange market\nbuilt on blockchain technology, utilizing a privacy-preserving Automated Market\nMaker (AMM) model to offer open and fair, and equal access to traders, and\nmitigates the most common trading-manipulation attacks. Additionally, it\nincorporates a scalable architecture based on geographical dynamic sharding,\nallowing for efficient resource allocation and improved performance as the\nmarket grows."}
{"id": "2508.18947", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.18947", "abs": "https://arxiv.org/abs/2508.18947", "authors": ["Ronal Singh", "Shahroz Tariq", "Fatemeh Jalalvand", "Mohan Baruwal Chhetri", "Surya Nepal", "Cecile Paris", "Martin Lochner"], "title": "LLMs in the SOC: An Empirical Study of Human-AI Collaboration in Security Operations Centres", "comment": "22 pages, 9 figures, under review", "summary": "The integration of Large Language Models (LLMs) into Security Operations\nCentres (SOCs) presents a transformative, yet still evolving, opportunity to\nreduce analyst workload through human-AI collaboration. However, their\nreal-world application in SOCs remains underexplored. To address this gap, we\npresent a longitudinal study of 3,090 analyst queries from 45 SOC analysts over\n10 months. Our analysis reveals that analysts use LLMs as on-demand aids for\nsensemaking and context-building, rather than for making high-stakes\ndeterminations, preserving analyst decision authority. The majority of queries\nare related to interpreting low-level telemetry (e.g., commands) and refining\ntechnical communication through short (1-3 turn) interactions. Notably, 93% of\nqueries align with established cybersecurity competencies (NICE Framework),\nunderscoring the relevance of LLM use for SOC-related tasks. Despite variations\nin tasks and engagement, usage trends indicate a shift from occasional\nexploration to routine integration, with growing adoption and sustained use\namong a subset of analysts. We find that LLMs function as flexible, on-demand\ncognitive aids that augment, rather than replace, SOC expertise. Our study\nprovides actionable guidance for designing context-aware, human-centred AI\nassistance in security operations, highlighting the need for further\nin-the-wild research on real-world analyst-LLM collaboration, challenges, and\nimpacts."}
{"id": "2508.18976", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18976", "abs": "https://arxiv.org/abs/2508.18976", "authors": ["Stephen Meisenbacher", "Alexandra Klymenko", "Andreea-Elena Bodea", "Florian Matthes"], "title": "The Double-edged Sword of LLM-based Data Reconstruction: Understanding and Mitigating Contextual Vulnerability in Word-level Differential Privacy Text Sanitization", "comment": "15 pages, 4 figures, 8 tables. Accepted to WPES @ CCS 2025", "summary": "Differentially private text sanitization refers to the process of privatizing\ntexts under the framework of Differential Privacy (DP), providing provable\nprivacy guarantees while also empirically defending against adversaries seeking\nto harm privacy. Despite their simplicity, DP text sanitization methods\noperating at the word level exhibit a number of shortcomings, among them the\ntendency to leave contextual clues from the original texts due to randomization\nduring sanitization $\\unicode{x2013}$ this we refer to as $\\textit{contextual\nvulnerability}$. Given the powerful contextual understanding and inference\ncapabilities of Large Language Models (LLMs), we explore to what extent LLMs\ncan be leveraged to exploit the contextual vulnerability of DP-sanitized texts.\nWe expand on previous work not only in the use of advanced LLMs, but also in\ntesting a broader range of sanitization mechanisms at various privacy levels.\nOur experiments uncover a double-edged sword effect of LLM-based data\nreconstruction attacks on privacy and utility: while LLMs can indeed infer\noriginal semantics and sometimes degrade empirical privacy protections, they\ncan also be used for good, to improve the quality and privacy of DP-sanitized\ntexts. Based on our findings, we propose recommendations for using LLM data\nreconstruction as a post-processing step, serving to increase privacy\nprotection by thinking adversarially."}
{"id": "2508.19072", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19072", "abs": "https://arxiv.org/abs/2508.19072", "authors": ["Sidahmed Benabderrahmane", "Talal Rahwan"], "title": "Attackers Strike Back? Not Anymore -- An Ensemble of RL Defenders Awakens for APT Detection", "comment": null, "summary": "Advanced Persistent Threats (APTs) represent a growing menace to modern\ndigital infrastructure. Unlike traditional cyberattacks, APTs are stealthy,\nadaptive, and long-lasting, often bypassing signature-based detection systems.\nThis paper introduces a novel framework for APT detection that unites deep\nlearning, reinforcement learning (RL), and active learning into a cohesive,\nadaptive defense system. Our system combines auto-encoders for latent\nbehavioral encoding with a multi-agent ensemble of RL-based defenders, each\ntrained to distinguish between benign and malicious process behaviors. We\nidentify a critical challenge in existing detection systems: their static\nnature and inability to adapt to evolving attack strategies. To this end, our\narchitecture includes multiple RL agents (Q-Learning, PPO, DQN, adversarial\ndefenders), each analyzing latent vectors generated by an auto-encoder. When\nany agent is uncertain about its decision, the system triggers an active\nlearning loop to simulate expert feedback, thus refining decision boundaries.\nAn ensemble voting mechanism, weighted by each agent's performance, ensures\nrobust final predictions."}
{"id": "2508.19115", "categories": ["cs.CR", "cs.AI", "E.3; I.2.6; I.5.1; F.1.2"], "pdf": "https://arxiv.org/pdf/2508.19115", "abs": "https://arxiv.org/abs/2508.19115", "authors": ["Joshua Lee", "Ali Arastehfard", "Weiran Liu", "Xuegang Ban", "Yuan Hong"], "title": "SecureV2X: An Efficient and Privacy-Preserving System for Vehicle-to-Everything (V2X) Applications", "comment": "10 pages, 3 figures", "summary": "Autonomous driving and V2X technologies have developed rapidly in the past\ndecade, leading to improved safety and efficiency in modern transportation.\nThese systems interact with extensive networks of vehicles, roadside\ninfrastructure, and cloud resources to support their machine learning\ncapabilities. However, the widespread use of machine learning in V2X systems\nraises issues over the privacy of the data involved. This is particularly\nconcerning for smart-transit and driver safety applications which can\nimplicitly reveal user locations or explicitly disclose medical data such as\nEEG signals. To resolve these issues, we propose SecureV2X, a scalable,\nmulti-agent system for secure neural network inferences deployed between the\nserver and each vehicle. Under this setting, we study two multi-agent V2X\napplications: secure drowsiness detection, and secure red-light violation\ndetection. Our system achieves strong performance relative to baselines, and\nscales efficiently to support a large number of secure computation interactions\nsimultaneously. For instance, SecureV2X is $9.4 \\times$ faster, requires\n$143\\times$ fewer computational rounds, and involves $16.6\\times$ less\ncommunication on drowsiness detection compared to other secure systems.\nMoreover, it achieves a runtime nearly $100\\times$ faster than state-of-the-art\nbenchmarks in object detection tasks for red light violation detection."}
{"id": "2508.19219", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.19219", "abs": "https://arxiv.org/abs/2508.19219", "authors": ["Faezeh Dehghan Tarzjani", "Mostafa Salehi"], "title": "An Efficient Lightweight Blockchain for Decentralized IoT", "comment": null, "summary": "The Internet of Things (IoT) is applied in various fields, and the number of\nphysical devices connected to the IoT is increasingly growing. There are\nsignificant challenges to the IoT's growth and development, mainly due to the\ncentralized nature and large-scale IoT networks. The emphasis on the\ndecentralization of IoT's architecture can overcome challenges to IoT's\ncapabilities. A promising decentralized platform for IoT is blockchain. Owing\nto IoT devices' limited resources, traditional consensus algorithms such as PoW\nand PoS in the blockchain are computationally expensive. Therefore, the PoA\nconsensus algorithm is proposed in the blockchain consensus network for IoT.\nThe PoA selects the validator as Turn-based selection (TBS) that needs\noptimization and faces system reliability, energy consumption, latency, and low\nscalability. We propose an efficient, lightweight blockchain for decentralizing\nIoT architecture by using virtualization and clustering to increase\nproductivity and scalability to address these issues. We also introduce a novel\nPoA based on the Weight-Based-Selection (WBS) method for validators to validate\ntransactions and add them to the blockchain. By simulation, we evaluated the\nperformance of our proposed WBS method as opposed to TBS. The results show\nreduced energy consumption, and response time, and increased throughput."}
