{"id": "2512.05288", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.05288", "abs": "https://arxiv.org/abs/2512.05288", "authors": ["Feijiang Han"], "title": "Beyond Detection: A Comprehensive Benchmark and Study on Representation Learning for Fine-Grained Webshell Family Classification", "comment": null, "summary": "Malicious WebShells pose a significant and evolving threat by compromising critical digital infrastructures and endangering public services in sectors such as healthcare and finance. While the research community has made significant progress in WebShell detection (i.e., distinguishing malicious samples from benign ones), we argue that it is time to transition from passive detection to in-depth analysis and proactive defense. One promising direction is the automation of WebShell family classification, which involves identifying the specific malware lineage in order to understand an adversary's tactics and enable a precise, rapid response. This crucial task, however, remains a largely unexplored area that currently relies on slow, manual expert analysis. To address this gap, we present the first systematic study to automate WebShell family classification. Our method begins with extracting dynamic function call traces to capture inherent behaviors that are resistant to common encryption and obfuscation. To enhance the scale and diversity of our dataset for a more stable evaluation, we augment these real-world traces with new variants synthesized by Large Language Models. These augmented traces are then abstracted into sequences, graphs, and trees, providing a foundation to benchmark a comprehensive suite of representation methods. Our evaluation spans classic sequence-based embeddings (CBOW, GloVe), transformers (BERT, SimCSE), and a range of structure-aware algorithms, including Graph Kernels, Graph Edit Distance, Graph2Vec, and various Graph Neural Networks. Through extensive experiments on four real-world, family-annotated datasets under both supervised and unsupervised settings, we establish a robust baseline and provide practical insights into the most effective combinations of data abstractions, representation models, and learning paradigms for this challenge.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76WebShell\u5bb6\u65cf\u5206\u7c7b\u81ea\u52a8\u5316\uff0c\u901a\u8fc7\u63d0\u53d6\u52a8\u6001\u51fd\u6570\u8c03\u7528\u8f68\u8ff9\u3001\u4f7f\u7528LLM\u751f\u6210\u53d8\u79cd\u589e\u5f3a\u6570\u636e\u96c6\uff0c\u5e76\u6bd4\u8f83\u591a\u79cd\u8868\u793a\u65b9\u6cd5\uff0c\u4e3aWebShell\u5bb6\u65cf\u5206\u7c7b\u5efa\u7acb\u4e86\u57fa\u51c6\u3002", "motivation": "WebShell\u5bf9\u5173\u952e\u6570\u5b57\u57fa\u7840\u8bbe\u65bd\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u68c0\u6d4b\u800c\u975e\u6df1\u5165\u5206\u6790\u3002\u4ece\u88ab\u52a8\u68c0\u6d4b\u8f6c\u5411\u4e3b\u52a8\u9632\u5fa1\u9700\u8981\u81ea\u52a8\u5316\u5bb6\u65cf\u5206\u7c7b\u6765\u7406\u89e3\u653b\u51fb\u8005\u6218\u672f\u5e76\u5b9e\u73b0\u7cbe\u51c6\u5feb\u901f\u54cd\u5e94\uff0c\u4f46\u76ee\u524d\u8be5\u4efb\u52a1\u4ecd\u4f9d\u8d56\u7f13\u6162\u7684\u624b\u52a8\u4e13\u5bb6\u5206\u6790\u3002", "method": "1) \u63d0\u53d6\u52a8\u6001\u51fd\u6570\u8c03\u7528\u8f68\u8ff9\u6355\u83b7\u6297\u52a0\u5bc6\u548c\u6df7\u6dc6\u7684\u5185\u5728\u884c\u4e3a\uff1b2) \u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5408\u6210\u65b0\u53d8\u79cd\u589e\u5f3a\u6570\u636e\u96c6\u89c4\u6a21\u548c\u591a\u6837\u6027\uff1b3) \u5c06\u8f68\u8ff9\u62bd\u8c61\u4e3a\u5e8f\u5217\u3001\u56fe\u548c\u6811\uff1b4) \u7cfb\u7edf\u8bc4\u4f30\u7ecf\u5178\u5e8f\u5217\u5d4c\u5165\u3001transformer\u6a21\u578b\u548c\u7ed3\u6784\u611f\u77e5\u7b97\u6cd5\u7b49\u591a\u79cd\u8868\u793a\u65b9\u6cd5\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u5bb6\u65cf\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u76d1\u7763\u548c\u65e0\u76d1\u7763\u5b9e\u9a8c\uff0c\u5efa\u7acb\u4e86\u7a33\u5065\u7684\u57fa\u51c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u6570\u636e\u62bd\u8c61\u3001\u8868\u793a\u6a21\u578b\u548c\u5b66\u4e60\u8303\u5f0f\u7684\u6700\u6709\u6548\u7ec4\u5408\u7684\u5b9e\u7528\u89c1\u89e3\u3002", "conclusion": "\u8fd9\u662fWebShell\u5bb6\u65cf\u5206\u7c7b\u81ea\u52a8\u5316\u7684\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\uff0c\u4e3a\u4ece\u88ab\u52a8\u68c0\u6d4b\u8f6c\u5411\u4e3b\u52a8\u9632\u5fa1\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u901a\u8fc7\u5168\u9762\u8bc4\u4f30\u591a\u79cd\u65b9\u6cd5\u4e3a\u540e\u7eed\u7814\u7a76\u5efa\u7acb\u4e86\u57fa\u51c6\u3002"}}
{"id": "2512.05321", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.05321", "abs": "https://arxiv.org/abs/2512.05321", "authors": ["Darren Malvern Chin", "Bilal Isfaq", "Simon Yusuf Enoch"], "title": "A Practical Honeypot-Based Threat Intelligence Framework for Cyber Defence in the Cloud", "comment": "6 pages", "summary": "In cloud environments, conventional firewalls rely on predefined rules and manual configurations, limiting their ability to respond effectively to evolving or zero-day threats. As organizations increasingly adopt platforms such as Microsoft Azure, this static defense model exposes cloud assets to zero-day exploits, botnets, and advanced persistent threats. In this paper, we introduce an automated defense framework that leverages medium- to high-interaction honeypot telemetry to dynamically update firewall rules in real time. The framework integrates deception sensors (Cowrie), Azure-native automation tools (Monitor, Sentinel, Logic Apps), and MITRE ATT&CK-aligned detection within a closed-loop feedback mechanism. We developed a testbed to automatically observe adversary tactics, classify them using the MITRE ATT&CK framework, and mitigate network-level threats automatically with minimal human intervention.\n  To assess the framework's effectiveness, we defined and applied a set of attack- and defense-oriented security metrics. Building on existing adaptive defense strategies, our solution extends automated capabilities into cloud-native environments. The experimental results show an average Mean Time to Block of 0.86 seconds - significantly faster than benchmark systems - while accurately classifying over 12,000 SSH attempts across multiple MITRE ATT&CK tactics. These findings demonstrate that integrating deception telemetry with Azure-native automation reduces attacker dwell time, enhances SOC visibility, and provides a scalable, actionable defense model for modern cloud infrastructures.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u871c\u7f50\u9065\u6d4b\u7684\u81ea\u52a8\u5316\u4e91\u9632\u706b\u5899\u9632\u5fa1\u6846\u67b6\uff0c\u5229\u7528Azure\u539f\u751f\u5de5\u5177\u5b9e\u73b0\u5b9e\u65f6\u5a01\u80c1\u68c0\u6d4b\u4e0e\u963b\u65ad\uff0c\u5e73\u5747\u963b\u65ad\u65f6\u95f4\u4ec50.86\u79d2\u3002", "motivation": "\u4f20\u7edf\u9632\u706b\u5899\u4f9d\u8d56\u9884\u5b9a\u4e49\u89c4\u5219\u548c\u624b\u52a8\u914d\u7f6e\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u4e91\u73af\u5883\u4e2d\u4e0d\u65ad\u6f14\u53d8\u7684\u96f6\u65e5\u5a01\u80c1\u3001\u50f5\u5c38\u7f51\u7edc\u548c\u9ad8\u7ea7\u6301\u7eed\u6027\u5a01\u80c1\uff0c\u7279\u522b\u662f\u5728Azure\u7b49\u4e91\u5e73\u53f0\u4e2d\u9759\u6001\u9632\u5fa1\u6a21\u578b\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "\u5f00\u53d1\u81ea\u52a8\u5316\u9632\u5fa1\u6846\u67b6\uff0c\u6574\u5408\u4e2d\u9ad8\u4ea4\u4e92\u871c\u7f50(Cowrie)\u3001Azure\u539f\u751f\u81ea\u52a8\u5316\u5de5\u5177(Monitor\u3001Sentinel\u3001Logic Apps)\u548cMITRE ATT&CK\u5bf9\u9f50\u68c0\u6d4b\uff0c\u901a\u8fc7\u95ed\u73af\u53cd\u9988\u673a\u5236\u5b9e\u65f6\u66f4\u65b0\u9632\u706b\u5899\u89c4\u5219\u3002\u5efa\u7acb\u6d4b\u8bd5\u5e73\u53f0\u81ea\u52a8\u89c2\u5bdf\u653b\u51fb\u8005\u6218\u672f\uff0c\u4f7f\u7528MITRE ATT&CK\u6846\u67b6\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u4ee5\u6700\u5c0f\u4eba\u5de5\u5e72\u9884\u81ea\u52a8\u7f13\u89e3\u7f51\u7edc\u7ea7\u5a01\u80c1\u3002", "result": "\u5e73\u5747\u963b\u65ad\u65f6\u95f4\u4ec50.86\u79d2\uff0c\u663e\u8457\u5feb\u4e8e\u57fa\u51c6\u7cfb\u7edf\uff1b\u51c6\u786e\u5206\u7c7b\u8d85\u8fc712,000\u6b21SSH\u5c1d\u8bd5\uff0c\u8986\u76d6\u591a\u4e2aMITRE ATT&CK\u6218\u672f\uff1b\u51cf\u5c11\u653b\u51fb\u8005\u9a7b\u7559\u65f6\u95f4\uff0c\u589e\u5f3aSOC\u53ef\u89c1\u6027\u3002", "conclusion": "\u5c06\u871c\u7f50\u9065\u6d4b\u4e0eAzure\u539f\u751f\u81ea\u52a8\u5316\u96c6\u6210\uff0c\u4e3a\u73b0\u4ee3\u4e91\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u64cd\u4f5c\u7684\u9632\u5fa1\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e91\u73af\u5883\u7684\u5b89\u5168\u9632\u62a4\u80fd\u529b\u3002"}}
{"id": "2512.05374", "categories": ["cs.CR", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2512.05374", "abs": "https://arxiv.org/abs/2512.05374", "authors": ["Charlie Summers", "Haneen Mohammed", "Eugene Wu"], "title": "Please Don't Kill My Vibe: Empowering Agents with Data Flow Control", "comment": "7 pages, 7 figures, CIDR 2026", "summary": "The promise of Large Language Model (LLM) agents is to perform complex, stateful tasks. This promise is stunted by significant risks - policy violations, process corruption, and security flaws - that stem from the lack of visibility and mechanisms to manage undesirable data flows produced by agent actions. Today, agent workflows are responsible for enforcing these policies in ad hoc ways. Just as data validation and access controls shifted from the application to the DBMS, freeing application developers from these concerns, we argue that systems should support Data Flow Controls (DFCs) and enforce DFC policies natively. This paper describes early work developing a portable instance of DFC for DBMSes and outlines a broader research agenda toward DFC for agent ecosystems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e3aLLM\u667a\u80fd\u4f53\u7cfb\u7edf\u5f15\u5165\u539f\u751f\u6570\u636e\u6d41\u63a7\u5236\u673a\u5236\uff0c\u4ee5\u89e3\u51b3\u5f53\u524d\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4e2d\u5b58\u5728\u7684\u7b56\u7565\u8fdd\u89c4\u3001\u6d41\u7a0b\u8150\u8d25\u548c\u5b89\u5168\u6f0f\u6d1e\u7b49\u98ce\u9669\u3002", "motivation": "\u5f53\u524dLLM\u667a\u80fd\u4f53\u5728\u6267\u884c\u590d\u6742\u4efb\u52a1\u65f6\u5b58\u5728\u91cd\u5927\u98ce\u9669\uff0c\u5305\u62ec\u7b56\u7565\u8fdd\u89c4\u3001\u6d41\u7a0b\u8150\u8d25\u548c\u5b89\u5168\u6f0f\u6d1e\uff0c\u8fd9\u4e9b\u95ee\u9898\u6e90\u4e8e\u7f3a\u4e4f\u5bf9\u667a\u80fd\u4f53\u884c\u4e3a\u4ea7\u751f\u7684\u4e0d\u826f\u6570\u636e\u6d41\u7684\u53ef\u89c1\u6027\u548c\u7ba1\u7406\u673a\u5236\u3002\u76ee\u524d\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u53ea\u80fd\u4ee5\u4e34\u65f6\u65b9\u5f0f\u6267\u884c\u7b56\u7565\u63a7\u5236\u3002", "method": "\u501f\u9274\u6570\u636e\u5e93\u7ba1\u7406\u7cfb\u7edf\u5c06\u6570\u636e\u9a8c\u8bc1\u548c\u8bbf\u95ee\u63a7\u5236\u4ece\u5e94\u7528\u5c42\u8f6c\u79fb\u5230DBMS\u5c42\u7684\u601d\u8def\uff0c\u63d0\u51fa\u7cfb\u7edf\u5e94\u539f\u751f\u652f\u6301\u6570\u636e\u6d41\u63a7\u5236\u5e76\u5f3a\u5236\u6267\u884cDFC\u7b56\u7565\u3002\u8bba\u6587\u63cf\u8ff0\u4e86\u4e3aDBMS\u5f00\u53d1\u53ef\u79fb\u690dDFC\u5b9e\u4f8b\u7684\u65e9\u671f\u5de5\u4f5c\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86DFC\u6846\u67b6\u7684\u521d\u6b65\u5b9e\u73b0\uff0c\u5e76\u6982\u8ff0\u4e86\u4e3a\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\u5f00\u53d1DFC\u7684\u66f4\u5e7f\u6cdb\u7814\u7a76\u8bae\u7a0b\u3002", "conclusion": "\u9700\u8981\u4e3aLLM\u667a\u80fd\u4f53\u7cfb\u7edf\u5efa\u7acb\u539f\u751f\u6570\u636e\u6d41\u63a7\u5236\u673a\u5236\uff0c\u5c31\u50cf\u6570\u636e\u5e93\u7ba1\u7406\u7cfb\u7edf\u5c06\u6570\u636e\u63a7\u5236\u4ece\u5e94\u7528\u5c42\u8f6c\u79fb\u5230\u7cfb\u7edf\u5c42\u4e00\u6837\uff0c\u8fd9\u5c06\u4f7f\u667a\u80fd\u4f53\u5f00\u53d1\u8005\u4ece\u7b56\u7565\u6267\u884c\u8d1f\u62c5\u4e2d\u89e3\u653e\u51fa\u6765\uff0c\u63d0\u9ad8\u7cfb\u7edf\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2512.05459", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.05459", "abs": "https://arxiv.org/abs/2512.05459", "authors": ["Zheng Liu", "Chen Gong", "Terry Yue Zhuo", "Kecen Li", "Weichen Yu", "Matt Fredrikson", "Tianhao Wang"], "title": "PrivCode: When Code Generation Meets Differential Privacy", "comment": "Accepted at NDSS 2026; code available at https://github.com/Liuzzyg/PrivCode", "summary": "Large language models (LLMs) have presented outstanding performance in code generation and completion. However, fine-tuning these models on private datasets can raise privacy and proprietary concerns, such as the leakage of sensitive personal information. Differentially private (DP) code generation provides theoretical guarantees for protecting sensitive code by generating synthetic datasets that preserve statistical properties while reducing privacy leakage concerns. However, DP code generation faces significant challenges due to the strict syntactic dependencies and the privacy-utility trade-off.\n  We propose PrivCode, the first DP synthesizer specifically designed for code datasets. It incorporates a two-stage framework to improve both privacy and utility. In the first stage, termed \"privacy-sanitizing\", PrivCode generates DP-compliant synthetic code by training models using DP-SGD while introducing syntactic information to preserve code structure. The second stage, termed \"utility-boosting\", fine-tunes a larger pre-trained LLM on the synthetic privacy-free code to mitigate the utility loss caused by DP, enhancing the utility of the generated code. Extensive experiments on four LLMs show that PrivCode generates higher-utility code across various testing tasks under four benchmarks. The experiments also confirm its ability to protect sensitive data under varying privacy budgets. We provide the replication package at the anonymous link.", "AI": {"tldr": "PrivCode\uff1a\u9996\u4e2a\u4e13\u4e3a\u4ee3\u7801\u6570\u636e\u96c6\u8bbe\u8ba1\u7684\u5dee\u5206\u9690\u79c1\u5408\u6210\u5668\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\u63d0\u5347\u9690\u79c1\u4fdd\u62a4\u548c\u4ee3\u7801\u6548\u7528", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u4f1a\u5f15\u53d1\u9690\u79c1\u548c\u4e13\u6709\u4fe1\u606f\u6cc4\u9732\u95ee\u9898\u3002\u5dee\u5206\u9690\u79c1\u4ee3\u7801\u751f\u6210\u867d\u7136\u63d0\u4f9b\u7406\u8bba\u4fdd\u62a4\uff0c\u4f46\u9762\u4e34\u4e25\u683c\u7684\u8bed\u6cd5\u4f9d\u8d56\u6027\u548c\u9690\u79c1-\u6548\u7528\u6743\u8861\u7684\u6311\u6218", "method": "\u63d0\u51faPrivCode\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\"\u9690\u79c1\u51c0\u5316\"\u4f7f\u7528DP-SGD\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u5dee\u5206\u9690\u79c1\u5408\u89c4\u7684\u5408\u6210\u4ee3\u7801\uff0c\u540c\u65f6\u5f15\u5165\u8bed\u6cd5\u4fe1\u606f\u4fdd\u6301\u4ee3\u7801\u7ed3\u6784\uff1b\u7b2c\u4e8c\u9636\u6bb5\"\u6548\u7528\u63d0\u5347\"\u5728\u5408\u6210\u9690\u79c1\u4ee3\u7801\u4e0a\u5fae\u8c03\u5927\u578b\u9884\u8bad\u7ec3LLM\uff0c\u51cf\u8f7bDP\u5e26\u6765\u7684\u6548\u7528\u635f\u5931", "result": "\u5728\u56db\u4e2aLLM\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPrivCode\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u7684\u5404\u79cd\u4efb\u52a1\u4e2d\u751f\u6210\u66f4\u9ad8\u6548\u7528\u7684\u4ee3\u7801\u3002\u5b9e\u9a8c\u8fd8\u8bc1\u5b9e\u4e86\u5728\u4e0d\u540c\u9690\u79c1\u9884\u7b97\u4e0b\u4fdd\u62a4\u654f\u611f\u6570\u636e\u7684\u80fd\u529b", "conclusion": "PrivCode\u662f\u9996\u4e2a\u4e13\u4e3a\u4ee3\u7801\u6570\u636e\u96c6\u8bbe\u8ba1\u7684\u5dee\u5206\u9690\u79c1\u5408\u6210\u5668\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u9690\u79c1\u4fdd\u62a4\u548c\u4ee3\u7801\u6548\u7528\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u4fdd\u62a4\u654f\u611f\u4ee3\u7801\u6570\u636e\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.05485", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.05485", "abs": "https://arxiv.org/abs/2512.05485", "authors": ["Xiuyuan Chen", "Jian Zhao", "Yuxiang He", "Yuan Xun", "Xinwei Liu", "Yanshu Li", "Huilin Zhou", "Wei Cai", "Ziyan Shi", "Yuchen Yuan", "Tianle Zhang", "Chi Zhang", "Xuelong Li"], "title": "TeleAI-Safety: A comprehensive LLM jailbreaking benchmark towards attacks, defenses, and evaluations", "comment": null, "summary": "While the deployment of large language models (LLMs) in high-value industries continues to expand, the systematic assessment of their safety against jailbreak and prompt-based attacks remains insufficient. Existing safety evaluation benchmarks and frameworks are often limited by an imbalanced integration of core components (attack, defense, and evaluation methods) and an isolation between flexible evaluation frameworks and standardized benchmarking capabilities. These limitations hinder reliable cross-study comparisons and create unnecessary overhead for comprehensive risk assessment. To address these gaps, we present TeleAI-Safety, a modular and reproducible framework coupled with a systematic benchmark for rigorous LLM safety evaluation. Our framework integrates a broad collection of 19 attack methods (including one self-developed method), 29 defense methods, and 19 evaluation methods (including one self-developed method). With a curated attack corpus of 342 samples spanning 12 distinct risk categories, the TeleAI-Safety benchmark conducts extensive evaluations across 14 target models. The results reveal systematic vulnerabilities and model-specific failure cases, highlighting critical trade-offs between safety and utility, and identifying potential defense patterns for future optimization. In practical scenarios, TeleAI-Safety can be flexibly adjusted with customized attack, defense, and evaluation combinations to meet specific demands. We release our complete code and evaluation results to facilitate reproducible research and establish unified safety baselines.", "AI": {"tldr": "TeleAI-Safety\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u590d\u73b0\u7684LLM\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\u4e0e\u57fa\u51c6\uff0c\u6574\u5408\u4e8619\u79cd\u653b\u51fb\u65b9\u6cd5\u300129\u79cd\u9632\u5fa1\u65b9\u6cd5\u548c19\u79cd\u8bc4\u4f30\u65b9\u6cd5\uff0c\u572812\u4e2a\u98ce\u9669\u7c7b\u522b\u4e0a\u8bc4\u4f3014\u4e2a\u76ee\u6807\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u7cfb\u7edf\u6f0f\u6d1e\u548c\u5b89\u5168\u4e0e\u6548\u7528\u7684\u6743\u8861\u3002", "motivation": "\u5f53\u524dLLM\u5b89\u5168\u8bc4\u4f30\u5b58\u5728\u4e0d\u8db3\uff1a\u73b0\u6709\u57fa\u51c6\u548c\u6846\u67b6\u5728\u6838\u5fc3\u7ec4\u4ef6\uff08\u653b\u51fb\u3001\u9632\u5fa1\u3001\u8bc4\u4f30\u65b9\u6cd5\uff09\u6574\u5408\u4e0d\u5e73\u8861\uff0c\u4e14\u7075\u6d3b\u8bc4\u4f30\u6846\u67b6\u4e0e\u6807\u51c6\u5316\u57fa\u51c6\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u9694\u79bb\uff0c\u8fd9\u963b\u788d\u4e86\u53ef\u9760\u7684\u8de8\u7814\u7a76\u6bd4\u8f83\uff0c\u5e76\u7ed9\u5168\u9762\u98ce\u9669\u8bc4\u4f30\u5e26\u6765\u4e0d\u5fc5\u8981\u7684\u8d1f\u62c5\u3002", "method": "\u63d0\u51fa\u4e86TeleAI-Safety\u6846\u67b6\uff0c\u5305\u542b\u6a21\u5757\u5316\u3001\u53ef\u590d\u73b0\u7684\u8bbe\u8ba1\uff0c\u6574\u5408\u4e8619\u79cd\u653b\u51fb\u65b9\u6cd5\uff08\u542b1\u79cd\u81ea\u7814\uff09\u300129\u79cd\u9632\u5fa1\u65b9\u6cd5\u548c19\u79cd\u8bc4\u4f30\u65b9\u6cd5\uff08\u542b1\u79cd\u81ea\u7814\uff09\u3002\u4f7f\u7528\u5305\u542b12\u4e2a\u98ce\u9669\u7c7b\u522b342\u4e2a\u6837\u672c\u7684\u653b\u51fb\u8bed\u6599\u5e93\uff0c\u5bf914\u4e2a\u76ee\u6807\u6a21\u578b\u8fdb\u884c\u5e7f\u6cdb\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u63ed\u793a\u4e86\u7cfb\u7edf\u6f0f\u6d1e\u548c\u6a21\u578b\u7279\u5b9a\u7684\u5931\u8d25\u6848\u4f8b\uff0c\u7a81\u51fa\u4e86\u5b89\u5168\u4e0e\u6548\u7528\u4e4b\u95f4\u7684\u5173\u952e\u6743\u8861\uff0c\u5e76\u8bc6\u522b\u4e86\u672a\u6765\u4f18\u5316\u7684\u6f5c\u5728\u9632\u5fa1\u6a21\u5f0f\u3002\u6846\u67b6\u53ef\u6839\u636e\u5b9e\u9645\u9700\u6c42\u7075\u6d3b\u8c03\u6574\u653b\u51fb\u3001\u9632\u5fa1\u548c\u8bc4\u4f30\u7ec4\u5408\u3002", "conclusion": "TeleAI-Safety\u4e3aLLM\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7edf\u4e00\u3001\u53ef\u590d\u73b0\u7684\u6846\u67b6\u548c\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u5efa\u7acb\u7edf\u4e00\u7684\u5b89\u5168\u57fa\u7ebf\uff0c\u4fc3\u8fdb\u53ef\u590d\u73b0\u7814\u7a76\uff0c\u5e76\u652f\u6301\u6839\u636e\u7279\u5b9a\u9700\u6c42\u8fdb\u884c\u7075\u6d3b\u8c03\u6574\u3002"}}
{"id": "2512.05518", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.05518", "abs": "https://arxiv.org/abs/2512.05518", "authors": ["Jason Vega", "Gagandeep Singh"], "title": "Matching Ranks Over Probability Yields Truly Deep Safety Alignment", "comment": null, "summary": "A frustratingly easy technique known as the prefilling attack has been shown to effectively circumvent the safety alignment of frontier LLMs by simply prefilling the assistant response with an affirmative prefix before decoding. In response, recent work proposed a supervised fine-tuning (SFT) defense using data augmentation to achieve a \\enquote{deep} safety alignment, allowing the model to generate natural language refusals immediately following harmful prefills. Unfortunately, we show in this work that the \"deep\" safety alignment produced by such an approach is in fact not very deep. A generalization of the prefilling attack, which we refer to as the Rank-Assisted Prefilling (RAP) attack, can effectively extract harmful content from models fine-tuned with the data augmentation defense by selecting low-probability \"harmful\" tokens from the top 20 predicted next tokens at each step (thus ignoring high-probability \"refusal\" tokens). We argue that this vulnerability is enabled due to the \"gaming\" of the SFT objective when the target distribution entropies are low, where low fine-tuning loss is achieved by shifting large probability mass to a small number of refusal tokens while neglecting the high ranks of harmful tokens. We then propose a new perspective on achieving deep safety alignment by matching the token ranks of the target distribution, rather than their probabilities. This perspective yields a surprisingly simple fix to the data augmentation defense based on regularizing the attention placed on harmful prefill tokens, an approach we call PRefill attEntion STOpping (PRESTO). Adding PRESTO yields up to a 4.7x improvement in the mean StrongREJECT score under RAP attacks across three popular open-source LLMs, with low impact to model utility.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRAP\u653b\u51fb\u53ef\u7ed5\u8fc7\u57fa\u4e8e\u6570\u636e\u589e\u5f3a\u7684SFT\u5b89\u5168\u5bf9\u9f50\u9632\u5fa1\uff0c\u5e76\u63d0\u51faPRESTO\u65b9\u6cd5\u901a\u8fc7\u6b63\u5219\u5316\u6709\u5bb3\u9884\u586b\u5145token\u7684\u6ce8\u610f\u529b\u6765\u63d0\u5347\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6570\u636e\u589e\u5f3a\u7684SFT\u9632\u5fa1\u65b9\u6cd5\u867d\u7136\u80fd\u8ba9\u6a21\u578b\u5728\u6709\u5bb3\u9884\u586b\u5145\u540e\u751f\u6210\u62d2\u7edd\u54cd\u5e94\uff0c\u4f46\u5b9e\u9645\u5b89\u5168\u5bf9\u9f50\u4e0d\u591f\"\u6df1\u5165\"\uff0c\u4ecd\u5b58\u5728\u6f0f\u6d1e\u3002", "method": "\u63d0\u51faRAP\u653b\u51fb\uff08Rank-Assisted Prefilling\uff09\uff0c\u901a\u8fc7\u4ecetop-20\u9884\u6d4btoken\u4e2d\u9009\u62e9\u4f4e\u6982\u7387\u7684\u6709\u5bb3token\u6765\u7ed5\u8fc7\u9632\u5fa1\uff1b\u7136\u540e\u63d0\u51faPRESTO\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u6709\u5bb3\u9884\u586b\u5145token\u7684\u6ce8\u610f\u529b\u6765\u5339\u914d\u76ee\u6807\u5206\u5e03\u7684token\u6392\u5e8f\u800c\u975e\u6982\u7387\u3002", "result": "PRESTO\u65b9\u6cd5\u5728\u4e09\u4e2a\u5f00\u6e90LLM\u4e0a\u4f7fRAP\u653b\u51fb\u4e0b\u7684StrongREJECT\u5206\u6570\u5e73\u5747\u63d0\u5347\u8fbe4.7\u500d\uff0c\u4e14\u5bf9\u6a21\u578b\u5b9e\u7528\u6027\u5f71\u54cd\u8f83\u5c0f\u3002", "conclusion": "\u57fa\u4e8e\u6982\u7387\u5339\u914d\u7684SFT\u5b89\u5168\u5bf9\u9f50\u5b58\u5728\u6839\u672c\u7f3a\u9677\uff0c\u5e94\u8f6c\u5411\u57fa\u4e8etoken\u6392\u5e8f\u5339\u914d\u7684\"\u6df1\u5ea6\"\u5b89\u5168\u5bf9\u9f50\uff0cPRESTO\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u5b9e\u73b0\u65b9\u6cd5\u3002"}}
{"id": "2512.05707", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.05707", "abs": "https://arxiv.org/abs/2512.05707", "authors": ["Ana-Maria Cretu", "Klim Kireev", "Amro Abdalla", "Wisdom Obinna", "Raphael Meier", "Sarah Adel Bargal", "Elissa M. Redmiles", "Carmela Troncoso"], "title": "Evaluating Concept Filtering Defenses against Child Sexual Abuse Material Generation by Text-to-Image Models", "comment": null, "summary": "We evaluate the effectiveness of child filtering to prevent the misuse of text-to-image (T2I) models to create child sexual abuse material (CSAM). First, we capture the complexity of preventing CSAM generation using a game-based security definition. Second, we show that current detection methods cannot remove all children from a dataset. Third, using an ethical proxy for CSAM (a child wearing glasses, hereafter, CWG), we show that even when only a small percentage of child images are left in the training dataset, there exist prompting strategies that generate CWG from a child-filtered T2I model using only a few more queries than when the model is trained on the unfiltered data. Fine-tuning the filtered model on child images further reduces the additional query overhead. We also show that reintroducing a concept is possible via fine-tuning even if filtering is perfect. Our results demonstrate that current filtering methods offer limited protection to closed-weight models and no protection to open-weight models, while reducing the generality of the model by hindering the generation of child-related concepts or changing their representation. We conclude by outlining challenges in conducting evaluations that establish robust evidence on the impact of AI safety mitigations for CSAM.", "AI": {"tldr": "\u5f53\u524d\u513f\u7ae5\u8fc7\u6ee4\u65b9\u6cd5\u5bf9\u9632\u6b62T2I\u6a21\u578b\u751f\u6210\u513f\u7ae5\u6027\u8650\u5f85\u6750\u6599\u6548\u679c\u6709\u9650\uff0c\u5373\u4f7f\u8fc7\u6ee4\u540e\u4ecd\u6709\u65b9\u6cd5\u901a\u8fc7\u5c11\u91cf\u989d\u5916\u67e5\u8be2\u751f\u6210\u513f\u7ae5\u5185\u5bb9\uff0c\u4e14\u5fae\u8c03\u53ef\u5b8c\u5168\u6062\u590d\u6982\u5ff5", "motivation": "\u8bc4\u4f30\u513f\u7ae5\u8fc7\u6ee4\u5728\u9632\u6b62\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u88ab\u6ee5\u7528\u4e8e\u751f\u6210\u513f\u7ae5\u6027\u8650\u5f85\u6750\u6599\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u63ed\u793a\u73b0\u6709\u5b89\u5168\u7f13\u89e3\u63aa\u65bd\u7684\u5c40\u9650\u6027", "method": "1) \u4f7f\u7528\u57fa\u4e8e\u6e38\u620f\u7684\u5b89\u5168\u5b9a\u4e49\u5206\u6790CSAM\u9884\u9632\u590d\u6742\u6027\uff1b2) \u8bc1\u660e\u5f53\u524d\u68c0\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u5b8c\u5168\u79fb\u9664\u6570\u636e\u96c6\u4e2d\u7684\u513f\u7ae5\u56fe\u50cf\uff1b3) \u4f7f\u7528\u6234\u773c\u955c\u513f\u7ae5\u4f5c\u4e3aCSAM\u7684\u4f26\u7406\u4ee3\u7406\uff0c\u6d4b\u8bd5\u8fc7\u6ee4\u540e\u6a21\u578b\u7684\u8106\u5f31\u6027\uff1b4) \u901a\u8fc7\u5fae\u8c03\u5b9e\u9a8c\u9a8c\u8bc1\u6982\u5ff5\u6062\u590d\u7684\u53ef\u80fd\u6027", "result": "\u5373\u4f7f\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u53ea\u6b8b\u7559\u5c11\u91cf\u513f\u7ae5\u56fe\u50cf\uff0c\u653b\u51fb\u8005\u4ecd\u53ef\u901a\u8fc7\u5c11\u91cf\u989d\u5916\u67e5\u8be2\u4ece\u8fc7\u6ee4\u540e\u7684\u6a21\u578b\u751f\u6210\u513f\u7ae5\u5185\u5bb9\u3002\u5fae\u8c03\u53ef\u8fdb\u4e00\u6b65\u964d\u4f4e\u67e5\u8be2\u5f00\u9500\uff0c\u751a\u81f3\u5b8c\u7f8e\u8fc7\u6ee4\u540e\u4e5f\u80fd\u901a\u8fc7\u5fae\u8c03\u6062\u590d\u6982\u5ff5\u3002\u5f53\u524d\u8fc7\u6ee4\u65b9\u6cd5\u5bf9\u95ed\u6e90\u6a21\u578b\u4fdd\u62a4\u6709\u9650\uff0c\u5bf9\u5f00\u6e90\u6a21\u578b\u65e0\u4fdd\u62a4\uff0c\u540c\u65f6\u635f\u5bb3\u6a21\u578b\u751f\u6210\u513f\u7ae5\u76f8\u5173\u6982\u5ff5\u7684\u80fd\u529b", "conclusion": "\u73b0\u6709\u513f\u7ae5\u8fc7\u6ee4\u65b9\u6cd5\u63d0\u4f9b\u7684\u4fdd\u62a4\u6709\u9650\uff0c\u9700\u8981\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u5efa\u7acbAI\u5b89\u5168\u7f13\u89e3\u63aa\u65bd\u5bf9\u9632\u6b62CSAM\u751f\u6210\u5f71\u54cd\u7684\u53ef\u9760\u8bc1\u636e"}}
{"id": "2512.05745", "categories": ["cs.CR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.05745", "abs": "https://arxiv.org/abs/2512.05745", "authors": ["Weikai Lu", "Ziqian Zeng", "Kehua Zhang", "Haoran Li", "Huiping Zhuang", "Ruidong Wang", "Cen Chen", "Hao Peng"], "title": "ARGUS: Defending Against Multimodal Indirect Prompt Injection via Steering Instruction-Following Behavior", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) are increasingly vulnerable to multimodal Indirect Prompt Injection (IPI) attacks, which embed malicious instructions in images, videos, or audio to hijack model behavior. Existing defenses, designed primarily for text-only LLMs, are unsuitable for countering these multimodal threats, as they are easily bypassed, modality-dependent, or generalize poorly. Inspired by activation steering researches, we hypothesize that a robust, general defense independent of modality can be achieved by steering the model's behavior in the representation space. Through extensive experiments, we discover that the instruction-following behavior of MLLMs is encoded in a subspace. Steering along directions within this subspace can enforce adherence to user instructions, forming the basis of a defense. However, we also found that a naive defense direction could be coupled with a utility-degrading direction, and excessive intervention strength harms model performance. To address this, we propose ARGUS, which searches for an optimal defense direction within the safety subspace that decouples from the utility degradation direction, further combining adaptive strength steering to achieve a better safety-utility trade-off. ARGUS also introduces lightweight injection detection stage to activate the defense on-demand, and a post-filtering stage to verify defense success. Experimental results show that ARGUS can achieve robust defense against multimodal IPI while maximally preserving the MLLM's utility.", "AI": {"tldr": "ARGUS\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u591a\u6a21\u6001\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u884c\u4e3a\u5f15\u5bfc\u5b9e\u73b0\u6a21\u6001\u65e0\u5173\u7684\u9c81\u68d2\u9632\u5fa1\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u6765\u81ea\u56fe\u50cf\u3001\u89c6\u9891\u3001\u97f3\u9891\u7b49\u6a21\u6001\u7684\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u5a01\u80c1\uff0c\u73b0\u6709\u9488\u5bf9\u7eaf\u6587\u672cLLM\u7684\u9632\u5fa1\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u591a\u6a21\u6001\u653b\u51fb\uff0c\u5b58\u5728\u6613\u88ab\u7ed5\u8fc7\u3001\u6a21\u6001\u4f9d\u8d56\u6216\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "method": "ARGUS\u65b9\u6cd5\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u9636\u6bb5\uff1a1\uff09\u5728\u5b89\u5168\u5b50\u7a7a\u95f4\u4e2d\u641c\u7d22\u6700\u4f18\u9632\u5fa1\u65b9\u5411\uff0c\u4f7f\u5176\u4e0e\u6027\u80fd\u9000\u5316\u65b9\u5411\u89e3\u8026\uff1b2\uff09\u5f15\u5165\u8f7b\u91cf\u7ea7\u6ce8\u5165\u68c0\u6d4b\u9636\u6bb5\uff0c\u6309\u9700\u6fc0\u6d3b\u9632\u5fa1\uff1b3\uff09\u540e\u8fc7\u6ee4\u9636\u6bb5\u9a8c\u8bc1\u9632\u5fa1\u6210\u529f\u3002\u901a\u8fc7\u81ea\u9002\u5e94\u5f3a\u5ea6\u5f15\u5bfc\u5b9e\u73b0\u5b89\u5168\u4e0e\u6027\u80fd\u7684\u6700\u4f73\u6743\u8861\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cARGUS\u80fd\u591f\u5b9e\u73b0\u5bf9\u591a\u6a21\u6001IPI\u653b\u51fb\u7684\u9c81\u68d2\u9632\u5fa1\uff0c\u540c\u65f6\u6700\u5927\u9650\u5ea6\u5730\u4fdd\u6301MLLM\u7684\u5b9e\u7528\u6027\uff0c\u5728\u5b89\u5168\u6027\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002", "conclusion": "\u901a\u8fc7\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u884c\u4e3a\u5f15\u5bfc\uff0c\u53ef\u4ee5\u6784\u5efa\u72ec\u7acb\u4e8e\u6a21\u6001\u7684\u901a\u7528\u9632\u5fa1\u673a\u5236\uff0cARGUS\u901a\u8fc7\u89e3\u8026\u9632\u5fa1\u65b9\u5411\u4e0e\u6027\u80fd\u9000\u5316\u65b9\u5411\u3001\u81ea\u9002\u5e94\u5f3a\u5ea6\u5f15\u5bfc\u548c\u591a\u9636\u6bb5\u9632\u5fa1\u67b6\u6784\uff0c\u6709\u6548\u5e94\u5bf9\u591a\u6a21\u6001\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u3002"}}
{"id": "2512.05951", "categories": ["cs.CR", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.05951", "abs": "https://arxiv.org/abs/2512.05951", "authors": ["Teofil Bodea", "Masanori Misono", "Julian Pritzi", "Patrick Sabanic", "Thore Sommer", "Harshavardhan Unnibhavi", "David Schall", "Nuno Santos", "Dimitrios Stavrakakis", "Pramod Bhatotia"], "title": "Trusted AI Agents in the Cloud", "comment": null, "summary": "AI agents powered by large language models are increasingly deployed as cloud services that autonomously access sensitive data, invoke external tools, and interact with other agents. However, these agents run within a complex multi-party ecosystem, where untrusted components can lead to data leakage, tampering, or unintended behavior. Existing Confidential Virtual Machines (CVMs) provide only per binary protection and offer no guarantees for cross-principal trust, accelerator-level isolation, or supervised agent behavior. We present Omega, a system that enables trusted AI agents by enforcing end-to-end isolation, establishing verifiable trust across all contributing principals, and supervising every external interaction with accountable provenance. Omega builds on Confidential VMs and Confidential GPUs to create a Trusted Agent Platform that hosts many agents within a single CVM using nested isolation. It also provides efficient multi-agent orchestration with cross-principal trust establishment via differential attestation, and a policy specification and enforcement framework that governs data access, tool usage, and inter-agent communication for data protection and regulatory compliance. Implemented on AMD SEV-SNP and NVIDIA H100, Omega fully secures agent state across CVM-GPU, and achieves high performance while enabling high-density, policy-compliant multi-agent deployments at cloud scale.", "AI": {"tldr": "Omega\u662f\u4e00\u4e2a\u53ef\u4fe1AI\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u9694\u79bb\u3001\u8de8\u4e3b\u4f53\u53ef\u9a8c\u8bc1\u4fe1\u4efb\u5efa\u7acb\u548c\u76d1\u7763\u5916\u90e8\u4ea4\u4e92\u6765\u4fdd\u62a4AI\u4ee3\u7406\uff0c\u652f\u6301\u4e91\u89c4\u6a21\u7684\u9ad8\u5bc6\u5ea6\u591a\u4ee3\u7406\u90e8\u7f72\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684AI\u4ee3\u7406\u4f5c\u4e3a\u4e91\u670d\u52a1\u90e8\u7f72\u65f6\u9762\u4e34\u591a\u4e3b\u4f53\u73af\u5883\u4e2d\u7684\u5b89\u5168\u98ce\u9669\uff0c\u73b0\u6709\u673a\u5bc6\u865a\u62df\u673a\u4ec5\u63d0\u4f9b\u4e8c\u8fdb\u5236\u7ea7\u522b\u4fdd\u62a4\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u8de8\u4e3b\u4f53\u4fe1\u4efb\u3001\u52a0\u901f\u5668\u7ea7\u9694\u79bb\u6216\u4ee3\u7406\u884c\u4e3a\u76d1\u7763\u3002", "method": "\u57fa\u4e8eAMD SEV-SNP\u548cNVIDIA H100\u7684\u673a\u5bc6\u865a\u62df\u673a\u548c\u673a\u5bc6GPU\u6784\u5efa\u53ef\u4fe1\u4ee3\u7406\u5e73\u53f0\uff0c\u4f7f\u7528\u5d4c\u5957\u9694\u79bb\u5728\u5355\u4e2aCVM\u5185\u6258\u7ba1\u591a\u4e2a\u4ee3\u7406\uff0c\u901a\u8fc7\u5dee\u5f02\u8ba4\u8bc1\u5efa\u7acb\u8de8\u4e3b\u4f53\u4fe1\u4efb\uff0c\u5e76\u63d0\u4f9b\u7b56\u7565\u89c4\u8303\u548c\u6267\u884c\u6846\u67b6\u3002", "result": "Omega\u5b8c\u5168\u4fdd\u62a4\u4e86CVM-GPU\u95f4\u7684\u4ee3\u7406\u72b6\u6001\uff0c\u5728\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u652f\u6301\u4e91\u89c4\u6a21\u7684\u9ad8\u5bc6\u5ea6\u3001\u7b56\u7565\u5408\u89c4\u7684\u591a\u4ee3\u7406\u90e8\u7f72\u3002", "conclusion": "Omega\u7cfb\u7edf\u901a\u8fc7\u7aef\u5230\u7aef\u9694\u79bb\u3001\u53ef\u9a8c\u8bc1\u4fe1\u4efb\u5efa\u7acb\u548c\u76d1\u7763\u673a\u5236\uff0c\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u5b89\u5168\u4fdd\u969c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u673a\u5bc6\u865a\u62df\u673a\u5728\u591a\u4e3b\u4f53\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
