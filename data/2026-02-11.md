<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 25]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Framework for Integrating Zero Trust in Cloud-Based Endpoint Security for Critical Infrastructure](https://arxiv.org/abs/2602.09078)
*Shyam Kumar Gajula*

Main category: cs.CR

TL;DR: 论文提出一个针对关键基础设施的零信任架构框架，特别关注云环境中的端点管理，以应对日益复杂的网络威胁。


<details>
  <summary>Details</summary>
Motivation: 关键基础设施（如发电厂、医疗系统、金融系统、供水系统和军事资产）面临高度复杂的网络威胁和钓鱼攻击，需要更强大的安全模型来保护端点安全。当前在云环境中应用零信任架构进行端点管理存在空白。

Method: 提出一个综合框架，将定制的零信任架构集成到管理敏感操作的组织中。该框架将每个访问请求视为新的，不假设任何隐式信任。

Result: 零信任架构框架能够增强合规性，实现持续保护，从而减少攻击面。该框架特别针对云环境中的关键基础设施端点管理。

Conclusion: 零信任架构是应对关键基础设施网络安全挑战的有效安全模型，特别是在云环境端点管理方面。提出的框架能够填补现有应用空白，提供更强大的安全保护。

Abstract: Cyber threats have become highly sophisticated, prompting a heightened concern for endpoint security, especially in critical infrastructure, to new heights. A security model, such as Zero Trust Architecture (ZTA), is required to overcome this challenge. ZTA treats every access request as new and assumes no implicit trust. Critical infrastructure like power plants, healthcare systems, financial systems, water supply, and military assets are especially prone to becoming targets for hackers and phishing attacks. This proposes a comprehensive framework for integrating tailored ZTA into organizations that manage sensitive operations. The paper highlights how the ZTA framework can enhance compliance, enabling continuous protection, thereby reducing attack surfaces. This paper aims to address the gap that exists in applying ZTA to endpoint management within cloud environments for critical infrastructure.

</details>


### [2] [PICASSO: Scaling CHERI Use-After-Free Protection to Millions of Allocations using Colored Capabilities](https://arxiv.org/abs/2602.09131)
*Merve Gülmez,Ruben Sturm,Hossam ElAtali,Håkan Englund,Jonathan Woodruff,N. Asokan,Thomas Nyman*

Main category: cs.CR

TL;DR: PICASSO提出彩色能力机制，为CHERI架构添加时间内存安全，通过硬件管理的来源有效性表追踪能力来源，批量回收悬空指针，无需隔离释放内存，显著减少能力撤销扫描频率。


<details>
  <summary>Details</summary>
Motivation: CHERI架构扩展提供了强大的空间内存安全，但缺乏时间安全，特别是堆分配。现有增强CHERI时间安全的方法在可扩展性、内存开销和安全性方面存在不足，需要定期扫描系统内存来撤销陈旧能力。

Method: 引入彩色能力，为CHERI能力模型添加受控的间接层。通过硬件管理的来源有效性表追踪能力到各自分配的来源，实现批量回收悬空指针而无需隔离释放内存。在CHERI-RISC-V架构上实现PICASSO扩展，并集成到CheriBSD OS和CHERI-enabled Clang/LLVM工具链。

Result: 有效缓解NIST Juliet测试用例中所有基于堆的时间内存安全漏洞（use-after-free和double-free）。SPEC CPU基准测试仅5%几何平均性能开销，在SQLite、PostgreSQL和gRPC长时间运行工作负载中延迟更低、性能更稳定。

Conclusion: 彩色能力机制显著减少了能力撤销扫描频率，同时提高了安全性，为CHERI架构提供了高效的时间内存安全解决方案。

Abstract: While the CHERI instruction-set architecture extensions for capabilities enable strong spatial memory safety, CHERI lacks built-in temporal safety, particularly for heap allocations. Prior attempts to augment CHERI with temporal safety fall short in terms of scalability, memory overhead, and incomplete security guarantees due to periodical sweeps of the system's memory to individually revoke stale capabilities. We address these limitations by introducing colored capabilities that add a controlled form of indirection to CHERI's capability model. This enables provenance tracking of capabilities to their respective allocations via a hardware-managed provenance-validity table, allowing bulk retraction of dangling pointers without needing to quarantine freed memory. Colored capabilities significantly reduce the frequency of capability revocation sweeps while improving security. We realize colored capabilities in PICASSO, an extension of the CHERI-RISC-V architecture on a speculative out-of-order FPGA softcore (CHERI-Toooba). We also integrate colored-capability support into the CheriBSD OS and CHERI-enabled Clang/LLVM toolchain. Our evaluation shows effective mitigation of use-after-free and double-free bugs across all heap-based temporal memory-safety vulnerabilities in NIST Juliet test cases, with only a small performance overhead on SPEC CPU benchmarks (5% g.m.), less latency, and more consistent performance in long-running SQLite, PostgreSQL, and gRPC workloads compared to prior work.

</details>


### [3] [One RNG to Rule Them All: How Randomness Becomes an Attack Vector in Machine Learning](https://arxiv.org/abs/2602.09182)
*Kotekar Annapoorna Prabhu,Andrew Gan,Zahra Ghodsi*

Main category: cs.CR

TL;DR: RNGGuard是一个静态分析和运行时监控工具，用于保护机器学习系统中的随机数生成器安全，防止因框架、依赖和硬件差异导致的攻击向量。


<details>
  <summary>Details</summary>
Motivation: 机器学习严重依赖随机性（数据采样、增强、权重初始化等），但不同框架、软件依赖和硬件后端的伪随机数生成器实现差异以及缺乏统计验证，可能形成未探索的攻击向量。这些攻击隐蔽性强，在现实系统中已有被利用的历史。

Method: RNGGuard通过静态分析目标库的源代码，识别随机函数及其使用模块。在运行时，RNGGuard通过将不安全的函数调用替换为符合安全规范的RNGGuard实现来强制执行随机函数的安全执行。

Result: 评估表明RNGGuard提供了一种实用的方法来弥补机器学习系统中随机源安全方面的现有差距。

Conclusion: RNGGuard能够以较低的工作量帮助机器学习工程师保护其系统，有效防范针对随机数生成器的攻击。

Abstract: Machine learning relies on randomness as a fundamental component in various steps such as data sampling, data augmentation, weight initialization, and optimization. Most machine learning frameworks use pseudorandom number generators as the source of randomness. However, variations in design choices and implementations across different frameworks, software dependencies, and hardware backends along with the lack of statistical validation can lead to previously unexplored attack vectors on machine learning systems. Such attacks on randomness sources can be extremely covert, and have a history of exploitation in real-world systems. In this work, we examine the role of randomness in the machine learning development pipeline from an adversarial point of view, and analyze the implementations of PRNGs in major machine learning frameworks. We present RNGGuard to help machine learning engineers secure their systems with low effort. RNGGuard statically analyzes a target library's source code and identifies instances of random functions and modules that use them. At runtime, RNGGuard enforces secure execution of random functions by replacing insecure function calls with RNGGuard's implementations that meet security specifications. Our evaluations show that RNGGuard presents a practical approach to close existing gaps in securing randomness sources in machine learning systems.

</details>


### [4] [MUZZLE: Adaptive Agentic Red-Teaming of Web Agents Against Indirect Prompt Injection Attacks](https://arxiv.org/abs/2602.09222)
*Georgios Syros,Evan Rose,Brian Grinstead,Christoph Kerschbaumer,William Robertson,Cristina Nita-Rotaru,Alina Oprea*

Main category: cs.CR

TL;DR: MUZZLE是一个自动化框架，用于评估基于LLM的网页代理对间接提示注入攻击的安全性，通过分析代理轨迹自适应生成攻击，发现了多种新攻击类型。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的网页代理在处理不受信任的网页内容时容易受到间接提示注入攻击，现有评估方法依赖固定攻击模板或手动选择注入点，无法捕捉实际中的自适应攻击。

Method: MUZZLE利用代理的执行轨迹自动识别高显著性的注入点，自适应生成针对机密性、完整性和可用性违规的上下文感知恶意指令，并根据观察到的执行轨迹调整攻击策略，通过失败执行的反馈迭代优化攻击。

Result: 在多样化的网页应用、用户任务和代理配置中，MUZZLE自动发现了37个新攻击，涉及4个网页应用和10个对抗目标，包括跨应用提示注入攻击和针对代理的钓鱼场景。

Conclusion: MUZZLE能够以最少的人工干预自动、自适应地评估网页代理的安全性，有效发现现有方法遗漏的新型攻击策略，为网页代理安全评估提供了更全面的框架。

Abstract: Large language model (LLM) based web agents are increasingly deployed to automate complex online tasks by directly interacting with web sites and performing actions on users' behalf. While these agents offer powerful capabilities, their design exposes them to indirect prompt injection attacks embedded in untrusted web content, enabling adversaries to hijack agent behavior and violate user intent. Despite growing awareness of this threat, existing evaluations rely on fixed attack templates, manually selected injection surfaces, or narrowly scoped scenarios, limiting their ability to capture realistic, adaptive attacks encountered in practice. We present MUZZLE, an automated agentic framework for evaluating the security of web agents against indirect prompt injection attacks. MUZZLE utilizes the agent's trajectories to automatically identify high-salience injection surfaces, and adaptively generate context-aware malicious instructions that target violations of confidentiality, integrity, and availability. Unlike prior approaches, MUZZLE adapts its attack strategy based on the agent's observed execution trajectory and iteratively refines attacks using feedback from failed executions. We evaluate MUZZLE across diverse web applications, user tasks, and agent configurations, demonstrating its ability to automatically and adaptively assess the security of web agents with minimal human intervention. Our results show that MUZZLE effectively discovers 37 new attacks on 4 web applications with 10 adversarial objectives that violate confidentiality, availability, or privacy properties. MUZZLE also identifies novel attack strategies, including 2 cross-application prompt injection attacks and an agent-tailored phishing scenario.

</details>


### [5] [Atlas: Enabling Cross-Vendor Authentication for IoT](https://arxiv.org/abs/2602.09263)
*Sanket Goutam,Omar Chowdhury,Amir Rahmati*

Main category: cs.CR

TL;DR: Atlas框架通过扩展Web公钥基础设施到物联网，为设备颁发X.509证书，实现跨供应商的直接设备间认证和通信，减少云依赖和延迟。


<details>
  <summary>Details</summary>
Motivation: 当前云中介的物联网架构存在认证碎片化、跨供应商设备交互延迟高和可用性瓶颈问题，需要一种去中心化的认证解决方案。

Method: 扩展Web PKI到物联网，通过供应商操作的ACME客户端和DNS命名空间为设备颁发X.509证书，设备获得全局可验证身份，建立跨管理域的相互TLS通道。

Result: 证书配置每设备6秒内完成，mTLS仅增加约17ms延迟和适度CPU开销，相比云中介基准，Atlas应用保持低且可预测的延迟。

Conclusion: Atlas提供立即可部署的解决方案，无需硬件更改，最小化基础设施变更，许多主要供应商已具备ACME兼容CA，可直接应用。

Abstract: Cloud-mediated IoT architectures fragment authentication across vendor silos and create latency and availability bottlenecks for cross-vendor device-to-device (D2D) interactions. We present Atlas, a framework that extends the Web public-key infrastructure to IoT by issuing X.509 certificates to devices via vendor-operated ACME clients and vendor-controlled DNS namespaces. Devices obtain globally verifiable identities without hardware changes and establish mutual TLS channels directly across administrative domains, decoupling runtime authentication from cloud reachability. We prototype Atlas on ESP32 and Raspberry Pi, integrate it with an MQTT-based IoT stack and an Atlas-aware cloud, and evaluate it in smart-home and smart-city workloads. Certificate provisioning completes in under 6s per device, mTLS adds only about 17ms of latency and modest CPU overhead, and Atlas-based applications sustain low, predictable latency compared to cloud-mediated baselines. Because many major vendors already rely on ACME-compatible CAs for their web services, Atlas is immediately deployable with minimal infrastructure changes.

</details>


### [6] [Benchmarking Knowledge-Extraction Attack and Defense on Retrieval-Augmented Generation](https://arxiv.org/abs/2602.09319)
*Zhisheng Qi,Utkarsh Sahu,Li Ma,Haoyu Han,Ryan Rossi,Franck Dernoncourt,Mahantesh Halappanavar,Nesreen Ahmed,Yushun Dong,Yue Zhao,Yu Zhang,Yu Wang*

Main category: cs.CR

TL;DR: 该论文提出了首个针对RAG系统知识提取攻击的系统性基准测试，统一了碎片化的攻击与防御研究，为开发隐私保护的RAG系统提供可复现的评估框架。


<details>
  <summary>Details</summary>
Motivation: RAG系统在知识密集型应用中广泛应用，但研究表明恶意查询可以提取敏感知识库内容，导致知识产权盗窃和隐私泄露。现有研究存在碎片化问题，缺乏统一的评估标准和数据集。

Method: 构建了首个系统性基准测试，涵盖广泛的攻击与防御策略、代表性检索嵌入模型、开源和闭源生成器，在统一实验框架下使用标准化协议和多个数据集进行评估。

Result: 通过整合实验环境并实现可复现、可比较的评估，该基准测试为面对知识提取威胁时开发隐私保护RAG系统提供了实用基础和可行见解。

Conclusion: 该基准测试填补了RAG系统安全评估的空白，为研究和开发隐私保护的RAG系统提供了系统性的评估工具和框架。

Abstract: Retrieval-Augmented Generation (RAG) has become a cornerstone of knowledge-intensive applications, including enterprise chatbots, healthcare assistants, and agentic memory management. However, recent studies show that knowledge-extraction attacks can recover sensitive knowledge-base content through maliciously crafted queries, raising serious concerns about intellectual property theft and privacy leakage. While prior work has explored individual attack and defense techniques, the research landscape remains fragmented, spanning heterogeneous retrieval embeddings, diverse generation models, and evaluations based on non-standardized metrics and inconsistent datasets. To address this gap, we introduce the first systematic benchmark for knowledge-extraction attacks on RAG systems. Our benchmark covers a broad spectrum of attack and defense strategies, representative retrieval embedding models, and both open- and closed-source generators, all evaluated under a unified experimental framework with standardized protocols across multiple datasets. By consolidating the experimental landscape and enabling reproducible, comparable evaluation, this benchmark provides actionable insights and a practical foundation for developing privacy-preserving RAG systems in the face of emerging knowledge extraction threats. Our code is available here.

</details>


### [7] [XMap: Fast Internet-wide IPv4 and IPv6 Network Scanner](https://arxiv.org/abs/2602.09333)
*Xiang Li,Zixuan Xie,Lu Sun,Yuqi Qiu,Zuyao Xu,Zheli Liu*

Main category: cs.CR

TL;DR: XMap是一款开源的网络扫描器，支持快速互联网范围的IPv4和IPv6网络研究扫描，是2020年首个支持快速IPv6全网扫描的工具，在学术界、工业界和政府领域产生了重要影响。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够进行快速互联网范围IPv4和IPv6网络研究扫描的工具，填补IPv6全网扫描工具的空白，为网络研究和安全分析提供高效支持。

Method: XMap作为开源网络扫描器，采用特定的架构设计实现快速扫描功能，最初作为2021年DSN会议论文的研究成果开发，并在GitHub上开源发布。

Result: XMap在五年内产生了重大影响：被52篇研究论文引用（包括15篇顶级安全会议和11篇主要网络学会论文），获得450+ GitHub星标，被多家国际公司部署或推荐，并促进了RFC文档的实施和多个漏洞的发现。

Conclusion: XMap是一款成功的开源网络扫描工具，在IPv6全网扫描领域具有开创性意义，对学术研究、工业应用和政府网络安全都产生了深远影响，证明了其技术价值和实用性。

Abstract: XMap is an open-source network scanner designed for performing fast Internet-wide IPv4 and IPv6 network research scanning. XMap was initially developed as the research artifact of a paper published at 2021 IEEE/IFIP International Conference on Dependable Systems and Networks (DSN '21) and then made available on GitHub. XMap is the first tool to support fast Internet-wide IPv6 network scanning in 2020. During the last five years, XMap has made substantial impact in academia, industry, and government. It has been referenced in 52 research papers (15 published at top-tier security venues and 11 in leading networking societies), received over 450 GitHub stars, featured in multiple news outlets, and deployed or recommended by international companies up to date. Additionally, XMap has contributed to the implementation of RFC documents and the discovery of various vulnerabilities. This paper provides fundamental details about XMap, its architecture, and its impact.

</details>


### [8] [Privacy Amplification for BandMF via $b$-Min-Sep Subsampling](https://arxiv.org/abs/2602.09338)
*Andy Dong,Arun Ganesh*

Main category: cs.CR

TL;DR: 提出b-min-sep子采样方案，用于带相关噪声的DP-SGD（BandMF），相比现有方案在低噪声区域提供更强的隐私放大效果，并支持多归属用户级隐私设置。


<details>
  <summary>Details</summary>
Motivation: 现有BandMF（带相关噪声的DP-SGD）的子采样方案如循环泊松采样存在局限性，需要更优的子采样策略来增强隐私放大效果，特别是在中低噪声区域，同时需要支持多归属用户级隐私设置。

Method: 提出b-min-sep子采样方案，推广了泊松和balls-in-bins子采样，扩展了BandMF的批处理策略。使用基于马尔可夫结构的动态规划和蒙特卡洛计算进行近精确的隐私分析。

Result: b-min-sep在高噪声区域与循环泊松采样效果相当，在中低噪声区域提供严格更好的隐私保证。实验验证了该方案的有效性，并能自然扩展到多归属用户级隐私设置。

Conclusion: b-min-sep子采样方案为BandMF提供了更优的隐私放大效果，特别是在中低噪声区域，同时保持了分析所需的结构特性，并支持更广泛的多归属用户级隐私应用场景。

Abstract: We study privacy amplification for BandMF, i.e., DP-SGD with correlated noise across iterations via a banded correlation matrix. We propose $b$-min-sep subsampling, a new subsampling scheme that generalizes Poisson and balls-in-bins subsampling, extends prior practical batching strategies for BandMF, and enables stronger privacy amplification than cyclic Poisson while preserving the structural properties needed for analysis. We give a near-exact privacy analysis using Monte Carlo accounting, based on a dynamic program that leverages the Markovian structure in the subsampling procedure. We show that $b$-min-sep matches cyclic Poisson subsampling in the high noise regime and achieves strictly better guarantees in the mid-to-low noise regime, with experimental results that bolster our claims. We further show that unlike previous BandMF subsampling schemes, our $b$-min-sep subsampling naturally extends to the multi-attribution user-level privacy setting.

</details>


### [9] [Timing and Memory Telemetry on GPUs for AI Governance](https://arxiv.org/abs/2602.09369)
*Saleh K. Monfared,Fatemeh Ganji,Dan Holcomb,Shahin Tajik*

Main category: cs.CR

TL;DR: 提出基于GPU计算活动的测量框架，通过四种原语生成可观测信号，用于监控已部署GPU的使用情况，解决硬件不可信环境下的治理问题。


<details>
  <summary>Details</summary>
Motivation: 随着GPU加速计算的快速发展，大规模AI应用日益普及，但部署后的GPU缺乏可信监控机制，存在被滥用于训练模型、规避使用政策或脱离法律监管的风险。当前GPU提供的可信遥测有限，且可能被对手修改或虚拟化，因此需要探索在主机和设备都不可信的情况下，基于计算活动提供可操作的利用率信号。

Method: 设计了一个测量框架，利用现代GPU的架构特性生成基于时间和内存的可观测信号。该框架包含四种互补的原语：1）受工作量证明启发的概率性、工作负载驱动机制，用于暴露并行计算努力；2）基于可验证延迟函数的顺序、延迟敏感工作负载，用于表征标量执行压力；3）基于通用矩阵乘法的张量核心测量，反映密集线性代数吞吐量；4）VRAM驻留测试，通过带宽依赖的哈希区分设备内存局部性和片外访问。

Result: 评估了这些原语在竞争、架构对齐、内存压力和功耗开销下的响应，表明时间偏移和驻留延迟能够揭示有意义的利用率模式。结果显示基于计算的遥测可以暴露与GPU治理相关的架构信号。

Conclusion: 基于计算的遥测可以补充未来的问责机制，为部署后GPU治理提供架构层面的可观测信号，即使在缺乏可信固件、安全飞地或厂商控制计数器的情况下也能保持可观测性。

Abstract: The rapid expansion of GPU-accelerated computing has enabled major advances in large-scale artificial intelligence (AI), while heightening concerns about how accelerators are observed or governed once deployed. Governance is essential to ensure that large-scale compute infrastructure is not silently repurposed for training models, circumventing usage policies, or operating outside legal oversight. Because current GPUs expose limited trusted telemetry and can be modified or virtualized by adversaries, we explore whether compute-based measurements can provide actionable signals of utilization when host and device are untrusted. We introduce a measurement framework that leverages architectural characteristics of modern GPUs to generate timing- and memory-based observables that correlate with compute activity. Our design draws on four complementary primitives: (1) a probabilistic, workload-driven mechanism inspired by Proof-of-Work (PoW) to expose parallel effort, (2) sequential, latency-sensitive workloads derived via Verifiable Delay Functions (VDFs) to characterize scalar execution pressure, (3) General Matrix Multiplication (GEMM)-based tensor-core measurements that reflect dense linear-algebra throughput, and (4) a VRAM-residency test that distinguishes on-device memory locality from off-chip access through bandwidth-dependent hashing. These primitives provide statistical and behavioral indicators of GPU engagement that remain observable even without trusted firmware, enclaves, or vendor-controlled counters. We evaluate their responses to contention, architectural alignment, memory pressure, and power overhead, showing that timing shifts and residency latencies reveal meaningful utilization patterns. Our results illustrate why compute-based telemetry can complement future accountability mechanisms by exposing architectural signals relevant to post-deployment GPU governance.

</details>


### [10] [LLMAC: A Global and Explainable Access Control Framework with Large Language Model](https://arxiv.org/abs/2602.09392)
*Sharif Noor Zisad,Ragib Hasan*

Main category: cs.CR

TL;DR: LLMAC：利用大语言模型统一RBAC、ABAC、DAC等多种访问控制方法，实现动态、可解释的访问决策，准确率达98.5%


<details>
  <summary>Details</summary>
Motivation: 传统访问控制方法（RBAC、ABAC、DAC）各自为政，无法有效处理现代系统所需的动态、情境依赖的工作流程，需要统一、灵活且可解释的解决方案

Method: 提出LLMAC框架，利用大语言模型（LLM）统一多种访问控制方法；使用包含所有权验证、版本管理、工作流流程、动态角色分离等复杂场景的合成数据集训练Mistral 7B模型

Result: LLMAC模型达到98.5%的准确率，显著优于传统方法（RBAC: 14.5%、ABAC: 58.5%、DAC: 27.5%），并能提供清晰的人类可读决策解释；性能测试显示系统具有实际部署的可行性

Conclusion: LLMAC为复杂动态访问控制提供了统一、可解释的解决方案，在准确性和可理解性方面显著优于传统方法，具有实际部署价值

Abstract: Today's business organizations need access control systems that can handle complex, changing security requirements that go beyond what traditional methods can manage. Current approaches, such as Role-Based Access Control (RBAC), Attribute-Based Access Control (ABAC), and Discretionary Access Control (DAC), were designed for specific purposes. They cannot effectively manage the dynamic, situation-dependent workflows that modern systems require. In this research, we introduce LLMAC, a new unified approach using Large Language Models (LLMs) to combine these different access control methods into one comprehensive, understandable system. We used an extensive synthetic dataset that represents complex real-world scenarios, including policies for ownership verification, version management, workflow processes, and dynamic role separation. Using Mistral 7B, our trained LLM model achieved outstanding results with 98.5% accuracy, significantly outperforming traditional methods (RBAC: 14.5%, ABAC: 58.5%, DAC: 27.5%) while providing clear, human readable explanations for each decision. Performance testing shows that the system can be practically deployed with reasonable response times and computing resources.

</details>


### [11] [Understanding and Enhancing Encoder-based Adversarial Transferability against Large Vision-Language Models](https://arxiv.org/abs/2602.09431)
*Xinwei Zhang,Li Bai,Tianwei Zhang,Youqian Zhang,Qingqing Ye,Yingnan Zhao,Ruochen Du,Haibo Hu*

Main category: cs.CR

TL;DR: 本文首次系统研究了大视觉语言模型（LVLMs）中基于编码器的对抗样本可迁移性，发现现有攻击方法可迁移性有限，并提出新的语义引导多模态攻击框架SGMA来提升可迁移性。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型在多模态任务中取得了显著成功，但其对视觉输入的依赖使其面临严重的对抗威胁。现有基于编码器的攻击方法虽然计算效率高，但在实际黑盒场景中跨不同LVLM架构的可迁移性尚不清楚，需要系统研究。

Method: 首先在8个不同的LVLM上进行大规模基准测试，揭示现有攻击方法的可迁移性限制。然后进行深入分析，发现阻碍可迁移性的两个根本原因：1）模型间视觉定位不一致；2）模型内语义对齐冗余。最后提出语义引导多模态攻击（SGMA）框架，将扰动引导到语义关键区域，并在全局和局部层面破坏跨模态定位。

Result: 实验表明，现有攻击方法在跨模型可迁移性方面表现严重受限。SGMA在不同受害模型和任务上均实现了比现有攻击方法更高的可迁移性，暴露了LVLM部署中的关键安全风险。

Conclusion: 本研究首次系统探索了LVLMs中基于编码器的对抗样本可迁移性，揭示了现有攻击方法的局限性及其根本原因。提出的SGMA框架显著提升了攻击可迁移性，强调了开发鲁棒多模态防御的紧迫性。

Abstract: Large vision-language models (LVLMs) have achieved impressive success across multimodal tasks, but their reliance on visual inputs exposes them to significant adversarial threats. Existing encoder-based attacks perturb the input image by optimizing solely on the vision encoder, rather than the entire LVLM, offering a computationally efficient alternative to end-to-end optimization. However, their transferability across different LVLM architectures in realistic black-box scenarios remains poorly understood. To address this gap, we present the first systematic study towards encoder-based adversarial transferability in LVLMs. Our contributions are threefold. First, through large-scale benchmarking over eight diverse LVLMs, we reveal that existing attacks exhibit severely limited transferability. Second, we perform in-depth analysis, disclosing two root causes that hinder the transferability: (1) inconsistent visual grounding across models, where different models focus their attention on distinct regions; (2) redundant semantic alignment within models, where a single object is dispersed across multiple overlapping token representations. Third, we propose Semantic-Guided Multimodal Attack (SGMA), a novel framework to enhance the transferability. Inspired by the discovered causes in our analysis, SGMA directs perturbations toward semantically critical regions and disrupts cross-modal grounding at both global and local levels. Extensive experiments across different victim models and tasks show that SGMA achieves higher transferability than existing attacks. These results expose critical security risks in LVLM deployment and underscore the urgent need for robust multimodal defenses.

</details>


### [12] [Autonomous Action Runtime Management(AARM):A System Specification for Securing AI-Driven Actions at Runtime](https://arxiv.org/abs/2602.09433)
*Herman Errico*

Main category: cs.CR

TL;DR: AARM（自主行动运行时管理）是一个开放规范，用于在运行时保护AI驱动的行动，通过拦截行动、评估策略、执行授权决策并记录防篡改收据来应对AI代理安全挑战。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统从被动助手演变为能够执行重要行动的自主代理，安全边界从模型输出转移到工具执行。传统的安全范式（日志聚合、边界防御、事后取证）无法保护AI驱动的行动，因为这些行动可能是不可逆的、以机器速度执行的，并且可能来自受损的编排层。

Method: 提出AARM规范，定义运行时安全系统：1）在执行前拦截行动；2）累积会话上下文；3）评估策略和意图对齐；4）执行授权决策；5）记录防篡改收据用于取证重建。形式化威胁模型（提示注入、混淆代理攻击、数据泄露、意图漂移），引入行动分类框架（禁止、上下文相关拒绝、上下文相关允许），并提出四种实现架构（协议网关、SDK插桩、内核eBPF、供应商集成）。

Result: AARM规范是模型无关、框架无关和供应商中立的，将行动执行视为稳定的安全边界。该规范旨在在专有碎片化破坏互操作性之前建立行业范围的要求。

Conclusion: AARM为保护AI驱动的自主行动提供了一个标准化、可互操作的运行时安全框架，解决了传统安全方法无法应对的新兴威胁，并为行业建立了统一的安全标准。

Abstract: As artificial intelligence systems evolve from passive assistants into autonomous agents capable of executing consequential actions, the security boundary shifts from model outputs to tool execution. Traditional security paradigms - log aggregation, perimeter defense, and post-hoc forensics - cannot protect systems where AI-driven actions are irreversible, execute at machine speed, and originate from potentially compromised orchestration layers. This paper introduces Autonomous Action Runtime Management (AARM), an open specification for securing AI-driven actions at runtime. AARM defines a runtime security system that intercepts actions before execution, accumulates session context, evaluates against policy and intent alignment, enforces authorization decisions, and records tamper-evident receipts for forensic reconstruction. We formalize a threat model addressing prompt injection, confused deputy attacks, data exfiltration, and intent drift. We introduce an action classification framework distinguishing forbidden, context-dependent deny, and context-dependent allow actions. We propose four implementation architectures - protocol gateway, SDK instrumentation, kernel eBPF, and vendor integration - with distinct trust properties, and specify minimum conformance requirements for AARM-compliant systems. AARM is model-agnostic, framework-agnostic, and vendor-neutral, treating action execution as the stable security boundary. This specification aims to establish industry-wide requirements before proprietary fragmentation forecloses interoperability.

</details>


### [13] [A Behavioral Fingerprint for Large Language Models: Provenance Tracking via Refusal Vectors](https://arxiv.org/abs/2602.09434)
*Zhenyu Xu,Victor S. Sheng*

Main category: cs.CR

TL;DR: 提出基于安全对齐诱导行为模式的LLM指纹框架，利用拒绝向量进行溯源追踪，在76个衍生模型中实现100%准确识别，并探讨公开验证方案。


<details>
  <summary>Details</summary>
Motivation: 保护大语言模型知识产权，应对未经授权衍生模型的扩散问题，需要有效的溯源追踪方法。

Method: 利用安全对齐诱导的行为模式，从模型内部表示中提取拒绝向量作为行为指纹，通过有害与无害提示的定向模式差异构建指纹系统。

Result: 行为指纹对微调、合并、量化等常见修改具有高度鲁棒性，在76个衍生模型中实现100%准确识别，不同独立训练模型间余弦相似度低。

Conclusion: 基于安全对齐行为模式的指纹框架有效保护LLM知识产权，提出通过局部敏感哈希和零知识证明实现隐私保护的公开验证方案。

Abstract: Protecting the intellectual property of large language models (LLMs) is a critical challenge due to the proliferation of unauthorized derivative models. We introduce a novel fingerprinting framework that leverages the behavioral patterns induced by safety alignment, applying the concept of refusal vectors for LLM provenance tracking. These vectors, extracted from directional patterns in a model's internal representations when processing harmful versus harmless prompts, serve as robust behavioral fingerprints. Our contribution lies in developing a fingerprinting system around this concept and conducting extensive validation of its effectiveness for IP protection. We demonstrate that these behavioral fingerprints are highly robust against common modifications, including finetunes, merges, and quantization. Our experiments show that the fingerprint is unique to each model family, with low cosine similarity between independently trained models. In a large-scale identification task across 76 offspring models, our method achieves 100\% accuracy in identifying the correct base model family. Furthermore, we analyze the fingerprint's behavior under alignment-breaking attacks, finding that while performance degrades significantly, detectable traces remain. Finally, we propose a theoretical framework to transform this private fingerprint into a publicly verifiable, privacy-preserving artifact using locality-sensitive hashing and zero-knowledge proofs.

</details>


### [14] [ReSIM: Re-ranking Binary Similarity Embeddings to Improve Function Search Performance](https://arxiv.org/abs/2602.09548)
*Gianluca Capozzi,Anna Paola Giancaspro,Fabio Petroni,Leonardo Querzoni,Giuseppe Antonio Di Luna*

Main category: cs.CR

TL;DR: ReSIM：一种增强的二进制函数相似性搜索系统，通过神经重排序器补充嵌入搜索，显著提升搜索效果


<details>
  <summary>Details</summary>
Motivation: 现有二进制函数相似性系统使用双编码器架构独立嵌入函数，无法捕捉跨函数关系和相似性，限制了搜索准确性

Method: 提出ReSIM系统，在嵌入搜索基础上增加神经重排序模块，联合处理查询-候选对计算基于相互表示的排序分数

Result: 在7个嵌入模型和2个基准数据集上评估，平均nDCG提升21.7%，Recall提升27.8%

Conclusion: 神经重排序器能有效补充嵌入搜索，捕捉双编码器无法捕获的细粒度关系信息，显著提升二进制函数相似性搜索效果

Abstract: Binary Function Similarity (BFS), the problem of determining whether two binary functions originate from the same source code, has been extensively studied in recent research across security, software engineering, and machine learning communities. This interest arises from its central role in developing vulnerability detection systems, copyright infringement analysis, and malware phylogeny tools. Nearly all binary function similarity systems embed assembly functions into real-valued vectors, where similar functions map to points that lie close to each other in the metric space. These embeddings enable function search: a query function is embedded and compared against a database of candidate embeddings to retrieve the most similar matches.
  Despite their effectiveness, such systems rely on bi-encoder architectures that embed functions independently, limiting their ability to capture cross-function relationships and similarities. To address this limitation, we introduce ReSIM, a novel and enhanced function search system that complements embedding-based search with a neural re-ranker. Unlike traditional embedding models, our reranking module jointly processes query-candidate pairs to compute ranking scores based on their mutual representation, allowing for more accurate similarity assessment. By re-ranking the top results from embedding-based retrieval, ReSIM leverages fine-grained relation information that bi-encoders cannot capture.
  We evaluate ReSIM across seven embedding models on two benchmark datasets, demonstrating consistent improvements in search effectiveness, with average gains of 21.7% in terms of nDCG and 27.8% in terms of Recall.

</details>


### [15] [When Handshakes Tell the Truth: Detecting Web Bad Bots via TLS Fingerprints](https://arxiv.org/abs/2602.09606)
*Ghalia Jarad,Kemal Bicakci*

Main category: cs.CR

TL;DR: 使用JA4 TLS指纹识别技术结合梯度提升机器学习模型（XGBoost和CatBoost）来检测恶意机器人流量，CatBoost模型在测试集上达到0.9863的准确率。


<details>
  <summary>Details</summary>
Motivation: 网络自动化流量已超过人工流量，其中恶意机器人比例上升。现有方法可能被规避（如解决验证码、模仿人类行为），需要更隐蔽的协议层检测方法。

Method: 采用JA4 TLS指纹识别技术，从TLS握手参数提取特征。使用两个梯度提升机器学习分类器（XGBoost和CatBoost）在真实TLS指纹数据集（JA4DB）上进行训练和评估。

Result: CatBoost模型表现最佳，AUC为0.998，F1分数0.9734，测试集准确率0.9863。XGBoost模型结果相近。特征重要性分析显示ja4_b、cipher_count和ext_count对模型效果影响最大。

Conclusion: JA4 TLS指纹结合机器学习能有效区分机器人和真实用户。未来研究将扩展到新协议（如HTTP/3）并增加设备指纹特征，以应对更高级的机器人规避策略。

Abstract: Automated traffic continued to surpass human-generated traffic on the web, and a rising proportion of this automation was explicitly malicious. Evasive bots could pretend to be real users, even solve Captchas and mimic human interaction patterns. This work explores a less intrusive, protocol-level method: using TLS fingerprinting with the JA4 technique to tell apart bots from real users. Two gradient-boosted machine learning classifiers (XGBoost and CatBoost) were trained and evaluated on a dataset of real TLS fingerprints (JA4DB) after feature extraction, which derived informative signals from JA4 fingerprints that describe TLS handshake parameters. The CatBoost model performed better, achieving an AUC of 0.998 and an F1 score of 0.9734. It was accurate 0.9863 of the time on the test set. The XGBoost model showed almost similar results. Feature significance analyses identified JA4 components, especially ja4\_b, cipher\_count, and ext\_count, as the most influential on model effectiveness. Future research will extend this method to new protocols, such as HTTP/3, and add additional device-fingerprinting features to test how well the system resists advanced bot evasion tactics.

</details>


### [16] [Parallel Composition for Statistical Privacy](https://arxiv.org/abs/2602.09627)
*Dennis Breutigam,Rüdiger Reischuk*

Main category: cs.CR

TL;DR: 该论文研究了统计隐私(SP)下的多查询组合问题，提出了一种基于子采样和随机分区数据库的隐私机制，首次获得了对有限对手的无数据库限制的隐私上界，展示了在现实场景中考虑分布熵可以改善隐私和精度保证。


<details>
  <summary>Details</summary>
Motivation: 传统差分隐私(DP)假设对手几乎完全了解数据库条目，这种最坏情况假设可能高估实际中的隐私威胁。相比之下，统计隐私(SP)考虑对手只知道数据库条目的分布而非具体实现，需要分析分布熵与隐私机制之间的交互作用，这在多查询组合时尤为复杂。

Method: 提出基于子采样和随机分区数据库的隐私机制，通过限制查询之间的依赖关系来保护隐私。该方法首次在不限制数据库的情况下获得了对抗有限对手的隐私上界。

Result: 获得的隐私界限表明，在现实应用场景中考虑分布熵可以改善隐私和精度保证。示例显示，在固定隐私参数和效用损失下，SP允许的查询数量显著多于DP。

Conclusion: 统计隐私框架比传统差分隐私更贴近实际应用场景，通过考虑对手的有限知识（只知道分布而非具体数据），能够在保持相同隐私保护水平下支持更多查询，为实际隐私保护提供了更实用的理论框架。

Abstract: Differential Privacy (DP) considers a scenario in which an adversary has almost complete information about the entries of a database. This worst-case assumption is likely to overestimate the privacy threat faced by an individual in practice. In contrast, Statistical Privacy (SP), as well as related notions such as noiseless privacy or limited background knowledge privacy, describe a setting in which the adversary knows the distribution of the database entries, but not their exact realizations. In this case, privacy analysis must account for the interaction between uncertainty induced by the entropy of the underlying distributions and privacy mechanisms that distort query answers, which can be highly non-trivial.
  This paper investigates this problem for multiple queries (composition). A privacy mechanism is proposed that is based on subsampling and randomly partitioning the database to bound the dependency among queries. This way for the first time, to the best of our knowledge, upper privacy bounds against limited adversaries are obtained without any further restriction on the database.
  These bounds show that in realistic application scenarios taking the entropy of distributions into account yields improvements of privacy and precision guarantees. We illustrate examples where for fixed privacy parameters and utility loss SP allows significantly more queries than DP.

</details>


### [17] [Stop Testing Attacks, Start Diagnosing Defenses: The Four-Checkpoint Framework Reveals Where LLM Safety Breaks](https://arxiv.org/abs/2602.09629)
*Hayfa Dhabhi,Kashyap Thimmaraju*

Main category: cs.CR

TL;DR: 论文提出四检查点框架分析LLM安全防御，发现输出阶段防御最弱，传统二元评估低估了2.3倍漏洞风险。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽然证明越狱攻击能成功，但无法解释防御在何处失效以及为何失效。需要系统分析LLM安全防御的薄弱环节。

Method: 提出四检查点框架，按处理阶段（输入vs输出）和检测级别（字面vs意图）划分四个防御层。设计13种针对性规避技术，使用LLM-as-judge评估和加权攻击成功率指标。

Result: 传统二元攻击成功率22.6%，但加权攻击成功率显示52.7%漏洞（2.3倍更高）。输出阶段防御最弱（72-79% WASR），输入字面防御最强（13% WASR）。Claude安全性最佳（42.8% WASR）。

Conclusion: 当前防御在输入字面检查点最强，但对意图级操纵和输出阶段技术仍脆弱。四检查点框架为识别和解决部署系统中的安全漏洞提供了结构化方法。

Abstract: Large Language Models (LLMs) deploy safety mechanisms to prevent harmful outputs, yet these defenses remain vulnerable to adversarial prompts. While existing research demonstrates that jailbreak attacks succeed, it does not explain \textit{where} defenses fail or \textit{why}.
  To address this gap, we propose that LLM safety operates as a sequential pipeline with distinct checkpoints. We introduce the \textbf{Four-Checkpoint Framework}, which organizes safety mechanisms along two dimensions: processing stage (input vs.\ output) and detection level (literal vs.\ intent). This creates four checkpoints, CP1 through CP4, each representing a defensive layer that can be independently evaluated. We design 13 evasion techniques, each targeting a specific checkpoint, enabling controlled testing of individual defensive layers.
  Using this framework, we evaluate GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro across 3,312 single-turn, black-box test cases. We employ an LLM-as-judge approach for response classification and introduce Weighted Attack Success Rate (WASR), a severity-adjusted metric that captures partial information leakage overlooked by binary evaluation.
  Our evaluation reveals clear patterns. Traditional Binary ASR reports 22.6\% attack success. However, WASR reveals 52.7\%, a 2.3$\times$ higher vulnerability. Output-stage defenses (CP3, CP4) prove weakest at 72--79\% WASR, while input-literal defenses (CP1) are strongest at 13\% WASR. Claude achieves the strongest safety (42.8\% WASR), followed by GPT-5 (55.9\%) and Gemini (59.5\%).
  These findings suggest that current defenses are strongest at input-literal checkpoints but remain vulnerable to intent-level manipulation and output-stage techniques. The Four-Checkpoint Framework provides a structured approach for identifying and addressing safety vulnerabilities in deployed systems.

</details>


### [18] [PiTPM: Partially Interactive Signatures for Multi-Device TPM Operations](https://arxiv.org/abs/2602.09707)
*Yunusa Simpa Abdulsalam,Mustapha Hedabou*

Main category: cs.CR

TL;DR: PiTPM是一个基于TPM 2.0的非交互式多方签名框架，通过混合信任架构和预共享随机种子消除传统方案中的交互需求，实现常数大小的签名和显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有TPM-based多方签名方案存在根本性限制：需要所有参与者在承诺阶段进行交互式协调，导致同步瓶颈、二次通信复杂度和参与者失败导致的协议中止问题，这在跨设备密码操作应用中尤为突出。

Method: 基于Schnorr数字签名构建聚合器框架，采用混合信任架构，利用安全存储在聚合器中的预共享随机种子，实现无需参与者间通信的确定性全局承诺计算。

Result: 提出的框架产生常数大小的签名（与签名者数量无关），实验结果表明在TPM-based密码系统设计中可能实现范式转变，混合信任架构在保持严格安全保证的同时实现显著性能改进。

Conclusion: PiTPM框架通过消除交互需求解决了TPM-based多方签名的关键限制，在随机预言模型下基于离散对数假设证明了EU-CMA安全性，为分布式系统中的高效密码操作提供了新方向。

Abstract: Trusted Platform Module (TPM) 2.0 devices provide efficient hardware-based cryptographic security through tamper-resistant key storage and computation, making them ideal building blocks for multi-party signature schemes in distributed systems. However, existing TPM-based multi-signature constructions suffer from a fundamental limitation, they require interactive protocols where all participants must coordinate during the commitment phase, before any signature can be computed. This interactive requirement creates several critical problems, such as synchronization bottlenecks, quadratic communication complexity, and aborted protocols as a result of participant failure. These limitations become particularly heightened for applications that require cross-device cryptographic operations. This paper presents PiTPM, an Aggregator Framework built upon Schnorr's digital signature. Our protocol eliminates the interactive requirement using a hybrid trust architecture. The proposed framework uses pre-shared randomness seeds stored securely in an Aggregator, enabling deterministic computation of global commitments without inter-participant communication. The resulting signatures of the proposed framework are of constant size regardless of signer count. Our experimental results show a possible paradigm shift in TPM-based cryptographic system design, demonstrating that hybrid trust architectures can achieve significant performance improvements while maintaining rigorous security guarantees. We provide a comprehensive formal security analysis proving EU-CMA security under the discrete logarithm assumption in the random oracle model.

</details>


### [19] [QRS: A Rule-Synthesizing Neuro-Symbolic Triad for Autonomous Vulnerability Discovery](https://arxiv.org/abs/2602.09774)
*George Tsigkourakos,Constantinos Patsakis*

Main category: cs.CR

TL;DR: QRS是一个神经符号框架，通过三个自主代理生成CodeQL查询、验证发现并自动合成漏洞利用，显著提高SAST工具的检测准确率并减少误报。


<details>
  <summary>Details</summary>
Motivation: 现有SAST工具（如CodeQL、Semgrep、SonarQube）存在三个主要限制：需要专家编写查询、产生过多误报、只能检测预定义的漏洞模式。虽然已有研究尝试用LLM增强SAST，但通常只是用LLM筛选现有工具输出，而非直接推理漏洞语义。

Method: QRS采用神经符号框架，包含三个自主代理：1) 从结构化模式定义和少量示例生成CodeQL查询；2) 通过语义推理验证发现；3) 自动合成漏洞利用进行验证。该框架反转了传统范式，不是过滤静态规则结果，而是直接生成查询并验证。

Result: 在20个历史CVE测试中达到90.6%检测准确率。在100个最受欢迎的PyPI包中，发现39个中高危漏洞：5个获得新CVE编号，5个导致文档更新，29个被其他研究者独立发现。QRS具有低时间开销和可控的token成本。

Conclusion: LLM驱动的查询合成和代码审查可以补充人工编写的规则集，发现现有行业工具无法检测的漏洞模式，为SAST工具提供了新的发展方向。

Abstract: Static Application Security Testing (SAST) tools are integral to modern DevSecOps pipelines, yet tools like CodeQL, Semgrep, and SonarQube remain fundamentally constrained: they require expert-crafted queries, generate excessive false positives, and detect only predefined vulnerability patterns. Recent work has explored augmenting SAST with Large Language Models (LLMs), but these approaches typically use LLMs to triage existing tool outputs rather than to reason about vulnerability semantics directly. We introduce QRS (Query, Review, Sanitize), a neuro-symbolic framework that inverts this paradigm. Rather than filtering results from static rules, QRS employs three autonomous agents that generate CodeQL queries from a structured schema definition and few-shot examples, then validate findings through semantic reasoning and automated exploit synthesis. This architecture enables QRS to discover vulnerability classes beyond predefined patterns while substantially reducing false positives. We evaluate QRS on full Python packages rather than isolated snippets. In 20 historical CVEs in popular PyPI libraries, QRS achieves 90.6% detection accuracy. Applied to the 100 most-downloaded PyPI packages, QRS identified 39 medium-to-high-severity vulnerabilities, 5 of which were assigned new CVEs, 5 received documentation updates, while the remaining 29 were independently discovered by concurrent researchers, validating both the severity and discoverability of these findings. QRS accomplishes this with low time overhead and manageable token costs, demonstrating that LLM-driven query synthesis and code review can complement manually curated rule sets and uncover vulnerability patterns that evade existing industry tools.

</details>


### [20] [From Multi-sig to DLCs: Modern Oracle Designs on Bitcoin](https://arxiv.org/abs/2602.09822)
*Giulio Caldarelli*

Main category: cs.CR

TL;DR: 比特币Layer 1的预言机设计从多签模式转向基于证明的设计，特别是Discreet Log Contracts，在现实场景中展现出更强的可行性和社区支持。


<details>
  <summary>Details</summary>
Motivation: 比特币作为主要交易账本，其可编程性有限，难以支持依赖外部事件的智能合约。需要研究自2015年以太坊智能合约时代以来，比特币Layer 1是否出现了新的预言机设计，以及后续改进提案是否扩展了预言机的可实现性。

Method: 使用Scopus和Web of Science进行文献检索，辅以Google Scholar捕获协议提案，分析比特币Layer 1预言机设计的演变。

Result: 学术文献覆盖有限，许多贡献在期刊外流通。主要变化是从多签模式（预言机作为共同签名者）转向基于证明的设计，特别是Discreet Log Contracts，这些设计在比特币社区中具有更强的合规性、工具支持和实际应用证据。

Conclusion: 比特币Layer 1的预言机设计经历了显著演变，Discreet Log Contracts成为主流，在赌博和预测市场等现实场景中展现出实际可行性，但学术研究仍需加强。

Abstract: Unlike Ethereum, which was conceived as a general-purpose smart-contract platform, Bitcoin was designed primarily as a transaction ledger for its native currency, which limits programmability for conditional applications. This constraint is particularly evident when considering oracles, mechanisms that enable Bitcoin contracts to depend on exogenous events. This paper investigates whether new oracle designs have emerged for Bitcoin Layer 1 since the 2015 transition to the Ethereum smart contracts era and whether subsequent Bitcoin improvement proposals have expanded oracles' implementability. Using Scopus and Web of Science searches, complemented by Google Scholar to capture protocol proposals, we observe that the indexed academic coverage remains limited, and many contributions circulate outside journal venues. Within the retrieved corpus, the main post-2015 shift is from multisig-style, which envisioned oracles as co-signers, toward attestation-based designs, mainly represented by Discreet Log Contracts (DLCs), which show stronger Bitcoin community compliance, tool support, and evidence of practical implementations in real-world scenarios such as betting and prediction-market mechanisms.

</details>


### [21] [Spinel: A Post-Quantum Signature Scheme Based on SLn(Fp) Hashing](https://arxiv.org/abs/2602.09882)
*Asmaa Cherkaoui,Faraz Heravi,Delaram Kahrobaei,Siamak F. Shahandashti*

Main category: cs.CR

TL;DR: Spinel是一个后量子数字签名方案，结合了SPHINCS+的安全性和基于SL_n(F_p)上扩展图导航问题的新代数哈希函数家族，该问题被认为对量子攻击者也是困难的。


<details>
  <summary>Details</summary>
Motivation: 量子计算的出现要求密码学界设计超越经典困难假设的数字签名方案。需要开发基于代数哈希函数的后量子安全签名方案，扩展后量子密码学的工具箱。

Method: 1. 引入基于Tillich-Zemor范式的新代数哈希函数家族，其安全性根植于SL_n(F_p)上扩展图导航问题的困难性；2. 将该哈希函数集成到SPHINCS+框架中；3. 建模分析方案的安全退化，指导参数选择；4. 实现哈希函数和Spinel签名方案。

Result: 提供了哈希函数安全性的实证证据，展示了如何将哈希函数集成到SPHINCS+框架中形成安全签名方案，分析了安全退化并确定了参数选择，实现了Spinel并提供了详细的性能实证结果，证明了其实用可行性。

Conclusion: Spinel为代数哈希基签名方案的设计奠定了基础，扩展了后量子密码学的工具箱，展示了将代数哈希函数与SPHINCS+框架结合的可行性。

Abstract: The advent of quantum computation compels the cryptographic community to design digital signature schemes whose security extends beyond the classical hardness assumptions. In this work, we introduce Spinel, a post-quantum digital signature scheme that combines the proven security of SPHINCS+ (CCS 2019) with a new family of algebraic hash functions (Adv. Math. Commun. 2025) derived from the Tillich-Zemor paradigm (Eurocrypt 2008) with security rooted in the hardness of navigating expander graphs over SL_n(F_p), a problem believed to be hard even for quantum adversaries. We first provide empirical evidence of the security of this hash function, complementing the original theoretical analysis. We then show how the hash function can be integrated within the SPHINCS+ framework to give a secure signature scheme. We then model and analyze the security degradation of the proposed scheme, which informs the parameter selection we discuss next. Finally, we provide an implementation of the hash function and the proposed signature scheme Spinel as well as detailed empirical results for the performance of Spinel showing its feasibility in practice. Our approach lays the foundations for the design of algebraic hash-based signature schemes, expanding the toolkit of post-quantum cryptography.

</details>


### [22] [The Need for Standardized Evidence Sampling in CMMC Assessments: A Survey-Based Analysis of Assessor Practices](https://arxiv.org/abs/2602.09905)
*Logan Therrien,John Hastings*

Main category: cs.CR

TL;DR: CMMC评估中证据抽样实践缺乏标准化，主要依赖评估员主观判断而非正式统计方法，导致评估结果不一致，需要开发风险导向的标准化抽样框架。


<details>
  <summary>Details</summary>
Motivation: CMMC框架为国防合同中的敏感非机密信息保护提供了标准，但缺乏关于证据抽样的正式指导。证据抽样是评估员选择、审查和验证合规性证据的过程，其不一致性可能影响评估可靠性和认证结果的可信度。

Method: 通过匿名调查收集CMMC认证评估员和首席评估员的数据，进行探索性研究。分析了17份有效调查回复，研究证据抽样实践中是否存在不一致性，并评估是否需要风险导向的标准化抽样方法。

Result: 证据抽样实践主要由评估员判断、感知风险和环境影响驱动，而非正式标准或统计模型。参与者普遍报告评估间存在不一致性，广泛支持开发标准化指导，但反对僵化的百分比要求。

Conclusion: 缺乏统一的证据抽样框架引入了影响评估可靠性和认证结果可信度的变异性。研究结果为未来CMMC评估方法开发和实证研究提供了建议。

Abstract: The Cybersecurity Maturity Model Certification (CMMC) framework provides a common standard for protecting sensitive unclassified information in defense contracting. While CMMC defines assessment objectives and control requirements, limited formal guidance exists regarding evidence sampling, the process by which assessors select, review, and validate artifacts to substantiate compliance. Analyzing data collected through an anonymous survey of CMMC-certified assessors and lead assessors, this exploratory study investigates whether inconsistencies in evidence sampling practices exist within the CMMC assessment ecosystem and evaluates the need for a risk-informed standardized sampling methodology. Across 17 usable survey responses, results indicate that evidence sampling practices are predominantly driven by assessor judgment, perceived risk, and environmental complexity rather than formalized standards, with formal statistical sampling models rarely referenced. Participants frequently reported inconsistencies across assessments and expressed broad support for the development of standardized guidance, while generally opposing rigid percentage-based requirements. The findings support the conclusion that the absence of a uniform evidence sampling framework introduces variability that may affect assessment reliability and confidence in certification outcomes. Recommendations are provided to inform future CMMC assessment methodology development and further empirical research.

</details>


### [23] [Focus Session: LLM4PQC -- An Agentic Framework for Accurate and Efficient Synthesis of PQC Cores](https://arxiv.org/abs/2602.09919)
*Buddhi Perera,Zeng Wang,Weihua Xiao,Mohammed Nabeel,Ozgur Sinanoglu,Johann Knechtel,Ramesh Karri*

Main category: cs.CR

TL;DR: LLM4PQC：基于LLM的代理框架，将PQC参考C代码自动重构为HLS就绪的C代码，减少后量子密码硬件设计的手动工作量


<details>
  <summary>Details</summary>
Motivation: 后量子密码硬件设计复杂，主要瓶颈包括：1) 将PQC参考C代码转换为HLS规范需要大量手动重构；2) 复杂PQC原语（如NTT加速器和宽内存接口）的综合可扩展性问题。传统方法效率低下，需要自动化解决方案。

Method: 提出LLM4PQC框架：基于大语言模型的代理框架，采用反馈驱动和代理集成方法，将高级PQC规范和参考C代码重构为HLS就绪且可综合的C代码。框架生成并验证RTL代码，通过多层次检查确保正确性（包括快速C编译/仿真和RTL仿真）。

Result: 在NIST PQC参考设计上的案例研究表明，与传统流程相比，LLM4PQC显著减少了手动工作量，加速了设计空间探索。框架为复杂硬件加速器的综合提供了强大高效的途径。

Conclusion: LLM4PQC成功解决了后量子密码硬件设计中的关键瓶颈，通过LLM驱动的自动化重构和验证流程，大幅提升了设计效率，为复杂密码硬件加速器的开发提供了创新解决方案。

Abstract: The design of post-quantum cryptography (PQC) hardware is a complex and hierarchical process with many challenges. A primary bottleneck is the conversion of PQC reference codes from C to high-level synthesis (HLS) specifications, which requires extensive manual refactoring [1]-[3]. Another bottleneck is the scalability of synthesis for complex PQC primitives, including number theoretic transform (NTT) accelerators and wide memory interfaces. While large language models (LLMs) have shown remarkable results for coding in general-purpose languages like Python, coding for hardware design is more challenging; feedback-driven and agentic integration are key principles of successful state-of-the-art approaches. Here, we propose LLM4PQC, an LLM-based agentic framework that refactors high-level PQC specifications and reference C codes into HLS-ready and synthesizable C code. Our framework generates and verifies the resulting RTL code. For correctness, we leverage a hierarchy of checks, covering fast C compilation and simulation as well as RTL simulation. Case studies on NIST PQC reference designs demonstrate a reduction in manual effort and accelerated design-space exploration compared to traditional flows. Overall, LLM4PQC provides a powerful and efficient pathway for synthesizing complex hardware accelerators.

</details>


### [24] [Trustworthy Agentic AI Requires Deterministic Architectural Boundaries](https://arxiv.org/abs/2602.09947)
*Manish Bhattarai,Minh Vu*

Main category: cs.CR

TL;DR: 论文提出当前基于自回归语言模型的智能体架构存在安全缺陷，无法满足高风险科学工作流的安全要求，需要采用确定性架构而非概率学习来实现可信AI辅助科学。


<details>
  <summary>Details</summary>
Motivation: 当前基于自回归语言模型的AI智能体架构无法满足高风险科学工作流的安全和认识论要求。问题不是对齐不足或护栏不够，而是架构性的：自回归语言模型统一处理所有标记，仅通过训练无法实现确定性的命令-数据分离。需要确定性架构强制执行而非概率学习行为。

Method: 提出三位一体防御架构，通过三种机制强制执行安全：1) 通过有限动作演算和参考监视器执行的动作治理；2) 通过强制访问标签防止跨范围泄漏的信息流控制；3) 隔离感知与执行的特权分离。

Result: 论文表明，如果没有不可伪造的来源和确定性中介，"致命三要素"（不可信输入、特权数据访问、外部行动能力）会将授权安全转变为漏洞发现问题。基于训练的防御可能降低经验攻击率，但无法提供确定性保证。

Conclusion: 机器学习社区必须认识到对齐对于授权安全是不够的，在将智能体AI安全部署到重要科学领域之前，需要架构中介。确定性架构强制执行是可信AI辅助科学的必要条件。

Abstract: Current agentic AI architectures are fundamentally incompatible with the security and epistemological requirements of high-stakes scientific workflows. The problem is not inadequate alignment or insufficient guardrails, it is architectural: autoregressive language models process all tokens uniformly, making deterministic command--data separation unattainable through training alone. We argue that deterministic, architectural enforcement, not probabilistic learned behavior, is a necessary condition for trustworthy AI-assisted science. We introduce the Trinity Defense Architecture, which enforces security through three mechanisms: action governance via a finite action calculus with reference-monitor enforcement, information-flow control via mandatory access labels preventing cross-scope leakage, and privilege separation isolating perception from execution. We show that without unforgeable provenance and deterministic mediation, the ``Lethal Trifecta'' (untrusted inputs, privileged data access, external action capability) turns authorization security into an exploit-discovery problem: training-based defenses may reduce empirical attack rates but cannot provide deterministic guarantees. The ML community must recognize that alignment is insufficient for authorization security, and that architectural mediation is required before agentic AI can be safely deployed in consequential scientific domains.

</details>


### [25] [CAPID: Context-Aware PII Detection for Question-Answering Systems](https://arxiv.org/abs/2602.10074)
*Mariia Ponomarenko,Sepideh Abedini,Masoumeh Shafieinejad,D. B. Emerson,Shubhankar Mohapatra,Xi He*

Main category: cs.CR

TL;DR: CAPID：一种通过微调本地小型语言模型实现隐私保护PII检测的方法，能识别PII的上下文相关性，在保护隐私的同时保持问答质量


<details>
  <summary>Details</summary>
Motivation: 现有PII检测方法通常直接屏蔽所有个人信息，但某些PII可能与用户问题上下文相关，全部屏蔽会降低回答质量。大语言模型虽能判断相关性，但由于闭源和隐私问题不适合处理敏感数据。

Method: 提出CAPID方法：1）设计合成数据生成管道，利用LLM创建包含多种PII类型和相关性级别的多样化数据集；2）微调本地小型语言模型，使其能检测PII范围、分类类型并评估上下文相关性；3）在敏感信息传递给LLM进行问答前进行过滤。

Result: 实验表明，基于微调SLM的相关性感知PII检测在范围、相关性和类型准确性方面显著优于现有基线方法，同时在匿名化处理下保持更高的下游应用效用。

Conclusion: CAPID提供了一种实用的隐私保护PII检测方案，通过本地微调的小型语言模型实现了对敏感信息的智能过滤，在保护隐私的同时维持了问答系统的回答质量。

Abstract: Detecting personally identifiable information (PII) in user queries is critical for ensuring privacy in question-answering systems. Current approaches mainly redact all PII, disregarding the fact that some of them may be contextually relevant to the user's question, resulting in a degradation of response quality. Large language models (LLMs) might be able to help determine which PII are relevant, but due to their closed source nature and lack of privacy guarantees, they are unsuitable for sensitive data processing. To achieve privacy-preserving PII detection, we propose CAPID, a practical approach that fine-tunes a locally owned small language model (SLM) that filters sensitive information before it is passed to LLMs for QA. However, existing datasets do not capture the context-dependent relevance of PII needed to train such a model effectively. To fill this gap, we propose a synthetic data generation pipeline that leverages LLMs to produce a diverse, domain-rich dataset spanning multiple PII types and relevance levels. Using this dataset, we fine-tune an SLM to detect PII spans, classify their types, and estimate contextual relevance. Our experiments show that relevance-aware PII detection with a fine-tuned SLM substantially outperforms existing baselines in span, relevance and type accuracy while preserving significantly higher downstream utility under anonymization.

</details>
