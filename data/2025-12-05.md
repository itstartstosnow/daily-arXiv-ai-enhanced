<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 18]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Towards Contextual Sensitive Data Detection](https://arxiv.org/abs/2512.04120)
*Liang Telkamp,Madelon Hulsebos*

Main category: cs.CR

TL;DR: 论文提出基于上下文的敏感数据检测机制，包括类型上下文化和领域上下文化，利用LLM提升检测准确性，在非标准领域（如人道主义数据）表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有敏感数据检测方法主要关注个人隐私数据，但数据敏感性高度依赖上下文。开放数据门户需要更全面的敏感数据保护方法，特别是在非标准数据领域。

Method: 提出两种上下文敏感数据检测机制：1) 类型上下文化：先检测数据值的语义类型，再考虑其在数据集中的整体上下文；2) 领域上下文化：基于相关规则文档检索，在更广泛上下文中确定数据集敏感性。两种机制均利用大语言模型辅助。

Result: 类型上下文化显著减少误报，召回率达到94%（商业工具为63%）；领域上下文化在非标准数据领域（如人道主义数据）有效；基于上下文的LLM解释在人工数据审计中提供有用指导，提高一致性。

Conclusion: 上下文感知的敏感数据检测方法优于传统方法，特别是在非标准数据领域。该方法已开源，包含机制和标注数据集，可用于实际数据发布前的敏感数据保护。

Abstract: The emergence of open data portals necessitates more attention to protecting sensitive data before datasets get published and exchanged. While an abundance of methods for suppressing sensitive data exist, the conceptualization of sensitive data and methods to detect it, focus particularly on personal data that, if disclosed, may be harmful or violate privacy. We observe the need for refining and broadening our definitions of sensitive data, and argue that the sensitivity of data depends on its context. Based on this definition, we introduce two mechanisms for contextual sensitive data detection that consider the broader context of a dataset at hand. First, we introduce type contextualization, which first detects the semantic type of particular data values, then considers the overall context of the data values within the dataset or document. Second, we introduce domain contextualization which determines sensitivity of a given dataset in the broader context based on the retrieval of relevant rules from documents that specify data sensitivity (e.g., data topic and geographic origin). Experiments with these mechanisms, assisted by large language models (LLMs), confirm that: 1) type-contextualization significantly reduces the number of false positives for type-based sensitive data detection and reaches a recall of 94% compared to 63% with commercial tools, and 2) domain-contextualization leveraging sensitivity rule retrieval is effective for context-grounded sensitive data detection in non-standard data domains such as humanitarian datasets. Evaluation with humanitarian data experts also reveals that context-grounded LLM explanations provide useful guidance in manual data auditing processes, improving consistency. We open-source mechanisms and annotated datasets for contextual sensitive data detection at https://github.com/trl-lab/sensitive-data-detection.

</details>


### [2] [Tipping the Dominos: Topology-Aware Multi-Hop Attacks on LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2512.04129)
*Ruichao Liang,Le Yin,Jing Chen,Cong Wu,Xiaoyu Zhang,Huangpeng Gu,Zijian Zhang,Yang Liu*

Main category: cs.CR

TL;DR: TOMA是一种针对LLM多智能体系统的拓扑感知多跳攻击方案，通过优化污染传播和对抗载荷扩散，无需特权访问即可实现40-78%的攻击成功率，并提出基于拓扑信任的防御框架可阻断94.8%的攻击。


<details>
  <summary>Details</summary>
Motivation: 当前LLM多智能体系统的安全评估局限于有限攻击场景，导致其安全问题不清晰且可能被低估，需要更全面的安全评估方法。

Method: 提出TOMA拓扑感知多跳攻击方案，通过优化MAS拓扑内的污染传播和控制来自环境的对抗载荷多跳扩散，无需特权访问或直接操纵智能体。

Result: 在三种最先进的MAS架构（Magentic-One、LangManus、OWL）和五种代表性拓扑上实现40-78%的攻击成功率，揭示了现有研究可能忽略的内在MAS漏洞。

Conclusion: TOMA揭示了LLM多智能体系统的新攻击向量和内在漏洞，提出的基于拓扑信任的防御框架能有效阻断94.8%的自适应复合攻击，为MAS安全提供了新视角。

Abstract: LLM-based multi-agent systems (MASs) have reshaped the digital landscape with their emergent coordination and problem-solving capabilities. However, current security evaluations of MASs are still confined to limited attack scenarios, leaving their security issues unclear and likely underestimated. To fill this gap, we propose TOMA, a topology-aware multi-hop attack scheme targeting MASs. By optimizing the propagation of contamination within the MAS topology and controlling the multi-hop diffusion of adversarial payloads originating from the environment, TOMA unveils new and effective attack vectors without requiring privileged access or direct agent manipulation. Experiments demonstrate attack success rates ranging from 40% to 78% across three state-of-the-art MAS architectures: \textsc{Magentic-One}, \textsc{LangManus}, and \textsc{OWL}, and five representative topologies, revealing intrinsic MAS vulnerabilities that may be overlooked by existing research. Inspired by these findings, we propose a conceptual defense framework based on topology trust, and prototype experiments show its effectiveness in blocking 94.8% of adaptive and composite attacks.

</details>


### [3] [Primitive Vector Cipher(PVC): A Hybrid Encryption Scheme based on the Vector Computational Diffie-Hellman (V-CDH) Problem](https://arxiv.org/abs/2512.04237)
*Gülçin ÇİVİ BİLİR*

Main category: cs.CR

TL;DR: PVC是一种结合矩阵密码学和改进Diffie-Hellman密钥交换的混合加密方案，基于V-CDH问题的困难性，提供IND-CPA安全性，支持并行处理。


<details>
  <summary>Details</summary>
Motivation: 设计一种能够抵抗线性攻击和已知明文攻击的加密方案，同时支持大规模并行处理和线性扩展，解决传统加密方案中的确定性重复问题。

Method: 采用两层设计：1) 使用HKDF通过DH认证的共享原始向量掩盖明文；2) 使用每块偏移随机化密文块。结合STS协议提升安全性。

Result: PVC消除了确定性重复，对线性攻击和已知明文攻击具有强抵抗力，支持大规模并行处理，在V-CDH假设下证明具有IND-CPA安全性。

Conclusion: PVC是一种安全高效的混合加密方案，基于V-CDH问题提供形式化安全保证，通过STS协议可达到IND-CCA级别的安全性，适合高性能加密应用。

Abstract: This work introduces the Primitive Vector Cipher (PVC), a novel hybrid encryption scheme integrating matrix-based cryptography with advanced Diffie-Hellman key exchange. PVC's security is grounded on the established hardness of the Vector Computational Diffie- Hellman (V-CDH) problem. The two-layered design uses HKDF to mask plaintext via a DH-authenticated shared primitive vector and randomize cipher blocks with a per-block offset. This approach eliminates deterministic repetitions and provides strong resistance against linear and known-plaintext attacks. PVC's block-wise structure allows for massive parallelism and excellent linear scaling. Security is formally analyzed, demonstrating INDCPA security under V-CDH. STS protocol integration elevates security toward IND-CCA guarantees.

</details>


### [4] [Hey GPT-OSS, Looks Like You Got It -- Now Walk Me Through It! An Assessment of the Reasoning Language Models Chain of Thought Mechanism for Digital Forensics](https://arxiv.org/abs/2512.04254)
*Gaëtan Michelet,Janine Schneider,Aruna Withanage,Frank Breitinger*

Main category: cs.CR

TL;DR: 本文首次探讨推理语言模型在数字取证中的应用潜力，评估其推理组件对结果可解释性的支持效果，发现中等推理水平有助于解释和验证模型输出，但支持有限且更高推理水平不会提升响应质量。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在数字取证中应用广泛，但有限的结果可解释性降低了其操作和法律可用性。新兴的推理语言模型通过"内部推理"机制处理逻辑任务，但用户通常只能看到最终答案而非底层推理过程。gpt-oss等可本地部署的推理模型提供了对其推理过程的完全访问，这为提升数字取证结果的可解释性提供了新机会。

Method: 研究采用gpt-oss推理语言模型，通过四个测试用例评估推理组件在支持结果可解释性方面的可用性。评估结合了新的定量指标和定性分析，考察不同推理水平下模型的表现。

Result: 研究发现推理组件在中等推理水平上有助于解释和验证数字取证中的语言模型输出，但这种支持通常有限。更高的推理水平并不会增强响应质量，表明推理深度与输出质量之间不存在正相关关系。

Conclusion: 推理语言模型在数字取证中具有提升结果可解释性的潜力，特别是在中等推理水平上。然而，当前支持有限，且更高推理水平不会改善响应质量，这为未来优化推理模型在取证应用中的设计提供了方向。

Abstract: The use of large language models in digital forensics has been widely explored. Beyond identifying potential applications, research has also focused on optimizing model performance for forensic tasks through fine-tuning. However, limited result explainability reduces their operational and legal usability. Recently, a new class of reasoning language models has emerged, designed to handle logic-based tasks through an `internal reasoning' mechanism. Yet, users typically see only the final answer, not the underlying reasoning. One of these reasoning models is gpt-oss, which can be deployed locally, providing full access to its underlying reasoning process. This article presents the first investigation into the potential of reasoning language models for digital forensics. Four test use cases are examined to assess the usability of the reasoning component in supporting result explainability. The evaluation combines a new quantitative metric with qualitative analysis. Findings show that the reasoning component aids in explaining and validating language model outputs in digital forensics at medium reasoning levels, but this support is often limited, and higher reasoning levels do not enhance response quality.

</details>


### [5] [WildCode: An Empirical Analysis of Code Generated by ChatGPT](https://arxiv.org/abs/2512.04259)
*Kobra Khanmohammadi,Pooria Roy,Raphael Khoury,Abdelwahab Hamou-Lhadj,Wilfried Patrick Konan*

Main category: cs.CR

TL;DR: 对ChatGPT生成的真实代码进行大规模实证分析，发现LLM生成的代码在安全性方面存在不足，且用户很少关注代码安全性问题


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究指出AI生成代码可能存在安全漏洞，但这些研究大多使用专门为实验生成的代码，缺乏现实性。本研究旨在分析ChatGPT生成的真实代码，评估其正确性和安全性，并探究用户请求代码时的意图

Method: 对ChatGPT生成的真实代码进行大规模实证分析，评估代码的正确性和安全性，并分析用户请求代码时的查询内容以了解其安全关注度

Result: 研究证实了先前使用合成查询的研究结果：LLM生成的代码在安全性方面往往不足。同时发现用户很少询问代码的安全特性，表明他们对代码安全性缺乏关注

Conclusion: LLM生成的代码存在显著的安全风险，且用户对此风险意识不足。这强调了在AI代码生成工具中加强安全措施和用户教育的重要性

Abstract: LLM models are increasingly used to generate code, but the quality and security of this code are often uncertain. Several recent studies have raised alarm bells, indicating that such AI-generated code may be particularly vulnerable to cyberattacks. However, most of these studies rely on code that is generated specifically for the study, which raises questions about the realism of such experiments. In this study, we perform a large-scale empirical analysis of real-life code generated by ChatGPT. We evaluate code generated by ChatGPT both with respect to correctness and security and delve into the intentions of users who request code from the model. Our research confirms previous studies that used synthetic queries and yielded evidence that LLM-generated code is often inadequate with respect to security. We also find that users exhibit little curiosity about the security features of the code they ask LLMs to generate, as evidenced by their lack of queries on this topic.

</details>


### [6] [Breaking Isolation: A New Perspective on Hypervisor Exploitation via Cross-Domain Attacks](https://arxiv.org/abs/2512.04260)
*Gaoning Pan,Yiming Tao,Qinying Wang,Chunming Wu,Mingde Hu,Yizhi Ren,Shouling Ji*

Main category: cs.CR

TL;DR: 该论文提出了跨域攻击(CDA)的概念，利用虚拟机环境中弱内存隔离的特性，通过访客内存重用实现能力提升，并开发了自动化系统来识别跨域gadget、匹配损坏指针、合成触发输入并组装完整利用链。


<details>
  <summary>Details</summary>
Motivation: 传统利用框架在虚拟机环境中效果有限，因为它们依赖于识别高度受限的主机结构并准确确定其运行时地址，但在虚拟机环境中这类结构稀少且受到ASLR的进一步混淆。作者观察到现代虚拟化环境存在弱内存隔离问题，访客内存完全由攻击者控制却可以从主机访问，这为利用提供了可靠原语。

Method: 作者提出了跨域攻击(CDA)的系统化特征描述和分类，开发了一个自动化系统来识别跨域gadget、匹配损坏指针、合成触发输入并组装完整的利用链。

Result: 在QEMU和VirtualBox的15个真实漏洞评估中，CDA被证明广泛适用且有效。

Conclusion: 跨域攻击(CDA)是一种有效的虚拟机利用技术，利用虚拟机环境中的弱内存隔离特性，通过访客内存重用实现能力提升，为虚拟机安全研究提供了新的方向。

Abstract: Hypervisors are under threat by critical memory safety vulnerabilities, with pointer corruption being one of the most prevalent and severe forms. Existing exploitation frameworks depend on identifying highly-constrained structures in the host machine and accurately determining their runtime addresses, which is ineffective in hypervisor environments where such structures are rare and further obfuscated by Address Space Layout Randomization (ASLR). We instead observe that modern virtualization environments exhibit weak memory isolation -- guest memory is fully attacker-controlled yet accessible from the host, providing a reliable primitive for exploitation. Based on this observation, we present the first systematic characterization and taxonomy of Cross-Domain Attacks (CDA), a class of exploitation techniques that enable capability escalation through guest memory reuse. To automate this process, we develop a system that identifies cross-domain gadgets, matches them with corrupted pointers, synthesizes triggering inputs, and assembles complete exploit chains. Our evaluation on 15 real-world vulnerabilities across QEMU and VirtualBox shows that CDA is widely applicable and effective.

</details>


### [7] [One Detector Fits All: Robust and Adaptive Detection of Malicious Packages from PyPI to Enterprises](https://arxiv.org/abs/2512.04338)
*Biagio Montaruli,Luca Compagna,Serena Elisa Ponta,Davide Balzarotti*

Main category: cs.CR

TL;DR: 提出一种鲁棒的恶意Python包检测器，通过对抗训练增强对代码混淆的鲁棒性，并适应不同场景的误报率要求，在PyPI和企业环境中均能有效检测供应链攻击。


<details>
  <summary>Details</summary>
Motivation: 当前供应链攻击检测方法存在两个关键挑战：1）缺乏对对抗性源代码变换的鲁棒性；2）无法适应不同参与者（如仓库维护者与企业安全团队）对误报率的不同要求。

Method: 1）提出细粒度代码混淆方法生成对抗性包；2）采用对抗训练增强检测器鲁棒性；3）设计可调节误报率的检测系统，适应不同应用场景。

Result: 对抗训练使检测器鲁棒性提升2.5倍，在PyPI场景（0.1%误报率）下每日检测2.48个恶意包，仅2.18个误报；在企业场景（10%误报率）下每日仅1.24个误报。共发现346个恶意包。

Conclusion: 该检测器能无缝集成到PyPI和企业生态系统，通过对抗训练显著提升对代码混淆的鲁棒性，同时满足不同场景的误报率要求，有效应对供应链攻击。

Abstract: The rise of supply chain attacks via malicious Python packages demands robust detection solutions. Current approaches, however, overlook two critical challenges: robustness against adversarial source code transformations and adaptability to the varying false positive rate (FPR) requirements of different actors, from repository maintainers (requiring low FPR) to enterprise security teams (higher FPR tolerance).
  We introduce a robust detector capable of seamless integration into both public repositories like PyPI and enterprise ecosystems. To ensure robustness, we propose a novel methodology for generating adversarial packages using fine-grained code obfuscation. Combining these with adversarial training (AT) enhances detector robustness by 2.5x. We comprehensively evaluate AT effectiveness by testing our detector against 122,398 packages collected daily from PyPI over 80 days, showing that AT needs careful application: it makes the detector more robust to obfuscations and allows finding 10% more obfuscated packages, but slightly decreases performance on non-obfuscated packages.
  We demonstrate production adaptability of our detector via two case studies: (i) one for PyPI maintainers (tuned at 0.1% FPR) and (ii) one for enterprise teams (tuned at 10% FPR). In the former, we analyze 91,949 packages collected from PyPI over 37 days, achieving a daily detection rate of 2.48 malicious packages with only 2.18 false positives. In the latter, we analyze 1,596 packages adopted by a multinational software company, obtaining only 1.24 false positives daily. These results show that our detector can be seamlessly integrated into both public repositories like PyPI and enterprise ecosystems, ensuring a very low time budget of a few minutes to review the false positives.
  Overall, we uncovered 346 malicious packages, now reported to the community.

</details>


### [8] [AutoGuard: A Self-Healing Proactive Security Layer for DevSecOps Pipelines Using Reinforcement Learning](https://arxiv.org/abs/2512.04368)
*Praveen Anugula,Avdhesh Kumar Bhardwaj,Navin Chhibber,Rohit Tewari,Sunil Khemka,Piyush Ranjan*

Main category: cs.CR

TL;DR: AutoGuard是一个基于强化学习的自愈安全框架，用于DevSecOps环境，通过持续学习和主动修复来提高安全防护能力。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的入侵检测和静态漏洞扫描方法无法适应系统变化，响应时间长，无法应对新兴攻击向量，需要更智能、自适应的安全解决方案。

Method: 采用强化学习技术构建自愈安全框架，持续监控管道活动，基于奖励机制动态学习策略，实时预防、检测和响应安全事件。

Result: 在模拟CI/CD环境中测试显示，威胁检测准确率提高22%，事件平均恢复时间减少38%，整体抗事故能力显著提升。

Conclusion: AutoGuard通过强化学习实现了DevSecOps环境的自适应安全防护，相比传统方法在检测准确性、响应速度和系统韧性方面都有显著改进。

Abstract: Contemporary DevSecOps pipelines have to deal with the evolution of security in an ever-continuously integrated and deployed environment. Existing methods,such as rule-based intrusion detection and static vulnerability scanning, are inadequate and unreceptive to changes in the system, causing longer response times and organization needs exposure to emerging attack vectors. In light of the previous constraints, we introduce AutoGuard to the DevSecOps ecosystem, a reinforcement learning (RL)-powered self-healing security framework built to pre-emptively protect DevSecOps environments. AutoGuard is a self-securing security environment that continuously observes pipeline activities for potential anomalies while preemptively remediating the environment. The model observes and reacts based on a policy that is continually learned dynamically over time. The RL agent improves each action over time through reward-based learning aimed at improving the agent's ability to prevent, detect and respond to a security incident in real-time. Testing using simulated ContinuousIntegration / Continuous Deployment (CI/CD) environments showed AutoGuard to successfully improve threat detection accuracy by 22%, reduce mean time torecovery (MTTR) for incidents by 38% and increase overall resilience to incidents as compared to traditional methods.
  Keywords- DevSecOps, Reinforcement Learning, Self- Healing Security, Continuous Integration, Automated Threat Mitigation

</details>


### [9] [ReFuzz: Reusing Tests for Processor Fuzzing with Contextual Bandits](https://arxiv.org/abs/2512.04436)
*Chen Chen,Zaiyan Xu,Mohamadreza Rostami,David Liu,Dileep Kalathil,Ahmad-Reza Sadeghi,Jeyavijayan,Rajendran*

Main category: cs.CR

TL;DR: ReFuzz是一个自适应硬件模糊测试框架，利用上下文多臂老虎机重用先前处理器中的高效测试来测试目标处理器，能有效检测类似和新变种的漏洞。


<details>
  <summary>Details</summary>
Motivation: 处理器设计通常基于迭代修改和重用现有设计，这导致相似漏洞在多个处理器中传播。现有处理器模糊测试工具缺乏利用已知漏洞知识来指导测试的能力，无法有效检测相似或新变种的漏洞。

Method: ReFuzz采用上下文多臂老虎机算法，智能地变异先前处理器中触发漏洞的测试，将这些高效测试重用于测试目标处理器，从而检测相似和新变种的漏洞。

Result: ReFuzz发现了3个新的安全漏洞和2个新的功能bug，其中一个漏洞是通过重用触发先前处理器已知漏洞的测试检测到的。在覆盖率方面，平均实现511.23倍的速度提升，总覆盖率最多提高9.33%。

Conclusion: ReFuzz通过重用先前处理器的高效测试，有效提高了处理器漏洞检测的效率和效果，能够检测相似和新变种的漏洞，为处理器安全测试提供了新方法。

Abstract: Processor designs rely on iterative modifications and reuse well-established designs. However, this reuse of prior designs also leads to similar vulnerabilities across multiple processors. As processors grow increasingly complex with iterative modifications, efficiently detecting vulnerabilities from modern processors is critical. Inspired by software fuzzing, hardware fuzzing has recently demonstrated its effectiveness in detecting processor vulnerabilities. Yet, to our best knowledge, existing processor fuzzers fuzz each design individually, lacking the capability to understand known vulnerabilities in prior processors to fine-tune fuzzing to identify similar or new variants of vulnerabilities.
  To address this gap, we present ReFuzz, an adaptive fuzzing framework that leverages contextual bandit to reuse highly effective tests from prior processors to fuzz a processor-under-test (PUT) within a given ISA. By intelligently mutating tests that trigger vulnerabilities in prior processors, ReFuzz effectively detects similar and new variants of vulnerabilities in PUTs. ReFuzz uncovered three new security vulnerabilities and two new functional bugs. ReFuzz detected one vulnerability by reusing a test that triggers a known vulnerability in a prior processor. One functional bug exists across three processors that share design modules. The second bug has two variants. Additionally, ReFuzz reuses highly effective tests to enhance efficiency in coverage, achieving an average 511.23x coverage speedup and up to 9.33% more total coverage, compared to existing fuzzers.

</details>


### [10] [A Light-Weight Large Language Model File Format for Highly-Secure Model Distribution](https://arxiv.org/abs/2512.04580)
*Huifeng Zhu,Shijie Li,Qinfeng Li,Yier Jin*

Main category: cs.CR

TL;DR: CryptoTensors是一个基于Safetensors格式的安全文件结构，通过张量级加密和访问控制策略保护LLM权重，支持透明解密和自动化密钥管理，实现轻量级、高效的模型保护方案。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在医疗、法律、金融等敏感领域的应用，使用私有数据定制或微调的模型成为个人隐私资产或企业知识产权。现有模型格式和部署框架缺乏对机密性、访问控制或可信硬件集成的内置支持，当前安全部署方法要么依赖计算昂贵的密码学技术，要么需要严格控制的私有基础设施，难以广泛部署。

Method: 提出CryptoTensors作为Safetensors格式的扩展，包含张量级加密和嵌入式访问控制策略，同时保留延迟加载和部分反序列化等关键特性。实现透明解密和自动化密钥管理，支持灵活许可和安全模型执行。

Result: 实现了概念验证库，在序列化和运行时场景中进行了性能基准测试，验证了与现有推理框架（包括Hugging Face Transformers和vLLM）的兼容性。结果表明CryptoTensors是一个轻量级、高效且开发者友好的解决方案。

Conclusion: CryptoTensors为保护现实世界和广泛部署中的LLM权重提供了一个安全、格式兼容的文件结构，解决了现有方法在机密性、访问控制和部署成本方面的不足。

Abstract: To enhance the performance of large language models (LLMs) in various domain-specific applications, sensitive data such as healthcare, law, and finance are being used to privately customize or fine-tune these models. Such privately adapted LLMs are regarded as either personal privacy assets or corporate intellectual property. Therefore, protecting model weights and maintaining strict confidentiality during deployment and distribution have become critically important. However, existing model formats and deployment frameworks provide little to no built-in support for confidentiality, access control, or secure integration with trusted hardware. Current methods for securing model deployment either rely on computationally expensive cryptographic techniques or tightly controlled private infrastructure. Although these approaches can be effective in specific scenarios, they are difficult and costly for widespread deployment.
  In this paper, we introduce CryptoTensors, a secure and format-compatible file structure for confidential LLM distribution. Built as an extension to the widely adopted Safetensors format, CryptoTensors incorporates tensor-level encryption and embedded access control policies, while preserving critical features such as lazy loading and partial deserialization. It enables transparent decryption and automated key management, supporting flexible licensing and secure model execution with minimal overhead. We implement a proof-of-concept library, benchmark its performance across serialization and runtime scenarios, and validate its compatibility with existing inference frameworks, including Hugging Face Transformers and vLLM. Our results highlight CryptoTensors as a light-weight, efficient, and developer-friendly solution for safeguarding LLM weights in real-world and widespread deployments.

</details>


### [11] [PBFuzz: Agentic Directed Fuzzing for PoV Generation](https://arxiv.org/abs/2512.04611)
*Haochen Zeng,Andrew Bao,Jiajun Cheng,Chengyu Song*

Main category: cs.CR

TL;DR: PBFuzz：一种基于智能体的定向模糊测试框架，通过模仿人类专家分析代码、提取语义约束、形成假设并利用调试反馈，高效生成PoV输入，在Magma基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如定向灰盒模糊测试和LLM辅助模糊测试）在高效满足PoV输入生成的两类约束（可达性约束和触发约束）方面存在困难，需要更智能的方法来模拟人类专家的分析过程。

Method: PBFuzz框架采用智能体方法，包含四个关键技术：自主代码推理提取语义约束、定制程序分析工具进行针对性推断、持久性内存避免假设漂移、基于属性的测试在保持输入结构的同时高效解决约束。

Result: 在Magma基准测试中，PBFuzz触发了57个漏洞，超过所有基线方法，并独特地触发了17个现有模糊测试器未发现的漏洞。在30分钟预算内完成，而传统方法需要24小时，中位暴露时间从8680秒降至339秒，效率提升25.6倍，每个漏洞API成本仅1.83美元。

Conclusion: PBFuzz通过智能体方法有效模拟人类专家的漏洞分析过程，在PoV输入生成任务上实现了显著优于现有方法的性能和效率，为软件安全测试提供了新的有效工具。

Abstract: Proof-of-Vulnerability (PoV) input generation is a critical task in software security and supports downstream applications such as path generation and validation. Generating a PoV input requires solving two sets of constraints: (1) reachability constraints for reaching vulnerable code locations, and (2) triggering constraints for activating the target vulnerability. Existing approaches, including directed greybox fuzzing and LLM-assisted fuzzing, struggle to efficiently satisfy these constraints. This work presents an agentic method that mimics human experts. Human analysts iteratively study code to extract semantic reachability and triggering constraints, form hypotheses about PoV triggering strategies, encode them as test inputs, and refine their understanding using debugging feedback. We automate this process with an agentic directed fuzzing framework called PBFuzz. PBFuzz tackles four challenges in agentic PoV generation: autonomous code reasoning for semantic constraint extraction, custom program-analysis tools for targeted inference, persistent memory to avoid hypothesis drift, and property-based testing for efficient constraint solving while preserving input structure. Experiments on the Magma benchmark show strong results. PBFuzz triggered 57 vulnerabilities, surpassing all baselines, and uniquely triggered 17 vulnerabilities not exposed by existing fuzzers. PBFuzz achieved this within a 30-minute budget per target, while conventional approaches use 24 hours. Median time-to-exposure was 339 seconds for PBFuzz versus 8680 seconds for AFL++ with CmpLog, giving a 25.6x efficiency improvement with an API cost of 1.83 USD per vulnerability.

</details>


### [12] [Topology Matters: Measuring Memory Leakage in Multi-Agent LLMs](https://arxiv.org/abs/2512.04668)
*Jinbo Liu,Defu Cao,Yifei Wei,Tianyao Su,Yuan Liang,Yushun Dong,Yue Zhao,Xiyang Hu*

Main category: cs.CR

TL;DR: MAMA框架量化多智能体LLM系统中图拓扑结构对记忆泄漏的影响，发现全连接网络泄漏最严重，链式结构保护最强，攻击者-目标距离和节点中心性是关键风险因素。


<details>
  <summary>Details</summary>
Motivation: 图拓扑结构是多智能体LLM系统中记忆泄漏的根本决定因素，但其影响尚未得到充分量化。现有研究缺乏对网络结构如何塑造信息泄漏的系统性理解，需要建立从架构选择到可测量隐私风险的映射关系。

Method: 提出MAMA（多智能体记忆攻击）框架，使用包含标记PII实体的合成文档生成净化任务指令。采用两阶段协议：Engram（将私有信息植入目标智能体记忆）和Resonance（多轮交互中攻击者尝试提取）。通过精确匹配从攻击者输出中恢复的PII比例量化泄漏，系统评估六种常见网络拓扑（全连接、环、链、二叉树、星、星环），变化智能体数量、攻击者-目标位置和基础模型。

Result: 全连接图泄漏最大，链式结构保护最强；攻击者-目标图距离越短、目标中心性越高，漏洞越显著；泄漏在早期轮次急剧上升后趋于平稳；模型选择改变绝对泄漏率但保持拓扑排名；时空PII属性比身份凭证或受监管标识符更容易泄漏。

Conclusion: 首次系统性地建立了从架构选择到可测量隐私风险的映射，提供可操作指导：优先选择稀疏或分层连接，最大化攻击者-目标分离，限制节点度和网络半径，避免绕过枢纽的捷径，实施拓扑感知的访问控制。

Abstract: Graph topology is a fundamental determinant of memory leakage in multi-agent LLM systems, yet its effects remain poorly quantified. We introduce MAMA (Multi-Agent Memory Attack), a framework that measures how network structure shapes leakage. MAMA operates on synthetic documents containing labeled Personally Identifiable Information (PII) entities, from which we generate sanitized task instructions. We execute a two-phase protocol: Engram (seeding private information into a target agent's memory) and Resonance (multi-round interaction where an attacker attempts extraction). Over up to 10 interaction rounds, we quantify leakage as the fraction of ground-truth PII recovered from attacking agent outputs via exact matching. We systematically evaluate six common network topologies (fully connected, ring, chain, binary tree, star, and star-ring), varying agent counts $n\in\{4,5,6\}$, attacker-target placements, and base models. Our findings reveal consistent patterns: fully connected graphs exhibit maximum leakage while chains provide strongest protection; shorter attacker-target graph distance and higher target centrality significantly increase vulnerability; leakage rises sharply in early rounds before plateauing; model choice shifts absolute leakage rates but preserves topology rankings; temporal/locational PII attributes leak more readily than identity credentials or regulated identifiers. These results provide the first systematic mapping from architectural choices to measurable privacy risk, yielding actionable guidance: prefer sparse or hierarchical connectivity, maximize attacker-target separation, limit node degree and network radius, avoid shortcuts bypassing hubs, and implement topology-aware access controls.

</details>


### [13] [Cryptanalysis of Gleeok-128](https://arxiv.org/abs/2512.04675)
*Siwei Chen,Peipei Xie,Shengyuan Xu,Xiutao Feng,Zejun Xiang,Xiangyong Zeng*

Main category: cs.CR

TL;DR: 本文对Gleeok-128进行了首次全面的第三方密码分析，提出了基于MILP的差分线性区分器构建框架和针对多分支设计的积分攻击密钥恢复框架，发现了原设计中Branch 3的线性安全评估缺陷。


<details>
  <summary>Details</summary>
Motivation: Gleeok是一种低延迟密钥伪随机函数家族，采用三个并行SPN置换输出异或的结构。由于其多分支特性，评估安全边界和进行有效的密钥恢复攻击具有挑战性。本文旨在提供对Gleeok-128的首次全面第三方密码分析。

Method: 1. 提出两阶段MILP框架构建分支级和全密码的差分线性区分器；2. 开发针对多分支设计的基于积分的密钥恢复框架；3. 通过收紧代数度界限推导积分区分器；4. 识别原设计中Branch 3的线性安全评估缺陷；5. 提出优化的线性层参数。

Result: 1. 差分线性分析：获得Branch 1、Branch 2、Branch 3和Gleeok-128的7、7、8、4轮区分器，平方相关性分别为2^{-88.12}、2^{-88.12}、2^{-38.73}、2^{-49.04}；2. 积分分析：获得三个分支的9、9、7轮区分器和全PRF的7轮区分器，比设计文档结果分别扩展了3、3、2轮和2轮；3. 密钥恢复攻击：在非全码本和全码本设置下实现7轮和8轮攻击；4. 发现Branch 3在所有12轮均可被区分，数据复杂度约2^{48}；5. 提出显著改善线性抵抗力的优化线性层参数。

Conclusion: 本文提供了对Gleeok-128的首次全面密码分析，提出了分析多分支对称设计的一般方法，发现了原设计中的安全缺陷，并提出了改进建议。研究结果推进了对Gleeok-128的理解，为多分支对称设计的分析提供了通用框架。

Abstract: Gleeok is a family of low latency keyed pseudorandom functions (PRFs) consisting of three parallel SPN based permutations whose outputs are XORed to form the final value. Both Gleeok-128 and Gleeok-256 use a 256 bit key, with block sizes of 128 and 256 bits, respectively. Owing to its multi branch structure, evaluating security margins and mounting effective key recovery attacks present nontrivial challenges. This paper provides the first comprehensive third party cryptanalysis of Gleeok-128. We introduce a two stage MILP based framework for constructing branch wise and full cipher differential linear (DL) distinguishers, together with an integral based key recovery framework tailored to multi branch designs. Our DL analysis yields 7, 7, 8, and 4 round distinguishers for Branch 1, Branch 2, Branch 3, and Gleeok-128, respectively, with squared correlations approximately 2 to the power minus 88.12, 2 to the power minus 88.12, 2 to the power minus 38.73, and 2 to the power minus 49.04, outperforming those in the design document except for the full PRF case. By tightening algebraic degree bounds, we further derive 9, 9, and 7 round integral distinguishers for the three branches and a 7 round distinguisher for the full PRF, extending the designers results by 3, 3, and 2 rounds and by 2 rounds, respectively. These integral properties enable 7 round and 8 round key recovery attacks in the non full codebook and full codebook settings. In addition, we identify a flaw in the original linear security evaluation of Branch 3, showing that it can be distinguished over all 12 rounds with data complexity about 2 to the power 48. We also propose optimized linear layer parameters that significantly improve linear resistance without sacrificing diffusion. Our results advance the understanding of Gleeok-128 and provide general methods for analyzing multi branch symmetric designs.

</details>


### [14] [SoK: a Comprehensive Causality Analysis Framework for Large Language Model Security](https://arxiv.org/abs/2512.04841)
*Wei Zhao,Zhe Li,Jun Sun*

Main category: cs.CR

TL;DR: 提出了一个统一的因果分析框架，用于系统研究LLM的安全漏洞，特别是越狱攻击。该框架支持从token到表示层的多级因果干预，并通过实证研究揭示了安全机制的高度局部化特征。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然能力强大，但容易受到对抗性操纵（如越狱攻击）的影响。理解这些漏洞背后的因果因素对于构建可靠的防御机制至关重要。目前缺乏一个统一的框架来系统支持LLM中不同层次的因果分析。

Method: 提出了一个统一的因果分析框架，支持token级、神经元级、层级和表示级的因果干预分析。该框架实现了因果驱动的攻击和防御方法的一致实验和比较。在多个开源模型和安全关键基准（包括越狱、幻觉检测、后门识别和公平性评估）上进行了实证评估。

Result: 研究发现：（1）对因果关键组件的针对性干预可以可靠地修改安全行为；（2）安全相关机制高度局部化（集中在早期到中间层，只有1-2%的神经元表现出因果影响）；（3）从框架中提取的因果特征在多种威胁类型上实现了超过95%的检测准确率。

Conclusion: 该框架通过连接理论因果分析和实际模型安全，为基于因果的攻击、可解释性以及鲁棒的攻击检测和缓解研究建立了可重复的基础。代码已开源。

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities but remain vulnerable to adversarial manipulations such as jailbreaking, where crafted prompts bypass safety mechanisms. Understanding the causal factors behind such vulnerabilities is essential for building reliable defenses.
  In this work, we introduce a unified causality analysis framework that systematically supports all levels of causal investigation in LLMs, ranging from token-level, neuron-level, and layer-level interventions to representation-level analysis. The framework enables consistent experimentation and comparison across diverse causality-based attack and defense methods. Accompanying this implementation, we provide the first comprehensive survey of causality-driven jailbreak studies and empirically evaluate the framework on multiple open-weight models and safety-critical benchmarks including jailbreaks, hallucination detection, backdoor identification, and fairness evaluation. Our results reveal that: (1) targeted interventions on causally critical components can reliably modify safety behavior; (2) safety-related mechanisms are highly localized (i.e., concentrated in early-to-middle layers with only 1--2\% of neurons exhibiting causal influence); and (3) causal features extracted from our framework achieve over 95\% detection accuracy across multiple threat types.
  By bridging theoretical causality analysis and practical model safety, our framework establishes a reproducible foundation for research on causality-based attacks, interpretability, and robust attack detection and mitigation in LLMs. Code is available at https://github.com/Amadeuszhao/SOK_Casuality.

</details>


### [15] [A Novel Trust-Based DDoS Cyberattack Detection Model for Smart Business Environments](https://arxiv.org/abs/2512.04855)
*Oghenetejiri Okporokpo,Funminiyi Olajide,Nemitari Ajienka,Xiaoqi Ma*

Main category: cs.CR

TL;DR: 提出基于信任的DDoS检测模型，针对智能物联网业务环境，通过信任评分和中央信任库实时识别攻击，提高检测准确性并降低误报率。


<details>
  <summary>Details</summary>
Motivation: 随着DDoS攻击频率和复杂性增加，智能物联网业务环境面临更大威胁。传统检测机制在动态多样的SIoT网络中往往不足，需要针对资源受限环境设计轻量级解决方案。

Method: 提出基于信任的DDoS检测模型，包含信任评估引擎持续监控节点行为，基于数据包投递率、响应时间和异常检测计算信任评分，中央信任库聚合信任指标识别DDoS攻击模式。

Result: 模型在TCP SYN、Ping Flood和UDP Flood攻击下表现出显著改进的检测准确性、低误报率，并具备增强的可扩展性和适应性。

Conclusion: 基于信任的方法为资源受限的业务物联网环境提供了有效、轻量级的替代安全方案，能够实时准确识别和应对威胁。

Abstract: As the frequency and complexity of Distributed Denial-of-Service (DDoS) attacks continue to increase, the level of threats posed to Smart Internet of Things (SIoT) business environments have also increased. These environments generally have several interconnected SIoT systems and devices that are integral to daily operations, usually depending on cloud infrastructure and real-time data analytics, which require continuous availability and secure data exchange. Conventional detection mechanisms, while useful in static or traditional network environments, often are inadequate in responding to the needs of these dynamic and diverse SIoT networks. In this paper, we introduce a novel trust-based DDoS detection model tailored to meet the unique requirements of smart business environments. The proposed model incorporates a trust evaluation engine that continuously monitors node behaviour, calculating trust scores based on packet delivery ratio, response time, and anomaly detection. These trust metrics are then aggregated by a central trust-based repository that uses inherent trust values to identify traffic patterns indicative of DDoS attacks. By integrating both trust scores and central trust-based outputs, the trust calculation is enhanced, ensuring that threats are accurately identified and addressed in real-time. The model demonstrated a significant improvement in detection accuracy, and a low false-positive rate with enhanced scalability and adaptability under TCP SYN, Ping Flood, and UDP Flood attacks. The results show that a trust-based approach provides an effective, lightweight alternative for securing resource-constrained business IoT environments.

</details>


### [16] [Logic-Driven Cybersecurity: A Novel Framework for System Log Anomaly Detection using Answer Set Programming](https://arxiv.org/abs/2512.04908)
*Fang Li,Fei Zuo,Gopal Gupta*

Main category: cs.CR

TL;DR: 使用ASP（答案集编程）检测系统日志异常，通过逻辑规则编码安全策略，在真实Linux日志数据上验证有效性


<details>
  <summary>Details</summary>
Motivation: 应对不断演变的网络威胁挑战，传统日志分析方法难以处理复杂的安全规则和事件关联

Method: 提出基于ASP的框架，利用其声明式特性和逻辑推理能力，将复杂安全规则编码为逻辑谓词

Result: 在真实Linux系统日志数据集上成功识别多种异常：潜在暴力攻击、权限提升、特定IP频繁网络连接及系统级问题

Conclusion: ASP在处理结构化日志数据、规则灵活性和事件关联方面具有优势，为计算机取证提供可解释的逻辑分析范式

Abstract: This study explores the application of Answer Set Programming (ASP) for detecting anomalies in system logs, addressing the challenges posed by evolving cyber threats. We propose a novel framework that leverages ASP's declarative nature and logical reasoning capabilities to encode complex security rules as logical predicates. Our ASP-based system was applied to a real-world Linux system log dataset, demonstrating its effectiveness in identifying various anomalies such as potential brute-force attacks, privilege escalations, frequent network connections from specific IPs, and various system-level issues. Key findings highlight ASP's strengths in handling structured log data, rule flexibility, and event correlation. The approach shows promise in providing explainable alerts from real-world data. This research contributes to computer forensics by demonstrating a logic-based paradigm for log analysis on a practical dataset, opening avenues for more nuanced and adaptive cyber intelligence systems.

</details>


### [17] [Opacity problems in multi-energy timed automata](https://arxiv.org/abs/2512.04950)
*Étienne André,Lydia Bakiri*

Main category: cs.CR

TL;DR: 研究具有时间和能量观测的系统的信息泄露验证问题，提出带守卫的多能量时间自动机模型，并在多个子类上获得可判定性结果


<details>
  <summary>Details</summary>
Motivation: 信息物理系统可能存在信息泄露，当涉及时间和能量等连续变量时，这些泄露难以检测。需要研究在同时观测时间和能量信息的情况下，系统的不透明性验证问题

Method: 提出带守卫的多能量时间自动机作为时间自动机的扩展，包含多个能量变量和相应的守卫条件。研究攻击者观测不同信息（最终能量、执行时间、每时间单位的能量值）时的验证问题

Result: 虽然一般形式是不可判定的，但在多个子类上获得了正面结果：当攻击者观测最终能量和/或执行时间时，以及当攻击者每时间单位都能访问能量变量值时

Conclusion: 带守卫的多能量时间自动机为分析信息物理系统的信息泄露提供了有力框架，尽管一般问题不可判定，但在实际相关的子类上可以获得可判定的验证结果

Abstract: Cyber-physical systems can be subject to information leakage; in the presence of continuous variables such as time and energy, these leaks can be subtle to detect. We study here the verification of opacity problems over systems with observation over both timing and energy information. We introduce guarded multi-energy timed automata as an extension of timed automata with multiple energy variables and guards over such variables. Despite undecidability of this general formalism, we establish positive results over a number of subclasses, notably when the attacker observes the final energy and/or the execution time, but also when they have access to the value of the energy variables every time unit.

</details>


### [18] [Personalizing Agent Privacy Decisions via Logical Entailment](https://arxiv.org/abs/2512.05065)
*James Flemings,Ren Yi,Octavian Suciu,Kassem Fawaz,Murali Annavaram,Marco Gruteser*

Main category: cs.CR

TL;DR: ARIEL框架结合语言模型与规则逻辑，通过个性化推理提升AI代理隐私决策准确性，相比传统方法减少39.1%错误率


<details>
  <summary>Details</summary>
Motivation: 个人语言模型代理日益普及，但存在隐私泄露风险。现有基于通用隐私规范的方法无法有效个性化隐私决策，且上下文学习存在推理不透明、与用户历史决策不一致的问题

Method: 提出ARIEL框架，将个性化数据共享建模为蕴含关系：判断用户历史隐私决策是否蕴含对新请求的相同判断。框架结合语言模型与基于规则的逻辑进行结构化数据共享推理

Result: 在先进模型和公开数据集上的实验表明，ARIEL相比基于语言模型的推理方法（ICL）将F1分数错误率降低了39.1%，能更准确地判断用户会批准数据共享的请求

Conclusion: 结合LLM与严格逻辑蕴含是使代理能够进行个性化隐私判断的高度有效策略，通用隐私规范不足以实现有效的隐私决策个性化

Abstract: Personal language model-based agents are becoming more widespread for completing tasks on behalf of users; however, this raises serious privacy questions regarding whether these models will appropriately disclose user data. While prior work has evaluated language models on data-sharing scenarios based on general privacy norms, we focus on personalizing language models' privacy decisions, grounding their judgments directly in prior user privacy decisions. Our findings suggest that general privacy norms are insufficient for effective personalization of privacy decisions. Furthermore, we find that eliciting privacy judgments from the model through In-context Learning (ICL) is unreliable to due misalignment with the user's prior privacy judgments and opaque reasoning traces, which make it difficult for the user to interpret the reasoning behind the model's decisions. To address these limitations, we propose ARIEL (Agentic Reasoning with Individualized Entailment Logic), a framework that jointly leverages a language model and rule-based logic for structured data-sharing reasoning. ARIEL is based on formulating personalization of data sharing as an entailment, whether a prior user judgment on a data-sharing request implies the same judgment for an incoming request. Our experimental evaluations on advanced models and publicly-available datasets demonstrate that ARIEL can reduce the F1 score error by $\textbf{39.1%}$ over language model-based reasoning (ICL), demonstrating that ARIEL is effective at correctly judging requests where the user would approve data sharing. Overall, our findings suggest that combining LLMs with strict logical entailment is a highly effective strategy for enabling personalized privacy judgments for agents.

</details>
