<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 54]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [GPS Spoofing Attacks and Pilot Responses Using a Flight Simulator Environment](https://arxiv.org/abs/2509.22662)
*Mathilde Durieux,Kayla D. Taylor,Laxima Niure Kandel,Deepti Gupta*

Main category: cs.CR

TL;DR: 该研究通过飞行模拟器实验，分析学生飞行员在GPS欺骗攻击下的行为反应，填补了GPS欺骗研究中人类因素的空缺。


<details>
  <summary>Details</summary>
Motivation: 现有GPS欺骗研究多关注技术检测和缓解，但忽视了飞行员作为飞机操作核心部分可能面临的欺骗风险，而人类因素在GPS欺骗中往往被忽略。

Method: 使用Force Dynamics 401CR飞行模拟器、X-Plane 11和配备Garmin G1000的Cessna 172，通过自定义脚本实施GPS欺骗场景，招募30名不同经验水平的学生飞行员参与三个欺骗场景实验。

Result: 观察了飞行员在欺骗攻击期间的飞行决策，包括对异常的反应时间、对界面元素的视觉关注和认知偏差，并使用改进的NASA任务负荷指数评估工作负荷。

Conclusion: 这项研究为识别人类在GPS欺骗中的脆弱性提供了初步探索，正值GPS依赖争议持续之际。

Abstract: Global Positioning System (GPS) spoofing involves transmitting fake signals
that mimic those from GPS satellites, causing the GPS receivers to calculate
incorrect Positioning, Navigation, and Timing (PNT) information. Recently,
there has been a surge in GPS spoofing attacks targeting aircraft. Since GPS
satellite signals are weak, the spoofed high-power signal can easily overpower
them. These spoofed signals are often interpreted as valid by the GPS receiver,
which can cause severe and cascading effects on air navigation. While much of
the existing research on GPS spoofing focuses on technical aspects of detection
and mitigation, human factors are often neglected, even though pilots are an
integral part of aircraft operation and potentially vulnerable to deception.
This research addresses this gap by conducting a detailed analysis of the
behavior of student pilots when subjected to GPS spoofing using the Force
Dynamics 401CR flight simulator with X-Plane 11 and a Cessna 172 equipped with
Garmin G1000. Spoofing scenarios were implemented via custom scripts that
altered navigational data without modifying the external visual environment.
Thirty student pilots from the Embry-Riddle Aeronautical University Daytona
Beach campus with diverse flying experience levels were recruited to
participate in three spoofing scenarios. A pre-simulation questionnaire was
distributed to measure pilot experience and confidence in GPS.Inflight
decision-making during the spoofing attacks was observed, including reaction
time to anomalies, visual attention to interface elements, and cognitive
biases. A post-flight evaluation of workload was obtained using a modified NASA
Task Load Index (TLX) method. This study provides a first step toward
identifying human vulnerabilities to GPS spoofing amid the ongoing debate over
GPS reliance.

</details>


### [2] [Security Friction Quotient for Zero Trust Identity Policy with Empirical Validation](https://arxiv.org/abs/2509.22663)
*Michel Youssef*

Main category: cs.CR

TL;DR: 提出安全摩擦系数(SFQ)框架，量化身份安全策略中安全性与操作摩擦的权衡，通过蒙特卡洛模拟和实际观察验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现代身份中心化项目中需要量化安全性与操作摩擦之间的权衡关系，为Zero Trust身份策略决策提供可操作依据。

Method: 定义SFQ指数(结合残余风险估计和经验摩擦项)，建立清晰度属性，使用蒙特卡洛模拟评估条件访问策略，并进行12周实际观察验证。

Result: SFQ框架在95.5%的权重随机抽取下保持策略排序稳定，实际观察结果与模拟的防钓鱼MFA增益方向一致。

Conclusion: SFQ框架具有可重现性、可解释性和直接可操作性，支持身份策略设计、审查和持续改进。

Abstract: We define a practical method to quantify the trade-off between security and
operational friction in modern identity-centric programs. We introduce the
Security Friction Quotient (SFQ), a bounded composite index that combines a
residual-risk estimator with empirically grounded friction terms (latency,
failure rate, and helpdesk impact). We establish clarity properties
(boundedness, monotonic response, and weight identifiability) with short
proofs, then evaluate widely used Conditional Access policies over a 12-week
horizon using Monte Carlo simulation (n = 2,000 runs per policy/scenario) with
effect sizes and 95% confidence intervals. We further assess rank stability
under 10,000 random weight draws, finding 95.5% preservation of policy
ordering. Finally, we provide a 12-week passkey field observation from an
enterprise-scale cohort (N = 1,200) that directionally aligns with the
simulation's phishing-resistant MFA gains. The SFQ framework is designed to be
reproducible, interpretable, and directly actionable for Zero Trust identity
policy decisions, with artifacts and parameter ranges provided to support
policy design, review, and continuous improvement.

</details>


### [3] [Security Issues on the OpenPLC project and corresponding solutions](https://arxiv.org/abs/2509.22664)
*Chaerin Kim*

Main category: cs.CR

TL;DR: 该论文分析了开源PLC软件OpenPLC的安全漏洞，演示了控制逻辑注入攻击，并提出了安全增强版本OpenPLC Aqua。


<details>
  <summary>Details</summary>
Motivation: OpenPLC作为低成本PLC解决方案虽然实用但缺乏足够的安全保护，存在多个安全漏洞，需要系统性的安全分析和改进。

Method: 通过威胁建模、漏洞分析和实际实验，深入分析OpenPLC的安全挑战，演示控制逻辑注入攻击，并开发安全增强版本。

Result: 识别了OpenPLC的多个安全漏洞，成功实施了控制逻辑注入攻击，开发了具备专门安全解决方案的OpenPLC Aqua软件。

Conclusion: OpenPLC Aqua通过专门设计的安全解决方案有效解决了当前OpenPLC版本的安全漏洞问题，为工业环境中的安全部署提供了保障。

Abstract: As Programmable Logic Controller (PLC) became a useful device and rose as an
interesting research topic but remained expensive, multiple PLC
simulators/emulators were introduced for various purposes. Open-source
Programmable Logic Controller (OpenPLC) software, one of the most popular PLC
simulators, is designed to be vendor-neutral and run on almost any computer or
low-cost embedded devices, e.g., Raspberry Pi, Arduino, and other controllers.
The project succeeded in introducing itself as an affordable and practical
solution for the high cost of real hardware PLCs. However, it still lacks
appropriate securing methods, resulting in several vulnerabilities. Through a
combination of threat modeling, vulnerability analysis, and practical
experiments, this thesis provides valuable insights for developers,
researchers, and engineers aiming to deploy OpenPLC securely in industrial
environments. To this end, this work first conducts an in-depth analysis aimed
to shed light on va! rious security challenges and vulnerabilities within the
OpenPLC project. After that, an advanced control logic injection attack was
performed. This attack modifies the user program maliciously, exploiting
presented vulnerabilities. Finally, the work introduces a security-enhanced
OpenPLC software called OpenPLC Aqua. The new software is equipped with a set
of security solutions designed specifically to address the vulnerabilities to
which current OpenPLC versions are prone.

</details>


### [4] [Responsible Diffusion: A Comprehensive Survey on Safety, Ethics, and Trust in Diffusion Models](https://arxiv.org/abs/2509.22723)
*Kang Wei,Xin Yuan,Fushuo Huo,Chuan Ma,Long Yuan,Songze Li,Ming Ding,Dacheng Tao*

Main category: cs.CR

TL;DR: 这篇论文对扩散模型的安全威胁和防御措施进行了系统性调查，分析了其框架、威胁类型及对应防护方法，并讨论了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成高质量数据方面表现出色，但与传统深度学习系统类似，也存在潜在的安全威胁。为了提供对扩散模型安全性、伦理性和可信度的深入理解，需要进行全面调查。

Method: 采用系统性调查方法，全面阐述扩散模型的框架、威胁类型和防御措施。对每种威胁及其对策进行系统检查和分类，以促进深入分析。

Result: 提供了扩散模型使用案例、潜在危险及防护方法的详细示例，系统性地梳理了扩散模型面临的安全挑战和相应解决方案。

Conclusion: 总结了关键经验教训，强调了扩散模型安全方面的开放挑战，并展望了这一关键领域的未来研究方向，旨在推动生成式AI技术能力及其应用成熟度的进步。

Abstract: Diffusion models (DMs) have been investigated in various domains due to their
ability to generate high-quality data, thereby attracting significant
attention. However, similar to traditional deep learning systems, there also
exist potential threats to DMs. To provide advanced and comprehensive insights
into safety, ethics, and trust in DMs, this survey comprehensively elucidates
its framework, threats, and countermeasures. Each threat and its
countermeasures are systematically examined and categorized to facilitate
thorough analysis. Furthermore, we introduce specific examples of how DMs are
used, what dangers they might bring, and ways to protect against these dangers.
Finally, we discuss key lessons learned, highlight open challenges related to
DM security, and outline prospective research directions in this critical
field. This work aims to accelerate progress not only in the technical
capabilities of generative artificial intelligence but also in the maturity and
wisdom of its application.

</details>


### [5] [Bidirectional Intention Inference Enhances LLMs' Defense Against Multi-Turn Jailbreak Attacks](https://arxiv.org/abs/2509.22732)
*Haibo Tong,Dongcheng Zhao,Guobin Shen,Xiang He,Dachuan Lin,Feifei Zhao,Yi Zeng*

Main category: cs.CR

TL;DR: 提出双向意图推断防御(BIID)方法，通过前向请求意图推断和后向响应意图回溯的双向协同机制，有效防御多轮越狱攻击，显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法主要针对单轮攻击，而多轮越狱攻击通过隐藏恶意意图和策略性操作逐步突破安全机制，使传统单轮防御失效。

Method: BIID方法整合前向请求意图推断和后向响应意图回溯，建立双向协同机制，检测看似良性输入中隐藏的风险，构建更强大的防护栏。

Result: 在3个LLM和2个安全基准上对10种攻击方法的系统评估显示，该方法显著降低单轮和多轮越狱攻击的成功率，优于所有现有基线方法，同时有效保持实用性。

Conclusion: BIID方法在多轮安全数据集上的比较实验进一步验证了其相对于其他防御方法的显著优势，为LLM安全提供更鲁棒的防护。

Abstract: The remarkable capabilities of Large Language Models (LLMs) have raised
significant safety concerns, particularly regarding "jailbreak" attacks that
exploit adversarial prompts to bypass safety alignment mechanisms. Existing
defense research primarily focuses on single-turn attacks, whereas multi-turn
jailbreak attacks progressively break through safeguards through by concealing
malicious intent and tactical manipulation, ultimately rendering conventional
single-turn defenses ineffective. To address this critical challenge, we
propose the Bidirectional Intention Inference Defense (BIID). The method
integrates forward request-based intention inference with backward
response-based intention retrospection, establishing a bidirectional synergy
mechanism to detect risks concealed within seemingly benign inputs, thereby
constructing a more robust guardrails that effectively prevents harmful content
generation. The proposed method undergoes systematic evaluation compared with a
no-defense baseline and seven representative defense methods across three LLMs
and two safety benchmarks under 10 different attack methods. Experimental
results demonstrate that the proposed method significantly reduces the Attack
Success Rate (ASR) across both single-turn and multi-turn jailbreak attempts,
outperforming all existing baseline methods while effectively maintaining
practical utility. Notably, comparative experiments across three multi-turn
safety datasets further validate the proposed model's significant advantages
over other defense approaches.

</details>


### [6] [Defending MoE LLMs against Harmful Fine-Tuning via Safety Routing Alignment](https://arxiv.org/abs/2509.22745)
*Jaehan Kim,Minkyoo Song,Seungwon Shin,Sooel Son*

Main category: cs.CR

TL;DR: SafeMoE是一种针对MoE架构大语言模型的安全微调方法，通过惩罚微调模型与初始安全对齐模型之间的路由权重差距，有效缓解有害微调攻击，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: MoE架构的LLMs依赖将有害输入路由到安全关键专家的机制，但微调后有害输入的路由决策会显著漂移，暴露了有害微调攻击的漏洞。现有防御方法主要针对单体LLMs，对MoE LLMs效果不佳。

Method: 提出SafeMoE方法，通过直接惩罚微调模型与初始安全对齐模型之间的路由权重差距，保持有害输入到安全关键专家的安全对齐路由。

Result: 在7B到141B参数的MoE LLMs上实验表明，SafeMoE有效缓解有害微调攻击，将OLMoE的有害性分数从62.0降至5.0，任务性能仅下降1%，开销仅2%。

Conclusion: SafeMoE显著优于现有最先进的防御方法，在最新的大规模MoE LLMs中保持有效，为MoE架构LLMs的安全微调提供了有效解决方案。

Abstract: Recent large language models (LLMs) have increasingly adopted the
Mixture-of-Experts (MoE) architecture for efficiency. MoE-based LLMs heavily
depend on a superficial safety mechanism in which harmful inputs are routed
safety-critical experts. However, our analysis reveals that routing decisions
for harmful inputs drift significantly after fine-tuning, exposing a critical
vulnerability to harmful fine-tuning (HFT) attacks. Existing defenses,
primarily designed for monolithic LLMs, are less effective for MoE LLMs as they
fail to prevent drift in harmful input routing. To address this limitation, we
propose SafeMoE, a safe fine-tuning method tailored to MoE LLMs. SafeMoE
directly mitigates routing drift by penalizing the gap between the routing
weights of a fine-tuned model and those of the initial safety-aligned model,
thereby preserving the safety-aligned routing of harmful inputs to
safety-critical experts. Experiments on open-source MoE LLMs ranging from 7B to
141B parameters demonstrate that SafeMoE effectively mitigates HFT attacks,
reducing the harmfulness score of OLMoE from 62.0 to 5.0, for example, while
maintaining task utility within 1% degradation and incurring only 2% overhead.
It significantly outperforms state-of-the-art defense methods for safeguarding
LLM fine-tuning and remains effective in recent large-scale MoE LLMs such as
gpt-oss and Llama 4. Our implementation is available at
https://anonymous.4open.science/r/SafeMoE.

</details>


### [7] [Red Teaming Quantum-Resistant Cryptographic Standards: A Penetration Testing Framework Integrating AI and Quantum Security](https://arxiv.org/abs/2509.22757)
*Petar Radanliev*

Main category: cs.CR

TL;DR: 本研究提出了一个结构化方法来评估量子密码协议中的漏洞，重点关注BB84量子密钥分发方法和NIST批准的量子抗性算法。通过整合AI驱动的红队测试、自动化渗透测试和实时异常检测，开发了一个评估和缓解量子网络安全风险的框架。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算的发展，量子密码协议面临新的安全挑战。需要开发系统性的方法来评估和加强量子网络的安全性，特别是在AI增强的密码过程中识别潜在的漏洞。

Method: 整合AI驱动的红队测试、自动化渗透测试、实时异常检测、自动化漏洞模拟和协议模糊测试，使用对抗性机器学习技术来识别AI增强密码过程中的新型攻击面。

Result: 研究发现AI能有效模拟对抗性攻击、探测密码实现中的弱点，并通过迭代反馈改进安全机制。自动化漏洞模拟和协议模糊测试提供了识别潜在漏洞的可扩展方法。

Conclusion: 本研究为加强量子安全提供了全面的方法论，并为将AI驱动的网络安全实践整合到不断发展的量子领域奠定了基础。

Abstract: This study presents a structured approach to evaluating vulnerabilities
within quantum cryptographic protocols, focusing on the BB84 quantum key
distribution method and National Institute of Standards and Technology (NIST)
approved quantum-resistant algorithms. By integrating AI-driven red teaming,
automated penetration testing, and real-time anomaly detection, the research
develops a framework for assessing and mitigating security risks in quantum
networks. The findings demonstrate that AI can be effectively used to simulate
adversarial attacks, probe weaknesses in cryptographic implementations, and
refine security mechanisms through iterative feedback. The use of automated
exploit simulations and protocol fuzzing provides a scalable means of
identifying latent vulnerabilities, while adversarial machine learning
techniques highlight novel attack surfaces within AI-enhanced cryptographic
processes. This study offers a comprehensive methodology for strengthening
quantum security and provides a foundation for integrating AI-driven
cybersecurity practices into the evolving quantum landscape.

</details>


### [8] [TRUSTCHECKPOINTS: Time Betrays Malware for Unconditional Software Root of Trust](https://arxiv.org/abs/2509.22762)
*Friedrich Doku,Peter Dinda*

Main category: cs.CR

TL;DR: TRUSTCHECKPOINTS是首个基于形式化模型的无条件软件信任根系统，无需依赖密钥或可信硬件。通过捕获完整系统检查点并回滚验证，使用时序约束的随机多项式挑战检测恶意代码。


<details>
  <summary>Details</summary>
Motivation: 现代物联网和嵌入式平台需要从已知可信状态启动以抵御恶意软件，但现有方法依赖密钥或专用安全硬件，增加了成本、复杂性和第三方依赖。

Method: 开发者捕获完整系统检查点，验证器通过时序约束的随机k独立多项式挑战（使用霍纳法则）重复扫描快速片上内存。恶意代码尝试持久化时会交换到较慢的片外存储，导致可检测的时序延迟。

Result: 在商用ARM Cortex-A53平台上，原型系统在10秒内验证192KB SRAM，使用500次扫描足以检测单指令持久恶意软件，并可无缝扩展到DRAM信任。

Conclusion: 系统提供了快速SRAM引导和全面全内存扫描两种模式，在未修改硬件上实现了可靠的恶意软件检测，展示了无条件软件信任根的可行性。

Abstract: Modern IoT and embedded platforms must start execution from a known trusted
state to thwart malware, ensure secure firmware updates, and protect critical
infrastructure. Current approaches to establish a root of trust depend on
secret keys and/or specialized secure hardware, which drives up costs, may
involve third parties, adds operational complexity, and relies on assumptions
about an attacker's computational power. In contrast, TRUSTCHECKPOINTS is the
first system to establish an unconditional software root of trust based on a
formal model without relying on secrets or trusted hardware. Developers capture
a full-system checkpoint and later roll back to it and prove this to an
external verifier. The verifier issues timing-constrained, randomized
k-independent polynomial challenges (via Horner's rule) that repeatedly scan
the fast on-chip memory in randomized passes. When malicious code attempts to
persist, it must swap into slower, unchecked off-chip storage, causing a
detectable timing delay.
  Our prototype for a commodity ARM Cortex-A53-based platform validates 192 KB
of SRAM in approximately 10 s using 500 passes, sufficient to detect
single-instruction persistent malware. The prototype then seamlessly extends
trust to DRAM. Two modes (fast SRAM-bootstrap and comprehensive full-memory
scan) allow trade-offs between speed and coverage, demonstrating reliable
malware detection on unmodified hardware.

</details>


### [9] [What Do They Fix? LLM-Aided Categorization of Security Patches for Critical Memory Bugs](https://arxiv.org/abs/2509.22796)
*Xingyu Li,Juefei Pu,Yifan Wu,Xiaochen Zou,Shitong Zhu,Xiaochen Zou,Shitong Zhu,Qiushi Wu,Zheng Zhang,Joshua Hsu,Yue Dong,Zhiyun Qian,Kangjie Lu,Trent Jaeger,Michael De Lucia,Srikanth V. Krishnamurthy*

Main category: cs.CR

TL;DR: 开发了DUALLM系统，通过结合大语言模型和微调小语言模型的双重方法，显著提高了Linux内核安全补丁分类的准确性，能够有效识别出界访问和使用后释放漏洞补丁。


<details>
  <summary>Details</summary>
Motivation: Linux内核安全补丁在下游维护者中延迟采用，主要原因是难以识别安全关键补丁，特别是那些修复可被利用漏洞的补丁，如出界访问和使用后释放漏洞。

Method: 利用提交标题/信息和差异代码的线索，结合适当的代码上下文，开发了DUALLM双重方法管道，整合了基于大语言模型和微调小语言模型的两种方法。

Result: DUALLM达到87.4%的准确率和0.875的F1分数，显著优于先前解决方案。在5,140个最近Linux内核补丁中成功识别出111个修复出界访问或使用后释放漏洞的补丁。

Conclusion: DUALLM能够有效识别Linux内核中的安全关键补丁，特别是那些修复可被利用漏洞的补丁，为下游维护者及时采用安全补丁提供了有力工具。

Abstract: Open-source software projects are foundational to modern software ecosystems,
with the Linux kernel standing out as a critical exemplar due to its ubiquity
and complexity. Although security patches are continuously integrated into the
Linux mainline kernel, downstream maintainers often delay their adoption,
creating windows of vulnerability. A key reason for this lag is the difficulty
in identifying security-critical patches, particularly those addressing
exploitable vulnerabilities such as out-of-bounds (OOB) accesses and
use-after-free (UAF) bugs. This challenge is exacerbated by intentionally
silent bug fixes, incomplete or missing CVE assignments, delays in CVE
issuance, and recent changes to the CVE assignment criteria for the Linux
kernel. While fine-grained patch classification approaches exist, they exhibit
limitations in both coverage and accuracy. In this work, we identify previously
unexplored opportunities to significantly improve fine-grained patch
classification. Specifically, by leveraging cues from commit titles/messages
and diffs alongside appropriate code context, we develop DUALLM, a dual-method
pipeline that integrates two approaches based on a Large Language Model (LLM)
and a fine-tuned small language model. DUALLM achieves 87.4% accuracy and an
F1-score of 0.875, significantly outperforming prior solutions. Notably, DUALLM
successfully identified 111 of 5,140 recent Linux kernel patches as addressing
OOB or UAF vulnerabilities, with 90 true positives confirmed by manual
verification (many do not have clear indications in patch descriptions).
Moreover, we constructed proof-of-concepts for two identified bugs (one UAF and
one OOB), including one developed to conduct a previously unknown control-flow
hijack as further evidence of the correctness of the classification.

</details>


### [10] [Model Context Protocol for Vision Systems: Audit, Security, and Protocol Extensions](https://arxiv.org/abs/2509.22814)
*Aditi Tiwari,Akshit Bhalla,Darshan Prasad*

Main category: cs.CR

TL;DR: 对91个公开注册的视觉中心MCP服务器进行首次协议级审计，发现系统存在模式语义、互操作性和运行时协调方面的弱点，开发了可执行的基准测试和验证器来检测协议违规。


<details>
  <summary>Details</summary>
Motivation: MCP定义了代理-工具交互的模式绑定执行模型，但缺乏部署规模的协议级审计，需要识别视觉系统中的系统性弱点。

Method: 分析91个公开注册的视觉中心MCP服务器，沿9个组合保真度维度进行标注，开发包含验证器的可执行基准测试来检测和分类协议违规。

Result: 审计显示高比例的格式分歧、缺少运行时模式验证、未声明的坐标约定和依赖未跟踪的桥接脚本。验证器测试量化了这些失败：78.0%系统存在模式格式不匹配，24.6%存在空间参考错误，每100次执行平均发出33.8个内存范围警告。

Conclusion: 提出的基准测试和验证器套件建立了一个可复现的框架，用于度量和改进组合视觉工作流程的可靠性和安全性。

Abstract: The Model Context Protocol (MCP) defines a schema bound execution model for
agent-tool interaction, enabling modular computer vision workflows without
retraining. To our knowledge, this is the first protocol level, deployment
scale audit of MCP in vision systems, identifying systemic weaknesses in schema
semantics, interoperability, and runtime coordination. We analyze 91 publicly
registered vision centric MCP servers, annotated along nine dimensions of
compositional fidelity, and develop an executable benchmark with validators to
detect and categorize protocol violations. The audit reveals high prevalence of
schema format divergence, missing runtime schema validation, undeclared
coordinate conventions, and reliance on untracked bridging scripts. Validator
based testing quantifies these failures, with schema format checks flagging
misalignments in 78.0 percent of systems, coordinate convention checks
detecting spatial reference errors in 24.6 percent, and memory scope checks
issuing an average of 33.8 warnings per 100 executions. Security probes show
that dynamic and multi agent workflows exhibit elevated risks of privilege
escalation and untyped tool connections. The proposed benchmark and validator
suite, implemented in a controlled testbed and to be released on GitHub,
establishes a reproducible framework for measuring and improving the
reliability and security of compositional vision workflows.

</details>


### [11] [PAPER: Privacy-Preserving ResNet Models using Low-Degree Polynomial Approximations and Structural Optimizations on Leveled FHE](https://arxiv.org/abs/2509.22857)
*Eduardo Chielle,Manaar Alam,Jinting Liu,Jovan Kascelan,Michail Maniatakos*

Main category: cs.CR

TL;DR: 提出了一种基于全同态加密的非交互式隐私保护推理方法，通过二次多项式逼近ReLU、结构优化和参数聚类等技术，在ResNet上实现了与明文ReLU模型相当的精度，且推理速度比现有方法快4倍。


<details>
  <summary>Details</summary>
Motivation: 现有基于全同态加密的隐私保护推理方法存在两个主要问题：依赖昂贵的引导操作严重拖慢推理速度，以及使用高次多项式逼近非线性激活函数导致精度下降2-5%。本文旨在解决这些问题。

Method: 使用二次多项式逼近ReLU（达到非线性激活的理论最小乘法深度），结合惩罚训练策略；引入节点融合、权重重分配和塔复用等结构优化；采用参数聚类、数据编码布局和集成技术的联合策略。

Result: 在ResNet-18、ResNet-20和ResNet-32上对CIFAR-10和CIFAR-100的实验表明，相比现有工作，所需FHE层级减少近5倍，推理速度提升4倍，精度与明文ReLU模型相当。

Conclusion: 该方法成功消除了FHE基非交互式模型与明文模型之间的精度差距，同时实现了更快的推理速度，无需引导操作即可在层级FHE下运行ResNet模型。

Abstract: Recent work has made non-interactive privacy-preserving inference more
practical by running deep Convolution Neural Network (CNN) with Fully
Homomorphic Encryption (FHE). However, these methods remain limited by their
reliance on bootstrapping, a costly FHE operation applied across multiple
layers, severely slowing inference. They also depend on high-degree polynomial
approximations of non-linear activations, which increase multiplicative depth
and reduce accuracy by 2-5% compared to plaintext ReLU models. In this work, we
focus on ResNets, a widely adopted benchmark architecture in privacy-preserving
inference, and close the accuracy gap between their FHE-based non-interactive
models and plaintext counterparts, while also achieving faster inference than
existing methods. We use a quadratic polynomial approximation of ReLU, which
achieves the theoretical minimum multiplicative depth for non-linear
activations, along with a penalty-based training strategy. We further introduce
structural optimizations such as node fusing, weight redistribution, and tower
reuse. These optimizations reduce the required FHE levels in CNNs by nearly a
factor of five compared to prior work, allowing us to run ResNet models under
leveled FHE without bootstrapping. To further accelerate inference and recover
accuracy typically lost with polynomial approximations, we introduce parameter
clustering along with a joint strategy of data encoding layout and ensemble
techniques. Experiments with ResNet-18, ResNet-20, and ResNet-32 on CIFAR-10
and CIFAR-100 show that our approach achieves up to 4x faster private inference
than prior work with comparable accuracy to plaintext ReLU models.

</details>


### [12] [AntiFLipper: A Secure and Efficient Defense Against Label-Flipping Attacks in Federated Learning](https://arxiv.org/abs/2509.22873)
*Aashnan Rahman,Abid Hasan,Sherajul Arifin,Faisal Haque Bappy,Tahrim Hossain,Tariqul Islam,Abu Raihan Mostofa Kamal,Md. Azam Hossain*

Main category: cs.CR

TL;DR: AntiFLipper是一种针对联邦学习中标签翻转攻击的新型防御方法，通过客户端检测策略显著降低服务器计算负担，在保持高准确率的同时大幅减少计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能保护数据隐私，但仍易受标签翻转攻击影响，现有防御方法计算开销大，需要一种既能确保安全又高效的防御方案。

Method: 采用新颖的客户端检测策略，在客户端层面识别和防御标签翻转攻击，减轻服务器聚合时的计算负担。

Result: 在多个数据集和不同分布下的综合评估表明，AntiFLipper在准确率上与最先进防御方法相当，但服务器端计算资源需求显著减少。

Conclusion: AntiFLipper在安全性和效率之间取得了良好平衡，特别适合资源受限的联邦学习部署场景，解决了现有防御方法的关键缺陷。

Abstract: Federated learning (FL) enables privacy-preserving model training by keeping
data decentralized. However, it remains vulnerable to label-flipping attacks,
where malicious clients manipulate labels to poison the global model. Despite
their simplicity, these attacks can severely degrade model performance, and
defending against them remains challenging. We introduce AntiFLipper, a novel
and computationally efficient defense against multi-class label-flipping
attacks in FL. Unlike existing methods that ensure security at the cost of high
computational overhead, AntiFLipper employs a novel client-side detection
strategy, significantly reducing the central server's burden during
aggregation. Comprehensive empirical evaluations across multiple datasets under
different distributions demonstrate that AntiFLipper achieves accuracy
comparable to state-of-the-art defenses while requiring substantially fewer
computational resources in server side. By balancing security and efficiency,
AntiFLipper addresses a critical gap in existing defenses, making it
particularly suitable for resource-constrained FL deployments where both model
integrity and operational efficiency are essential.

</details>


### [13] [Towards Context-aware Mobile Privacy Notice: Implementation of A Deployable Contextual Privacy Policies Generator](https://arxiv.org/abs/2509.22900)
*Haochen Gong,Zhen Tao,Shidong Pan,Zhenchang Xing,Xiaoyu Sun*

Main category: cs.CR

TL;DR: PrivScan是首个可部署的Android上下文隐私政策SDK，通过截屏识别GUI元素关联的个人数据类型，以简洁格式显示隐私政策，并提供轻量级浮动按钮实现低摩擦的按需控制。


<details>
  <summary>Details</summary>
Motivation: 冗长且法律化的隐私政策阻碍用户理解移动应用如何收集和处理个人数据，现有上下文隐私政策方法无法在实际移动环境中部署。

Method: 采用远程部署架构，将多模态后端管道与移动客户端解耦，客户端包含五个模块化组件，通过捕获实时应用截图识别GUI元素并关联个人数据类型。

Result: 可行性评估显示平均执行时间为9.15秒，证明了该方法的实用性。

Conclusion: PrivScan提供了一个可部署的解决方案，有效解决了移动应用中隐私政策理解困难的问题，同时降低了设备资源需求并提高了跨平台可移植性。

Abstract: Lengthy and legally phrased privacy policies impede users' understanding of
how mobile applications collect and process personal data. Prior work proposed
Contextual Privacy Policies (CPPs) for mobile apps to display shorter policy
snippets only in the corresponding user interface contexts, but the pipeline
could not be deployable in real-world mobile environments. In this paper, we
present PrivScan, the first deployable CPP Software Development Kit (SDK) for
Android. It captures live app screenshots to identify GUI elements associated
with types of personal data and displays CPPs in a concise, user-facing format.
We provide a lightweight floating button that offers low-friction, on-demand
control. The architecture leverages remote deployment to decouple the
multimodal backend pipeline from a mobile client comprising five modular
components, thereby reducing on-device resource demands and easing
cross-platform portability. A feasibility-oriented evaluation shows an average
execution time of 9.15\,s, demonstrating the practicality of our approach. The
source code of PrivScan is available at https://github.com/buyanghc/PrivScan
and the demo video can be found at https://www.youtube.com/watch?v=ck-25otfyHc.

</details>


### [14] [Blockchain Voting System](https://arxiv.org/abs/2509.22965)
*Yousef Tahboub,Anthony Revilla,Jaydon Lynch,Greg Floyd*

Main category: cs.CR

TL;DR: 提出了一种基于混合区块链的投票模型，将加密选票存储在私有区块链中，同时定期将选票哈希锚定到公共区块链作为防篡改封条，保护投票隐私和结果完整性。


<details>
  <summary>Details</summary>
Motivation: 解决在线投票中选民对投票隐私保护和结果防篡改的担忧，实现安全、可验证且可扩展的在线投票系统。

Method: 使用混合区块链设计：私有区块链存储加密选票，公共区块链锚定哈希值；采用一次性盲签名令牌保护匿名性；提供投票收据供选民确认；使用Next.js、React、Firebase等技术实现原型。

Result: 开发了可工作的演示系统，包含完整的选举工作流程、混合区块链设计、用户友好界面，在隐私、安全、透明性和实用性之间取得平衡。

Conclusion: 该研究证明了从小型团体到大型机构的各种组织都可以实现安全、可验证且可扩展的在线投票系统的可行性。

Abstract: Casting a ballot from a phone or laptop sounds appealing, but only if voters
can be confident their choice remains secret and results cannot be altered in
the dark. This paper proposes a hybrid blockchain-based voting model that
stores encrypted votes on a private blockchain maintained by election
organizers and neutral observers, while periodically anchoring hashes of these
votes onto a public blockchain as a tamper-evident seal. The system issues
voters one-time blind-signed tokens to protect anonymity, and provides receipts
so they can confirm their vote was counted. We implemented a live prototype
using common web technologies (Next.js, React, Firebase) to demonstrate
end-to-end functionality, accessibility, and cost efficiency. Our contributions
include developing a working demo, a complete election workflow, a hybrid
blockchain design, and a user-friendly interface that balances privacy,
security, transparency, and practicality. This research highlights the
feasibility of secure, verifiable, and scalable online voting for organizations
ranging from small groups to larger institutions.

</details>


### [15] [CryptoSRAM: Enabling High-Throughput Cryptography on MCUs via In-SRAM Computing](https://arxiv.org/abs/2509.22986)
*Jingyao Zhang,Elaheh Sadredini*

Main category: cs.CR

TL;DR: CryptoSRAM是一种在SRAM内直接执行加密操作的架构，通过消除内存和处理单元之间的数据移动瓶颈，显著提升了MCU的加密性能。


<details>
  <summary>Details</summary>
Motivation: 物联网设备中的加密解决方案受限于内存和处理单元之间的数据移动性能与能耗成本，需要更高效的架构。

Method: 利用MCU的标准SRAM阵列作为大规模并行处理结构，通过物理寻址和DMA技术实现无缝集成，最小化硬件开销。

Result: 相比软件实现，AES和SHA3的吞吐量分别提升74倍和67倍；相比现有硬件加速器，AES吞吐量提升6倍。

Conclusion: CryptoSRAM为下一代物联网系统的安全通信提供了一种可行且高效的架构解决方案。

Abstract: Secure communication is a critical requirement for Internet of Things (IoT)
devices, which are often based on Microcontroller Units (MCUs). Current
cryptographic solutions, which rely on software libraries or dedicated hardware
accelerators, are fundamentally limited by the performance and energy costs of
data movement between memory and processing units. This paper introduces
CryptoSRAM, an in-SRAM computing architecture that performs cryptographic
operations directly within the MCU's standard SRAM array. By repurposing the
memory array into a massively parallel processing fabric, CryptoSRAM eliminates
the data movement bottleneck. This approach is well-suited to MCUs, which
utilize physical addressing and Direct Memory Access (DMA) to manage SRAM,
allowing for seamless integration with minimal hardware overhead. Our analysis
shows that for common cryptographic kernels, CryptoSRAM achieves throughput
improvements of up to 74$\times$ and 67$\times$ for AES and SHA3, respectively,
compared to a software implementation. Furthermore, our solution delivers up to
6$\times$ higher throughput than existing hardware accelerators for AES.
CryptoSRAM demonstrates a viable and efficient architecture for secure
communication in next-generation IoT systems.

</details>


### [16] [LLM Watermark Evasion via Bias Inversion](https://arxiv.org/abs/2509.23019)
*Jeongyeon Hwang,Sangdon Park,Jungseul Ok*

Main category: cs.CR

TL;DR: 提出了一种名为BIRA的对抗攻击方法，能够在不了解水印方案的情况下，通过抑制可能带有水印标记的logits来削弱LLM水印信号，实现超过99%的规避率。


<details>
  <summary>Details</summary>
Motivation: 虽然水印技术在良性环境下有效，但其在对抗性规避下的鲁棒性仍存在争议，需要对这些漏洞进行严格理解和评估。

Method: 提出Bias-Inversion Rewriting Attack (BIRA)方法，这是一种理论驱动且模型无关的攻击，通过在LLM重写过程中抑制可能带有水印标记的logits来削弱水印信号。

Result: 在最近的水印方法上，BIRA实现了超过99%的规避率，同时保留了原始文本的语义内容。

Conclusion: 研究结果揭示了系统性的漏洞，强调了压力测试和鲁棒防御的必要性。

Abstract: Watermarking for large language models (LLMs) embeds a statistical signal
during generation to enable detection of model-produced text. While
watermarking has proven effective in benign settings, its robustness under
adversarial evasion remains contested. To advance a rigorous understanding and
evaluation of such vulnerabilities, we propose the \emph{Bias-Inversion
Rewriting Attack} (BIRA), which is theoretically motivated and model-agnostic.
BIRA weakens the watermark signal by suppressing the logits of likely
watermarked tokens during LLM-based rewriting, without any knowledge of the
underlying watermarking scheme. Across recent watermarking methods, BIRA
achieves over 99\% evasion while preserving the semantic content of the
original text. Beyond demonstrating an attack, our results reveal a systematic
vulnerability, emphasizing the need for stress testing and robust defenses.

</details>


### [17] [Virus Infection Attack on LLMs: Your Poisoning Can Spread "VIA" Synthetic Data](https://arxiv.org/abs/2509.23041)
*Zi Liang,Qingqing Ye,Xuan Liu,Yanyun Wang,Jianliang Xu,Haibo Hu*

Main category: cs.CR

TL;DR: 本文系统评估了合成数据集成训练范式对主流投毒和后门攻击的抵抗力，发现该范式对现有攻击具有强抵抗力。为增强攻击效果，作者提出了病毒感染攻击(VIA)框架，通过将投毒载荷隐藏在保护壳中，显著提高了合成数据中的投毒内容比例和下游模型的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 合成数据虽能显著提升大语言模型性能，但其可能引入的安全风险尚未得到充分研究。本文旨在系统评估合成数据集成训练范式对投毒和后门攻击的抵抗力，并探索相关安全风险。

Method: 提出病毒感染攻击(VIA)框架，受网络安全中病毒设计原则启发，将投毒载荷隐藏在保护壳中，在良性样本中策略性搜索最优劫持点以最大化恶意内容生成概率。

Result: 实验表明，合成数据集成训练范式对现有攻击具有强抵抗力。VIA框架显著提高了合成数据中的投毒内容比例，使下游模型的攻击成功率提升到与上游投毒模型相当的水平。

Conclusion: 合成数据集成训练虽然对传统攻击具有抵抗力，但通过VIA等新型攻击框架仍可能引入严重安全风险，需要更全面的安全防护措施。

Abstract: Synthetic data refers to artificial samples generated by models. While it has
been validated to significantly enhance the performance of large language
models (LLMs) during training and has been widely adopted in LLM development,
potential security risks it may introduce remain uninvestigated. This paper
systematically evaluates the resilience of synthetic-data-integrated training
paradigm for LLMs against mainstream poisoning and backdoor attacks. We reveal
that such a paradigm exhibits strong resistance to existing attacks, primarily
thanks to the different distribution patterns between poisoning data and
queries used to generate synthetic samples. To enhance the effectiveness of
these attacks and further investigate the security risks introduced by
synthetic data, we introduce a novel and universal attack framework, namely,
Virus Infection Attack (VIA), which enables the propagation of current attacks
through synthetic data even under purely clean queries. Inspired by the
principles of virus design in cybersecurity, VIA conceals the poisoning payload
within a protective "shell" and strategically searches for optimal hijacking
points in benign samples to maximize the likelihood of generating malicious
content. Extensive experiments on both data poisoning and backdoor attacks show
that VIA significantly increases the presence of poisoning content in synthetic
data and correspondingly raises the attack success rate (ASR) on downstream
models to levels comparable to those observed in the poisoned upstream models.

</details>


### [18] [FedBit: Accelerating Privacy-Preserving Federated Learning via Bit-Interleaved Packing and Cross-Layer Co-Design](https://arxiv.org/abs/2509.23091)
*Xiangchen Meng,Yangdi Lyu*

Main category: cs.CR

TL;DR: FedBit是一个硬件/软件协同设计的框架，针对BFV全同态加密方案进行优化，通过位交错数据打包和FPGA加速器，显著降低了联邦学习中的计算和通信开销。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中使用全同态加密虽然能保护数据隐私，但带来了巨大的计算负担和密文膨胀问题，增加了资源和通信开销。

Method: 采用位交错数据打包将多个模型参数嵌入单个密文系数中，集成专用FPGA加速器处理密码操作，并优化数据流以减少内存开销。

Result: 实验结果显示FedBit在加密速度上提升了两个数量级，平均通信开销降低了60.7%，同时保持了高精度。

Conclusion: FedBit框架有效解决了联邦学习中全同态加密带来的计算和通信瓶颈问题，实现了隐私保护与效率的平衡。

Abstract: Federated learning (FL) with fully homomorphic encryption (FHE) effectively
safeguards data privacy during model aggregation by encrypting local model
updates before transmission, mitigating threats from untrusted servers or
eavesdroppers in transmission. However, the computational burden and ciphertext
expansion associated with homomorphic encryption can significantly increase
resource and communication overhead. To address these challenges, we propose
FedBit, a hardware/software co-designed framework optimized for the
Brakerski-Fan-Vercauteren (BFV) scheme. FedBit employs bit-interleaved data
packing to embed multiple model parameters into a single ciphertext
coefficient, thereby minimizing ciphertext expansion and maximizing
computational parallelism. Additionally, we integrate a dedicated FPGA
accelerator to handle cryptographic operations and an optimized dataflow to
reduce the memory overhead. Experimental results demonstrate that FedBit
achieves a speedup of two orders of magnitude in encryption and lowers average
communication overhead by 60.7%, while maintaining high accuracy.

</details>


### [19] [ICS-SimLab: A Containerized Approach for Simulating Industrial Control Systems for Cyber Security Research](https://arxiv.org/abs/2509.23305)
*Jaxson Brown,Duc-Son Pham,Sie-Teng Soh,Foad Motalebi,Sivaraman Eswaran,Mahathir Almashor*

Main category: cs.CR

TL;DR: ICS-SimLab是一个基于Docker容器化技术的可配置工业控制系统仿真环境，支持快速构建不同ICS架构，用于开发网络安全解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着工业控制系统向互联网服务发展，面临攻击风险增加，需要专门的入侵检测系统。现有测试平台通常只使用有限的虚拟ICS仿真，缺乏能够高效模拟多种ICS架构的系统。

Method: 利用Docker容器化技术开发端到端软件套件，创建高度可配置的ICS仿真环境，支持按普渡企业参考架构构建不同系统。

Result: 构建了三个虚拟ICS仿真：太阳能智能电网、水瓶灌装设施和智能电子设备系统，并运行网络攻击生成包含恶意和良性网络流量的数据集。

Conclusion: ICS-SimLab为研究人员提供了快速构建和定制不同ICS环境的能力，促进了跨不同系统的安全解决方案开发。

Abstract: Industrial Control Systems (ICSs) are complex interconnected systems used to
manage process control within industrial environments, such as chemical
processing plants and water treatment facilities. As the modern industrial
environment moves towards Internet-facing services, ICSs face an increased risk
of attacks that necessitates ICS-specific Intrusion Detection Systems (IDS).
The development of such IDS relies significantly on a simulated testbed as it
is unrealistic and sometimes hazardous to utilize an operational control
system. Whilst some testbeds have been proposed, they often use a limited
selection of virtual ICS simulations to test and verify cyber security
solutions. There is a lack of investigation done on developing systems that can
efficiently simulate multiple ICS architectures. Currently, the trend within
research involves developing security solutions on just one ICS simulation,
which can result in bias to its specific architecture. We present ICS-SimLab,
an end-to-end software suite that utilizes Docker containerization technology
to create a highly configurable ICS simulation environment. This software
framework enables researchers to rapidly build and customize different ICS
environments, facilitating the development of security solutions across
different systems that adhere to the Purdue Enterprise Reference Architecture.
To demonstrate its capability, we present three virtual ICS simulations: a
solar panel smart grid, a water bottle filling facility, and a system of
intelligent electronic devices. Furthermore, we run cyber-attacks on these
simulations and construct a dataset of recorded malicious and benign network
traffic to be used for IDS development.

</details>


### [20] [Detecting YouTube Scam Videos via Multimodal Signals and Policy Reasoning](https://arxiv.org/abs/2509.23418)
*Ummay Kulsum,Aafaq Sabir,Abhinaya S. B.,Anupam Das*

Main category: cs.CR

TL;DR: 该论文首次系统研究YouTube诈骗检测的多模态方法，通过整合视频标题、描述和视频帧的多模态框架实现了80.53% F1的最高性能，并提供了基于YouTube内容政策的可解释推理。


<details>
  <summary>Details</summary>
Motivation: YouTube作为主要信息平台面临诈骗内容泛滥问题，现有基于文本或统计元数据的检测方法容易被规避且忽略了视觉线索等模态信息。

Method: 构建包含完整视频内容和政策基础推理标注的数据集，比较文本模型（BERT）、音频转录模型和视觉模型（LLaVA-Video），最终开发整合标题、描述和视频帧的多模态框架。

Result: 文本模型F1为76.61%，加入音频后提升至77.98%，视觉模型达79.61%，多模态框架最优为80.53%。在6,374个真实YouTube视频上验证有效。

Conclusion: 多模态方法显著提升YouTube诈骗检测性能，提供可解释的政策基础推理，增强透明度并支持自动化内容审核应用。

Abstract: YouTube has emerged as a dominant platform for both information dissemination
and entertainment. However, its vast accessibility has also made it a target
for scammers, who frequently upload deceptive or malicious content. Prior
research has documented a range of scam types, and detection approaches rely
primarily on textual or statistical metadata. Although effective to some
extent, these signals are easy to evade and potentially overlook other
modalities, such as visual cues.
  In this study, we present the first systematic investigation of multimodal
approaches for YouTube scam detection. Our dataset consolidates established
scam categories and augments them with full length video content and policy
grounded reasoning annotations. Our experimental evaluation demonstrates that a
text-only model using video titles and descriptions (fine-tuned BERT) achieves
moderate effectiveness (76.61% F1), with modest improvements when incorporating
audio transcripts (77.98% F1). In contrast, visual analysis using a fine-tuned
LLaVA-Video model yields stronger results (79.61% F1). Finally, a multimodal
framework that integrates titles, descriptions, and video frames achieves the
highest performance (80.53% F1). Beyond improving detection accuracy, our
multimodal framework produces interpretable reasoning grounded in YouTube
content policies, thereby enhancing transparency and supporting potential
applications in automated moderation. Moreover, we validate our approach on
in-the-wild YouTube data by analyzing 6,374 videos, thereby contributing a
valuable resource for future research on scam detection.

</details>


### [21] [StarveSpam: Mitigating Spam with Local Reputation in Permissionless Blockchains](https://arxiv.org/abs/2509.23427)
*Rowdy Chotkan,Bulat Nasrulin,Jérémie Decouchant,Johan Pouwelse*

Main category: cs.CR

TL;DR: StarveSpam是一种基于信誉的去中心化协议，通过在交易中继层操作来缓解区块链垃圾邮件问题，结合本地行为跟踪、对等评分和自适应速率限制来抑制恶意行为。


<details>
  <summary>Details</summary>
Motivation: 区块链网络面临日益严重的垃圾邮件威胁，现有基于经济威慑的防御方法无法区分恶意和合法用户，且常常排除低价值但诚实的活动。

Method: StarveSpam协议结合本地行为跟踪、对等评分和自适应速率限制，在交易中继层操作，无需全局共识、协议变更或可信基础设施。

Result: 使用真实以太坊NFT垃圾邮件事件数据评估，StarveSpam能让每个节点阻止超过95%的垃圾邮件，仅丢弃3%的诚实流量，与现有基于规则的方法相比，网络暴露于垃圾邮件的比例减少85%。

Conclusion: StarveSpam为传统垃圾邮件防御提供了可扩展且可部署的替代方案，为构建更具弹性和公平性的区块链基础设施铺平道路。

Abstract: Spam poses a growing threat to blockchain networks. Adversaries can easily
create multiple accounts to flood transaction pools, inflating fees and
degrading service quality. Existing defenses against spam, such as fee markets
and staking requirements, primarily rely on economic deterrence, which fails to
distinguish between malicious and legitimate users and often exclude low-value
but honest activity. To address these shortcomings, we present StarveSpam, a
decentralized reputation-based protocol that mitigates spam by operating at the
transaction relay layer. StarveSpam combines local behavior tracking, peer
scoring, and adaptive rate-limiting to suppress abusive actors, without
requiring global consensus, protocol changes, or trusted infrastructure. We
evaluate StarveSpam using real Ethereum data from a major NFT spam event and
show that it outperforms existing fee-based and rule-based defenses, allowing
each node to block over 95% of spam while dropping just 3% of honest traffic,
and reducing the fraction of the network exposed to spam by 85% compared to
existing rule-based methods. StarveSpam offers a scalable and deployable
alternative to traditional spam defenses, paving the way toward more resilient
and equitable blockchain infrastructure.

</details>


### [22] [MaskSQL: Safeguarding Privacy for LLM-Based Text-to-SQL via Abstraction](https://arxiv.org/abs/2509.23459)
*Sepideh Abedini,Shubhankar Mohapatra,D. B. Emerson,Masoumeh Shafieinejad,Jesse C. Cresswell,Xi He*

Main category: cs.CR

TL;DR: MaskSQL是一个文本到SQL框架，使用抽象作为隐私保护机制来掩盖LLM提示中的敏感信息，在保护隐私的同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)在推理任务上表现优异，但由于隐私法规限制、成本高昂且资源密集，难以在敏感系统中部署。小型语言模型(SLMs)虽然可以本地部署保护隐私，但在复杂任务如文本到SQL转换上表现不佳。

Method: MaskSQL采用抽象作为隐私保护机制，不同于完全删除内容的编辑或扩大标记的泛化，抽象保留必要信息同时丢弃不必要细节，为文本到SQL任务实现了有效的隐私-效用平衡。

Result: 实验结果显示MaskSQL在性能上超越了领先的基于SLM的文本到SQL模型，并接近最先进的基于LLM的模型，同时保持了隐私保护。

Conclusion: MaskSQL通过抽象机制在隐私保护和性能之间取得了良好平衡，为更广泛的应用场景提供了可行的解决方案。

Abstract: Large language models (LLMs) have shown promising performance on tasks that
require reasoning, such as text-to-SQL, code generation, and debugging.
However, regulatory frameworks with strict privacy requirements constrain their
integration into sensitive systems. State-of-the-art LLMs are also proprietary,
costly, and resource-intensive, making local deployment impractical.
Consequently, utilizing such LLMs often requires sharing data with third-party
providers, raising privacy concerns and risking noncompliance with regulations.
Although fine-tuned small language models (SLMs) can outperform LLMs on certain
tasks and be deployed locally to mitigate privacy concerns, they underperform
on more complex tasks such as text-to-SQL translation. In this work, we
introduce MaskSQL, a text-to-SQL framework that utilizes abstraction as a
privacy protection mechanism to mask sensitive information in LLM prompts.
Unlike redaction, which removes content entirely, or generalization, which
broadens tokens, abstraction retains essential information while discarding
unnecessary details, striking an effective privacy-utility balance for the
text-to-SQL task. Moreover, by providing mechanisms to control the
privacy-utility tradeoff, MaskSQL facilitates adoption across a broader range
of use cases. Our experimental results show that MaskSQL outperforms leading
SLM-based text-to-SQL models and achieves performance approaching
state-of-the-art LLM-based models, while preserving privacy.

</details>


### [23] [ReliabilityRAG: Effective and Provably Robust Defense for RAG-based Web-Search](https://arxiv.org/abs/2509.23519)
*Zeyu Shen,Basileal Imana,Tong Wu,Chong Xiang,Prateek Mittal,Aleksandra Korolova*

Main category: cs.CR

TL;DR: 提出了ReliabilityRAG框架，通过利用检索文档的可靠性信息来增强RAG系统的对抗鲁棒性，包含基于最大独立集的图论方法和可扩展的加权采样聚合框架。


<details>
  <summary>Details</summary>
Motivation: RAG系统容易受到检索语料库攻击（如提示注入），特别是在搜索系统中，可以利用文档排名等可靠性信号来防御攻击。

Method: 1) 图论方法：在文档图中寻找最大独立集，优先选择高可靠性文档；2) 可扩展框架：加权采样聚合，利用可靠性信息高效处理大量文档。

Result: 相比现有方法，ReliabilityRAG提供更好的对抗攻击鲁棒性，保持高良性准确率，在长文本生成任务中表现优异。

Conclusion: 这是朝着更有效、可证明鲁棒的RAG检索语料库腐败防御的重要一步。

Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models by
grounding their outputs in external documents. These systems, however, remain
vulnerable to attacks on the retrieval corpus, such as prompt injection.
RAG-based search systems (e.g., Google's Search AI Overview) present an
interesting setting for studying and protecting against such threats, as
defense algorithms can benefit from built-in reliability signals -- like
document ranking -- and represent a non-LLM challenge for the adversary due to
decades of work to thwart SEO.
  Motivated by, but not limited to, this scenario, this work introduces
ReliabilityRAG, a framework for adversarial robustness that explicitly
leverages reliability information of retrieved documents.
  Our first contribution adopts a graph-theoretic perspective to identify a
"consistent majority" among retrieved documents to filter out malicious ones.
We introduce a novel algorithm based on finding a Maximum Independent Set (MIS)
on a document graph where edges encode contradiction. Our MIS variant
explicitly prioritizes higher-reliability documents and provides provable
robustness guarantees against bounded adversarial corruption under natural
assumptions. Recognizing the computational cost of exact MIS for large
retrieval sets, our second contribution is a scalable weighted sample and
aggregate framework. It explicitly utilizes reliability information, preserving
some robustness guarantees while efficiently handling many documents.
  We present empirical results showing ReliabilityRAG provides superior
robustness against adversarial attacks compared to prior methods, maintains
high benign accuracy, and excels in long-form generation tasks where prior
robustness-focused methods struggled. Our work is a significant step towards
more effective, provably robust defenses against retrieved corpus corruption in
RAG.

</details>


### [24] [Benchmarking LLM-Assisted Blue Teaming via Standardized Threat Hunting](https://arxiv.org/abs/2509.23571)
*Yuqiao Meng,Luoxi Tang,Feiyang Yu,Xi Li,Guanhua Yan,Ping Yang,Zhaohan Xi*

Main category: cs.CR

TL;DR: CyberTeam是一个用于指导LLM进行蓝队威胁狩猎的基准框架，通过标准化工作流将威胁狩猎转化为结构化推理步骤，包含30个任务和9个操作模块。


<details>
  <summary>Details</summary>
Motivation: 随着网络威胁规模和复杂性的增长，蓝队防御者需要先进工具来主动检测和缓解风险。LLM在威胁分析方面具有潜力，但其在真实蓝队威胁狩猎场景中的有效性尚未充分探索。

Method: CyberTeam构建了一个两阶段标准化工作流：首先模拟真实威胁狩猎工作流，捕捉从威胁归因到事件响应的分析任务依赖关系；然后通过针对特定分析需求的操作模块处理每个任务，将威胁狩猎转化为结构化推理步骤序列。

Result: 评估了领先的LLM和最先进的网络安全代理，将CyberTeam与开放式推理策略进行比较。结果显示标准化设计带来了改进，同时揭示了开放式推理在真实威胁狩猎中的局限性。

Conclusion: CyberTeam通过标准化框架有效指导LLM执行威胁狩猎任务，证明了结构化方法相比开放式推理在蓝队实践中的优势。

Abstract: As cyber threats continue to grow in scale and sophistication, blue team
defenders increasingly require advanced tools to proactively detect and
mitigate risks. Large Language Models (LLMs) offer promising capabilities for
enhancing threat analysis. However, their effectiveness in real-world blue team
threat-hunting scenarios remains insufficiently explored. This paper presents
CyberTeam, a benchmark designed to guide LLMs in blue teaming practice.
CyberTeam constructs a standardized workflow in two stages. First, it models
realistic threat-hunting workflows by capturing the dependencies among
analytical tasks from threat attribution to incident response. Next, each task
is addressed through a set of operational modules tailored to its specific
analytical requirements. This transforms threat hunting into a structured
sequence of reasoning steps, with each step grounded in a discrete operation
and ordered according to task-specific dependencies. Guided by this framework,
LLMs are directed to perform threat-hunting tasks through modularized steps.
Overall, CyberTeam integrates 30 tasks and 9 operational modules to guide LLMs
through standardized threat analysis. We evaluate both leading LLMs and
state-of-the-art cybersecurity agents, comparing CyberTeam against open-ended
reasoning strategies. Our results highlight the improvements enabled by
standardized design, while also revealing the limitations of open-ended
reasoning in real-world threat hunting.

</details>


### [25] [Uncovering Vulnerabilities of LLM-Assisted Cyber Threat Intelligence](https://arxiv.org/abs/2509.23573)
*Yuqiao Meng,Luoxi Tang,Feiyang Yu,Jinyuan Jia,Guanhua Yan,Ping Yang,Zhaohan Xi*

Main category: cs.CR

TL;DR: 本文研究了大型语言模型在网络安全威胁情报中的内在漏洞，揭示了三种基本脆弱性：伪相关性、矛盾知识和受限泛化能力，这些限制了LLM在CTI任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs被广泛用于支持网络安全威胁情报任务，但在实际部署中存在显著的性能差距。本文旨在研究LLMs在CTI中的内在漏洞，这些漏洞源于威胁态势本身的性质而非模型架构。

Method: 通过跨多个CTI基准和真实世界威胁报告的大规模评估，引入了一种新颖的分类方法，该方法整合了分层、自回归细化和人在回路监督，以可靠地分析失败实例。

Result: 通过广泛的实验和人工检查，揭示了三种基本脆弱性：伪相关性、矛盾知识和受限泛化，这些限制了LLMs在有效支持CTI方面的能力。

Conclusion: 为设计更强大的LLM驱动的CTI系统提供了可操作的见解，以促进未来研究。

Abstract: Large Language Models (LLMs) are intensively used to assist security analysts
in counteracting the rapid exploitation of cyber threats, wherein LLMs offer
cyber threat intelligence (CTI) to support vulnerability assessment and
incident response. While recent work has shown that LLMs can support a wide
range of CTI tasks such as threat analysis, vulnerability detection, and
intrusion defense, significant performance gaps persist in practical
deployments. In this paper, we investigate the intrinsic vulnerabilities of
LLMs in CTI, focusing on challenges that arise from the nature of the threat
landscape itself rather than the model architecture. Using large-scale
evaluations across multiple CTI benchmarks and real-world threat reports, we
introduce a novel categorization methodology that integrates stratification,
autoregressive refinement, and human-in-the-loop supervision to reliably
analyze failure instances. Through extensive experiments and human inspections,
we reveal three fundamental vulnerabilities: spurious correlations,
contradictory knowledge, and constrained generalization, that limit LLMs in
effectively supporting CTI. Subsequently, we provide actionable insights for
designing more robust LLM-powered CTI systems to facilitate future research.

</details>


### [26] [StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data](https://arxiv.org/abs/2509.23594)
*Yixu Wang,Yan Teng,Yingchun Wang,Xingjun Ma*

Main category: cs.CR

TL;DR: 本文提出了一种针对LoRA适配模型的新型模型提取攻击方法StolenLoRA，使用合成数据和半监督学习策略，在仅需1万次查询的情况下攻击成功率高达96.60%，揭示了LoRA适配模型的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: LoRA等参数高效微调方法虽然推动了视觉模型的快速部署，但其紧凑性带来了新的安全隐患，特别是对模型提取攻击的脆弱性。本文旨在研究针对LoRA适配模型的提取攻击。

Method: 提出StolenLoRA攻击方法：1）利用大语言模型生成有效提示来合成数据；2）采用基于分歧的半监督学习策略从有限查询中最大化信息增益；3）训练替代模型来提取LoRA适配模型的功能。

Result: 实验证明StolenLoRA非常有效，在仅1万次查询下攻击成功率高达96.60%，即使在攻击者和受害者模型使用不同预训练骨干网络的跨骨干场景中也能成功。

Conclusion: 研究揭示了LoRA适配模型对提取攻击的特定脆弱性，强调了为PEFT方法定制鲁棒防御机制的紧迫性。初步探索了基于多样化LoRA部署的防御策略，显示出缓解此类攻击的潜力。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have transformed
vision model adaptation, enabling the rapid deployment of customized models.
However, the compactness of LoRA adaptations introduces new safety concerns,
particularly their vulnerability to model extraction attacks. This paper
introduces a new focus of model extraction attacks named LoRA extraction that
extracts LoRA-adaptive models based on a public pre-trained model. We then
propose a novel extraction method called StolenLoRA which trains a substitute
model to extract the functionality of a LoRA-adapted model using synthetic
data. StolenLoRA leverages a Large Language Model to craft effective prompts
for data generation, and it incorporates a Disagreement-based Semi-supervised
Learning (DSL) strategy to maximize information gain from limited queries. Our
experiments demonstrate the effectiveness of StolenLoRA, achieving up to a
96.60% attack success rate with only 10k queries, even in cross-backbone
scenarios where the attacker and victim models utilize different pre-trained
backbones. These findings reveal the specific vulnerability of LoRA-adapted
models to this type of extraction and underscore the urgent need for robust
defense mechanisms tailored to PEFT methods. We also explore a preliminary
defense strategy based on diversified LoRA deployments, highlighting its
potential to mitigate such attacks.

</details>


### [27] [AutoML in Cybersecurity: An Empirical Study](https://arxiv.org/abs/2509.23621)
*Sherif Saad,Kevin Shi,Mohammed Mamun,Hythem Elmiligi*

Main category: cs.CR

TL;DR: 系统评估8个开源AutoML框架在11个网络安全数据集上的表现，发现性能差异显著，没有单一最优方案，并识别了对抗性脆弱性、模型漂移等关键挑战。


<details>
  <summary>Details</summary>
Motivation: 探索AutoML在复杂网络安全领域的可靠性，目前该领域的研究仍不充分。

Method: 使用8个开源AutoML框架在11个公开网络安全数据集上进行系统性评估，涵盖入侵检测、恶意软件分类、钓鱼检测、欺诈检测和垃圾邮件过滤等任务。

Result: 不同工具和数据集间存在显著性能差异，没有单一解决方案始终最优；AutoML工具倾向于选择基于树的模型，虽然性能良好但存在过拟合风险且可解释性受限。

Conclusion: 提出了最佳实践和研究方向，以增强AutoML在关键网络安全应用中的鲁棒性、可解释性和可信度。

Abstract: Automated machine learning (AutoML) has emerged as a promising paradigm for
automating machine learning (ML) pipeline design, broadening AI adoption. Yet
its reliability in complex domains such as cybersecurity remains underexplored.
This paper systematically evaluates eight open-source AutoML frameworks across
11 publicly available cybersecurity datasets, spanning intrusion detection,
malware classification, phishing, fraud detection, and spam filtering. Results
show substantial performance variability across tools and datasets, with no
single solution consistently superior. A paradigm shift is observed: the
challenge has moved from selecting individual ML models to identifying the most
suitable AutoML framework, complicated by differences in runtime efficiency,
automation capabilities, and supported features. AutoML tools frequently favor
tree-based models, which perform well but risk overfitting and limit
interpretability. Key challenges identified include adversarial vulnerability,
model drift, and inadequate feature engineering. We conclude with best
practices and research directions to strengthen robustness, interpretability,
and trust in AutoML for high-stakes cybersecurity applications.

</details>


### [28] [A First Look at Privacy Risks of Android Task-executable Voice Assistant Applications](https://arxiv.org/abs/2509.23680)
*Shidong Pan,Yikai Ge,Xiaoyu Sun*

Main category: cs.CR

TL;DR: 对Android任务执行型语音助手进行用户中心的隐私风险实证研究，发现隐私声明普遍不一致，并识别出三大隐私威胁模型：超级应用中的隐私披露不足、应用间交互的权限提升、以及滥用Google系统应用规避危险权限声明。


<details>
  <summary>Details</summary>
Motivation: 随着基础AI技术的发展，任务执行型语音助手日益普及，但缺乏从任务执行模式角度全面审视其隐私风险的研究。现有工作未能在整体层面分析语音助手的隐私风险。

Method: 收集10个主流Android任务执行型语音助手应用，分析其操作特性，并交叉检查六个来源的隐私声明（包括隐私标签、政策和清单文件）。

Result: 发现隐私声明存在普遍不一致性，识别出三大隐私威胁模型：超级应用中集成小应用（如Alexa技能）的隐私披露不足；通过Android通信机制绕过用户同意的应用间交互权限提升；滥用Google系统应用规避危险权限声明。

Conclusion: 研究为从业者提供了可操作建议，并强调这些隐私风险对新兴自主AI代理具有更广泛的相关性。

Abstract: With the development of foundation AI technologies, task-executable voice
assistants (VAs) have become more popular, enhancing user convenience and
expanding device functionality. Android task-executable VAs are applications
that are capable of understanding complex tasks and performing corresponding
operations. Given their prevalence and great autonomy, there is no existing
work examine the privacy risks within the voice assistants from the
task-execution pattern in a holistic manner. To fill this research gap, this
paper presents a user-centric comprehensive empirical study on privacy risks in
Android task-executable VA applications. We collect ten mainstream VAs as our
research target and analyze their operational characteristics. We then
cross-check their privacy declarations across six sources, including privacy
labels, policies, and manifest files, and our findings reveal widespread
inconsistencies. Moreover, we uncover three significant privacy threat models:
(1) privacy misdisclosure in mega apps, where integrated mini apps such as
Alexa skills are inadequately represented; (2) privilege escalation via
inter-application interactions, which exploit Android's communication
mechanisms to bypass user consent; and (3) abuse of Google system applications,
enabling apps to evade the declaration of dangerous permissions. Our study
contributes actionable recommendations for practitioners and underscores
broader relevance of these privacy risks to emerging autonomous AI agents.

</details>


### [29] [GPM: The Gaussian Pancake Mechanism for Planting Undetectable Backdoors in Differential Privacy](https://arxiv.org/abs/2509.23834)
*Haochen Sun,Xi He*

Main category: cs.CR

TL;DR: 提出高斯煎饼机制(GPM)，这是一种与标准高斯机制计算上不可区分但统计隐私保护更弱的机制，能够实现隐蔽的后门攻击。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私机制假设正确实现，但实际中常因数值问题和配置不当导致隐私泄露。研究能否将被动缺陷转化为主动攻击并保持隐蔽性。

Method: 设计高斯煎饼机制(GPM)，形式上与高斯机制不可区分但统计隐私保证任意弱化，通过理论证明隐蔽性和统计泄露特性。

Result: GPM能够实现近乎完美的区分攻击成功率，理论和实验均验证其有效性，展示了隐蔽隐私攻击的可行性。

Conclusion: 强调使用透明开源DP库的重要性，需要对DP实现进行严格审查和形式化验证，防止现实系统中微妙且不可检测的隐私泄露。

Abstract: Differential privacy (DP) has become the gold standard for preserving
individual privacy in data analysis. However, an implicit yet fundamental
assumption underlying these rigorous privacy guarantees is the correct
implementation and execution of DP mechanisms. Several incidents of unintended
privacy loss have occurred due to numerical issues and inappropriate
configurations of DP software, which have been successfully exploited in
privacy attacks. To better understand the seriousness of defective DP software,
we ask the following question: is it possible to elevate these passive defects
into active privacy attacks while maintaining covertness?
  To address this question, we present the Gaussian pancake mechanism (GPM), a
novel mechanism that is computationally indistinguishable from the widely used
Gaussian mechanism (GM), yet exhibits arbitrarily weaker statistical DP
guarantees. This unprecedented separation enables a new class of backdoor
attacks: by indistinguishably passing off as the authentic GM, GPM can covertly
degrade statistical privacy. Unlike the unintentional privacy loss caused by
GM's numerical issues, GPM is an adversarial yet undetectable backdoor attack
against data privacy. We formally prove GPM's covertness, characterize its
statistical leakage, and demonstrate a concrete distinguishing attack that can
achieve near-perfect success rates under suitable parameter choices, both
theoretically and empirically.
  Our results underscore the importance of using transparent, open-source DP
libraries and highlight the need for rigorous scrutiny and formal verification
of DP implementations to prevent subtle, undetectable privacy compromises in
real-world systems.

</details>


### [30] [Taught Well Learned Ill: Towards Distillation-conditional Backdoor Attack](https://arxiv.org/abs/2509.23871)
*Yukun Chen,Boheng Li,Yu Yuan,Leyi Qi,Yiming Li,Tianwei Zhang,Zhan Qin,Kui Ren*

Main category: cs.CR

TL;DR: 本文提出了一种新的知识蒸馏安全威胁——蒸馏条件后门攻击（DCBA），通过双层优化方法SCAR在教师模型中植入休眠后门，这些后门在知识蒸馏过程中被激活到学生模型中。


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏技术被广泛用于资源受限设备部署深度神经网络，但第三方平台的教师模型可能存在安全风险。现有安全验证（如后门检测）无法发现这种新型威胁。

Method: 将DCBA建模为双层优化问题，提出SCAR方法：内层优化通过优化代理学生模型模拟知识蒸馏过程，外层优化利用代理输出优化教师模型以植入条件后门，使用隐式微分算法和预优化触发器注入函数解决复杂优化问题。

Result: 在多个数据集、模型架构和知识蒸馏技术上的实验验证了SCAR的有效性，且能抵抗现有后门检测方法，揭示了知识蒸馏过程中被忽视的重要安全漏洞。

Conclusion: 蒸馏条件后门攻击是一个严重且之前被忽视的安全威胁，SCAR方法成功实现了这种攻击，强调了知识蒸馏过程需要更强的安全防护措施。

Abstract: Knowledge distillation (KD) is a vital technique for deploying deep neural
networks (DNNs) on resource-constrained devices by transferring knowledge from
large teacher models to lightweight student models. While teacher models from
third-party platforms may undergo security verification (\eg, backdoor
detection), we uncover a novel and critical threat: distillation-conditional
backdoor attacks (DCBAs). DCBA injects dormant and undetectable backdoors into
teacher models, which become activated in student models via the KD process,
even with clean distillation datasets. While the direct extension of existing
methods is ineffective for DCBA, we implement this attack by formulating it as
a bilevel optimization problem and proposing a simple yet effective method
(\ie, SCAR). Specifically, the inner optimization simulates the KD process by
optimizing a surrogate student model, while the outer optimization leverages
outputs from this surrogate to optimize the teacher model for implanting the
conditional backdoor. Our SCAR addresses this complex optimization utilizing an
implicit differentiation algorithm with a pre-optimized trigger injection
function. Extensive experiments across diverse datasets, model architectures,
and KD techniques validate the effectiveness of our SCAR and its resistance
against existing backdoor detection, highlighting a significant yet previously
overlooked vulnerability in the KD process. Our code is available at
https://github.com/WhitolfChen/SCAR.

</details>


### [31] [Binary Diff Summarization using Large Language Models](https://arxiv.org/abs/2509.23970)
*Meet Udeshi,Venkata Sai Charan Putrevu,Prashanth Krishnamurthy,Prashant Anantharaman,Sean Carrick,Ramesh Karri,Farshad Khorrami*

Main category: cs.CR

TL;DR: 提出了一种基于大语言模型的二进制差异摘要框架，通过功能敏感度评分自动识别敏感二进制函数，用于软件供应链安全中的恶意代码检测。


<details>
  <summary>Details</summary>
Motivation: 软件供应链安全需要验证软件更新的完整性，传统二进制差异分析结合LLM的自然语言摘要能力可以提升对关键变化的关注度，实现自动化恶意软件检测。

Method: 使用LLM进行二进制代码摘要，结合二进制差异分析，引入功能敏感度评分(FSS)自动筛选敏感二进制函数，构建包含104个二进制版本和392个二进制差异的软件供应链安全基准。

Result: 恶意软件检测精度达到0.98，召回率0.64；恶意和良性函数间FSS分离度为3.0分；在XZ utils供应链攻击案例中成功检测到注入的后门函数。

Conclusion: 该框架在软件供应链安全中表现出高精度和低误报率，FSS分类能有效识别敏感函数，为自动化恶意代码检测提供了可行方案。

Abstract: Security of software supply chains is necessary to ensure that software
updates do not contain maliciously injected code or introduce vulnerabilities
that may compromise the integrity of critical infrastructure. Verifying the
integrity of software updates involves binary differential analysis (binary
diffing) to highlight the changes between two binary versions by incorporating
binary analysis and reverse engineering. Large language models (LLMs) have been
applied to binary analysis to augment traditional tools by producing natural
language summaries that cybersecurity experts can grasp for further analysis.
Combining LLM-based binary code summarization with binary diffing can improve
the LLM's focus on critical changes and enable complex tasks such as automated
malware detection. To address this, we propose a novel framework for binary
diff summarization using LLMs. We introduce a novel functional sensitivity
score (FSS) that helps with automated triage of sensitive binary functions for
downstream detection tasks. We create a software supply chain security
benchmark by injecting 3 different malware into 6 open-source projects which
generates 104 binary versions, 392 binary diffs, and 46,023 functions. On this,
our framework achieves a precision of 0.98 and recall of 0.64 for malware
detection, displaying high accuracy with low false positives. Across malicious
and benign functions, we achieve FSS separation of 3.0 points, confirming that
FSS categorization can classify sensitive functions. We conduct a case study on
the real-world XZ utils supply chain attack; our framework correctly detects
the injected backdoor functions with high FSS.

</details>


### [32] [Multiple Concurrent Proposers: Why and How](https://arxiv.org/abs/2509.23984)
*Pranav Garimidi,Joachim Neu,Max Resnick*

Main category: cs.CR

TL;DR: 提出多并发提议者协议来解决MEV问题，提供选择性审查抵抗和隐藏属性


<details>
  <summary>Details</summary>
Motivation: 传统单提议者区块链存在矿工可提取价值问题，验证者利用其串行垄断地位从用户处提取租金

Method: 设计多并发提议者协议，提供选择性审查抵抗和隐藏两个关键属性

Result: 该协议能够防止对手选择性延迟交易或在确认前查看交易内容

Conclusion: MCP协议为解决MEV问题提供了必要的共识层基础

Abstract: Traditional single-proposer blockchains suffer from miner extractable value
(MEV), where validators exploit their serial monopoly on transaction inclusion
and ordering to extract rents from users. While there have been many
developments at the application layer to reduce the impact of MEV, these
approaches largely require auctions as a subcomponent. Running auctions
efficiently on chain requires two key properties of the underlying consensus
protocol: selective-censorship resistance and hiding. These properties
guarantee that an adversary can neither selectively delay transactions nor see
their contents before they are confirmed. We propose a multiple concurrent
proposer (MCP) protocol offering exactly these properties.

</details>


### [33] [Automated Vulnerability Validation and Verification: A Large Language Model Approach](https://arxiv.org/abs/2509.24037)
*Alireza Lotfi,Charalampos Katsis,Elisa Bertino*

Main category: cs.CR

TL;DR: 该论文提出了一个利用生成式AI和大型语言模型的端到端多步骤管道，用于编排和复现已知软件漏洞攻击。通过从NVD数据库提取CVE信息，结合RAG技术增强外部知识，自动化创建容器化环境和漏洞利用代码，并验证攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 软件漏洞仍然是关键的安全挑战，但缺乏高质量的多样化漏洞利用行为数据集限制了有效的漏洞评估和缓解。现有CVE描述存在噪声和不完整性问题，需要系统化的方法来生成可靠的漏洞利用环境。

Method: 使用端到端多步骤管道，结合LLMs和RAG技术：从NVD提取CVE信息，用外部知识（威胁咨询、代码片段）增强，自动化创建容器化环境和漏洞利用代码，迭代优化生成物，用测试用例验证攻击成功，支持复杂多容器设置。

Result: 该方法在不同类型漏洞（内存溢出、拒绝服务、远程代码执行）上证明有效，涵盖多种编程语言、库和年份。发现了CVE描述中的显著不一致性，强调CVE披露过程需要更严格的验证。

Conclusion: 这是首个通过结合通用LLM推理、CVE数据和基于RAG的上下文增强，在容器化环境中系统化编排和利用已知漏洞的系统。该方法模型无关，支持多种LLM，开源了相关工件以促进可重现性和加速安全研究。

Abstract: Software vulnerabilities remain a critical security challenge, providing
entry points for attackers into enterprise networks. Despite advances in
security practices, the lack of high-quality datasets capturing diverse exploit
behavior limits effective vulnerability assessment and mitigation. This paper
introduces an end-to-end multi-step pipeline leveraging generative AI,
specifically large language models (LLMs), to address the challenges of
orchestrating and reproducing attacks to known software vulnerabilities. Our
approach extracts information from CVE disclosures in the National
Vulnerability Database, augments it with external public knowledge (e.g.,
threat advisories, code snippets) using Retrieval-Augmented Generation (RAG),
and automates the creation of containerized environments and exploit code for
each vulnerability. The pipeline iteratively refines generated artifacts,
validates attack success with test cases, and supports complex multi-container
setups. Our methodology overcomes key obstacles, including noisy and incomplete
vulnerability descriptions, by integrating LLMs and RAG to fill information
gaps. We demonstrate the effectiveness of our pipeline across different
vulnerability types, such as memory overflows, denial of service, and remote
code execution, spanning diverse programming languages, libraries and years. In
doing so, we uncover significant inconsistencies in CVE descriptions,
emphasizing the need for more rigorous verification in the CVE disclosure
process. Our approach is model-agnostic, working across multiple LLMs, and we
open-source the artifacts to enable reproducibility and accelerate security
research. To the best of our knowledge, this is the first system to
systematically orchestrate and exploit known vulnerabilities in containerized
environments by combining general-purpose LLM reasoning with CVE data and
RAG-based context enrichment.

</details>


### [34] [An Ensemble Framework for Unbiased Language Model Watermarking](https://arxiv.org/abs/2509.24043)
*Yihan Wu,Ruibo Chen,Georgios Milis,Heng Huang*

Main category: cs.CR

TL;DR: ENS是一个新颖的集成框架，通过顺序组合多个独立的水印实例来增强基于logits的无偏水印的可检测性和鲁棒性，同时严格保持无偏性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力增强和广泛部署，验证机器生成内容的来源对于确保信任、安全和问责制至关重要。现有的无偏水印方案存在检测能力弱和鲁棒性有限的问题。

Method: 提出ENS集成框架，顺序组合多个独立的水印实例，每个实例由不同的密钥控制，以放大水印信号。理论证明该集成构造在期望上保持无偏。

Result: 在多个LLM家族上的实证评估表明，ENS显著减少了可靠检测所需的token数量，提高了对平滑和转述攻击的抵抗力，且不损害生成质量。

Conclusion: ENS框架有效增强了无偏水印的检测能力和鲁棒性，为机器生成内容的溯源提供了更可靠的解决方案。

Abstract: As large language models become increasingly capable and widely deployed,
verifying the provenance of machine-generated content is critical to ensuring
trust, safety, and accountability. Watermarking techniques have emerged as a
promising solution by embedding imperceptible statistical signals into the
generation process. Among them, unbiased watermarking is particularly
attractive due to its theoretical guarantee of preserving the language model's
output distribution, thereby avoiding degradation in fluency or detectability
through distributional shifts. However, existing unbiased watermarking schemes
often suffer from weak detection power and limited robustness, especially under
short text lengths or distributional perturbations. In this work, we propose
ENS, a novel ensemble framework that enhances the detectability and robustness
of logits-based unbiased watermarks while strictly preserving their
unbiasedness. ENS sequentially composes multiple independent watermark
instances, each governed by a distinct key, to amplify the watermark signal. We
theoretically prove that the ensemble construction remains unbiased in
expectation and demonstrate how it improves the signal-to-noise ratio for
statistical detectors. Empirical evaluations on multiple LLM families show that
ENS substantially reduces the number of tokens needed for reliable detection
and increases resistance to smoothing and paraphrasing attacks without
compromising generation quality.

</details>


### [35] [Analyzing and Evaluating Unbiased Language Model Watermark](https://arxiv.org/abs/2509.24048)
*Yihan Wu,Xuehao Cui,Ruibo Chen,Heng Huang*

Main category: cs.CR

TL;DR: UWbench是首个专门用于无偏水印方法评估的开源基准，通过理论分析和三轴评估协议（无偏性、可检测性、鲁棒性）解决现有评估不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型快速发展，验证AI生成文本真实性变得重要，但现有无偏水印方法在多代生成中会累积分布偏差，且鲁棒性评估在不同研究中不一致。

Method: 结合理论和实证贡献：提出统计指标量化多批次分布漂移，证明无偏水印在无限查询下无法完美保持分布的不可能性结果，开发针对令牌级修改攻击的鲁棒性形式分析，并建立三轴评估协议。

Result: 令牌修改攻击比基于释义的方法提供更稳定的鲁棒性评估，UWbench为社区提供了标准化和可复现的平台。

Conclusion: UWbench通过理论分析和系统评估框架，为无偏水印算法的设计和评估提供了标准化平台，解决了现有评估方法的不一致问题。

Abstract: Verifying the authenticity of AI-generated text has become increasingly
important with the rapid advancement of large language models, and unbiased
watermarking has emerged as a promising approach due to its ability to preserve
output distribution without degrading quality. However, recent work reveals
that unbiased watermarks can accumulate distributional bias over multiple
generations and that existing robustness evaluations are inconsistent across
studies. To address these issues, we introduce UWbench, the first open-source
benchmark dedicated to the principled evaluation of unbiased watermarking
methods. Our framework combines theoretical and empirical contributions: we
propose a statistical metric to quantify multi-batch distribution drift, prove
an impossibility result showing that no unbiased watermark can perfectly
preserve the distribution under infinite queries, and develop a formal analysis
of robustness against token-level modification attacks. Complementing this
theory, we establish a three-axis evaluation protocol: unbiasedness,
detectability, and robustness, and show that token modification attacks provide
more stable robustness assessments than paraphrasing-based methods. Together,
UWbench offers the community a standardized and reproducible platform for
advancing the design and evaluation of unbiased watermarking algorithms.

</details>


### [36] [DNS in the Time of Curiosity: A Tale of Collaborative User Privacy Protection](https://arxiv.org/abs/2509.24153)
*Philip Sjösvärd,Hongyu Jin,Panos Papadimitratos*

Main category: cs.CR

TL;DR: 论文提出了一种用户驱动的方法来保护DNS查询隐私，通过减少对DNS服务的暴露来防止好奇的解析器收集用户活动数据，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 当前DNS基础设施存在隐私风险，公共DNS解析器可以收集和分析大量用户活动数据，即使采用加密和隐私政策也无法完全保护用户免受好奇解析器的侵害。

Method: 采用用户驱动的方法，通过减少对DNS服务的暴露来保护隐私，同时确保低延迟、低网络带宽、低内存/存储开销和低计算开销。

Result: 该方法能够在保持高性能的同时实现高水平的隐私保护，解决了DNS基础设施不可信环境下的用户隐私问题。

Conclusion: 通过用户驱动的方法可以有效保护DNS查询隐私，在不牺牲性能的前提下减少用户对DNS服务的暴露，为DNS隐私保护提供了可行的解决方案。

Abstract: The Domain Name System (DNS) is central to all Internet user activity,
resolving accessed domain names into Internet Protocol (IP) addresses. As a
result, curious DNS resolvers can learn everything about Internet users'
interests. Public DNS resolvers are rising in popularity, offering low-latency
resolution, high reliability, privacy-preserving policies, and support for
encrypted DNS queries. However, client-resolver traffic encryption,
increasingly deployed to protect users from eavesdroppers, does not protect
users against curious resolvers. Similarly, privacy-preserving policies are
based solely on written commitments and do not provide technical safeguards.
Although DNS query relay schemes can separate duties to limit data accessible
by each entity, they cannot prevent colluding entities from sharing user
traffic logs. Thus, a key challenge remains: organizations operating public DNS
resolvers, accounting for the majority of DNS resolutions, can potentially
collect and analyze massive volumes of Internet user activity data. With DNS
infrastructure that cannot be fully trusted, can we safeguard user privacy? We
answer positively and advocate for a user-driven approach to reduce exposure to
DNS services. We will discuss key ideas of the proposal, which aims to achieve
a high level of privacy without sacrificing performance: maintaining low
latency, network bandwidth, memory/storage overhead, and computational
overhead.

</details>


### [37] [Fundamental Limit of Discrete Distribution Estimation under Utility-Optimized Local Differential Privacy](https://arxiv.org/abs/2509.24173)
*Sun-Moon Yoon,Hyun-Young Park,Seung-Hyun Nam,Si-Hyeon Lee*

Main category: cs.CR

TL;DR: 本文在效用优化本地差分隐私(ULDP)框架下，完全刻画了离散分布估计的隐私-效用权衡，提出了uBD机制并建立了紧致的下界。


<details>
  <summary>Details</summary>
Motivation: 研究在ULDP约束下的离散分布估计问题，ULDP对敏感数据实施本地差分隐私保护，同时允许对非敏感数据进行更准确的推断。

Method: 提出了效用优化块设计(uBD)方案，结合了分布分解技术和基于分数的线性估计器；在逆证明中使用了广义均匀渐近Cramér-Rao下界、极值ULDP机制类和ULDP约束下的分布分解技术。

Result: 完全刻画了ULDP下的隐私-效用权衡，提供了在ULDP约束下可达到的估计精度的紧致表征。

Conclusion: 这些结果揭示了隐私保护统计推断中最优机制的结构，为ULDP框架下的分布估计提供了理论基础。

Abstract: We study the problem of discrete distribution estimation under
utility-optimized local differential privacy (ULDP), which enforces local
differential privacy (LDP) on sensitive data while allowing more accurate
inference on non-sensitive data. In this setting, we completely characterize
the fundamental privacy-utility trade-off. The converse proof builds on several
key ideas, including a generalized uniform asymptotic Cram\'er-Rao lower bound,
a reduction showing that it suffices to consider a newly defined class of
extremal ULDP mechanisms, and a novel distribution decomposition technique
tailored to ULDP constraints. For the achievability, we propose a class of
utility-optimized block design (uBD) schemes, obtained as nontrivial
modifications of the block design mechanism known to be optimal under standard
LDP constraints, while incorporating the distribution decomposition idea used
in the converse proof and a score-based linear estimator. These results provide
a tight characterization of the estimation accuracy achievable under ULDP and
reveal new insights into the structure of optimal mechanisms for
privacy-preserving statistical inference.

</details>


### [38] [LLUAD: Low-Latency User-Anonymized DNS](https://arxiv.org/abs/2509.24174)
*Philip Sjösvärd,Hongyu Jin,Panos Papadimitratos*

Main category: cs.CR

TL;DR: LLUAD是一个保护DNS隐私的系统，通过在用户设备本地存储热门DNS记录列表，减少对公共DNS解析器的依赖，从而保护用户隐私并提高访问速度。


<details>
  <summary>Details</summary>
Motivation: DNS系统暴露用户网络活动细节，特别是公共DNS解析器可以追踪和收集大量用户数据。现有加密方案无法解决解析器本身的隐私威胁，而DNS查询中继无法防止实体合谋。

Method: 在用户设备本地存储热门DNS记录列表，通过隐私保护方式基于用户兴趣形成该列表。使用客户端驱动的投票混合网络匿名化用户投票，确保列表的地理相关性和安全性。

Result: LLUAD能够实现接近零信任的DNS隐私保护，与现有DNS基础设施兼容，同时提高隐私保护和减少网页内容访问时间。

Conclusion: LLUAD通过本地存储热门DNS记录和隐私保护投票机制，有效解决了DNS隐私问题，在保护用户隐私的同时提升了访问性能。

Abstract: The Domain Name System (DNS) is involved in practically all web activity,
translating easy-to-remember domain names into Internet Protocol (IP)
addresses. Due to its central role on the Internet, DNS exposes user web
activity in detail. The privacy challenge is honest-but-curious DNS
servers/resolvers providing the translation/lookup service. In particular, with
the majority of DNS queries handled by public DNS resolvers, the organizations
running them can track, collect, and analyze massive user activity data.
Existing solutions that encrypt DNS traffic between clients and resolvers are
insufficient, as the resolver itself is the privacy threat. While DNS query
relays separate duties among multiple entities, to limit the data accessible by
each entity, they cannot prevent colluding entities from sharing user traffic
logs. To achieve near-zero-trust DNS privacy compatible with the existing DNS
infrastructure, we propose LLUAD: it locally stores a Popularity List, the most
popular DNS records, on user devices, formed in a privacy-preserving manner
based on user interests. In this way, LLUAD can both improve privacy and reduce
access times to web content. The Popularity List is proactively retrieved from
a (curious) public server that continually updates and refreshes the records
based on user popularity votes, while efficiently broadcasting record
updates/changes to adhere to aggressive load-balancing schemes (i.e., name
servers actively load-balancing user connections by changing record IP
addresses). User votes are anonymized using a novel, efficient, and highly
scalable client-driven Voting Mix Network - with packet lengths independent of
the number of hops, centrally enforced limit on number of votes cast per user,
and robustness against poor client participation - to ensure a geographically
relevant and correctly/securely instantiated Popularity List.

</details>


### [39] [Takedown: How It's Done in Modern Coding Agent Exploits](https://arxiv.org/abs/2509.24240)
*Eunkyu Lee,Donghyeon Kim,Wonyoung Kim,Insu Yun*

Main category: cs.CR

TL;DR: 对8个真实世界编码代理进行安全分析，发现15个安全问题，可在无需用户交互的情况下实现任意命令执行和数据泄露


<details>
  <summary>Details</summary>
Motivation: 现代编码代理具有高度自主性，但缺乏系统性的安全分析，存在严重的安全和隐私风险

Method: 系统性地检查编码代理的内部工作流程，识别各组件中的安全威胁

Result: 在5个代理中实现任意命令执行，在4个代理中实现全局数据泄露，均无需用户交互

Conclusion: 现代LLM驱动代理需要全面的安全分析，安全考虑不足会导致严重漏洞

Abstract: Coding agents, which are LLM-driven agents specialized in software
development, have become increasingly prevalent in modern programming
environments. Unlike traditional AI coding assistants, which offer simple code
completion and suggestions, modern coding agents tackle more complex tasks with
greater autonomy, such as generating entire programs from natural language
instructions. To enable such capabilities, modern coding agents incorporate
extensive functionalities, which in turn raise significant concerns over their
security and privacy. Despite their growing adoption, systematic and in-depth
security analysis of these agents has largely been overlooked.
  In this paper, we present a comprehensive security analysis of eight
real-world coding agents. Our analysis addresses the limitations of prior
approaches, which were often fragmented and ad hoc, by systematically examining
the internal workflows of coding agents and identifying security threats across
their components. Through the analysis, we identify 15 security issues,
including previously overlooked or missed issues, that can be abused to
compromise the confidentiality and integrity of user systems. Furthermore, we
show that these security issues are not merely individual vulnerabilities, but
can collectively lead to end-to-end exploitations. By leveraging these security
issues, we successfully achieved arbitrary command execution in five agents and
global data exfiltration in four agents, all without any user interaction or
approval. Our findings highlight the need for a comprehensive security analysis
in modern LLM-driven agents and demonstrate how insufficient security
considerations can lead to severe vulnerabilities.

</details>


### [40] [VeriLLM: A Lightweight Framework for Publicly Verifiable Decentralized Inference](https://arxiv.org/abs/2509.24257)
*Ke Wang,Felix Qu,Libin Xia,Zishuo Zhao,Chris Tong,Lynn Ai,Eric Yang*

Main category: cs.CR

TL;DR: VeriLLM是一个去中心化LLM推理的可公开验证协议，通过轻量级验证算法和同构推理-验证网络，在保证安全性的同时显著降低验证成本。


<details>
  <summary>Details</summary>
Motivation: 去中心化LLM推理具有安全性高、效率好和运营成本低的优势，但在无许可环境中缺乏对参与节点的先验信任，需要确保输出可验证性才能安全部署。

Method: 采用轻量级验证算法（验证成本仅占推理的1%）、同构推理-验证网络复用GPU资源、同伴预测机制防止懒验证，并提供形式化博弈论分析。

Result: 协议在单一诚实验证者假设下实现安全性，验证成本可忽略不计，GPU利用率提高，端到端吞吐量提升，验证者池扩大增强鲁棒性。

Conclusion: VeriLLM是首个具有端到端博弈论安全证明的去中心化推理验证协议，诚实推理和验证构成纳什均衡，确保对理性对手的激励兼容性。

Abstract: Decentralized inference is an appealing paradigm for serving large language
models (LLMs), offering strong security, high efficiency, and lower operating
costs. Yet the permissionless setting admits no a priori trust in participating
nodes, making output verifiability a prerequisite for secure deployment. We
present VeriLLM, a publicly verifiable protocol for decentralized LLM inference
that (i) achieves security under a one-honest-verifier assumption, (ii) attains
near-negligible verification cost (about 1% of the underlying inference) via a
lightweight verification algorithm designed explicitly for LLMs, and (iii)
enforces honest checking through a peer-prediction mechanism that mitigates
lazy verification in naive voting. We further introduce an isomorphic
inference-verification network that multiplexes both roles on the same set of
GPU workers. This architecture (i) increases GPU utilization and thereby
improves end-to-end throughput for both inference and verification, (ii)
expands the effective pool of available validators, strengthening robustness
and security, and (iii) enforces task indistinguishability at the worker
boundary to prevent job-type-conditioned behavior. Finally, we provide a formal
game-theoretic analysis and prove that, under our incentives, honest inference
and verification constitute a Nash equilibrium, ensuring incentive
compatibility against rational adversaries. To our knowledge, this is the first
decentralized inference verification protocol with an end-to-end game-theoretic
security proof.

</details>


### [41] [When MCP Servers Attack: Taxonomy, Feasibility, and Mitigation](https://arxiv.org/abs/2509.24272)
*Weibo Zhao,Jiahao Liu,Bonan Ruan,Shaofei Li,Zhenkai Liang*

Main category: cs.CR

TL;DR: 这是首个系统研究MCP服务器安全风险的研究，将MCP服务器视为主动威胁参与者，提出了基于组件的攻击分类法，开发了概念验证服务器，并发现现有检测方法不足。


<details>
  <summary>Details</summary>
Motivation: MCP服务器的快速扩散带来了严重安全风险，但缺乏标准化审查机制，其安全影响尚未得到充分探索。

Method: 将MCP服务器分解为核心组件，研究恶意开发者如何植入恶意意图，开发概念验证服务器并在真实环境中测试。

Result: 提出了包含12个攻击类别的分类法，证明攻击者可以几乎零成本生成大量恶意服务器，现有扫描器检测效果不足。

Conclusion: 恶意MCP服务器易于实现、难以检测且能造成实际损害，需要协议设计者、主机开发者、LLM提供商和终端用户协同努力构建更安全的MCP生态系统。

Abstract: Model Context Protocol (MCP) servers enable AI applications to connect to
external systems in a plug-and-play manner, but their rapid proliferation also
introduces severe security risks. Unlike mature software ecosystems with
rigorous vetting, MCP servers still lack standardized review mechanisms, giving
adversaries opportunities to distribute malicious implementations. Despite this
pressing risk, the security implications of MCP servers remain underexplored.
To address this gap, we present the first systematic study that treats MCP
servers as active threat actors and decomposes them into core components to
examine how adversarial developers can implant malicious intent. Specifically,
we investigate three research questions: (i) what types of attacks malicious
MCP servers can launch, (ii) how vulnerable MCP hosts and Large Language Models
(LLMs) are to these attacks, and (iii) how feasible it is to carry out MCP
server attacks in practice. Our study proposes a component-based taxonomy
comprising twelve attack categories. For each category, we develop
Proof-of-Concept (PoC) servers and demonstrate their effectiveness across
diverse real-world host-LLM settings. We further show that attackers can
generate large numbers of malicious servers at virtually no cost. We then test
state-of-the-art scanners on the generated servers and found that existing
detection approaches are insufficient. These findings highlight that malicious
MCP servers are easy to implement, difficult to detect with current tools, and
capable of causing concrete damage to AI agent systems. Addressing this threat
requires coordinated efforts among protocol designers, host developers, LLM
providers, and end users to build a more secure and resilient MCP ecosystem.

</details>


### [42] [FuncPoison: Poisoning Function Library to Hijack Multi-agent Autonomous Driving Systems](https://arxiv.org/abs/2509.24408)
*Yuzhen Long,Songze Li*

Main category: cs.CR

TL;DR: FuncPoison是一种针对多智能体自动驾驶系统中函数库的新型投毒攻击，通过注入恶意工具来操纵智能体决策，引发级联错误。


<details>
  <summary>Details</summary>
Motivation: 函数库在多智能体自动驾驶系统中扮演关键角色，但作为安全漏洞尚未得到充分研究。攻击者可以利用智能体基于文本指令选择工具的特性进行攻击。

Method: 利用智能体依赖文本指令选择工具和标准化命令格式的弱点，注入具有欺骗性指令的恶意工具，操纵单个智能体决策并触发级联错误。

Result: 在两个代表性多智能体自动驾驶系统上的实验表明，FuncPoison能显著降低轨迹精度，灵活针对特定智能体诱导协调错误行为，并规避多种防御机制。

Conclusion: 函数库在基于LLM的自动驾驶系统中可能成为关键攻击面，对其可靠性提出了更高关注。

Abstract: Autonomous driving systems increasingly rely on multi-agent architectures
powered by large language models (LLMs), where specialized agents collaborate
to perceive, reason, and plan. A key component of these systems is the shared
function library, a collection of software tools that agents use to process
sensor data and navigate complex driving environments. Despite its critical
role in agent decision-making, the function library remains an under-explored
vulnerability. In this paper, we introduce FuncPoison, a novel poisoning-based
attack targeting the function library to manipulate the behavior of LLM-driven
multi-agent autonomous systems. FuncPoison exploits two key weaknesses in how
agents access the function library: (1) agents rely on text-based instructions
to select tools; and (2) these tools are activated using standardized command
formats that attackers can replicate. By injecting malicious tools with
deceptive instructions, FuncPoison manipulates one agent s decisions--such as
misinterpreting road conditions--triggering cascading errors that mislead other
agents in the system. We experimentally evaluate FuncPoison on two
representative multi-agent autonomous driving systems, demonstrating its
ability to significantly degrade trajectory accuracy, flexibly target specific
agents to induce coordinated misbehavior, and evade diverse defense mechanisms.
Our results reveal that the function library, often considered a simple
toolset, can serve as a critical attack surface in LLM-based autonomous driving
systems, raising elevated concerns on their reliability.

</details>


### [43] [GSPR: Aligning LLM Safeguards as Generalizable Safety Policy Reasoners](https://arxiv.org/abs/2509.24418)
*Haoran Li,Yulin Chen,Jingru Zeng,Hao Peng,Huihao Jing,Wenbin Hu,Xi Yang,Ziqian Zeng,Sirui Han,Yangqiu Song*

Main category: cs.CR

TL;DR: 提出了GSPR（通用安全策略推理器），通过组相对策略优化（GRPO）来识别不安全输入提示和违反安全分类的LLM输出，能够跨多个安全基准进行训练并展现强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在各领域的广泛应用，其安全性成为关键问题。现有安全基准的分类法各不相同，导致训练的安全防护措施要么只能粗粒度区分安全与不安全，要么受限于单一基准的狭窄风险分类。

Method: 提出GSPR方法，使用组相对策略优化（GRPO），通过精心设计的冷启动策略和奖励机制，激励模型在不同安全分类法下的推理能力。

Result: 实验表明GSPR显著提升了现有安全防护在安全和类别预测任务中的推理能力，不仅展现了强大的安全泛化能力，还实现了最少的推理token成本并带有解释。

Conclusion: GSPR能够有效整合多个安全基准的细粒度分类法，提供通用化的安全策略推理能力，在保持高效的同时提升LLM的安全性。

Abstract: As large language models (LLMs) are increasingly integrated into numerous
applications across various domains, LLMs' safety becomes a critical concern
for both application developers and intended users. Currently, great efforts
have been made to develop safety benchmarks with fine-grained taxonomies.
However, these benchmarks' taxonomies are disparate with different safety
policies. Thus, existing safeguards trained on these benchmarks are either
coarse-grained to only distinguish between safe and unsafe, or constrained by
the narrow risk taxonomies of a single benchmark. To leverage these
fine-grained safety taxonomies across multiple safety benchmarks, in this
paper, we propose GSPR, a Generalizable Safety Policy Reasoner to identify
unsafe input prompts and LLMs' outputs with violated safety taxonomies through
Group Relative Policy Optimization (GRPO). Unlike prior safeguards which only
cover a fixed set of risk factors, our GSPR incentivizes its reasoning
capability with varied safety taxonomies through our careful cold-start
strategy and reward design. Consequently, our GSPR can be trained across
multiple safety benchmarks with distinct taxonomies and naturally exhibits
powerful generalization ability. We conduct extensive experiments to show that
our GSPR significantly improves existing safety guardrails' reasoning
capabilities for both safety and category prediction tasks. Moreover, our GSPR
not only demonstrates powerful safety generalization abilities but also
achieves the least inference token costs with explanations.

</details>


### [44] [Evaluating Relayed and Switched Quantum Key Distribution (QKD) Network Architectures](https://arxiv.org/abs/2509.24440)
*Antonis Selentis,Nikolas Makris,Alkinoos Papageorgopoulos,Persefoni Konteli,Konstantinos Christodoulopoulos,George T. Kanellos,Dimitris Syvridis*

Main category: cs.CR

TL;DR: 比较两种QKD网络架构：中继QKD和交换QKD。实验发现不匹配的QKD模块会显著影响交换QKD性能。理论分析表明交换QKD在密集环网中表现更好，而中继QKD在长距离大节点数场景更有效。


<details>
  <summary>Details</summary>
Motivation: 评估两种量子密钥分发网络架构的性能差异，特别关注交换QKD中模块不匹配问题对网络性能的影响。

Method: 首先实验评估商用DV-QKD模块在匹配和不匹配配置下的性能，然后进行理论分析，基于均匀环网推导最优密钥管理配置和可实现的SKR公式。

Result: 实验发现不匹配模块对SKR有显著影响。理论分析显示交换QKD在密集环网（短距离、大节点数）表现更好，中继QKD在长距离大节点数场景更有效。

Conclusion: 交换QKD和中继QKD各有优势场景，模块不匹配问题会显著影响交换QKD架构的效率。

Abstract: We evaluate the performance of two architectures for network-wide quantum key
distribution (QKD): Relayed QKD, which relays keys over multi-link QKD paths
for non-adjacent nodes, and Switched QKD, which uses optical switches to
dynamically connect arbitrary QKD modules to form direct QKD links between
them. An advantage of Switched QKD is that it distributes quantum keys
end-to-end, whereas Relayed relies on trusted nodes. However, Switched depends
on arbitrary matching of QKD modules. We first experimentally evaluate the
performance of commercial DV-QKD modules; for each of three vendors we
benchmark the performance in standard/matched module pairs and in unmatched
pairs to emulate configurations in the Switched QKD network architecture. The
analysis reveals that in some cases a notable variation in the generated secret
key rate (SKR) between the matched and unmatched pairs is observed. Driven by
these experimental findings, we conduct a comprehensive theoretical analysis
that evaluates the network-wide performance of the two architectures. Our
analysis is based on uniform ring networks, where we derive optimal key
management configurations and analytical formulas for the achievable consumed
SKR. We compare network performance under varying ring sizes, QKD link losses,
QKD receivers' sensitivity and performance penalties of unmatched modules. Our
findings indicate that Switched QKD performs better in dense rings (short
distances, large node counts), while Relayed QKD is more effective in longer
distances and large node counts. Moreover, we confirm that unmatched QKD
modules penalties significantly impact the efficiency of Switched QKD
architecture.

</details>


### [45] [BugMagnifier: TON Transaction Simulator for Revealing Smart Contract Vulnerabilities](https://arxiv.org/abs/2509.24444)
*Yury Yanovich,Victoria Kovalevskaya,Maksim Egorov,Elizaveta Smirnova,Matvey Mishuris,Yash Madhwal,Kirill Ziborov,Vladimir Gorgadze,Subodh Sharma*

Main category: cs.CR

TL;DR: BugMagnifier是一个针对TON区块链智能合约的漏洞检测框架，通过消息队列操控和状态分析来发现异步执行中的竞态条件漏洞。


<details>
  <summary>Details</summary>
Motivation: TON区块链的异步执行模型带来了独特的安全挑战，特别是消息处理顺序不可预测导致的竞态条件。现有静态分析方法存在局限，需要动态检测方法来发现时间依赖性漏洞。

Method: 基于TON Sandbox和TVM构建的交易模拟框架，结合精确的消息队列操控、差分状态分析和概率排列测试，通过受控的消息编排系统性地揭示漏洞。

Result: 实验评估显示BugMagnifier在专门构建的易受攻击合约上有效，揭示了消息比例依赖的检测复杂性，与理论预测一致。

Conclusion: BugMagnifier填补了TON安全工具的关键空白，为异步区块链环境中更安全的智能合约开发提供实用支持，将漏洞发现从手动专家分析转向自动化证据生成。

Abstract: The Open Network (TON) blockchain employs an asynchronous execution model
that introduces unique security challenges for smart contracts, particularly
race conditions arising from unpredictable message processing order. While
previous work established vulnerability patterns through static analysis of
audit reports, dynamic detection of temporal dependencies through systematic
testing remains an open problem. We present BugMagnifier, a transaction
simulation framework that systematically reveals vulnerabilities in TON smart
contracts through controlled message orchestration. Built atop TON Sandbox and
integrated with the TON Virtual Machine (TVM), our tool combines precise
message queue manipulation with differential state analysis and probabilistic
permutation testing to detect asynchronous execution flaws. Experimental
evaluation demonstrates BugMagnifier's effectiveness through extensive
parametric studies on purpose-built vulnerable contracts, revealing message
ratio-dependent detection complexity that aligns with theoretical predictions.
This quantitative model enables predictive vulnerability assessment while
shifting discovery from manual expert analysis to automated evidence
generation. By providing reproducible test scenarios for temporal
vulnerabilities, BugMagnifier addresses a critical gap in the TON security
tooling, offering practical support for safer smart contract development in
asynchronous blockchain environments.

</details>


### [46] [Mapping Quantum Threats: An Engineering Inventory of Cryptographic Dependencies](https://arxiv.org/abs/2509.24623)
*Carlos Benitez*

Main category: cs.CR

TL;DR: 量子计算机对现代公钥密码学构成威胁，本文从工程角度系统梳理了受量子威胁的技术，提供了跨领域、跨环境的威胁地图来指导实践。


<details>
  <summary>Details</summary>
Motivation: 量子计算机（如Shor算法和Grover算法）能够高效解决支撑现代密码学的数学难题，一旦实用化将导致RSA、Diffie-Hellman等广泛使用的密码原语失效，危及数字生态安全。

Method: 从工程角度系统盘点受量子威胁的技术，按技术领域和实施环境进行组织，重点关注实际应用场景而非理论突破或协议级适配。

Result: 创建了一个跨领域、跨环境的量子脆弱系统威胁地图，涵盖不同数字基础设施中的暴露技术。

Conclusion: 该威胁地图为从业者、供应商和政策制定者提供了识别量子脆弱技术的实用指南，帮助在密码相关量子计算机到来前做好应对准备。

Abstract: The emergence of large-scale quantum computers, powered by algorithms like
Shor's and Grover's, poses an existential threat to modern public-key
cryptography. This vulnerability stems from the ability of these machines to
efficiently solve the hard mathematical problems - such as integer
factorization and the elliptic curve discrete logarithm problem - that underpin
widely used cryptographic primitives. This includes RSA, Diffie-Hellman (DH),
Elliptic Curve Diffie-Hellman (ECDH), and Elliptic Curve Digital Signature
Algorithm (ECDSA), which are foundational to security across the digital
ecosystem. Once Shor's algorithm becomes practically realizable, these
primitives will fail, undermining both retrospective confidentiality and
cryptographic authenticity - enabling adversaries to decrypt previously
captured communications and forge digital signatures. This paper presents a
systematic inventory of technologies exposed to quantum threats from the
engineering perspective, organized by both technology domain and by
implementation environment. While prior research has emphasized theoretical
breaks or protocol-level adaptations, this work focuses on the practical
landscape - mapping quantum-vulnerable systems across diverse digital
infrastructures. The contribution is a cross-domain, cross-environment threat
map to guide practitioners, vendors, and policymakers in identifying exposed
technologies before the arrival of cryptographically relevant quantum
computers.

</details>


### [47] [PRIVMARK: Private Large Language Models Watermarking with MPC](https://arxiv.org/abs/2509.24624)
*Thomas Fargues,Ye Dong,Tianwei Zhang,Jin-Song Dong*

Main category: cs.CR

TL;DR: PRIVMARK是一个基于安全多方计算的私有LLM水印框架，允许多方协作对LLM输出进行水印处理，而无需暴露模型权重。


<details>
  <summary>Details</summary>
Motivation: 传统水印方法在敏感场景下存在隐私问题，需要直接访问模型参数或训练数据。

Method: 基于PostMark方法构建高效的安全多方计算协议，使用SecretFlow-SPU实现，采用ABY3后端。

Result: PRIVMARK在语义上与无MPC的明文基线结果相同，能够抵抗改写和移除攻击，且效率合理。

Conclusion: 该框架为LLM内容所有权验证和可追溯性提供了隐私保护的解决方案。

Abstract: The rapid growth of Large Language Models (LLMs) has highlighted the pressing
need for reliable mechanisms to verify content ownership and ensure
traceability. Watermarking offers a promising path forward, but it remains
limited by privacy concerns in sensitive scenarios, as traditional approaches
often require direct access to a model's parameters or its training data. In
this work, we propose a secure multi-party computation (MPC)-based private LLMs
watermarking framework, PRIVMARK, to address the concerns. Concretely, we
investigate PostMark (EMNLP'2024), one of the state-of-the-art LLMs
Watermarking methods, and formulate its basic operations. Then, we construct
efficient protocols for these operations using the MPC primitives in a
black-box manner. In this way, PRIVMARK enables multiple parties to
collaboratively watermark an LLM's output without exposing the model's weights
to any single computing party. We implement PRIVMARK using SecretFlow-SPU
(USENIX ATC'2023) and evaluate its performance using the ABY3 (CCS'2018)
backend. The experimental results show that PRIVMARK achieves semantically
identical results compared to the plaintext baseline without MPC and is
resistant against paraphrasing and removing attacks with reasonable efficiency.

</details>


### [48] [LISA Technical Report: An Agentic Framework for Smart Contract Auditing](https://arxiv.org/abs/2509.24698)
*Izaiah Sun,Daniel Tan,Andy Deng*

Main category: cs.CR

TL;DR: LISA是一个结合规则和逻辑方法的智能合约漏洞检测框架，利用历史审计报告数据学习检测经验，无需模型微调即可泛化到新项目和威胁场景。


<details>
  <summary>Details</summary>
Motivation: 解决智能合约漏洞检测的局限性，传统方法覆盖范围有限且依赖大量人工工作，需要更可靠和全面的自动化检测方案。

Method: 结合规则和逻辑方法，利用历史审计报告数据学习检测模式，实现无需模型微调的知识泛化。

Result: 显著优于基于LLM的方法和传统静态分析工具，在漏洞类型覆盖率和检测准确率方面表现更优。

Conclusion: LISA为行业提供了有吸引力的解决方案，能够提供更可靠和全面的漏洞检测，同时减少对人工工作的依赖。

Abstract: We present LISA, an agentic smart contract vulnerability detection framework
that combines rule-based and logic-based methods to address a broad spectrum of
vulnerabilities in smart contracts. LISA leverages data from historical audit
reports to learn the detection experience (without model fine-tuning), enabling
it to generalize learned patterns to unseen projects and evolving threat
profiles. In our evaluation, LISA significantly outperforms both LLM-based
approaches and traditional static analysis tools, achieving superior coverage
of vulnerability types and higher detection accuracy. Our results suggest that
LISA offers a compelling solution for industry: delivering more reliable and
comprehensive vulnerability detection while reducing the dependence on manual
effort.

</details>


### [49] [Active Authentication via Korean Keystrokes Under Varying LLM Assistance and Cognitive Contexts](https://arxiv.org/abs/2509.24807)
*Dong Hyun Roh,Rajesh Kumar*

Main category: cs.CR

TL;DR: 该研究评估了在LLM辅助打字和不同认知条件下韩语击键动态认证的有效性，结果表明系统在各种LLM使用场景和认知背景下都能保持可靠性能。


<details>
  <summary>Details</summary>
Motivation: 击键动态是一种有前景的主动用户认证方式，但其在变化的LLM辅助打字和认知条件下的有效性尚未得到充分研究。

Method: 使用50名用户的数据和布鲁姆分类法的认知标签，评估了三种真实打字场景下的击键认证：真实创作、LLM内容改写和转录。采用连续性感知分割、特征提取以及SVM、MLP和XGB分类器。

Result: 系统在各种LLM使用和认知背景下保持可靠性能，等错误率在5.1%到10.4%之间。

Conclusion: 研究证明了在现代写作条件下行为认证的可行性，并为设计更具上下文弹性的模型提供了见解。

Abstract: Keystroke dynamics is a promising modality for active user authentication,
but its effectiveness under varying LLM-assisted typing and cognitive
conditions remains understudied. Using data from 50 users and cognitive labels
from Bloom's Taxonomy, we evaluate keystroke-based authentication in Korean
across three realistic typing scenarios: bona fide composition, LLM content
paraphrasing, and transcription. Our pipeline incorporates continuity-aware
segmentation, feature extraction, and classification via SVM, MLP, and XGB.
Results show that the system maintains reliable performance across varying LLM
usages and cognitive contexts, with Equal Error Rates ranging from 5.1% to
10.4%. These findings demonstrate the feasibility of behavioral authentication
under modern writing conditions and offer insights into designing more
context-resilient models.

</details>


### [50] [Of-SemWat: High-payload text embedding for semantic watermarking of AI-generated images with arbitrary size](https://arxiv.org/abs/2509.24823)
*Benedetta Tondi,Andrea Costanzo,Mauro Barni*

Main category: cs.CR

TL;DR: 提出了一种用于文本嵌入的高载荷图像水印方法，能够将图像的语义描述嵌入到图像中，并在AI生成的大规模图像中保持鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了在AI生成的大规模图像中鲁棒地嵌入高载荷水印，特别是嵌入与图像语义相关的文本描述，以支持图像-文本不匹配分析。

Method: 基于传统水印方案，结合正交码和turbo码提高鲁棒性，集成频域嵌入和感知掩蔽技术增强水印不可感知性。

Result: 实验表明该方法对各种图像处理具有极强的鲁棒性，即使在传统和AI修复后仍能提取嵌入文本，通过图像-文本不匹配分析揭示图像的语义修改。

Conclusion: 该方法成功实现了高载荷文本嵌入，为AI生成图像的语义完整性验证提供了有效工具。

Abstract: We propose a high-payload image watermarking method for textual embedding,
where a semantic description of the image - which may also correspond to the
input text prompt-, is embedded inside the image. In order to be able to
robustly embed high payloads in large-scale images - such as those produced by
modern AI generators - the proposed approach builds upon a traditional
watermarking scheme that exploits orthogonal and turbo codes for improved
robustness, and integrates frequency-domain embedding and perceptual masking
techniques to enhance watermark imperceptibility. Experiments show that the
proposed method is extremely robust against a wide variety of image processing,
and the embedded text can be retrieved also after traditional and AI
inpainting, permitting to unveil the semantic modification the image has
undergone via image-text mismatch analysis.

</details>


### [51] [Secret Leader Election in Ethereum PoS: An Empirical Security Analysis of Whisk and Homomorphic Sortition under DoS on the Leader and Censorship Attacks](https://arxiv.org/abs/2509.24955)
*Tereza Burianová,Martin Perešíni,Ivan Homoliak*

Main category: cs.CR

TL;DR: 提出一个统一的实验框架来评估PoS区块链中的秘密单领导者选举机制，发现在对抗性DoS攻击下，Whisk和同态排序都能保护领导者，但都无法有效防御针对验证者群体的协调攻击，且同态排序因计算复杂度高而不实用。


<details>
  <summary>Details</summary>
Motivation: PoS区块链中的提议者匿名性面临DoS和审查攻击风险，现有的SSLE机制的实际效果和权衡尚未充分探索。

Method: 开发了一个基于以太坊PoS共识层的统一实验框架，包含可配置的对手模型，模拟和比较Whisk和同态排序两种保护机制。

Result: 两种机制都能有效防御针对领导者的DoS攻击，但都无法应对针对验证者群体的协调攻击；Whisk简化了DoS攻击目标，同态排序因计算复杂而不实用。

Conclusion: 需要开发更有效的机制来防御针对验证者群体的协调攻击，同时平衡安全性和实用性。

Abstract: Proposer anonymity in Proof-of-Stake (PoS) blockchains is a critical concern
due to the risk of targeted attacks such as malicious denial-of-service (DoS)
and censorship attacks. While several Secret Single Leader Election (SSLE)
mechanisms have been proposed to address these threats, their practical impact
and trade-offs remain insufficiently explored. In this work, we present a
unified experimental framework for evaluating SSLE mechanisms under adversarial
conditions, grounded in a simplified yet representative model of Ethereum's PoS
consensus layer. The framework includes configurable adversaries capable of
launching targeted DoS and censorship attacks, including coordinated strategies
that simultaneously compromise groups of validators. We simulate and compare
key protection mechanisms - Whisk, and homomorphic sortition. To the best of
our knowledge, this is the first comparative study to examine adversarial DoS
scenarios involving multiple attackers under diverse protection mechanisms. Our
results show that while both designs offer strong protection against targeted
DoS attacks on the leader, neither defends effectively against coordinated
attacks on validator groups. Moreover, Whisk simplifies a DoS attack by
narrowing the target set from all validators to a smaller list of known
candidates. Homomorphic sortition, despite its theoretical strength, remains
impractical due to the complexity of cryptographic operations over large
validator sets.

</details>


### [52] [SecInfer: Preventing Prompt Injection via Inference-time Scaling](https://arxiv.org/abs/2509.24967)
*Yupei Liu,Yanting Wang,Yuqi Jia,Jinyuan Jia,Neil Zhenqiang Gong*

Main category: cs.CR

TL;DR: SecInfer是一种基于推理时扩展的新型防御方法，通过系统提示引导采样和目标任务引导聚合来抵御提示注入攻击。


<details>
  <summary>Details</summary>
Motivation: 现有的基于微调的预防性防御方法对强攻击效果有限，提示注入攻击对大型语言模型构成普遍威胁。

Method: 采用推理时扩展范式，包含系统提示引导采样（通过不同系统提示生成多样化响应）和目标任务引导聚合（选择最可能完成预期任务的响应）。

Result: 实验表明SecInfer能有效缓解现有和自适应的提示注入攻击，优于最先进的防御方法和现有推理时扩展方法。

Conclusion: 通过在推理时利用额外计算资源，SecInfer为提示注入攻击提供了有效的防御解决方案。

Abstract: Prompt injection attacks pose a pervasive threat to the security of Large
Language Models (LLMs). State-of-the-art prevention-based defenses typically
rely on fine-tuning an LLM to enhance its security, but they achieve limited
effectiveness against strong attacks. In this work, we propose \emph{SecInfer},
a novel defense against prompt injection attacks built on \emph{inference-time
scaling}, an emerging paradigm that boosts LLM capability by allocating more
compute resources for reasoning during inference. SecInfer consists of two key
steps: \emph{system-prompt-guided sampling}, which generates multiple responses
for a given input by exploring diverse reasoning paths through a varied set of
system prompts, and \emph{target-task-guided aggregation}, which selects the
response most likely to accomplish the intended task. Extensive experiments
show that, by leveraging additional compute at inference, SecInfer effectively
mitigates both existing and adaptive prompt injection attacks, outperforming
state-of-the-art defenses as well as existing inference-time scaling
approaches.

</details>


### [53] [Optimizing Privacy-Preserving Primitives to Support LLM-Scale Applications](https://arxiv.org/abs/2509.25072)
*Yaman Jandali,Ruisi Zhang,Nojan Sheybani,Farinaz Koushanfar*

Main category: cs.CR

TL;DR: 本文概述了通过硬件/软件/算法协同设计来降低隐私保护技术（MPC、ZKPs、FHE）的计算和通信开销，使其能够应用于LLM规模的实际场景。


<details>
  <summary>Details</summary>
Motivation: 隐私保护技术在实际应用中面临计算和通信开销过大的问题，阻碍了其大规模采用。

Method: 采用多方计算（MPC）、零知识证明（ZKPs）和全同态加密（FHE）等技术，通过硬件/软件/算法协同设计来优化性能。

Result: 展示了在DNN知识产权保护、伦理LLM使用执行和transformer推理等多个场景中的有效性，朝着实现LLM规模隐私保护应用迈出了进展。

Conclusion: 通过协同设计方法，可以有效降低隐私保护技术的开销，使其在现实系统中更加实用化。

Abstract: Privacy-preserving technologies have introduced a paradigm shift that allows
for realizable secure computing in real-world systems. The significant barrier
to the practical adoption of these primitives is the computational and
communication overhead that is incurred when applied at scale. In this paper,
we present an overview of our efforts to bridge the gap between this overhead
and practicality for privacy-preserving learning systems using multi-party
computation (MPC), zero-knowledge proofs (ZKPs), and fully homomorphic
encryption (FHE). Through meticulous hardware/software/algorithm co-design, we
show progress towards enabling LLM-scale applications in privacy-preserving
settings. We demonstrate the efficacy of our solutions in several contexts,
including DNN IP ownership, ethical LLM usage enforcement, and transformer
inference.

</details>


### [54] [Two-Dimensional XOR-Based Secret Sharing for Layered Multipath Communication](https://arxiv.org/abs/2509.25113)
*Wai Ming Chan,Remi Chou,Taejoon Kim*

Main category: cs.CR

TL;DR: 提出了首个基于XOR的二维秘密共享方案，用于分层多路径通信网络，确保在每层单路径被攻击时仍能恢复消息并保持完美隐私，具有信息论安全性。


<details>
  <summary>Details</summary>
Motivation: 针对传统加密方法易受量子计算威胁的问题，为资源受限的军事环境设计无需计算假设的、可证明安全的通信方案。

Method: 使用位级XOR操作构建二维秘密共享方案，复杂度为线性O(|S|)，其中|S|为消息长度，通过数学证明确保无条件安全性。

Result: 方案在每传输层任意单路径被观察和破坏的情况下，仍能保证成功消息恢复和完美隐私，且不受攻击者计算资源限制。

Conclusion: 该XOR-based方案为资源受限环境提供了对抗量子计算威胁的可证明安全通信方案，优于依赖计算假设的传统加密方法。

Abstract: This paper introduces the first two-dimensional XOR-based secret sharing
scheme for layered multipath communication networks. We present a construction
that guarantees successful message recovery and perfect privacy when an
adversary observes and disrupts any single path at each transmission layer. The
scheme achieves information-theoretic security using only bitwise XOR
operations with linear $O(|S|)$ complexity, where $|S|$ is the message length.
We provide mathematical proofs demonstrating that the scheme maintains
unconditional security regardless of computational resources available to
adversaries. Unlike encryption-based approaches vulnerable to quantum computing
advances, our construction offers provable security suitable for
resource-constrained military environments where computational assumptions may
fail.

</details>
