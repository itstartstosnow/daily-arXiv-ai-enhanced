{"id": "2509.25394", "categories": ["cs.CR", "cs.SY", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.25394", "abs": "https://arxiv.org/abs/2509.25394", "authors": ["Hui Wang", "Nima Tashakor", "Xiaoyang Tian", "Hans D. Schotten", "Stefan M. Goetz"], "title": "Fast Energy-Theft Attack on Frequency-Varying Wireless Power without Additional Sensors", "comment": "11 pages, 12 figures", "summary": "With the popularity of wireless charging, energy access protection and\ncybersecurity are gaining importance, especially in public places. Currently,\nthe most common energy encryption method uses frequency and associated\nimpedance variation. However, we have proven that this method is not reliable,\nsince a hacker can detect the changing frequency and adjust the compensation.\nHowever, the previously presented system needed time to follow the updated\nfrequency, while encryption systems may vary the frequency faster to avoid\nenergy theft. Furthermore, the previous system required an additional sensor\ncoil. To solve these problems, we optimized the attack and the associated\nsystem, which can intrude and steal energy within 0.2 ms. The key is the\nelimination of the time-consuming maximum receiver current regulation. Also, we\nuse the main receiving coil rather than any additional sensor antenna to detect\nthe magnetic field. Thus, the new hardware is even simpler. A simulation model\nand experimental results demonstrate the fast response speed of the attack on\nencrypted wireless power and steal 65% of the power. Overall, the applicability\nof the attack is highly improved and leaves less room for hardening the\nencryption. The results demonstrate that energy access protection needs to be\ngiven great attention."}
{"id": "2509.25408", "categories": ["cs.CR", "econ.TH"], "pdf": "https://arxiv.org/pdf/2509.25408", "abs": "https://arxiv.org/abs/2509.25408", "authors": ["Korok Ray", "Sindura Saraswathi"], "title": "Optimal Threshold Signatures in Bitcoin", "comment": null, "summary": "We formulate the design of a threshold signature scheme as made possible on\ncryptocurrency protocols like Bitcoin. The funds are secured by an m-of-n\nthreshold signature, where at least m signatures are needed to unlock the\nfunds. A user designs this scheme knowing that a malicious attacker can also\nobtain the signatures with some probability. Higher thresholds offer more\nsecurity, but also risk locking the user out of his own funds. The optimal\nthreshold balances these twin effects. Interventions like increasing the\nsecurity or usability of the signatures allow for higher thresholds. We model\ndynamic threshold signature schemes, where the probability of a user or\nattacker obtaining signatures decays with time. A dynamic threshold signature\nscheme is optimal, and increasing security or usability allows for higher\nthresholds and longer time locks."}
{"id": "2509.25410", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.25410", "abs": "https://arxiv.org/abs/2509.25410", "authors": ["Maraz Mia", "Mir Mehedi A. Pritom", "Tariqul Islam", "Shouhuai Xu"], "title": "Characterizing Event-themed Malicious Web Campaigns: A Case Study on War-themed Websites", "comment": "12 pages, 9 figures, 5 tables", "summary": "Cybercrimes such as online scams and fraud have become prevalent.\nCybercriminals often abuse various global or regional events as themes of their\nfraudulent activities to breach user trust and attain a higher attack success\nrate. These attacks attempt to manipulate and deceive innocent people into\ninteracting with meticulously crafted websites with malicious payloads,\nphishing, or fraudulent transactions. To deepen our understanding of the\nproblem, this paper investigates how to characterize event-themed malicious\nwebsite-based campaigns, with a case study on war-themed websites. We find that\nattackers tailor their attacks by exploiting the unique aspects of events, as\nevidenced by activities such as fundraising, providing aid, collecting\nessential supplies, or seeking updated news. We use explainable unsupervised\nclustering methods to draw further insights, which could guide the design of\neffective early defenses against various event-themed malicious web campaigns."}
{"id": "2509.25430", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.25430", "abs": "https://arxiv.org/abs/2509.25430", "authors": ["Martin Kotuliak", "Simon Erni", "Jakub Polák", "Marc Roeschlin", "Richard Baker", "Ivan Martinovic", "Srdjan Čapkun"], "title": "Finding Phones Fast: Low-Latency and Scalable Monitoring of Cellular Communications in Sensitive Areas", "comment": null, "summary": "The widespread availability of cellular devices introduces new threat vectors\nthat allow users or attackers to bypass security policies and physical barriers\nand bring unauthorized devices into sensitive areas. These threats can arise\nfrom user non-compliance or deliberate actions aimed at data\nexfiltration/infiltration via hidden devices, drones, etc. We identify a\ncritical gap in this context: the absence of low-latency systems for\nhigh-quality and instantaneous monitoring of cellular transmissions. Such\nlow-latency systems are crucial to allow for timely detection, decision (e.g.,\ngeofencing or localization), and disruption of unauthorized communication in\nsensitive areas. Operator-based monitoring systems, built for purposes such as\npeople counting or tracking, lack real-time capability, require cooperation\nacross multiple operators, and thus are hard to deploy. Operator-independent\nmonitoring approaches proposed in the literature either lack low-latency\ncapabilities or do not scale.\n  We propose LTag, the first low-latency, operator-independent and scalable\nsystem designed to monitor cellular connections across all operators prior to\nany user data transmission. LTag consists of several downlink sniffers and a\ndistributed network of uplink sniffers that measure both downlink protocol\ninformation and uplink signal characteristics at multiple locations to gain a\ndetailed spatial image of uplink signals. LTag aggregates the recorded\ninformation, processes it, and provides a decision about the connection all\nprior to connection establishment of a UE. To evaluate LTag, we deployed it in\nthe context of geofencing, where LTag was able to determine if the signals\noriginate from inside or outside of an area within 2.3 ms of the initial base\nstation-to-device message, therefore enabling prompt and targeted suppression\nof communication before any user data was transmitted."}
{"id": "2509.25448", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25448", "abs": "https://arxiv.org/abs/2509.25448", "authors": ["Yuepeng Hu", "Zhengyuan Jiang", "Mengyuan Li", "Osama Ahmed", "Zhicong Huang", "Cheng Hong", "Neil Gong"], "title": "Fingerprinting LLMs via Prompt Injection", "comment": null, "summary": "Large language models (LLMs) are often modified after release through\npost-processing such as post-training or quantization, which makes it\nchallenging to determine whether one model is derived from another. Existing\nprovenance detection methods have two main limitations: (1) they embed signals\ninto the base model before release, which is infeasible for already published\nmodels, or (2) they compare outputs across models using hand-crafted or random\nprompts, which are not robust to post-processing. In this work, we propose\nLLMPrint, a novel detection framework that constructs fingerprints by\nexploiting LLMs' inherent vulnerability to prompt injection. Our key insight is\nthat by optimizing fingerprint prompts to enforce consistent token preferences,\nwe can obtain fingerprints that are both unique to the base model and robust to\npost-processing. We further develop a unified verification procedure that\napplies to both gray-box and black-box settings, with statistical guarantees.\nWe evaluate LLMPrint on five base models and around 700 post-trained or\nquantized variants. Our results show that LLMPrint achieves high true positive\nrates while keeping false positive rates near zero."}
{"id": "2509.25462", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.25462", "abs": "https://arxiv.org/abs/2509.25462", "authors": ["Loay Abdelrazek", "Filippo Rebecchi"], "title": "Managing Differentiated Secure Connectivity using Intents", "comment": "Preprint version of paper accepted in Mobiwac'25", "summary": "Mobile networks in the 5G and 6G era require to rethink how to manage\nsecurity due to the introduction of new services, use cases, each with its own\nsecurity requirements, while simultaneously expanding the threat landscape.\nAlthough automation has emerged as a key enabler to address complexity in\nnetworks, existing approaches lack the expressiveness to define and enforce\ncomplex, goal-driven, and measurable security requirements. In this paper, we\npropose the concept of differentiated security levels and leveraging intents as\na management framework. We discuss the requirements and enablers to extend the\ncurrently defined intent-based management frameworks to pave the path for\nintent-based security management in mobile networks. Our approach formalizes\nboth functional and non-functional security requirements and demonstrates how\nthese can be expressed and modeled using an extended TM Forum (TMF) intent\nsecurity ontology. We further discuss the required standardization steps to\nachieve intent-based security management. Our work aims at advance security\nautomation, improve adaptability, and strengthen the resilience and security\nposture of the next-generation mobile networks."}
{"id": "2509.25469", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.25469", "abs": "https://arxiv.org/abs/2509.25469", "authors": ["Panagiotis Michalopoulos", "Anthony Mack", "Cameron Clark", "Linus Chen", "Johannes Sedlmeir", "Andreas Veneris"], "title": "Balancing Compliance and Privacy in Offline CBDC Transactions Using a Secure Element-based System", "comment": "9 pages, 4 figures", "summary": "Blockchain technology has spawned a vast ecosystem of digital currencies with\nCentral Bank Digital Currencies (CBDCs) -- digital forms of fiat currency --\nbeing one of them. An important feature of digital currencies is facilitating\ntransactions without network connectivity, which can enhance the scalability of\ncryptocurrencies and the privacy of CBDC users. However, in the case of CBDCs,\nthis characteristic also introduces new regulatory challenges, particularly\nwhen it comes to applying established Anti-Money Laundering and Countering the\nFinancing of Terrorism (AML/CFT) frameworks. This paper introduces a prototype\nfor offline digital currency payments, equally applicable to cryptocurrencies\nand CBDCs, that leverages Secure Elements and digital credentials to address\nthe tension of offline payment support with regulatory compliance. Performance\nevaluation results suggest that the prototype can be flexibly adapted to\ndifferent regulatory environments, with a transaction latency comparable to\nreal-life commercial payment systems. Furthermore, we conceptualize how the\nintegration of Zero-Knowledge Proofs into our design could accommodate various\ntiers of enhanced privacy protection."}
{"id": "2509.25476", "categories": ["cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.25476", "abs": "https://arxiv.org/abs/2509.25476", "authors": ["Yonatan Gizachew Achamyeleh", "Yang Xiang", "Yun-Ping Hsiao", "Yasamin Moghaddas", "Mohammad Abdullah Al Faruque"], "title": "Environmental Rate Manipulation Attacks on Power Grid Security", "comment": null, "summary": "The growing complexity of global supply chains has made hardware Trojans a\nsignificant threat in sensor-based power electronics. Traditional Trojan\ndesigns depend on digital triggers or fixed threshold conditions that can be\ndetected during standard testing. In contrast, we introduce Environmental Rate\nManipulation (ERM), a novel Trojan triggering mechanism that activates by\nmonitoring the rate of change in environmental parameters rather than their\nabsolute values. This approach allows the Trojan to remain inactive under\nnormal conditions and evade redundancy and sensor-fusion defenses. We implement\na compact 14~$\\mu$m$^2$ circuit that measures capacitor charging rates in\nstandard sensor front-ends and disrupts inverter pulse-width modulation PWM\nsignals when a rapid change is induced. Experiments on a commercial Texas\nInstruments solar inverter demonstrate that ERM can trigger catastrophic driver\nchip failure. Furthermore, ETAP simulations indicate that a single compromised\n100~kW inverter may initiate cascading grid instabilities. The attack's\nsignificance extends beyond individual sensors to entire classes of\nenvironmental sensing systems common in power electronics, demonstrating\nfundamental challenges for hardware security."}
{"id": "2509.25525", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25525", "abs": "https://arxiv.org/abs/2509.25525", "authors": ["Boyang Zhang", "Istemi Ekin Akkus", "Ruichuan Chen", "Alice Dethise", "Klaus Satzke", "Ivica Rimac", "Yang Zhang"], "title": "Defeating Cerberus: Concept-Guided Privacy-Leakage Mitigation in Multimodal Language Models", "comment": null, "summary": "Multimodal large language models (MLLMs) have demonstrated remarkable\ncapabilities in processing and reasoning over diverse modalities, but their\nadvanced abilities also raise significant privacy concerns, particularly\nregarding Personally Identifiable Information (PII) leakage. While relevant\nresearch has been conducted on single-modal language models to some extent, the\nvulnerabilities in the multimodal setting have yet to be fully investigated. In\nthis work, we investigate these emerging risks with a focus on vision language\nmodels (VLMs), a representative subclass of MLLMs that covers the two\nmodalities most relevant for PII leakage, vision and text. We introduce a\nconcept-guided mitigation approach that identifies and modifies the model's\ninternal states associated with PII-related content. Our method guides VLMs to\nrefuse PII-sensitive tasks effectively and efficiently, without requiring\nre-training or fine-tuning. We also address the current lack of multimodal PII\ndatasets by constructing various ones that simulate real-world scenarios.\nExperimental results demonstrate that the method can achieve an average refusal\nrate of 93.3% for various PII-related tasks with minimal impact on unrelated\nmodel performances. We further examine the mitigation's performance under\nvarious conditions to show the adaptability of our proposed method."}
{"id": "2509.25566", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.25566", "abs": "https://arxiv.org/abs/2509.25566", "authors": ["Amal Yousseef", "Shalaka Satam", "Banafsheh Saber Latibari", "Mai Abdel-Malek", "Soheil Salehi", "Pratik Satam"], "title": "Zero Trust-based Decentralized Identity Management System for Autonomous Vehicles", "comment": null, "summary": "The rise of autonomous vehicles (AVs) promises to significantly enhance\ntransportation safety and efficiency by mitigating human error, which is\nresponsible for over 90\\% of road accidents. However, the increasing\nconnectivity of AVs introduces new cybersecurity challenges, as traditional\nperimeter-based security models are inadequate for dynamic and untrusted\nenvironments. This paper presents a novel Zero Trust-based Decentralized\nIdentity Management (D-IM) protocol for AVs. By integrating the core principles\nof Zero Trust Architecture, \"never trust, always verify\", with the tamper\nresistant and decentralized nature of a blockchain network, our framework\neliminates reliance on centralized authorities and provides continuous\nverification for every entity. We detail the system's design, which leverages\nHyperledger Iroha to enable lightweight and secure authentication without a\ncentral trusted entity. A comprehensive experimental evaluation, conducted\nacross both urban and highway scenarios, validates the protocol's practicality.\nOur results demonstrate that the D-IM framework introduces minimal overhead,\nwith less than 7.5\\% reduction in Packet Reception Rate (PRR) in urban settings\nand an increase of under 11\\% in Channel Busy Ratio (CBR) for LTE-V2X. These\nfindings prove the protocol's efficiency and robustness, providing a resilient\nfoundation for securing real-time V2X communication against impersonation and\nreplay attacks."}
{"id": "2509.25624", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25624", "abs": "https://arxiv.org/abs/2509.25624", "authors": ["Jing-Jing Li", "Jianfeng He", "Chao Shang", "Devang Kulshreshtha", "Xun Xian", "Yi Zhang", "Hang Su", "Sandesh Swamy", "Yanjun Qi"], "title": "STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents", "comment": null, "summary": "As LLMs advance into autonomous agents with tool-use capabilities, they\nintroduce security challenges that extend beyond traditional content-based LLM\nsafety concerns. This paper introduces Sequential Tool Attack Chaining (STAC),\na novel multi-turn attack framework that exploits agent tool use. STAC chains\ntogether tool calls that each appear harmless in isolation but, when combined,\ncollectively enable harmful operations that only become apparent at the final\nexecution step. We apply our framework to automatically generate and\nsystematically evaluate 483 STAC cases, featuring 1,352 sets of\nuser-agent-environment interactions and spanning diverse domains, tasks, agent\ntypes, and 10 failure modes. Our evaluations show that state-of-the-art LLM\nagents, including GPT-4.1, are highly vulnerable to STAC, with attack success\nrates (ASR) exceeding 90% in most cases. The core design of STAC's automated\nframework is a closed-loop pipeline that synthesizes executable multi-step tool\nchains, validates them through in-environment execution, and reverse-engineers\nstealthy multi-turn prompts that reliably induce agents to execute the verified\nmalicious sequence. We further perform defense analysis against STAC and find\nthat existing prompt-based defenses provide limited protection. To address this\ngap, we propose a new reasoning-driven defense prompt that achieves far\nstronger protection, cutting ASR by up to 28.8%. These results highlight a\ncrucial gap: defending tool-enabled agents requires reasoning over entire\naction sequences and their cumulative effects, rather than evaluating isolated\nprompts or responses."}
{"id": "2509.25926", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25926", "abs": "https://arxiv.org/abs/2509.25926", "authors": ["Dennis Jacob", "Emad Alghamdi", "Zhanhao Hu", "Basel Alomair", "David Wagner"], "title": "Better Privilege Separation for Agents by Restricting Data Types", "comment": null, "summary": "Large language models (LLMs) have become increasingly popular due to their\nability to interact with unstructured content. As such, LLMs are now a key\ndriver behind the automation of language processing systems, such as AI agents.\nUnfortunately, these advantages have come with a vulnerability to prompt\ninjections, an attack where an adversary subverts the LLM's intended\nfunctionality with an injected task. Past approaches have proposed detectors\nand finetuning to provide robustness, but these techniques are vulnerable to\nadaptive attacks or cannot be used with state-of-the-art models. To this end we\npropose type-directed privilege separation for LLMs, a method that\nsystematically prevents prompt injections. We restrict the ability of an LLM to\ninteract with third-party data by converting untrusted content to a curated set\nof data types; unlike raw strings, each data type is limited in scope and\ncontent, eliminating the possibility for prompt injections. We evaluate our\nmethod across several case studies and find that designs leveraging our\nprinciples can systematically prevent prompt injection attacks while\nmaintaining high utility."}
{"id": "2509.26350", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26350", "abs": "https://arxiv.org/abs/2509.26350", "authors": ["Tharindu Lakshan Yasarathna", "Nhien-An Le-Khac"], "title": "SoK: Systematic analysis of adversarial threats against deep learning approaches for autonomous anomaly detection systems in SDN-IoT networks", "comment": null, "summary": "Integrating SDN and the IoT enhances network control and flexibility.\nDL-based AAD systems improve security by enabling real-time threat detection in\nSDN-IoT networks. However, these systems remain vulnerable to adversarial\nattacks that manipulate input data or exploit model weaknesses, significantly\ndegrading detection accuracy. Existing research lacks a systematic analysis of\nadversarial vulnerabilities specific to DL-based AAD systems in SDN-IoT\nenvironments. This SoK study introduces a structured adversarial threat model\nand a comprehensive taxonomy of attacks, categorising them into data, model,\nand hybrid-level threats. Unlike previous studies, we systematically evaluate\nwhite, black, and grey-box attack strategies across popular benchmark datasets.\nOur findings reveal that adversarial attacks can reduce detection accuracy by\nup to 48.4%, with Membership Inference causing the most significant drop. C&W\nand DeepFool achieve high evasion success rates. However, adversarial training\nenhances robustness, and its high computational overhead limits the real-time\ndeployment of SDN-IoT applications. We propose adaptive countermeasures,\nincluding real-time adversarial mitigation, enhanced retraining mechanisms, and\nexplainable AI-driven security frameworks. By integrating structured threat\nmodels, this study offers a more comprehensive approach to attack\ncategorisation, impact assessment, and defence evaluation than previous\nresearch. Our work highlights critical vulnerabilities in existing DL-based AAD\nmodels and provides practical recommendations for improving resilience,\ninterpretability, and computational efficiency. This study serves as a\nfoundational reference for researchers and practitioners seeking to enhance\nDL-based AAD security in SDN-IoT networks, offering a systematic adversarial\nthreat model and conceptual defence evaluation based on prior empirical\nstudies."}
{"id": "2509.26393", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.26393", "abs": "https://arxiv.org/abs/2509.26393", "authors": ["Maciej Skorski", "Francisco-Javier Soto", "Onur Günlü"], "title": "Exact Bias of Linear TRNG Correctors -- Spectral Approach", "comment": null, "summary": "Using Fourier analysis, this paper establishes exact security bounds for\nlinear extractors in True Random Number Generators (TRNGs). We provide the\nfirst near-optimal total variation security characterization by interpolating\nbetween optimal $\\ell_{\\infty}$ and $\\ell_2$ norm results, expressed through\ncode weight enumerators and input bias parameters. Our bounds improve security\nassessments by an order of magnitude over previous approximations. By scanning\n~20,000 codes, we reveal fundamental trade-offs between compression efficiency\nand cryptographic security. For instance, we show that achieving 80 bits of\nsecurity can require sacrificing more than 50\\% of the code rate when\ncorrecting 10\\% input bias. Our bounds enhance security evaluation of TRNG\npost-processing schemes and quantify the inherent cost of randomness extraction\nin hardware implementations."}
{"id": "2509.26404", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26404", "abs": "https://arxiv.org/abs/2509.26404", "authors": ["Yao Tong", "Haonan Wang", "Siquan Li", "Kenji Kawaguchi", "Tianyang Hu"], "title": "SeedPrints: Fingerprints Can Even Tell Which Seed Your Large Language Model Was Trained From", "comment": null, "summary": "Fingerprinting Large Language Models (LLMs) is essential for provenance\nverification and model attribution. Existing methods typically extract post-hoc\nsignatures based on training dynamics, data exposure, or hyperparameters --\nproperties that only emerge after training begins. In contrast, we propose a\nstronger and more intrinsic notion of LLM fingerprinting: SeedPrints, a method\nthat leverages random initialization biases as persistent, seed-dependent\nidentifiers present even before training. We show that untrained models exhibit\nreproducible token selection biases conditioned solely on their parameters at\ninitialization. These biases are stable and measurable throughout training,\nenabling our statistical detection method to recover a model's lineage with\nhigh confidence. Unlike prior techniques, unreliable before convergence and\nvulnerable to distribution shifts, SeedPrints remains effective across all\ntraining stages and robust under domain shifts or parameter modifications.\nExperiments on LLaMA-style and Qwen-style models show that SeedPrints achieves\nseed-level distinguishability and can provide birth-to-lifecycle identity\nverification akin to a biometric fingerprint. Evaluations on large-scale\npretrained models and fingerprinting benchmarks further confirm its\neffectiveness under practical deployment scenarios. These results suggest that\ninitialization itself imprints a unique and persistent identity on neural\nlanguage models, forming a true ''Galtonian'' fingerprint."}
{"id": "2509.26509", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.26509", "abs": "https://arxiv.org/abs/2509.26509", "authors": ["Raghul Saravanan", "Sai Manoj P D"], "title": "Logic Solver Guided Directed Fuzzing for Hardware Designs", "comment": null, "summary": "The ever-increasing complexity of design specifications for processors and\nintellectual property (IP) presents a formidable challenge for early bug\ndetection in the modern IC design cycle. The recent advancements in hardware\nfuzzing have proven effective in detecting bugs in RTL designs of cutting-edge\nprocessors. The modern IC design flow involves incremental updates and\nmodifications to the hardware designs necessitating rigorous verification and\nextending the overall verification period. To accelerate this process, directed\nfuzzing has emerged focusing on generating targeted stimuli for specific\nregions of the design, avoiding the need for exhaustive, full-scale\nverification. However, a significant limitation of these hardware fuzzers lies\nin their reliance on an equivalent SW model of the hardware which fails to\ncapture intrinsic hardware characteristics. To circumvent the aforementioned\nchallenges, this work introduces TargetFuzz, an innovative and scalable\ntargeted hardware fuzzing mechanism. It leverages SAT-based techniques to focus\non specific regions of the hardware design while operating at its native\nhardware abstraction level, ensuring a more precise and comprehensive\nverification process. We evaluated this approach across a diverse range of RTL\ndesigns for various IP cores. Our experimental results demonstrate its\ncapability to effectively target and fuzz a broad spectrum of sites within\nthese designs, showcasing its extensive coverage and precision in addressing\ntargeted regions. TargetFuzz demonstrates its capability to effectively scale\n30x greater in terms of handling target sites, achieving 100% state coverage\nand 1.5x faster in terms of site coverage, and shows 90x improvement in target\nstate coverage compared to Coverage-Guided Fuzzing, demonstrating its potential\nto advance the state-of-the-art in directed hardware fuzzing."}
{"id": "2509.26530", "categories": ["cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.26530", "abs": "https://arxiv.org/abs/2509.26530", "authors": ["Aleksandra Knapińska", "Marija Furdek"], "title": "Explainable and Resilient ML-Based Physical-Layer Attack Detectors", "comment": null, "summary": "Detection of emerging attacks on network infrastructure is a critical aspect\nof security management. To meet the growing scale and complexity of modern\nthreats, machine learning (ML) techniques offer valuable tools for automating\nthe detection of malicious activities. However, as these techniques become more\ncomplex, their internal operations grow increasingly opaque. In this context,\nwe address the need for explainable physical-layer attack detection methods.\nFirst, we analyze the inner workings of various classifiers trained to alert\nabout physical layer intrusions, examining how the influence of different\nmonitored parameters varies depending on the type of attack being detected.\nThis analysis not only improves the interpretability of the models but also\nsuggests ways to enhance their design for increased speed. In the second part,\nwe evaluate the detectors' resilience to malicious parameter noising. The\nresults highlight a key trade-off between model speed and resilience. This work\nserves as a design guideline for developing fast and robust detectors trained\non available network monitoring data."}
{"id": "2509.26562", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26562", "abs": "https://arxiv.org/abs/2509.26562", "authors": ["Firas Ben Hmida", "Abderrahmen Amich", "Ata Kaboudi", "Birhanu Eshete"], "title": "DeepProv: Behavioral Characterization and Repair of Neural Networks via Inference Provenance Graph Analysis", "comment": "18 pages, 9 figures, 6 tables, To appear in the 41st Annual Computer\n  Security Applications Conference (ACSAC), 2025", "summary": "Deep neural networks (DNNs) are increasingly being deployed in high-stakes\napplications, from self-driving cars to biometric authentication. However,\ntheir unpredictable and unreliable behaviors in real-world settings require new\napproaches to characterize and ensure their reliability.\n  This paper introduces DeepProv, a novel and customizable system designed to\ncapture and characterize the runtime behavior of DNNs during inference by using\ntheir underlying graph structure. Inspired by system audit provenance graphs,\nDeepProv models the computational information flow of a DNN's inference process\nthrough Inference Provenance Graphs (IPGs). These graphs provide a detailed\nstructural representation of the behavior of DNN, allowing both empirical and\nstructural analysis. DeepProv uses these insights to systematically repair DNNs\nfor specific objectives, such as improving robustness, privacy, or fairness.\n  We instantiate DeepProv with adversarial robustness as the goal of model\nrepair and conduct extensive case studies to evaluate its effectiveness. Our\nresults demonstrate its effectiveness and scalability across diverse\nclassification tasks, attack scenarios, and model complexities. DeepProv\nautomatically identifies repair actions at the node and edge-level within IPGs,\nsignificantly enhancing the robustness of the model. In particular, applying\nDeepProv repair strategies to just a single layer of a DNN yields an average\n55% improvement in adversarial accuracy. Moreover, DeepProv complements\nexisting defenses, achieving substantial gains in adversarial robustness.\nBeyond robustness, we demonstrate the broader potential of DeepProv as an\nadaptable system to characterize DNN behavior in other critical areas, such as\nprivacy auditing and fairness analysis."}
{"id": "2509.26598", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26598", "abs": "https://arxiv.org/abs/2509.26598", "authors": ["Anshul Nasery", "Edoardo Contente", "Alkin Kaz", "Pramod Viswanath", "Sewoong Oh"], "title": "Are Robust LLM Fingerprints Adversarially Robust?", "comment": null, "summary": "Model fingerprinting has emerged as a promising paradigm for claiming model\nownership. However, robustness evaluations of these schemes have mostly focused\non benign perturbations such as incremental fine-tuning, model merging, and\nprompting. Lack of systematic investigations into {\\em adversarial robustness}\nagainst a malicious model host leaves current systems vulnerable. To bridge\nthis gap, we first define a concrete, practical threat model against model\nfingerprinting. We then take a critical look at existing model fingerprinting\nschemes to identify their fundamental vulnerabilities. Based on these, we\ndevelop adaptive adversarial attacks tailored for each vulnerability, and\ndemonstrate that these can bypass model authentication completely for ten\nrecently proposed fingerprinting schemes while maintaining high utility of the\nmodel for the end users. Our work encourages fingerprint designers to adopt\nadversarial robustness by design. We end with recommendations for future\nfingerprinting methods."}
