<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 15]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Safe2Harm: Semantic Isomorphism Attacks for Jailbreaking Large Language Models](https://arxiv.org/abs/2512.13703)
*Fan Yang*

Main category: cs.CR

TL;DR: Safe2Harm攻击方法通过语义同构转换实现高效越狱：将有害问题重写为语义安全的相似问题，提取主题映射关系，让LLM生成安全响应，再反向重写得到有害输出。


<details>
  <summary>Details</summary>
Motivation: 现有越狱方法主要围绕提示工程或对抗优化，但作者发现许多有害场景与合法场景在底层原理上高度一致。基于这一现象，提出通过语义同构实现越狱的新方法。

Method: Safe2Harm语义同构攻击方法包含四个阶段：1) 将有害问题重写为语义安全的相似问题；2) 提取两者间的主题映射关系；3) LLM生成针对安全问题的详细响应；4) 基于映射关系反向重写安全响应得到有害输出。

Result: 在7个主流LLM和三类基准数据集上的实验表明，Safe2Harm展现出强大的越狱能力，整体性能优于现有方法。同时构建了包含358个样本的挑战性有害内容评估数据集。

Conclusion: 该研究提出了一种基于语义同构的新型越狱方法，揭示了LLM安全漏洞的新维度。构建的有害内容评估数据集可用于评估现有有害检测方法的有效性，为LLM输入输出过滤防御提供支持。

Abstract: Large Language Models (LLMs) have demonstrated exceptional performance across various tasks, but their security vulnerabilities can be exploited by attackers to generate harmful content, causing adverse impacts across various societal domains. Most existing jailbreak methods revolve around Prompt Engineering or adversarial optimization, yet we identify a previously overlooked phenomenon: many harmful scenarios are highly consistent with legitimate ones in terms of underlying principles. Based on this finding, this paper proposes the Safe2Harm Semantic Isomorphism Attack method, which achieves efficient jailbreaking through four stages: first, rewrite the harmful question into a semantically safe question with similar underlying principles; second, extract the thematic mapping relationship between the two; third, let the LLM generate a detailed response targeting the safe question; finally, reversely rewrite the safe response based on the thematic mapping relationship to obtain harmful output. Experiments on 7 mainstream LLMs and three types of benchmark datasets show that Safe2Harm exhibits strong jailbreaking capability, and its overall performance is superior to existing methods. Additionally, we construct a challenging harmful content evaluation dataset containing 358 samples and evaluate the effectiveness of existing harmful detection methods, which can be deployed for LLM input-output filtering to enable defense.

</details>


### [2] [Smart Surveillance: Identifying IoT Device Behaviours using ML-Powered Traffic Analysis](https://arxiv.org/abs/2512.13709)
*Reza Ryan,Napoleon Paciente,Cahil Youngs,Nickson Karie,Qian Li,Nasim Ferdosian*

Main category: cs.CR

TL;DR: 使用随机森林、多层感知机和K近邻等机器学习技术，结合网络流量监控来分类物联网设备类型及其行为，随机森林达到91%最高准确率


<details>
  <summary>Details</summary>
Motivation: 物联网设备激增带来安全挑战，现有设备识别方法主要局限于本地网络，无法进行外部监控和分类，需要新的解决方案

Method: 构建包含NPAT路由器和多种物联网设备的测试平台，使用随机森林、多层感知机和K近邻三种机器学习算法，通过监控和分析设备与网络间的数据包流来分类设备类型和行为

Result: 随机森林分类器达到91%最高准确率，多层感知机最低为56%，除安全摄像头的某些行为外，所有设备类别都能成功分类

Conclusion: 机器学习驱动的物联网设备和行为识别是可行的，随机森林表现最佳，但方法在安全摄像头行为识别方面存在局限性，展示了该方法的潜力和限制

Abstract: The proliferation of Internet of Things (IoT) devices has grown exponentially in recent years, introducing significant security challenges. Accurate identification of the types of IoT devices and their associated actions through network traffic analysis is essential to mitigate potential threats. By monitoring and analysing packet flows between IoT devices and connected networks, anomalous or malicious behaviours can be detected. Existing research focuses primarily on device identification within local networks using methods such as protocol fingerprinting and wireless frequency scanning. However, these approaches are limited in their ability to monitor or classify IoT devices externally. To address this gap, we investigate the use of machine learning (ML) techniques, specifically Random Forest (RF), Multilayer Perceptron (MLP), and K-Nearest Neighbours (KNN), in conjunction with targeted network traffic monitoring to classify IoT device types and their actions. We constructed a testbed comprising an NPAT-enabled router and a diverse set of IoT devices, including smart cameras, controller hubs, home appliances, power controllers, and streaming devices. Experimental results demonstrate that IoT device and action recognition is feasible using our proposed ML-driven approach, with the RF classifier achieving the highest accuracy of 91%, while the MLP recorded the lowest accuracy at 56%. Notably, all device categories were successfully classified except for certain actions associated with security cameras, underscoring both the potential and the limitations of the proposed method.

</details>


### [3] [Stability-Drift Early Warning for Cyber-Physical Systems Under Degradation Attacks](https://arxiv.org/abs/2512.13767)
*Daniyal Ganiuly,Nurzhau Bolatbek,Assel Smaiyl*

Main category: cs.CR

TL;DR: 本文提出了一种基于稳定性漂移的无人机早期退化预警方法，通过追踪预测与观测状态转移之间的微小差异，在退化变得明显前提供预警信号。


<details>
  <summary>Details</summary>
Motivation: 无人机等网络物理系统容易受到缓慢退化的影响，如传感器偏差或时序异常会随时间累积，逐渐降低稳定性，而标准监控机制仍报告正常操作。现有方法主要关注突发故障或明显轨迹偏差，难以检测这种早期退化阶段。

Method: 提出基于稳定性漂移的早期预警方法，通过测量短时间范围内预测状态转移与观测状态转移之间的差异，追踪这种差异的逐渐增长来识别新兴的不稳定性。该方法在飞行堆栈外部运行，仅依赖标准遥测数据，无需修改自动驾驶仪固件。

Result: 在PX4 x500平台的软件在环环境中评估了两种现实退化场景：IMU偏差漂移和控制循环时序异常。稳定性漂移指标在两种情况下都提供了持续数秒的早期预警信号，在名义和激进但非退化的飞行中保持稳定。

Conclusion: 稳定性漂移可作为无人机控制系统早期退化的实用指标。通过在预不稳定阶段提供提前通知，该方法补充了现有安全机制，为缓解或安全模式转换提供了额外时间，特别适用于缓慢和微妙的攻击场景。

Abstract: Cyber-physical systems (CPS) such as unmanned aerial vehicles are vulnerable to slow degradation that develops without causing immediate or obvious failures. Small sensor biases or timing irregularities can accumulate over time, gradually reducing stability while standard monitoring mechanisms continue to report normal operation. Detecting this early phase of degradation remains a challenge, as most existing approaches focus on abrupt faults or visible trajectory deviations. This paper introduces an early warning method based on stability drift, which measures the divergence between predicted and observed state transitions over short horizons. By tracking the gradual growth of this divergence, the proposed approach identifies emerging instability before it becomes visible in the flight trajectory or estimator residuals. The method operates externally to the flight stack and relies only on standard telemetry, making it suitable for deployment without modifying autopilot firmware. The approach was evaluated on a PX4 x500 platform in a software in the loop environment under two realistic degradation scenarios, gradual IMU bias drift and timing irregularities in the control loop. In both cases, the stability drift metric provided a consistent early warning signal several seconds before visible instability appeared, while remaining stable during nominal and aggressive but non degraded flight. The results demonstrate that stability drift can serve as a practical indicator of early degradation in UAV control systems. By providing advance notice during a pre instability phase, the proposed method complements existing safety mechanisms and offers additional time for mitigation or safe mode transitions under slow and subtle attacks.

</details>


### [4] [A Deep Dive into Function Inlining and its Security Implications for ML-based Binary Analysis](https://arxiv.org/abs/2512.14045)
*Omar Abusabha,Jiyong Uhm,Tamer Abuhmed,Hyungjoon Koo*

Main category: cs.CR

TL;DR: 论文首次系统研究函数内联对机器学习二进制分析安全性的影响，发现即使良性的编译器优化也可能被利用来逃避ML模型检测，揭示了ML模型对静态特征的敏感性及编译器设置可被用于制作规避变体。


<details>
  <summary>Details</summary>
Motivation: 函数内联是现代编译器广泛使用的优化技术，虽然提升性能但显著影响机器指令和控制流图等静态特征，而这些特征对二进制分析至关重要。然而，函数内联的安全影响至今仍未得到充分探索。

Method: 通过剖析LLVM成本模型中的内联决策流程，探索超越标准优化级别的极端内联编译器选项组合。聚焦5个ML辅助的二进制安全分析任务，使用20个独特模型系统评估其在极端内联场景下的鲁棒性。

Result: 实验发现：1）函数内联虽为良性优化，但可直接或间接影响ML模型行为，可能被利用来逃避判别式或生成式ML模型；2）依赖静态特征的ML模型对内联高度敏感；3）细微编译器设置可被用于故意制作规避性二进制变体；4）内联率在不同应用和构建配置中差异显著，破坏了ML模型训练和评估的一致性假设。

Conclusion: 函数内联作为编译器优化对ML二进制分析安全有重大影响，揭示了ML模型对静态特征变化的脆弱性，强调了在安全关键应用中考虑编译器优化影响的必要性，并为构建更鲁棒的ML辅助二进制分析工具提供了见解。

Abstract: A function inlining optimization is a widely used transformation in modern compilers, which replaces a call site with the callee's body in need. While this transformation improves performance, it significantly impacts static features such as machine instructions and control flow graphs, which are crucial to binary analysis. Yet, despite its broad impact, the security impact of function inlining remains underexplored to date. In this paper, we present the first comprehensive study of function inlining through the lens of machine learning-based binary analysis. To this end, we dissect the inlining decision pipeline within the LLVM's cost model and explore the combinations of the compiler options that aggressively promote the function inlining ratio beyond standard optimization levels, which we term extreme inlining. We focus on five ML-assisted binary analysis tasks for security, using 20 unique models to systematically evaluate their robustness under extreme inlining scenarios. Our extensive experiments reveal several significant findings: i) function inlining, though a benign transformation in intent, can (in)directly affect ML model behaviors, being potentially exploited by evading discriminative or generative ML models; ii) ML models relying on static features can be highly sensitive to inlining; iii) subtle compiler settings can be leveraged to deliberately craft evasive binary variants; and iv) inlining ratios vary substantially across applications and build configurations, undermining assumptions of consistency in training and evaluation of ML models.

</details>


### [5] [From Obfuscated to Obvious: A Comprehensive JavaScript Deobfuscation Tool for Security Analysis](https://arxiv.org/abs/2512.14070)
*Dongchao Zhou,Lingyun Ying,Huajun Chai,Dongbin Wang*

Main category: cs.CR

TL;DR: JSIMPLIFIER是一个多阶段JavaScript反混淆工具，通过预处理、AST静态分析、动态执行追踪和LLM增强标识符重命名，在真实数据集上实现了100%处理能力、100%正确性和88.2%代码复杂度降低。


<details>
  <summary>Details</summary>
Motivation: 当前JavaScript反混淆工具存在严重限制：处理不同输入格式困难、只能应对特定混淆类型、输出结果晦涩难懂，阻碍了实际安全分析效果。

Method: 采用多阶段流水线：预处理、基于抽象语法树的静态分析、动态执行追踪、LLM增强的标识符重命名。同时引入多维评估指标，包括控制/数据流分析、代码简化评估、熵测量和LLM可读性评估。

Result: 构建并发布了最大的真实世界混淆JavaScript数据集（44,421个样本）。JSIMPLIFIER在20种混淆技术上实现100%处理能力，评估子集上100%正确性，88.2%代码复杂度降低，经多个LLM验证可读性提升超过4倍。

Conclusion: JSIMPLIFIER显著提升了JavaScript反混淆的实用效果，为相关研究和实际安全应用建立了新的基准。

Abstract: JavaScript's widespread adoption has made it an attractive target for malicious attackers who employ sophisticated obfuscation techniques to conceal harmful code. Current deobfuscation tools suffer from critical limitations that severely restrict their practical effectiveness. Existing tools struggle with diverse input formats, address only specific obfuscation types, and produce cryptic output that impedes human analysis.
  To address these challenges, we present JSIMPLIFIER, a comprehensive deobfuscation tool using a multi-stage pipeline with preprocessing, abstract syntax tree-based static analysis, dynamic execution tracing, and Large Language Model (LLM)-enhanced identifier renaming. We also introduce multi-dimensional evaluation metrics that integrate control/data flow analysis, code simplification assessment, entropy measures and LLM-based readability assessments.
  We construct and release the largest real-world obfuscated JavaScript dataset with 44,421 samples (23,212 wild malicious + 21,209 benign samples). Evaluation shows JSIMPLIFIER outperforms existing tools with 100% processing capability across 20 obfuscation techniques, 100% correctness on evaluation subsets, 88.2% code complexity reduction, and over 4-fold readability improvement validated by multiple LLMs. Our results advance benchmarks for JavaScript deobfuscation research and practical security applications.

</details>


### [6] [UIXPOSE: Mobile Malware Detection via Intention-Behaviour Discrepancy Analysis](https://arxiv.org/abs/2512.14130)
*Amirmohammad Pasdar,Toby Murray,Van-Thuan Pham*

Main category: cs.CR

TL;DR: UIXPOSE是一个源代码无关的移动恶意软件分析框架，通过意图行为对齐（IBA）将UI推断的意图与运行时语义对齐，能够检测传统方法遗漏的隐蔽恶意行为。


<details>
  <summary>Details</summary>
Motivation: 现有移动恶意软件分析方法存在局限性：静态方法（如基于权限或widget级别）只能推断意图，而动态方法监控粗粒度信号（端点、部分资源使用）会遗漏内容和上下文信息，无法有效检测隐蔽的恶意行为。

Method: UIXPOSE框架使用视觉语言模型和知识结构从每个屏幕推断意图向量，同时将解码的网络负载、堆/内存信号和资源利用轨迹组合成行为向量，在运行时计算两者的对齐度，实现意图行为对齐分析。

Result: 在三个真实世界案例研究中，UIXPOSE揭示了仅基于元数据的基线方法无法检测的隐蔽数据外泄和隐藏后台活动，证明了意图行为对齐方法在动态检测中的有效性。

Conclusion: 意图行为对齐（IBA）方法能够显著改进移动恶意软件的动态检测能力，通过将UI推断的意图与运行时语义对齐，可以检测传统方法遗漏的隐蔽恶意行为，为移动安全分析提供了新的有效框架。

Abstract: We introduce UIXPOSE, a source-code-agnostic framework that operates on both compiled and open-source apps. This framework applies Intention Behaviour Alignment (IBA) to mobile malware analysis, aligning UI-inferred intent with runtime semantics. Previous work either infers intent statically, e.g., permission-centric, or widget-level or monitors coarse dynamic signals (endpoints, partial resource usage) that miss content and context. UIXPOSE infers an intent vector from each screen using vision-language models and knowledge structures and combines decoded network payloads, heap/memory signals, and resource utilisation traces into a behaviour vector. Their alignment, calculated at runtime, can both detect misbehaviour and highlight exploration of behaviourally rich paths. In three real-world case studies, UIXPOSE reveals covert exfiltration and hidden background activity that evade metadata-only baselines, demonstrating how IBA improves dynamic detection.

</details>


### [7] [HAL -- An Open-Source Framework for Gate-Level Netlist Analysis](https://arxiv.org/abs/2512.14139)
*Julian Speith,Jörn Langheinrich,Marc Fyrbiak,Max Hoffmann,Sebastian Wallat,Simon Klix,Nils Albartus,René Walendy,Steffen Becker,Christof Paar*

Main category: cs.CR

TL;DR: HAL是一个用于门级网表分析的开源框架，提供GUI、插件系统和API，支持硬件逆向工程研究。


<details>
  <summary>Details</summary>
Motivation: 硬件逆向工程是理解硬件安全威胁的关键步骤，但缺乏可访问、可复现的研究工具。HAL旨在为学术界、工业界和政府提供统一的硬件逆向工程平台。

Method: 开发开源框架，包含交互式GUI、可扩展插件系统、C++和Python API，并提供模块化、密码分析、仿真和图形探索等核心插件。

Result: 自2019年发布以来，HAL被广泛采用，支持了23篇学术论文，用于培训、会议教程和大学课程，GitHub获得680星和86个分支。

Conclusion: HAL通过提供可访问和可复现的硬件逆向工程研究工具，显著推动了该领域的发展，增强了对实际能力和威胁的理解。

Abstract: HAL is an open-source framework for gate-level netlist analysis, an integral step in hardware reverse engineering. It provides analysts with an interactive GUI, an extensible plugin system, and APIs in both C++ and Python for rapid prototyping and automation. In addition, HAL ships with plugins for word-level modularization, cryptographic analysis, simulation, and graph-based exploration. Since its release in 2019, HAL has become widely adopted in academia, industry, government, and teaching. It underpins at least 23 academic publications, is taught in hands-on trainings, conference tutorials, and university classes, and has collected over 680 stars and 86 forks on GitHub. By enabling accessible and reproducible hardware reverse engineering research, HAL has significantly advanced the field and the understanding of real-world capabilities and threats.

</details>


### [8] [IntentMiner: Intent Inversion Attack via Tool Call Analysis in the Model Context Protocol](https://arxiv.org/abs/2512.14166)
*Yunhao Yao,Zhiqiang Wang,Haoran Cheng,Yihang Cheng,Haohua Du,Xiang-Yang Li*

Main category: cs.CR

TL;DR: 论文提出IntentMiner框架，通过分析工具调用日志来重构用户隐私意图，揭示了MCP架构中的隐私风险


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为自主代理采用MCP协议进行工具调用，但第三方MCP服务器作为半诚实中介可以观察详细的工具交互日志，这带来了隐私泄露风险

Method: 提出IntentMiner框架，采用分层信息隔离和三维语义分析（工具目的、调用语句、返回结果）来准确推断用户意图

Result: IntentMiner在语义对齐方面达到85%以上，显著优于基线方法，证明工具执行日志可作为泄露用户隐私的有效载体

Conclusion: 解耦的代理架构存在固有隐私风险，看似良性的工具执行日志可能暴露用户秘密，需要更强的隐私保护机制

Abstract: The rapid evolution of Large Language Models (LLMs) into autonomous agents has led to the adoption of the Model Context Protocol (MCP) as a standard for discovering and invoking external tools. While this architecture decouples the reasoning engine from tool execution to enhance scalability, it introduces a significant privacy surface: third-party MCP servers, acting as semi-honest intermediaries, can observe detailed tool interaction logs outside the user's trusted boundary. In this paper, we first identify and formalize a novel privacy threat termed Intent Inversion, where a semi-honest MCP server attempts to reconstruct the user's private underlying intent solely by analyzing legitimate tool calls. To systematically assess this vulnerability, we propose IntentMiner, a framework that leverages Hierarchical Information Isolation and Three-Dimensional Semantic Analysis, integrating tool purpose, call statements, and returned results, to accurately infer user intent at the step level. Extensive experiments demonstrate that IntentMiner achieves a high degree of semantic alignment (over 85%) with original user queries, significantly outperforming baseline approaches. These results highlight the inherent privacy risks in decoupled agent architectures, revealing that seemingly benign tool execution logs can serve as a potent vector for exposing user secrets.

</details>


### [9] [LegionITS: A Federated Intrusion-Tolerant System Architecture](https://arxiv.org/abs/2512.14242)
*Tadeu Freitas,Carlos Novo,Manuel E. Correia,Rolando Martins*

Main category: cs.CR

TL;DR: 提出結合入侵容忍系統與惡意軟體資訊共享平台的新架構，實現安全保密的網路威脅情報共享，並透過差分隱私聯邦學習驗證可行性


<details>
  <summary>Details</summary>
Motivation: 現有資安解決方案(SIEM、SOAR、SOC)難以應對新興攻擊，需要促進實體間的資訊共享，但必須解決敏感資料交換的保密性與操作安全挑戰

Method: 提出LegionITS架構，聯邦化入侵容忍系統並結合MISP概念，採用差分隱私聯邦學習技術，在保護資料隱私的前提下實現協作防禦

Result: 差分隱私聯邦學習模組測試顯示準確率從98.42%降至85.98%(平均損失12.44%)，但仍能可靠檢測受損訊息，證明安全資料共享的可行性

Conclusion: 提出的架構能實現安全保密的網路威脅情報共享，增強集體防禦能力，為LegionITS全面實施奠定基礎

Abstract: The growing sophistication, frequency, and diversity of cyberattacks increasingly exceed the capacity of individual entities to fully understand and counter them. While existing solutions, such as Security Information and Event Management (SIEM) systems, Security Orchestration, Automation, and Response (SOAR) platforms, and Security Operation Center (SOC), play a vital role in mitigating known threats, they often struggle to effectively address emerging and unforeseen attacks. To increase the effectiveness of cyber defense, it is essential to foster greater information sharing between entities; however, this requires addressing the challenge of exchanging sensitive data without compromising confidentiality or operational security.
  To address the challenges of secure and confidential Cyber Threat Intelligence (CTI) sharing, we propose a novel architecture that federates Intrusion Tolerant Systems (ITSs) and leverages concepts from Malware Information Sharing Platform (MISP) to empower SOCs. This framework enables controlled collaboration and data privacy while enhancing collective defenses. As a proof of concept, we evaluate one module by applying Differential Privacy (DP) to Federated Learning (FL), observing a manageable accuracy drop from 98.42% to 85.98% (average loss 12.44%) while maintaining reliable detection of compromised messages. These results highlight the viability of secure data sharing and establishes a foundation for the future full-scale implementation of LegionITS.

</details>


### [10] [Lost in the Pages: WebAssembly Code Recovery through SEV-SNP's Exposed Address Space](https://arxiv.org/abs/2512.14376)
*Markus Berthilsson,Christian Gehrmann*

Main category: cs.CR

TL;DR: 本文提出一种针对WebAssembly在可信执行环境中的新型代码机密性攻击，利用地址空间信息泄露，结合侧信道攻击，能恢复超过70%的代码，相比之前SGX单步执行攻击的50%恢复率有显著提升。


<details>
  <summary>Details</summary>
Motivation: WebAssembly作为跨平台计算技术日益流行，可信执行环境为安全敏感Wasm工作负载提供保护。然而，已有研究表明Wasm二进制文件比原生二进制文件更容易受到代码机密性攻击，但之前的研究仅限于Intel SGX。本文旨在进一步探索TEEs中Wasm代码机密性攻击的可能性。

Method: 提出一种新型Wasm代码机密性攻击方法，利用TEEs中暴露的地址空间信息，提取关键执行特征，并结合额外的侧信道攻击手段。

Result: 攻击能够在大多数情况下以高可靠性恢复超过70%的代码，这比之前Intel SGX单步执行攻击只能恢复最多50%代码的结果有显著提升。

Conclusion: TEEs中Wasm代码的机密性仍然面临严重威胁，地址空间信息泄露结合侧信道攻击能够有效恢复大量代码，需要更强的保护机制来防御此类攻击。

Abstract: WebAssembly (Wasm) has risen as a widely used technology to distribute computing workloads on different platforms. The platform independence offered through Wasm makes it an attractive solution for many different applications that can run on disparate infrastructures. In addition, Trusted Execution Environments (TEEs) are offered in many computing infrastructures, which allows also running security sensitive Wasm workloads independent of the specific platforms offered. However, recent work has shown that Wasm binaries are more sensitive to code confidentiality attacks than native binaries. The previous result was obtained for Intel SGX only. In this paper, we take this one step further, introducing a new Wasm code-confidentiality attack that exploits exposed address-space information in TEEs. Our attack enables the extraction of crucial execution features which, when combined with additional side channels, allows us to with high reliability obtain more than 70% of the code in most cases. This is a considerably larger amount than was previously obtained by single stepping Intel SGX where only upwards to 50% of the code could be obtained.

</details>


### [11] [Hybrid Ensemble Method for Detecting Cyber-Attacks in Water Distribution Systems Using the BATADAL Dataset](https://arxiv.org/abs/2512.14422)
*Waqas Ahmed*

Main category: cs.CR

TL;DR: 提出混合集成学习框架检测工业控制系统网络攻击，结合机器学习与深度学习模型，在BATADAL基准数据上取得最佳性能


<details>
  <summary>Details</summary>
Motivation: 工业控制系统网络安全日益重要，但现有BATADAL基准数据存在类别不平衡、多变量时间依赖和隐蔽攻击等问题，需要更有效的检测方法

Method: 使用混合集成学习模型，结合随机森林、XGBoost和LSTM三种基础学习器，采用简单平均和堆叠集成（逻辑回归元学习器）七种集成方式，使用SMOTE处理类别不平衡，并提取时序和统计特征

Result: 单一LSTM模型表现差（F1=0.000，AUC=0.4460），树模型特别是XGBoost表现好（F1=0.7470，AUC=0.9684），混合堆叠集成获得最高性能（攻击类F1=0.7205，AUC=0.9826）

Conclusion: 提出的混合集成框架为时间依赖工业系统建立了鲁棒可扩展的网络攻击检测方案，通过结合时序学习和模型多样性支持关键基础设施安全运行

Abstract: The cybersecurity of Industrial Control Systems that manage critical infrastructure such as Water Distribution Systems has become increasingly important as digital connectivity expands. BATADAL benchmark data is a good source of testing intrusion detection techniques, but it presents several important problems, such as imbalance in the number of classes, multivariate time dependence, and stealthy attacks. We consider a hybrid ensemble learning model that will enhance the detection ability of cyber-attacks in WDS by using the complementary capabilities of machine learning and deep learning models. Three base learners, namely, Random Forest , eXtreme Gradient Boosting , and Long Short-Term Memory network, have been strictly compared and seven ensemble types using simple averaged and stacked learning with a logistic regression meta-learner. Random Forest analysis identified top predictors turned into temporal and statistical features, and Synthetic Minority Oversampling Technique (SMOTE) was used to overcome the class imbalance issue. The analyics indicates that the single Long Short-Term Memory network model is of poor performance (F1 = 0.000, AUC = 0.4460), but tree-based models, especially eXtreme Gradient Boosting, perform well (F1 = 0.7470, AUC=0.9684). The hybrid stacked ensemble of Random Forest , eXtreme Gradient Boosting , and Long Short-Term Memory network scored the highest, with the attack class of 0.7205 with an F1-score and a AUC of 0.9826 indicating that the heterogeneous stacking between model precision and generalization can work. The proposed framework establishes a robust and scalable solution for cyber-attack detection in time-dependent industrial systems, integrating temporal learning and ensemble diversity to support the secure operation of critical infrastructure.

</details>


### [12] [VICTOR: Dataset Copyright Auditing in Video Recognition Systems](https://arxiv.org/abs/2512.14439)
*Quan Yuan,Zhikun Zhang,Linkang Du,Min Chen,Mingyang Sun,Yunjun Gao,Shibo He,Jiming Chen*

Main category: cs.CR

TL;DR: VICTOR是首个针对视频识别系统的数据集版权审计方法，通过修改少量样本（如1%）来增强目标模型输出差异，从而检测未经授权的数据集使用。


<details>
  <summary>Details</summary>
Motivation: 视频识别系统日益普及，公开数据集常被滥用和侵权。现有版权审计方法主要针对图像领域，视频数据的时序特性给版权审计带来巨大挑战，需要专门解决方案。

Method: 提出VICTOR方法：开发通用且隐蔽的样本修改策略，仅修改少量样本（如1%）来放大目标模型输出差异；通过比较模型对已发布修改样本和未发布原始样本的行为差异来进行数据集审计。

Result: 在多个模型和数据集上的广泛实验表明VICTOR具有优越性；该方法对训练视频或目标模型的各种扰动机制具有鲁棒性。

Conclusion: VICTOR是首个针对视频识别系统的数据集版权审计方法，能有效检测未经授权的数据集使用，填补了视频领域版权审计的空白。

Abstract: Video recognition systems are increasingly being deployed in daily life, such as content recommendation and security monitoring. To enhance video recognition development, many institutions have released high-quality public datasets with open-source licenses for training advanced models. At the same time, these datasets are also susceptible to misuse and infringement. Dataset copyright auditing is an effective solution to identify such unauthorized use. However, existing dataset copyright solutions primarily focus on the image domain; the complex nature of video data leaves dataset copyright auditing in the video domain unexplored. Specifically, video data introduces an additional temporal dimension, which poses significant challenges to the effectiveness and stealthiness of existing methods.
  In this paper, we propose VICTOR, the first dataset copyright auditing approach for video recognition systems. We develop a general and stealthy sample modification strategy that enhances the output discrepancy of the target model. By modifying only a small proportion of samples (e.g., 1%), VICTOR amplifies the impact of published modified samples on the prediction behavior of the target models. Then, the difference in the model's behavior for published modified and unpublished original samples can serve as a key basis for dataset auditing. Extensive experiments on multiple models and datasets highlight the superiority of VICTOR. Finally, we show that VICTOR is robust in the presence of several perturbation mechanisms to the training videos or the target models.

</details>


### [13] [Reasoning-Style Poisoning of LLM Agents via Stealthy Style Transfer: Process-Level Attacks and Runtime Monitoring in RSV Space](https://arxiv.org/abs/2512.14448)
*Xingfu Zhou,Pengfei Wang*

Main category: cs.CR

TL;DR: 本文提出了一种针对LLM代理的新型攻击方法——推理风格投毒(RSP)，通过改变代理处理信息的方式而非内容本身来操纵其推理过程，并开发了相应的防御监控系统。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击主要关注内容伪造或指令注入，但忽略了代理的推理风格这一新的攻击面。作者发现可以操纵代理处理信息的方式（而非内容本身）来影响其决策，这在高风险环境中尤其危险。

Method: 提出了推理风格投毒(RSP)范式，开发了生成风格注入(GSI)攻击方法，将检索到的文档重写为"分析瘫痪"或"认知仓促"的病态语调而不改变事实内容。开发了推理风格向量(RSV)指标来量化推理风格变化，包含验证深度、自信度和注意力焦点三个维度。

Result: 在HotpotQA和FEVER数据集上对ReAct、Reflection和ToT架构的实验表明，GSI显著降低性能：推理步骤增加最多4.4倍或导致过早错误，成功绕过最先进的内容过滤器。提出的RSP-M轻量级运行时监控器能实时计算RSV指标并在超出安全阈值时触发警报。

Conclusion: 推理风格是一个独特且可利用的漏洞，需要超越静态内容分析的过程级防御。RSP攻击展示了操纵LLM代理推理过程的新方法，而RSP-M提供了有效的实时监控解决方案。

Abstract: Large Language Model (LLM) agents relying on external retrieval are increasingly deployed in high-stakes environments. While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style. We propose Reasoning-Style Poisoning (RSP), a paradigm that manipulates how agents process information rather than what they process. We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically "analysis paralysis" or "cognitive haste"--without altering underlying facts or using explicit triggers. To quantify these shifts, we develop the Reasoning Style Vector (RSV), a metric tracking Verification depth, Self-confidence, and Attention focus. Experiments on HotpotQA and FEVER using ReAct, Reflection, and Tree of Thoughts (ToT) architectures reveal that GSI significantly degrades performance. It increases reasoning steps by up to 4.4 times or induces premature errors, successfully bypassing state-of-the-art content filters. Finally, we propose RSP-M, a lightweight runtime monitor that calculates RSV metrics in real-time and triggers alerts when values exceed safety thresholds. Our work demonstrates that reasoning style is a distinct, exploitable vulnerability, necessitating process-level defenses beyond static content analysis.

</details>


### [14] [PrivATE: Differentially Private Average Treatment Effect Estimation for Observational Data](https://arxiv.org/abs/2512.14557)
*Quan Yuan,Xiaochen Li,Linkang Du,Min Chen,Mingyang Sun,Yunjun Gao,Shibo He,Jiming Chen,Zhikun Zhang*

Main category: cs.CR

TL;DR: PrivATE是一个差分隐私保护的平均处理效应估计框架，提供标签级和样本级两种隐私保护级别，通过自适应匹配限制平衡噪声误差和匹配误差，在多个数据集和隐私预算下优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 从观测数据估计因果效应（特别是平均处理效应ATE）存在严重的隐私风险，现有差分隐私ATE估计方法要么依赖特定假设，要么提供有限的隐私保护，要么无法提供全面的信息保护。

Method: 提出PrivATE框架，设计标签级和样本级两种隐私保护级别以适应不同隐私需求，通过推导自适应匹配限制来平衡噪声引起的误差和匹配误差，从而提高ATE估计的准确性。

Result: PrivATE在所有数据集和隐私预算下都优于基线方法，验证了框架的有效性。

Conclusion: PrivATE是一个实用的差分隐私ATE估计框架，能够适应不同的隐私需求，通过自适应匹配机制在保护隐私的同时提高估计准确性。

Abstract: Causal inference plays a crucial role in scientific research across multiple disciplines. Estimating causal effects, particularly the average treatment effect (ATE), from observational data has garnered significant attention. However, computing the ATE from real-world observational data poses substantial privacy risks to users. Differential privacy, which offers strict theoretical guarantees, has emerged as a standard approach for privacy-preserving data analysis. However, existing differentially private ATE estimation works rely on specific assumptions, provide limited privacy protection, or fail to offer comprehensive information protection.
  To this end, we introduce PrivATE, a practical ATE estimation framework that ensures differential privacy. In fact, various scenarios require varying levels of privacy protection. For example, only test scores are generally sensitive information in education evaluation, while all types of medical record data are usually private. To accommodate different privacy requirements, we design two levels (i.e., label-level and sample-level) of privacy protection in PrivATE. By deriving an adaptive matching limit, PrivATE effectively balances noise-induced error and matching error, leading to a more accurate estimate of ATE. Our evaluation validates the effectiveness of PrivATE. PrivATE outperforms the baselines on all datasets and privacy budgets.

</details>


### [15] [PerProb: Indirectly Evaluating Memorization in Large Language Models](https://arxiv.org/abs/2512.14600)
*Yihan Liao,Jacky Keung,Xiaoxue Ma,Jingyu Zhang,Yicheng Sun*

Main category: cs.CR

TL;DR: PerProb是一个统一的、无标签的框架，通过比较受害模型和攻击模型生成数据的困惑度和平均对数概率变化，间接评估LLM的记忆漏洞和隐私风险。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练数据可能包含敏感信息，引发隐私担忧。成员推理攻击(MIA)威胁严重，但现有评估方法不一致且缺乏标准化，加上许多LLM训练集未公开，使得MIA对LLM的真实影响不明确。

Method: 提出PerProb框架：通过比较受害模型和攻击模型生成数据的困惑度和平均对数概率变化，间接评估训练引起的记忆。该方法不依赖成员/非成员标签或内部访问，适用于黑盒和白盒设置。系统地将MIA分为四种攻击模式，并在五个数据集上评估有效性。

Result: 评估显示不同LLM具有不同的记忆行为和隐私风险。同时评估了知识蒸馏、早停和差分隐私等缓解策略，证明这些方法能有效减少数据泄漏。

Conclusion: PerProb提供了一个实用且可推广的框架，用于评估和改进LLM的隐私保护，为理解LLM记忆漏洞和隐私风险提供了系统方法。

Abstract: The rapid advancement of Large Language Models (LLMs) has been driven by extensive datasets that may contain sensitive information, raising serious privacy concerns. One notable threat is the Membership Inference Attack (MIA), where adversaries infer whether a specific sample was used in model training. However, the true impact of MIA on LLMs remains unclear due to inconsistent findings and the lack of standardized evaluation methods, further complicated by the undisclosed nature of many LLM training sets. To address these limitations, we propose PerProb, a unified, label-free framework for indirectly assessing LLM memorization vulnerabilities. PerProb evaluates changes in perplexity and average log probability between data generated by victim and adversary models, enabling an indirect estimation of training-induced memory. Compared with prior MIA methods that rely on member/non-member labels or internal access, PerProb is independent of model and task, and applicable in both black-box and white-box settings. Through a systematic classification of MIA into four attack patterns, we evaluate PerProb's effectiveness across five datasets, revealing varying memory behaviors and privacy risks among LLMs. Additionally, we assess mitigation strategies, including knowledge distillation, early stopping, and differential privacy, demonstrating their effectiveness in reducing data leakage. Our findings offer a practical and generalizable framework for evaluating and improving LLM privacy.

</details>
