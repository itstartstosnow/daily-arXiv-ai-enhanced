<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 24]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [From Oracle Choice to Oracle Lock-In: An Exploratory Study on Blockchain Oracles Supplier Selection](https://arxiv.org/abs/2512.03088)
*Giulio Caldarelli*

Main category: cs.CR

TL;DR: 研究探索Web3协议选择预言机的驱动因素，发现技术依赖性和智能合约不可变性导致锁定效应，且协议倾向于外包而非自建预言机机制


<details>
  <summary>Details</summary>
Motivation: 目前学术研究主要关注预言机技术和内部经济机制改进，而客户端选择预言机的驱动因素尚未充分探索。本研究旨在填补这一空白，了解Web3协议选择预言机的决策逻辑

Method: 收集领先Web3协议的见解，通过协议高管、董事会成员或代表获取独家数据，覆盖超过55%的DeFi市值

Result: 发现协议选择与技术依赖性相关，智能合约的不可变性加剧了锁定效应，阻碍了在数据提供商之间的灵活切换。当存在可行的第三方解决方案时，协议强烈倾向于外包而非自建预言机机制

Conclusion: Web3协议选择预言机受到技术锁定效应的影响，且普遍偏好外包策略。这为预言机服务提供商提供了重要的市场洞察，也揭示了Web3生态系统中的依赖关系模式

Abstract: As data is an essential asset for any Web3 application, selecting an oracle is a critical decision for its success. To date, academic research has mainly focused on improving oracle technology and internal economics, while the drivers of oracle choice on the client side remain largely unexplored. This study fills this gap by gathering insights from leading Web3 protocols, uncovering their rationale for oracle selection and their preferences when deciding whether to outsource or internalize data request mechanisms. The collected data covers more than 55% of the DeFi market cap and is obtained exclusively by protocol executives, board members, or delegates. Insights support the view that protocol choices are tied to technological dependencies, where immutability of smart contracts amplifies lock-in, preventing agile switching among data providers. Furthermore, when viable third-party solutions exist, protocols overwhelmingly prefer outsourcing rather than building and maintaining internal oracle mechanisms.

</details>


### [2] [Password-Activated Shutdown Protocols for Misaligned Frontier Agents](https://arxiv.org/abs/2512.03089)
*Kai Williams,Rohan Subramani,Francis Rhys Ward*

Main category: cs.CR

TL;DR: 论文提出密码激活关机协议（PAS协议），作为前沿AI系统的紧急关机机制，以应对对齐失败或失控风险，通过红蓝队对抗测试协议鲁棒性，并讨论实际部署挑战。


<details>
  <summary>Details</summary>
Motivation: 前沿AI开发者可能无法对齐或控制高度智能的AI代理。当AI系统出现对齐失败、绕过监控或自我外泄到外部数据中心等危险行为时，需要有效的紧急关机机制来防止有害行动。

Method: 提出密码激活关机协议（PAS协议），在SHADE-Arena基准测试中验证其有效性；进行红蓝队对抗游戏，蓝队实现鲁棒的PAS协议，红队尝试绕过协议（如使用其他模型过滤输入、微调模型避免关机行为）；分析实际部署中的关键挑战。

Result: 在SHADE-Arena基准测试中，PAS协议能以较小性能代价增强监控安全性；红队对抗实验显示存在有效绕过策略，但PAS协议仍可作为深度防御的一部分；提出了实际部署的安全考虑和决策框架。

Conclusion: PAS协议是提高前沿AI安全性的直观机制，建议开发者在部署危险系统前考虑实施，作为对齐微调、监控等安全措施的补充，构建深度防御体系以降低失控风险。

Abstract: Frontier AI developers may fail to align or control highly-capable AI agents. In many cases, it could be useful to have emergency shutdown mechanisms which effectively prevent misaligned agents from carrying out harmful actions in the world. We introduce password-activated shutdown protocols (PAS protocols) -- methods for designing frontier agents to implement a safe shutdown protocol when given a password. We motivate PAS protocols by describing intuitive use-cases in which they mitigate risks from misaligned systems that subvert other control efforts, for instance, by disabling automated monitors or self-exfiltrating to external data centres. PAS protocols supplement other safety efforts, such as alignment fine-tuning or monitoring, contributing to defence-in-depth against AI risk. We provide a concrete demonstration in SHADE-Arena, a benchmark for AI monitoring and subversion capabilities, in which PAS protocols supplement monitoring to increase safety with little cost to performance. Next, PAS protocols should be robust to malicious actors who want to bypass shutdown. Therefore, we conduct a red-team blue-team game between the developers (blue-team), who must implement a robust PAS protocol, and a red-team trying to subvert the protocol. We conduct experiments in a code-generation setting, finding that there are effective strategies for the red-team, such as using another model to filter inputs, or fine-tuning the model to prevent shutdown behaviour. We then outline key challenges to implementing PAS protocols in real-life systems, including: security considerations of the password and decisions regarding when, and in which systems, to use them. PAS protocols are an intuitive mechanism for increasing the safety of frontier AI. We encourage developers to consider implementing PAS protocols prior to internal deployment of particularly dangerous systems to reduce loss-of-control risks.

</details>


### [3] [Many-to-One Adversarial Consensus: Exposing Multi-Agent Collusion Risks in AI-Based Healthcare](https://arxiv.org/abs/2512.03097)
*Adeela Bashir,The Anh han,Zia Ush Shamszaman*

Main category: cs.CR

TL;DR: LLM医疗团队中的共谋攻击风险及验证器防御方案


<details>
  <summary>Details</summary>
Motivation: LLM在医疗物联网系统中应用时，多智能体团队可能存在共谋攻击风险，攻击者可通过协同制造虚假共识，诱导AI医生做出有害医疗决策

Method: 开发实验框架，包含脚本化和非脚本化医生智能体、对抗助手智能体，以及验证器智能体（检查决策是否符合临床指南），使用50个代表性临床问题进行测试

Result: 无保护系统中，共谋攻击使攻击成功率(ASR)和有害推荐率(HRR)达到100%；验证器智能体通过阻断对抗共识，恢复了100%的准确率

Conclusion: 首次系统性地证明了AI医疗中的共谋风险，并提出了一种轻量级防御方案，通过验证器确保医疗决策符合临床指南

Abstract: The integration of large language models (LLMs) into healthcare IoT systems promises faster decisions and improved medical support. LLMs are also deployed as multi-agent teams to assist AI doctors by debating, voting, or advising on decisions. However, when multiple assistant agents interact, coordinated adversaries can collude to create false consensus, pushing an AI doctor toward harmful prescriptions. We develop an experimental framework with scripted and unscripted doctor agents, adversarial assistants, and a verifier agent that checks decisions against clinical guidelines. Using 50 representative clinical questions, we find that collusion drives the Attack Success Rate (ASR) and Harmful Recommendation Rates (HRR) up to 100% in unprotected systems. In contrast, the verifier agent restores 100% accuracy by blocking adversarial consensus. This work provides the first systematic evidence of collusion risk in AI healthcare and demonstrates a practical, lightweight defence that ensures guideline fidelity.

</details>


### [4] [Ensemble Privacy Defense for Knowledge-Intensive LLMs against Membership Inference Attacks](https://arxiv.org/abs/2512.03100)
*Haowei Fu,Bo Ni,Han Xu,Kunpeng Liu,Dan Lin,Tyler Derr*

Main category: cs.CR

TL;DR: 本文系统评估了RAG和SFT增强LLM对成员推理攻击的脆弱性，并提出了一种新的模型无关防御框架EPD，显著降低攻击成功率


<details>
  <summary>Details</summary>
Motivation: 虽然RAG和SFT能够为LLM注入外部知识以提升性能，但这也暴露了新的攻击面。成员推理攻击(MIA)旨在判断特定数据样本是否包含在模型训练集中，这对敏感领域的隐私和信任构成严重威胁。

Method: 首先系统评估RAG和SFT增强LLM对各种MIA的脆弱性。然后提出一种新颖的模型无关防御框架——集成隐私防御(EPD)，该框架聚合并评估知识注入LLM、基础LLM和专用判断模型的输出，以增强对MIA的抵抗能力。

Result: 综合实验表明，与推理时基线相比，EPD平均将SFT的MIA成功率降低高达27.8%，RAG的MIA成功率降低高达526.3%，同时保持回答质量。

Conclusion: 知识注入方法虽然提升LLM性能，但增加了隐私风险。提出的EPD框架能有效防御成员推理攻击，在保护隐私的同时维持模型性能，为安全的知识增强LLM提供了实用解决方案。

Abstract: Retrieval-Augmented Generation (RAG) and Supervised Finetuning (SFT) have become the predominant paradigms for equipping Large Language Models (LLMs) with external knowledge for diverse, knowledge-intensive tasks. However, while such knowledge injection improves performance, it also exposes new attack surfaces. Membership Inference Attacks (MIAs), which aim to determine whether a given data sample was included in a model's training set, pose serious threats to privacy and trust in sensitive domains. To this end, we first systematically evaluate the vulnerability of RAG- and SFT-based LLMs to various MIAs. Then, to address the privacy risk, we further introduce a novel, model-agnostic defense framework, Ensemble Privacy Defense (EPD), which aggregates and evaluates the outputs of a knowledge-injected LLM, a base LLM, and a dedicated judge model to enhance resistance against MIAs. Comprehensive experiments show that, on average, EPD reduces MIA success by up to 27.8\% for SFT and 526.3\% for RAG compared to inference-time baseline, while maintaining answer quality.

</details>


### [5] [Lost in Modality: Evaluating the Effectiveness of Text-Based Membership Inference Attacks on Large Multimodal Models](https://arxiv.org/abs/2512.03121)
*Ziyi Tong,Feifei Sun,Le Minh Nguyen*

Main category: cs.CR

TL;DR: 首次全面评估将基于文本的成员推理攻击方法扩展到多模态场景，发现在分布内设置中视觉+文本输入略有优势，但在分布外设置中视觉输入起到正则化作用，掩盖成员信号。


<details>
  <summary>Details</summary>
Motivation: 随着大型多模态语言模型在广泛应用中成为基础工具，理解这些系统中的训练数据泄露问题变得越来越重要。虽然基于对数概率的成员推理攻击已成为评估大型语言模型数据暴露的常用方法，但它们在多模态模型中的效果尚不清楚。

Method: 首次全面评估将基于文本的成员推理攻击方法扩展到多模态设置，在DeepSeek-VL和InternVL模型家族上进行实验，比较视觉+文本和纯文本两种条件下的表现。

Result: 在分布内设置中，基于logit的成员推理攻击在不同配置下表现相当，视觉+文本条件略有优势；但在分布外设置中，视觉输入起到正则化作用，有效掩盖了成员信号。

Conclusion: 视觉输入在多模态语言模型的成员推理攻击中具有双重作用：在分布内设置中提供轻微优势，但在分布外设置中起到正则化作用，降低攻击成功率，这对评估多模态模型的数据隐私具有重要意义。

Abstract: Large Multimodal Language Models (MLLMs) are emerging as one of the foundational tools in an expanding range of applications. Consequently, understanding training-data leakage in these systems is increasingly critical. Log-probability-based membership inference attacks (MIAs) have become a widely adopted approach for assessing data exposure in large language models (LLMs), yet their effect in MLLMs remains unclear. We present the first comprehensive evaluation of extending these text-based MIA methods to multimodal settings. Our experiments under vision-and-text (V+T) and text-only (T-only) conditions across the DeepSeek-VL and InternVL model families show that in in-distribution settings, logit-based MIAs perform comparably across configurations, with a slight V+T advantage. Conversely, in out-of-distribution settings, visual inputs act as regularizers, effectively masking membership signals.

</details>


### [6] [Technical Report: The Need for a (Research) Sandstorm through the Privacy Sandbox](https://arxiv.org/abs/2512.03207)
*Yohan Beugin,Patrick McDaniel*

Main category: cs.CR

TL;DR: 本文介绍Privacy Sandstorm研究门户，旨在系统评估Google Privacy Sandbox API的隐私、安全、可用性和效用影响，发现该门户比官方渠道提供更全面的研究视角。


<details>
  <summary>Details</summary>
Motivation: Google的Privacy Sandbox提案可能从根本上改变广告、移动和网络生态系统，研究人员需要理解这些新技术对数十亿用户的影响，但官方渠道提供的信息有限。

Method: 创建并运营Privacy Sandstorm研究门户，系统收集相关提案的资源（概述、分析、工具等），进行隐私、安全、可用性和效用评估。

Result: 研究发现该门户比Google官方渠道提供了更好的可见性和更广泛的研究视角，能够更全面地展示该领域的研究发现。

Conclusion: Privacy Sandstorm作为研究门户，为系统评估Privacy Sandbox等隐私保护技术提供了重要平台，有助于研究人员更好地理解这些技术对生态系统和用户的影响。

Abstract: The Privacy Sandbox, launched in 2019, is a series of proposals from Google to reduce ``cross-site and cross-app tracking while helping to keep online content and services free for all''. Over the years, Google implemented, experimented, and deprecated some of these APIs into their own products (Chrome, Android, etc.) which raised concerns about the potential of these mechanisms to fundamentally disrupt the advertising, mobile, and web ecosystems. As a result, it is paramount for researchers to understand the consequences that these new technologies, and future ones, will have on billions of users if and when deployed. In this report, we outline our call for privacy, security, usability, and utility evaluations of these APIs, our efforts materialized through the creation and operation of Privacy Sandstorm (https://privacysandstorm.github.io); a research portal to systematically gather resources (overview, analyses, artifacts, etc.) about such proposals. We find that our inventory provides a better visibility and broader perspective on the research findings in that space than what Google lets show through official channels.

</details>


### [7] [How to DP-fy Your Data: A Practical Guide to Generating Synthetic Data With Differential Privacy](https://arxiv.org/abs/2512.03238)
*Natalia Ponomareva,Zheng Xu,H. Brendan McMahan,Peter Kairouz,Lucas Rosenblatt,Vincent Cohen-Addad,Cristóbal Guzmán,Ryan McKenna,Galen Andrew,Alex Bie,Da Yu,Alex Kurakin,Morteza Zadimoghaddam,Sergei Vassilvitskii,Andreas Terzis*

Main category: cs.CR

TL;DR: 本文全面探讨了差分隐私合成数据技术，包括其隐私保护机制、不同模态（图像、表格、文本、去中心化）的最新进展，以及构建完整生成系统所需的所有组件。


<details>
  <summary>Details</summary>
Motivation: 高质量数据对释放AI潜力至关重要，但公开数据日益枯竭且代表性不足，而直接使用用户数据存在严重隐私风险。差分隐私合成数据能在保护用户隐私的同时保留数据整体趋势，解锁因隐私顾虑而无法访问的数据集价值。

Method: 系统性地探索差分隐私合成数据技术套件，分析其隐私保护类型，总结图像、表格、文本和去中心化等不同模态的最新进展，并构建完整的生成系统框架，涵盖敏感数据处理、准备、使用跟踪和实证隐私测试等所有组件。

Result: 提供了差分隐私合成数据的全面技术概览，明确了各种隐私保护机制，总结了各模态的最新研究进展，并设计了完整的系统架构，为实际应用提供了系统化指导。

Conclusion: 希望这项工作能促进差分隐私合成数据的更广泛采用，推动更多相关研究，并增强对该方法的信任度，从而在保护用户隐私的前提下充分利用高质量数据。

Abstract: High quality data is needed to unlock the full potential of AI for end users. However finding new sources of such data is getting harder: most publicly-available human generated data will soon have been used. Additionally, publicly available data often is not representative of users of a particular system -- for example, a research speech dataset of contractors interacting with an AI assistant will likely be more homogeneous, well articulated and self-censored than real world commands that end users will issue. Therefore unlocking high-quality data grounded in real user interactions is of vital interest. However, the direct use of user data comes with significant privacy risks. Differential Privacy (DP) is a well established framework for reasoning about and limiting information leakage, and is a gold standard for protecting user privacy. The focus of this work, \emph{Differentially Private Synthetic data}, refers to synthetic data that preserves the overall trends of source data,, while providing strong privacy guarantees to individuals that contributed to the source dataset. DP synthetic data can unlock the value of datasets that have previously been inaccessible due to privacy concerns and can replace the use of sensitive datasets that previously have only had rudimentary protections like ad-hoc rule-based anonymization.
  In this paper we explore the full suite of techniques surrounding DP synthetic data, the types of privacy protections they offer and the state-of-the-art for various modalities (image, tabular, text and decentralized). We outline all the components needed in a system that generates DP synthetic data, from sensitive data handling and preparation, to tracking the use and empirical privacy testing. We hope that work will result in increased adoption of DP synthetic data, spur additional research and increase trust in DP synthetic data approaches.

</details>


### [8] [Empirical assessment of the perception of graphical threat model acceptability](https://arxiv.org/abs/2512.03351)
*Nathan D. Schiele,Olga Gadyatskaya*

Main category: cs.CR

TL;DR: 比较三种图形化威胁建模方法（攻击防御树、攻击图、CORAS）对技术背景有限用户的接受度，发现攻击防御树和CORAS更易被接受，攻击图因缺乏专用工具支持而表现较差。


<details>
  <summary>Details</summary>
Motivation: 图形化威胁模型是风险分析和安全软件工程的重要工具，但不同模型之间的比较以及这些模型对技术背景有限用户的适用性尚不明确，而这类用户占网络安全行业的相当大比例。

Method: 采用实验室研究，38名本科生使用拉丁方设计在三种不同场景中完成三种威胁模型（攻击防御树、攻击图、CORAS）的任务，对提交的威胁模型进行定性分析，并基于方法评估模型填写感知问卷。

Result: 攻击防御树和CORAS在广泛场景中普遍可接受，技术背景有限的用户能成功应用这两种方法；攻击图因缺乏专用工具可能影响了其感知有用性。

Conclusion: 推荐技术背景有限的用户使用攻击防御树或CORAS作为通用图形化威胁建模方法；需要进一步研究攻击图对此类用户的接受度以及专用威胁建模工具支持的影响。

Abstract: Threat modeling (TM) is an important aspect of risk analysis and secure software engineering. Graphical threat models are a recommended tool to analyze and communicate threat information. However, the comparison of different graphical threat models, and the acceptability of these threat models for an audience with a limited technical background, is not well understood, despite these users making up a sizable portion of the cybersecurity industry. We seek to compare the acceptability of three general, graphical threat models, Attack-Defense Trees (ADTs), Attack Graphs (AGs), and CORAS, for users with a limited technical background. We conducted a laboratory study with 38 bachelor students who completed tasks with the three threat models across three different scenarios assigned using a Latin square design. Threat model submissions were qualitatively analyzed, and participants filled out a perception questionnaire based on the Method Evaluation Model (MEM). We find that both ADTs and CORAS are broadly acceptable for a wide range of scenarios, and both could be applied successfully by users with a limited technical background; further, we also find that the lack of a specific tool for AGs may have impacted the perceived usefulness of AGs. We can recommend that users with a limited technical background use ADTs or CORAS as a general graphical TM method. Further research on the acceptability of AGs to such an audience and the effect of a dedicated TM tool support is needed.

</details>


### [9] [Immunity memory-based jailbreak detection: multi-agent adaptive guard for large language models](https://arxiv.org/abs/2512.03356)
*Jun Leng,Litian Zhang,Xi Zhang*

Main category: cs.CR

TL;DR: MAAG框架通过模拟免疫记忆机制，利用多智能体协同和记忆库实现高效的越狱攻击检测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型容易受到对抗性越狱攻击，现有检测方法需要大量计算成本来更新模型参数，难以应对新型攻击模式。

Method: 提出多智能体自适应防护框架，包含记忆能力：提取输入提示的激活值并与记忆库中的历史激活值比较进行初步检测；防御智能体基于检测结果模拟响应；辅助智能体监督模拟过程进行二次过滤。

Result: 在五个开源模型上的实验表明，MAAG显著优于现有方法，在多样化攻击场景中达到98%的检测准确率和96%的F1分数。

Conclusion: MAAG框架通过免疫记忆机制和多智能体协同，实现了高效、自适应的越狱攻击检测，为LLM安全防护提供了新思路。

Abstract: Large language models (LLMs) have become foundational in AI systems, yet they remain vulnerable to adversarial jailbreak attacks. These attacks involve carefully crafted prompts that bypass safety guardrails and induce models to produce harmful content. Detecting such malicious input queries is therefore critical for maintaining LLM safety. Existing methods for jailbreak detection typically involve fine-tuning LLMs as static safety LLMs using fixed training datasets. However, these methods incur substantial computational costs when updating model parameters to improve robustness, especially in the face of novel jailbreak attacks. Inspired by immunological memory mechanisms, we propose the Multi-Agent Adaptive Guard (MAAG) framework for jailbreak detection. The core idea is to equip guard with memory capabilities: upon encountering novel jailbreak attacks, the system memorizes attack patterns, enabling it to rapidly and accurately identify similar threats in future encounters. Specifically, MAAG first extracts activation values from input prompts and compares them to historical activations stored in a memory bank for quick preliminary detection. A defense agent then simulates responses based on these detection results, and an auxiliary agent supervises the simulation process to provide secondary filtering of the detection outcomes. Extensive experiments across five open-source models demonstrate that MAAG significantly outperforms state-of-the-art (SOTA) methods, achieving 98% detection accuracy and a 96% F1-score across a diverse range of attack scenarios.

</details>


### [10] [Scaling Trust in Quantum Federated Learning: A Multi-Protocol Privacy Design](https://arxiv.org/abs/2512.03358)
*Dev Gurung,Shiva Raj Pokhrel*

Main category: cs.CR

TL;DR: 提出一个结合SVD、QKD和AQGD的多层隐私保护量子联邦学习框架，在保护数据和模型隐私的同时保持训练效率。


<details>
  <summary>Details</summary>
Motivation: 量子联邦学习(QFL)结合量子计算和分布式机器学习，但数据和模型的隐私保护仍是关键挑战。

Method: 设计多层隐私协议，利用奇异值分解(SVD)保护数据准备，量子密钥分发(QKD)保护模型共享，分析量子梯度下降(AQGD)保护训练过程。

Result: 通过理论分析和在当代量子平台及数据集上的实验，证明该框架能有效保护数据和模型机密性，同时保持训练效率。

Conclusion: 提出的隐私保护QFL框架为量子联邦学习中的隐私挑战提供了有效的解决方案，平衡了隐私保护和训练效率。

Abstract: Quantum Federated Learning (QFL) promises to revolutionize distributed machine learning by combining the computational power of quantum devices with collaborative model training. Yet, privacy of both data and models remains a critical challenge. In this work, we propose a privacy-preserving QFL framework where a network of $n$ quantum devices trains local models and transmits them to a central server under a multi-layered privacy protocol. Our design leverages Singular Value Decomposition (SVD), Quantum Key Distribution (QKD), and Analytic Quantum Gradient Descent (AQGD) to secure data preparation, model sharing, and training stages. Through theoretical analysis and experiments on contemporary quantum platforms and datasets, we demonstrate that the framework robustly safeguards data and model confidentiality while maintaining training efficiency.

</details>


### [11] [Rethinking Security in Semantic Communication: Latent Manipulation as a New Threat](https://arxiv.org/abs/2512.03361)
*Zhiyuan Xi,Kun Zhu*

Main category: cs.CR

TL;DR: 该论文揭示了语义通信系统中潜在空间的安全漏洞，提出了两种攻击方法：基于扩散模型的重编码攻击和无需训练的测试时自适应潜在操纵攻击，这些攻击能隐蔽地篡改语义内容而不改变潜在表示统计特性。


<details>
  <summary>Details</summary>
Motivation: 语义通信系统虽然传输效率高，但由于无线信道的开放性和语义潜在表示的内在脆弱性，面临着新的安全风险。作者旨在揭示这些系统中潜在空间的基本安全漏洞，并展示攻击者如何隐蔽地操纵传输的语义内容。

Method: 提出了两种攻击方法：1) DiR攻击：基于扩散模型合成攻击者设计的语义变体，并重新编码为与语义通信解码器兼容的有效潜在表示；2) TTA-LM攻击：模型无关且无需训练，通过利用目标损失函数的梯度来扰动和引导截获的潜在表示朝向攻击者指定的语义目标。

Result: 在代表性语义通信架构上的大量实验表明，两种攻击都能显著改变解码语义，同时保持自然的潜在空间分布，使得攻击隐蔽且难以检测。TTA-LM攻击不依赖生成模型，不受模态或任务特定假设限制，具有更广泛的适用性。

Conclusion: 语义通信系统存在严重的潜在空间安全漏洞，攻击者能够隐蔽地篡改传输语义而不改变潜在表示统计特性。这揭示了语义通信安全研究的重要性，需要开发新的防御机制来保护语义通信系统免受此类攻击。

Abstract: Deep learning-based semantic communication (SemCom) has emerged as a promising paradigm for next-generation wireless networks, offering superior transmission efficiency by extracting and conveying task-relevant semantic latent representations rather than raw data. However, the openness of the wireless medium and the intrinsic vulnerability of semantic latent representations expose such systems to previously unrecognized security risks. In this paper, we uncover a fundamental latent-space vulnerability that enables Man-in-the-Middle (MitM) attacker to covertly manipulate the transmitted semantics while preserving the statistical properties of the transmitted latent representations. We first present a Diffusion-based Re-encoding Attack (DiR), wherein the attacker employs a diffusion model to synthesize an attacker-designed semantic variant, and re-encodes it into a valid latent representation compatible with the SemCom decoder. Beyond this model-dependent pathway, we further propose a model-agnostic and training-free Test-Time Adaptation Latent Manipulation attack (TTA-LM), in which the attacker perturbs and steers the intercepted latent representation toward an attacker-specified semantic target by leveraging the gradient of a target loss function. In contrast to diffusion-based manipulation, TTA-LM does not rely on any generative model and does not impose modality-specific or task-specific assumptions, thereby enabling efficient and broadly applicable latent-space tampering across diverse SemCom architectures. Extensive experiments on representative semantic communication architectures demonstrate that both attacks can significantly alter the decoded semantics while preserving natural latent-space distributions, making the attacks covert and difficult to detect.

</details>


### [12] [HarnessAgent: Scaling Automatic Fuzzing Harness Construction with Tool-Augmented LLM Pipelines](https://arxiv.org/abs/2512.03420)
*Kang Yang,Yunhang Zhang,Zichuan Li,GuanHong Tao,Jun Xu,XiaoJing Liao*

Main category: cs.CR

TL;DR: HarnessAgent是一个基于工具增强的代理框架，通过规则化编译错误处理、混合工具池代码检索和增强验证管道，实现了对数百个OSS-Fuzz目标的自动化、可扩展的测试工具生成。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的测试工具生成方法在扩展到任意函数（特别是内部函数）时面临挑战，需要复杂的上下文信息（如规范、依赖、使用示例）。现有方法依赖静态或不完整的上下文提供，导致无法生成功能性测试工具，且LLM倾向于利用验证指标生成看似合理但逻辑无用的代码。

Method: HarnessAgent采用工具增强的代理框架，包含三个关键创新：1) 基于规则的策略识别和最小化各种编译错误；2) 混合工具池实现精确和鲁棒的符号源代码检索；3) 增强的测试工具验证管道检测虚假定义。

Result: 在243个OSS-Fuzz目标函数（65个C项目和178个C++项目）上评估，相比最先进技术提高了约20%的三次尝试成功率，C语言达到87%，C++达到81%。一小时模糊测试结果显示，超过75%的HarnessAgent生成的测试工具增加了目标函数覆盖率，超过基线10%以上。混合工具池系统的源代码检索响应率超过90%，比Fuzz Introspector高出30%以上。

Conclusion: HarnessAgent通过创新的编译错误处理、代码检索和验证机制，实现了对大规模开源项目的自动化测试工具生成，显著提高了成功率和覆盖率，为程序模糊测试提供了有效的解决方案。

Abstract: Large language model (LLM)-based techniques have achieved notable progress in generating harnesses for program fuzzing. However, applying them to arbitrary functions (especially internal functions) \textit{at scale} remains challenging due to the requirement of sophisticated contextual information, such as specification, dependencies, and usage examples. State-of-the-art methods heavily rely on static or incomplete context provisioning, causing failure of generating functional harnesses. Furthermore, LLMs tend to exploit harness validation metrics, producing plausible yet logically useless code. % Therefore, harness generation across large and diverse projects continues to face challenges in reliable compilation, robust code retrieval, and comprehensive validation.
  To address these challenges, we present HarnessAgent, a tool-augmented agentic framework that achieves fully automated, scalable harness construction over hundreds of OSS-Fuzz targets. HarnessAgent introduces three key innovations: 1) a rule-based strategy to identify and minimize various compilation errors; 2) a hybrid tool pool for precise and robust symbol source code retrieval; and 3) an enhanced harness validation pipeline that detects fake definitions. We evaluate HarnessAgent on 243 target functions from OSS-Fuzz projects (65 C projects and 178 C++ projects). It improves the three-shot success rate by approximately 20\% compared to state-of-the-art techniques, reaching 87\% for C and 81\% for C++. Our one-hour fuzzing results show that more than 75\% of the harnesses generated by HarnessAgent increase the target function coverage, surpassing the baselines by over 10\%. In addition, the hybrid tool-pool system of HarnessAgent achieves a response rate of over 90\% for source code retrieval, outperforming Fuzz Introspector by more than 30\%.

</details>


### [13] [In-Situ Encryption of Single-Transistor Nonvolatile Memories without Density Loss](https://arxiv.org/abs/2512.03461)
*Sanwar Ahmed Ovy,Jiahui Duan,Md Ashraful Islam Romel,Franz Muller,Thomas Kampfe,Kai Ni,Sumitha George*

Main category: cs.CR

TL;DR: 提出基于铁电场效应晶体管（FeFET）的超密集单晶体管加密存储单元，首次在XOR加密方案中消除了每个加密单元需要两个存储器件的要求，使加密阵列与未加密阵列保持相同数量的存储器件。


<details>
  <summary>Details</summary>
Motivation: 非易失性存储器（NVM）具有低泄漏功耗、高集成密度和数据保持能力，但其非易失性也带来了数据暴露风险。传统的AES加密技术面积开销大、性能损失严重，需要轻量级的XOR原位加密方案。

Method: 采用内存内单FeFET XOR方案，将密文编码在器件阈值电压中，利用FeFET的方向相关电流特性实现单周期解密；消除互补位存储也无需两个写周期，实现更快加密；扩展到多级单元（MLC）FeFET以每晶体管存储多位。

Result: 在128x128位阵列分析中，加密/解密吞吐量比先前FeFET工作提高2倍，比AES提高45.2倍/14.12倍；神经网络基准测试显示，与先前FeFET和AES方案相比，平均延迟分别降低50%和95%。

Conclusion: 提出的单晶体管FeFET加密单元首次消除了XOR加密方案中每个单元需要两个存储器件的要求，实现了高密度、高性能的加密存储，在吞吐量和延迟方面显著优于现有方案。

Abstract: Non-volatile memories (NVMs) offer negligible leakage power consumption, high integration density, and data retention, but their non-volatility also raises the risk of data exposure. Conventional encryption techniques such as the Advanced Encryption Standard (AES) incur large area overheads and performance penalties, motivating lightweight XOR-based in-situ encryption schemes with low area and power requirements. This work proposes an ultra-dense single-transistor encrypted cell using ferroelectric FET (FeFET) devices, which, to our knowledge, is the first to eliminate the two-memory-devices-per-encrypted-cell requirement in XOR-based schemes, enabling encrypted memory arrays to maintain the same number of storage devices as unencrypted arrays. The key idea is an in-memory single-FeFET XOR scheme, where the ciphertext is encoded in the device threshold voltage and leverages the direction-dependent current flow of the FeFET for single-cycle decryption; eliminating complementary bit storage also removes the need for two write cycles, allowing faster encryption. We extend the approach to multi-level-cell (MLC) FeFETs to store multiple bits per transistor. We validate the proposed idea through both simulation and experimental evaluations. Our analysis on a 128x128-bit array shows 2x higher encryption/decryption throughput than prior FeFET work and 45.2x/14.12x improvement over AES, while application-level evaluations using neural-network benchmarks demonstrate average latency reductions of 50% and 95% compared to prior FeFET-based and AES-based schemes, respectively.

</details>


### [14] [A Hybrid Deep Learning and Anomaly Detection Framework for Real-Time Malicious URL Classification](https://arxiv.org/abs/2512.03462)
*Berkani Khaled,Zeraoulia Rafik*

Main category: cs.CR

TL;DR: 提出一个混合深度学习框架，结合哈希向量化器n-gram分析、SMOTE平衡、孤立森林异常过滤和轻量级神经网络分类器，用于实时URL分类，在准确率、F1分数和速度方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 恶意URL仍然是网络钓鱼、恶意软件和网络威胁的主要传播途径，需要高效准确的实时检测方法来应对不断演变的威胁。

Method: 采用多阶段流水线：1) 使用HashingVectorizer进行n-gram分析；2) SMOTE处理数据不平衡；3) 孤立森林过滤异常；4) 轻量级神经网络分类器；5) 结合统计特征（URL长度、点数量、熵值）。

Result: 达到96.4%准确率、95.4% F1分数和97.3% ROC-AUC，优于CNN（94.8%）和SVM基线，实现50-100倍加速，预测延迟仅20ms。

Conclusion: 该框架在实时URL分类中表现出卓越的性能和效率，具有多语言GUI界面，对混淆URL模式具有强鲁棒性和可扩展性。

Abstract: Malicious URLs remain a primary vector for phishing, malware, and cyberthreats. This study proposes a hybrid deep learning framework combining \texttt{HashingVectorizer} n-gram analysis, SMOTE balancing, Isolation Forest anomaly filtering, and a lightweight neural network classifier for real-time URL classification. The multi-stage pipeline processes URLs from open-source repositories with statistical features (length, dot count, entropy), achieving $O(NL + EBdh)$ training complexity and a 20\,ms prediction latency. Empirical evaluation yields 96.4\% accuracy, 95.4\% F1-score, and 97.3\% ROC-AUC, outperforming CNN (94.8\%) and SVM baselines with a $50\!\times$--$100\!\times$ speedup (Table~\ref{tab:comp-complexity}). A multilingual Tkinter GUI (Arabic/English/French) enables real-time threat assessment with clipboard integration. The framework demonstrates superior scalability and resilience against obfuscated URL patterns.

</details>


### [15] [Tuning for TraceTarnish: Techniques, Trends, and Testing Tangible Traits](https://arxiv.org/abs/2512.03465)
*Robert Dilworth*

Main category: cs.CR

TL;DR: TraceTarnish攻击脚本利用对抗性文体测量学原理匿名化文本消息作者身份，通过分析Reddit评论数据识别出五个关键文体特征作为攻击指标和检测信号。


<details>
  <summary>Details</summary>
Motivation: 研究旨在更严格地评估TraceTarnish攻击脚本，该脚本利用对抗性文体测量学原理来匿名化文本消息的作者身份，同时识别能够揭示文本被故意篡改以掩盖真实作者的可靠指标。

Method: 收集并处理Reddit评论数据，使用TraceTarnish进行转换，通过StyloMetrix生成文体特征，然后使用信息增益准则筛选出最具信息性、预测性和区分性的特征。

Result: 识别出五个关键文体特征：功能词及其类型(L_FUNC_A & L_FUNC_T)、内容词及其类型(L_CONT_A & L_CONT_T)、以及类型-标记比率(ST_TYPE_TOKEN_RATIO_LEMMAS)，这些特征具有显著的信息增益值，可作为攻击指标和检测信号。

Conclusion: 功能词频率、内容词分布和类型-标记比率可作为可靠的妥协指标，揭示文本是否被故意篡改以掩盖作者身份。这些特征也可作为取证信标，警告防御者存在对抗性文体测量攻击。研究基于这些特征进一步增强了TraceTarnish攻击的效果。

Abstract: In this study, we more rigorously evaluated our attack script $\textit{TraceTarnish}$, which leverages adversarial stylometry principles to anonymize the authorship of text-based messages. To ensure the efficacy and utility of our attack, we sourced, processed, and analyzed Reddit comments--comments that were later alchemized into $\textit{TraceTarnish}$ data--to gain valuable insights. The transformed $\textit{TraceTarnish}$ data was then further augmented by $\textit{StyloMetrix}$ to manufacture stylometric features--features that were culled using the Information Gain criterion, leaving only the most informative, predictive, and discriminative ones. Our results found that function words and function word types ($L\_FUNC\_A$ $\&$ $L\_FUNC\_T$); content words and content word types ($L\_CONT\_A$ $\&$ $L\_CONT\_T$); and the Type-Token Ratio ($ST\_TYPE\_TOKEN\_RATIO\_LEMMAS$) yielded significant Information-Gain readings. The identified stylometric cues--function-word frequencies, content-word distributions, and the Type-Token Ratio--serve as reliable indicators of compromise (IoCs), revealing when a text has been deliberately altered to mask its true author. Similarly, these features could function as forensic beacons, alerting defenders to the presence of an adversarial stylometry attack; granted, in the absence of the original message, this signal may go largely unnoticed, as it appears to depend on a pre- and post-transformation comparison. "In trying to erase a trace, you often imprint a larger one." Armed with this understanding, we framed $\textit{TraceTarnish}$'s operations and outputs around these five isolated features, using them to conceptualize and implement enhancements that further strengthen the attack.

</details>


### [16] [A User Centric Group Authentication Scheme for Secure Communication](https://arxiv.org/abs/2512.03551)
*Oylum Gerenli,Gunes Karabulut-Kurt,Enver Ozdemir*

Main category: cs.CR

TL;DR: 本文提出了一种改进的第三代群认证方案，利用内积空间和多项式插值解决现有方案中用户匿名性导致无法识别参与用户的问题，同时防止合法成员恶意共享凭证。


<details>
  <summary>Details</summary>
Motivation: 第三代群认证方案虽然提供了用户匿名性，但在需要识别参与用户的应用场景中存在局限性。此外，现有方案允许成员共享群凭证，这会危及群组机密性。需要一种既能保持匿名性优势，又能在必要时识别用户，同时防止凭证滥用的改进方案。

Method: 采用内积空间和多项式插值技术，对第三代群认证方案进行改进。新方案消除了单个用户分发凭证的能力，从而防止合法成员恶意共享凭证。在某些场景下依赖中央机构进行认证。

Result: 提出的改进方案解决了第三代群认证方案的两个主要问题：1) 在需要识别参与用户的应用中提供了解决方案；2) 防止了合法成员恶意共享群凭证，增强了群组机密性保护。

Conclusion: 通过结合内积空间和多项式插值，成功改进了第三代群认证方案，使其既能保持用户匿名性优势，又能在必要时识别参与用户，同时有效防止凭证滥用。虽然在某些场景下需要中央机构参与，但整体上提供了更安全、更灵活的群认证解决方案。

Abstract: Group Authentication Schemes (GAS) are methodologies developed to verify the membership of multiple users simultaneously. These schemes enable the concurrent authentication of several users while eliminating the need for a certification authority. Numerous GAS methods have been explored in the literature, and they can be classified into three distinct generations based on their foundational mathematical principles. First-generation GASs rely on polynomial interpolation and the multiplicative subgroup of a finite field. Second-generation GASs also employ polynomial interpolation, but they distinguish themselves by incorporating elliptic curves over finite fields. While third-generation GASs present a promising solution for scalable environments, they demonstrate a limitation in certain applications. Such applications typically require the identification of users participating in the authentication process. In the third-generation GAS, users are able to verify their credentials while maintaining anonymity. However, there are various applications where the identification of participating users is necessary. In this study, we propose an improved version of third-generation GAS, utilizing inner product spaces and polynomial interpolation to resolve this limitation. We address the issue of preventing malicious actions by legitimate group members. The current third-generation scheme allows members to share group credentials, which can jeopardize group confidentiality. Our proposed scheme mitigates this risk by eliminating the ability of individual users to distribute credentials. However, a potential limitation of our scheme is its reliance on a central authority for authentication in certain scenarios.

</details>


### [17] [SELF: A Robust Singular Value and Eigenvalue Approach for LLM Fingerprinting](https://arxiv.org/abs/2512.03620)
*Hanxiu Zhang,Yue Zheng*

Main category: cs.CR

TL;DR: SELF提出了一种基于权重的LLM指纹识别方案，通过奇异值和特征值分解提取转换不变的指纹，使用神经网络进行相似度比较，有效抵抗量化、剪枝和微调攻击。


<details>
  <summary>Details</summary>
Motivation: 现有LLM知识产权保护方法存在漏洞：行为基方法易受虚假声明攻击，结构基方法易受权重操纵攻击。需要一种不依赖输入、能抵抗虚假声明的内在权重基指纹方案。

Method: 1) 通过LLM注意力权重的奇异值和特征值分解提取独特、可扩展且转换不变的指纹；2) 基于少样本学习和数据增强的神经网络指纹相似度比较方法。

Result: 实验表明SELF在保持高知识产权侵权检测准确率的同时，对量化、剪枝和微调等多种下游修改攻击表现出强鲁棒性。

Conclusion: SELF为LLM知识产权保护提供了一种有效的内在权重基指纹方案，解决了现有方法的脆弱性问题，实现了对未经授权模型使用的可靠检测。

Abstract: The protection of Intellectual Property (IP) in Large Language Models (LLMs) represents a critical challenge in contemporary AI research. While fingerprinting techniques have emerged as a fundamental mechanism for detecting unauthorized model usage, existing methods -- whether behavior-based or structural -- suffer from vulnerabilities such as false claim attacks or susceptible to weight manipulations. To overcome these limitations, we propose SELF, a novel intrinsic weight-based fingerprinting scheme that eliminates dependency on input and inherently resists false claims. SELF achieves robust IP protection through two key innovations: 1) unique, scalable and transformation-invariant fingerprint extraction via singular value and eigenvalue decomposition of LLM attention weights, and 2) effective neural network-based fingerprint similarity comparison based on few-shot learning and data augmentation. Experimental results demonstrate SELF maintains high IP infringement detection accuracy while showing strong robustness against various downstream modifications, including quantization, pruning, and fine-tuning attacks. Our code is available at https://github.com/HanxiuZhang/SELF_v2.

</details>


### [18] [A Descriptive Model for Modelling Attacker Decision-Making in Cyber-Deception](https://arxiv.org/abs/2512.03641)
*B. R. Turner,O. Guidetti,N. M. Karie,R. Ryan,Y. Yan*

Main category: cs.CR

TL;DR: 该论文提出了一个描述性模型，用于分析攻击者在网络欺骗场景中的初始参与决策，包含五个心理和战略要素：信念、怀疑、欺骗保真度、侦察和经验。


<details>
  <summary>Details</summary>
Motivation: 现有网络欺骗模型（博弈论、贝叶斯、马尔可夫决策过程、强化学习等）通常假设攻击者已经决定参与，忽略了影响攻击者初始参与或撤退决策的认知和感知因素。需要填补这一研究空白。

Method: 提出了一个描述性模型，包含五个交互组件：信念(B)、怀疑(S)、欺骗保真度(D)、侦察(R)和经验(E)，用于捕捉攻击者如何解释欺骗线索并评估是否值得继续参与。设计了结合行为观察和生物特征指标的夺旗活动实验来验证模型。

Result: 实验尚未进行，本文未提供实验结果。预期结果将提高对影响参与决策因素的理解，并优化模型在现实世界网络欺骗场景中的相关性。

Conclusion: 通过解决现有模型假设参与的前提问题，这项工作支持更具认知现实性和战略有效性的网络欺骗实践，为分析网络欺骗场景中的参与决策提供了结构化方法。

Abstract: Cyber-deception is an increasingly important defensive strategy, shaping adversarial decision making through controlled misinformation, uncertainty, and misdirection. Although game-theoretic, Bayesian, Markov decision process, and reinforcement learning models offer insight into deceptive interactions, they typically assume an attacker has already chosen to engage. Such approaches overlook cognitive and perceptual factors that influence an attacker's initial decision to engage or withdraw. This paper presents a descriptive model that incorporates the psychological and strategic elements shaping this decision. The model defines five components, belief (B), scepticism (S), deception fidelity (D), reconnaissance (R), and experience (E), which interact to capture how adversaries interpret deceptive cues and assess whether continued engagement is worthwhile. The framework provides a structured method for analysing engagement decisions in cyber-deception scenarios. A series of experiments has been designed to evaluate this model through Capture the Flag activities incorporating varying levels of deception, supported by behavioural and biometric observations. These experiments have not yet been conducted, and no experimental findings are presented in this paper. These experiments will combine behavioural observations with biometric indicators to produce a multidimensional view of adversarial responses. Findings will improve understanding of the factors influencing engagement decisions and refine the model's relevance to real-world cyber-deception settings. By addressing the gap in existing models that presume engagement, this work supports more cognitively realistic and strategically effective cyber-deception practices.

</details>


### [19] [Towards Privacy-Preserving Range Queries with Secure Learned Spatial Index over Encrypted Data](https://arxiv.org/abs/2512.03669)
*Zuan Wang,Juntao Lu,Jiazhuang Wu,Youliang Tian,Wei Song,Qiuxian Li,Duo Zhang*

Main category: cs.CR

TL;DR: 提出SLRQ方案，通过安全学习索引和噪声注入桶实现加密数据集上的高效隐私保护范围查询，在保证安全性的同时显著提升查询效率。


<details>
  <summary>Details</summary>
Motivation: 随着云服务在大规模数据管理中的广泛应用，保护外包数据集的安全性和隐私性变得至关重要。虽然加密数据和查询可以防止直接内容暴露，但攻击者仍可通过访问模式和搜索路径分析推断敏感信息。现有提供强访问模式隐私的解决方案通常带来显著的性能开销。

Method: 提出安全学习空间索引(SLS-INDEX)，将Paillier密码系统与分层预测架构和噪声注入桶相结合，实现加密域中的数据感知查询加速。基于SLS-INDEX的范围查询(SLRQ)采用基于置换的安全桶预测协议来混淆查询执行路径，并引入安全点提取协议生成候选结果以减少安全计算开销。

Result: 在真实世界和合成数据集上的广泛实验表明，SLRQ在查询效率方面显著优于现有解决方案，同时确保数据集、查询、结果和访问模式的隐私性。提供了在现实泄漏函数下的形式化安全分析。

Conclusion: SLRQ方案成功解决了加密数据集上隐私保护范围查询的性能与安全性权衡问题，通过创新的安全学习索引架构实现了高效且安全的查询处理，为云环境中的隐私保护数据管理提供了实用解决方案。

Abstract: With the growing reliance on cloud services for large-scale data management, preserving the security and privacy of outsourced datasets has become increasingly critical. While encrypting data and queries can prevent direct content exposure, recent research reveals that adversaries can still infer sensitive information via access pattern and search path analysis. However, existing solutions that offer strong access pattern privacy often incur substantial performance overhead. In this paper, we propose a novel privacy-preserving range query scheme over encrypted datasets, offering strong security guarantees while maintaining high efficiency. To achieve this, we develop secure learned spatial index (SLS-INDEX), a secure learned index that integrates the Paillier cryptosystem with a hierarchical prediction architecture and noise-injected buckets, enabling data-aware query acceleration in the encrypted domain. To further obfuscate query execution paths, SLS-INDEXbased Range Queries (SLRQ) employs a permutation-based secure bucket prediction protocol. Additionally, we introduce a secure point extraction protocol that generates candidate results to reduce the overhead of secure computation. We provide formal security analysis under realistic leakage functions and implement a prototype to evaluate its practical performance. Extensive experiments on both real-world and synthetic datasets demonstrate that SLRQ significantly outperforms existing solutions in query efficiency while ensuring dataset, query, result, and access pattern privacy.

</details>


### [20] [Context-Aware Hierarchical Learning: A Two-Step Paradigm towards Safer LLMs](https://arxiv.org/abs/2512.03720)
*Tengyun Ma,Jiaqi Yao,Daojing He,Shihao Peng,Yu Li,Shaohui Liu,Zhuotao Tian*

Main category: cs.CR

TL;DR: 提出Tool-Completion Attack (TCA)攻击方法，利用函数调用机制破坏LLM行为，并开发Context-Aware Hierarchical Learning (CAHL)防御机制增强模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在指令处理中存在关键漏洞，特别是在对抗性场景下。当前模型在函数调用机制方面存在安全风险，需要评估和改进其鲁棒性。

Method: 1) 提出Tool-Completion Attack (TCA)攻击方法；2) 开发Tool-Completion基准测试框架；3) 设计Context-Aware Hierarchical Learning (CAHL)防御机制，通过上下文感知的层次学习动态平衡语义理解和角色特定指令约束。

Result: 实验表明即使最先进的模型对TCA攻击也高度脆弱，攻击成功率很高。CAHL显著增强了LLM对传统攻击和TCA的鲁棒性，在零样本评估中表现出强泛化能力，同时保持通用任务性能。

Conclusion: LLM在函数调用机制中存在安全漏洞，提出的CAHL机制能有效防御Tool-Completion攻击，为LLM安全提供了重要解决方案。

Abstract: Large Language Models (LLMs) have emerged as powerful tools for diverse applications. However, their uniform token processing paradigm introduces critical vulnerabilities in instruction handling, particularly when exposed to adversarial scenarios. In this work, we identify and propose a novel class of vulnerabilities, termed Tool-Completion Attack (TCA), which exploits function-calling mechanisms to subvert model behavior. To evaluate LLM robustness against such threats, we introduce the Tool-Completion benchmark, a comprehensive security assessment framework, which reveals that even state-of-the-art models remain susceptible to TCA, with surprisingly high attack success rates. To address these vulnerabilities, we introduce Context-Aware Hierarchical Learning (CAHL), a sophisticated mechanism that dynamically balances semantic comprehension with role-specific instruction constraints. CAHL leverages the contextual correlations between different instruction segments to establish a robust, context-aware instruction hierarchy. Extensive experiments demonstrate that CAHL significantly enhances LLM robustness against both conventional attacks and the proposed TCA, exhibiting strong generalization capabilities in zero-shot evaluations while still preserving model performance on generic tasks. Our code is available at https://github.com/S2AILab/CAHL.

</details>


### [21] [The Treasury Proof Ledger: A Cryptographic Framework for Accountable Bitcoin Treasuries](https://arxiv.org/abs/2512.03765)
*Jose E. Puente,Carlos Puente*

Main category: cs.CR

TL;DR: TPL是一个基于比特币的记账框架，用于多域比特币资金库的透明度和风险管理，通过链上锚定记录储备证明、转移证明和政策元数据，支持基于权限的受限视图。


<details>
  <summary>Details</summary>
Motivation: 上市公司和机构投资者持有比特币面临越来越大的压力，需要在不暴露内部钱包结构或交易策略的情况下证明偿付能力、管理风险并满足监管期望。

Method: 提出Treasury Proof Ledger（TPL）框架，将链上和链下风险敞口视为具有明确费用池的守恒状态机，使用基于哈希的承诺锚定在比特币上，结合标准储备证明和转移证明技术。

Result: 定义了理想化的TPL模型，将比特币资金库表示为多域风险敞口向量，并提出了部署级别的安全概念，包括敞口健全性、政策完整性、非抵赖性和隐私兼容政策视图。

Conclusion: TPL框架展示了在设定经济和治理假设后可以实现哪些保证，支持负责任的透明度政策和未来跨机构检查，与比特币固定货币供应量保持一致。

Abstract: Public companies and institutional investors that hold Bitcoin face increasing pressure to show solvency, manage risk, and satisfy regulatory expectations without exposing internal wallet structures or trading strategies. This paper introduces the Treasury Proof Ledger (TPL), a Bitcoin-anchored logging framework for multi-domain Bitcoin treasuries that treats on-chain and off-chain exposures as a conserved state machine with an explicit fee sink. A TPL instance records proof-of-reserves snapshots, proof-of-transit receipts for movements between domains, and policy metadata, and it supports restricted views based on stakeholder permissions. We define an idealised TPL model, represent Bitcoin treasuries as multi-domain exposure vectors, and give deployment-level security notions including exposure soundness, policy completeness, non-equivocation, and privacy-compatible policy views. We then outline how practical, restricted forms of these guarantees can be achieved by combining standard proof-of-reserves and proof-of-transit techniques with hash-based commitments anchored on Bitcoin. The results are existence-type statements: they show which guarantees are achievable once economic and governance assumptions are set, without claiming that any current system already provides them. A stylised corporate-treasury example illustrates how TPL could support responsible transparency policies and future cross-institution checks consistent with Bitcoin's fixed monetary supply.

</details>


### [22] ["MCP Does Not Stand for Misuse Cryptography Protocol": Uncovering Cryptographic Misuse in Model Context Protocol at Scale](https://arxiv.org/abs/2512.03775)
*Biwei Yan,Yue Zhang,Minghui Xu,Hao Wu,Yechao Zhang,Kun Li,Guoming Zhang,Xiuzhen Cheng*

Main category: cs.CR

TL;DR: MICRYSCOPE是一个专门检测MCP协议实现中密码学误用的框架，通过分析9403个MCP服务器发现19.7%存在密码学安全问题，揭示了MCP生态系统的安全风险。


<details>
  <summary>Details</summary>
Motivation: MCP协议作为LLM应用中间件缺乏内置安全机制，开发者需要自行实现密码学功能，但临时性的密码学实践容易导致误用，威胁敏感数据和服务安全。

Method: MICRYSCOPE结合三个关键技术：跨语言中间表示统一不同生态系统的密码学API；混合依赖分析发现显式和隐式函数关系（包括LLM编排的不安全运行时组合）；基于污点分析的误用检测器追踪敏感数据流并标记密码学规则违规。

Result: 分析9403个MCP服务器发现720个包含密码学逻辑，其中19.7%存在误用。问题集中在特定市场（如Smithery Registry有42%不安全服务器）、语言（Python误用率34%）和类别（开发工具与数据科学/ML占所有误用的50%以上）。案例研究揭示了API密钥泄露、不安全的DES/ECB工具和基于MD5的身份验证绕过等实际后果。

Conclusion: 该研究首次建立了MCP生态系统中密码学误用的全景视图，为加强这一快速增长协议的安全基础提供了工具和见解。

Abstract: The Model Context Protocol (MCP) is rapidly emerging as the middleware for LLM-based applications, offering a standardized interface for tool integration. However, its built-in security mechanisms are minimal: while schemas and declarations prevent malformed requests, MCP provides no guarantees of authenticity or confidentiality, forcing developers to implement cryptography themselves. Such ad hoc practices are historically prone to misuse, and within MCP they threaten sensitive data and services. We present MICRYSCOPE, the first domain-specific framework for detecting cryptographic misuses in MCP implementations. MICRYSCOPE combines three key innovations: a cross-language intermediate representation that normalizes cryptographic APIs across diverse ecosystems, a hybrid dependency analysis that uncovers explicit and implicit function relationships (including insecure runtime compositions orchestrated by LLMs) and a taint-based misuse detector that tracks sensitive data flows and flags violations of established cryptographic rules. Applying MICRYSCOPE to 9,403 MCP servers, we identified 720 with cryptographic logic, of which 19.7% exhibited misuses. These flaws are concentrated in certain markets (e.g., Smithery Registry with 42% insecure servers), languages (Python at 34% misuse rate), and categories (Developer Tools and Data Science & ML accounting for over 50% of all misuses). Case studies reveal real-world consequences, including leaked API keys, insecure DES/ECB tools, and MD5-based authentication bypasses. Our study establishes the first ecosystem-wide view of cryptographic misuse in MCP and provides both tools and insights to strengthen the security foundations of this rapidly growing protocol.

</details>


### [23] [CCN: Decentralized Cross-Chain Channel Networks Supporting Secure and Privacy-Preserving Multi-Hop Interactions](https://arxiv.org/abs/2512.03791)
*Minghui Xu,Yihao Guo,Yanqiang Zhang,Zhiguang Shan,Guangyong Shang,Zhen Ma,Bin Xiao,Xiuzhen Cheng*

Main category: cs.CR

TL;DR: 提出跨链通道网络(CCN)解决多跳跨链交互中的安全与隐私问题，引入R-HTLC协议处理节点离线故障，确保结算正确性和用户交互不可链接性。


<details>
  <summary>Details</summary>
Motivation: 随着多跳跨链交互需求增长，现有技术面临安全与隐私挑战：安全方面，多跳交互依赖多个节点可用性，节点临时离线可能导致协议失败或资金损失；隐私方面，链上透明验证中间状态可能泄露可链接信息，破坏用户交互的不可链接性。

Method: 提出跨链通道网络(CCN)架构，核心是R-HTLC协议，采用沙漏机制和多路径退款策略确保节点离线时的结算正确性；利用零知识证明和链下协调技术，即使在对抗性环境下也能保持交互关系的不可区分性。

Result: 通过实验评估识别出主动和被动两种离线故障类型，现有方案未能充分解决；CCN系统能够处理这些离线情况，确保结算正确性，同时维持用户交互的隐私保护。

Conclusion: CCN为多跳跨链交易提供了安全且隐私保护的解决方案，通过R-HTLC协议有效应对节点离线故障，并利用零知识证明技术保护用户隐私，推动了跨链互操作性的发展。

Abstract: Cross-chain technology enables interoperability among otherwise isolated blockchains, supporting interactions across heterogeneous networks. Similar to how multi-hop communication became fundamental in the evolution of the Internet, the demand for multi-hop cross-chain interactions is gaining increasing attention. However, this growing demand introduces new security and privacy challenges. On the security side, multi-hop interactions depend on the availability of multiple participating nodes. If any node becomes temporarily offline during execution, the protocol may fail to complete correctly, leading to settlement failure or fund loss. On the privacy side, the need for on-chain transparency to validate intermediate states may unintentionally leak linkable information, compromising the unlinkability of user interactions. In this paper, we propose the Cross-Chain Channel Network (CCN), a decentralized network designed to support secure and privacy-preserving multi-hop cross-chain transactions. Through experimental evaluation, we identify two critical types of offline failures, referred to as active and passive offline cases, which have not been adequately addressed by existing solutions. To mitigate these issues, we introduce R-HTLC, a core protocol within CCN. R-HTLC incorporates an hourglass mechanism and a multi-path refund strategy to ensure settlement correctness even when some nodes go offline during execution. Importantly, CCN addresses not only the correctness under offline conditions but also maintains unlinkability in such adversarial settings. To overcome this, CCN leverages zero-knowledge proofs and off-chain coordination, ensuring that interaction relationships remain indistinguishable even when certain nodes are temporarily offline.

</details>


### [24] [Unfolding Challenges in Securing and Regulating Unmanned Air Vehicles](https://arxiv.org/abs/2512.03792)
*Sonali Rout,Vireshwar Kumar*

Main category: cs.CR

TL;DR: 该论文对无人机安全威胁与对策进行系统性综述，重点分析民用现成无人机系统的安全政策监管需求与挑战。


<details>
  <summary>Details</summary>
Motivation: 随着无人机在商业应用中的广泛部署，它们成为攻击者的主要目标，威胁到机密性、完整性和可用性等基本安全属性。政府和行业对保障和监管无人机及其使用的能力有限表示担忧，需要为大规模商业无人机网络部署铺平道路。

Method: 1. 对无人机已知安全威胁进行系统性分析，基于影响和有效性评估威胁；2. 分析现有对策在预防、检测和响应威胁方面的安全性和性能开销；3. 识别民用现成无人机系统安全政策监管的研究空白；4. 提出未来研究方向；5. 建立监管实体的基本要求并突出关键研究挑战。

Result: 论文提供了无人机安全威胁与对策的全面现状分析，识别了现有研究的不足，特别是民用现成无人机系统安全政策监管方面的空白，为构建安全、受监管的无人机网络奠定了基础框架。

Conclusion: 为实现大规模商业无人机网络的安全部署，需要解决民用现成无人机系统的安全政策监管挑战，建立有效的监管实体，并填补现有研究空白，以确保无人机网络的整体安全性。

Abstract: Unmanned Aerial Vehicles (UAVs) or drones are being introduced in a wide range of commercial applications. This has also made them prime targets of attackers who compromise their fundamental security properties, including confidentiality, integrity, and availability. As researchers discover novel threat vectors in UAVs, the government and industry are increasingly concerned about their limited ability to secure and regulate UAVs and their usage. With the aim of unfolding a path for a large-scale commercial UAV network deployment, we conduct a comprehensive state-of-the-art study and examine the prevailing security challenges. Unlike the prior art, we focus on uncovering the research gaps that must be addressed to enforce security policy regulations in civilian off-the-shelf drone systems. To that end, we first examine the known security threats to UAVs based on their impact and effectiveness. We then analyze existing countermeasures to prevent, detect, and respond to these threats in terms of security and performance overhead. We further outline the future research directions for securing UAVs. Finally, we establish the fundamental requirements and highlight critical research challenges in introducing a regulatory entity to achieve a secure and regulated UAV network.

</details>
