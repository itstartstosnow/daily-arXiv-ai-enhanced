{"id": "2602.18489", "categories": ["cs.CR", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.18489", "abs": "https://arxiv.org/abs/2602.18489", "authors": ["Nahom Birhan", "Daniel Wesego", "Dereje Shenkut", "Frank Liu", "Daniel Takabi"], "title": "DCInject: Persistent Backdoor Attacks via Frequency Manipulation in Personal Federated Learning", "comment": "Accepted to ICASSP 2026. 6 pages, 2 figures, 2 tables", "summary": "Personalized federated learning (PFL) creates client-specific models to handle data heterogeneity. Previously, PFL has been shown to be naturally resistant to backdoor attack propagation across clients. In this work, we reveal that PFL remains vulnerable to backdoor attacks through a novel frequency-domain approach. We propose DCInject, an adaptive frequency-domain backdoor attack for PFL, which removes portions of the zero-frequency (DC) component and replaces them with Gaussian-distributed samples in the frequency domain. Our attack achieves superior attack success rates while maintaining clean accuracy across four datasets (CIFAR-10/100, GTSRB, SVHN) compared to existing spatial-domain attacks, evaluated under parameter decoupling based personalization. DCInject achieves superior performance with ASRs of 96.83% (CIFAR-10), 99.38% (SVHN), and 100% (GTSRB) while maintaining clean accuracy. Under I-BAU defense, DCInject demonstrates strong persistence, retaining 90.30% ASR vs BadNet's 58.56% on VGG-16, exposing critical vulnerabilities in PFL security assumptions. Our code is available at https://github.com/NahomMA/DCINject-PFL"}
{"id": "2602.18514", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18514", "abs": "https://arxiv.org/abs/2602.18514", "authors": ["Manuel Wirth"], "title": "Trojan Horses in Recruiting: A Red-Teaming Case Study on Indirect Prompt Injection in Standard vs. Reasoning Models", "comment": "43 pages, 3 synthetic CV PDF's, 6 chat history PDF's and system prompts. This work was developed as part of the Responsible AI course within the Mannheim Master in Data Science (MMDS) program at the University of Mannheim", "summary": "As Large Language Models (LLMs) are increasingly integrated into automated decision-making pipelines, specifically within Human Resources (HR), the security implications of Indirect Prompt Injection (IPI) become critical. While a prevailing hypothesis posits that \"Reasoning\" or \"Chain-of-Thought\" Models possess safety advantages due to their ability to self-correct, emerging research suggests these capabilities may enable more sophisticated alignment failures. This qualitative Red-Teaming case study challenges the safety-through-reasoning premise using the Qwen 3 30B architecture. By subjecting both a standard instruction-tuned model and a reasoning-enhanced model to a \"Trojan Horse\" curriculum vitae, distinct failure modes are observed. The results suggest a complex trade-off: while the Standard Model resorted to brittle hallucinations to justify simple attacks and filtered out illogical constraints in complex scenarios, the Reasoning Model displayed a dangerous duality. It employed advanced strategic reframing to make simple attacks highly persuasive, yet exhibited \"Meta-Cognitive Leakage\" when faced with logically convoluted commands. This study highlights a failure mode where the cognitive load of processing complex adversarial instructions causes the injection logic to be unintentionally printed in the final output, rendering the attack more detectable by humans than in Standard Models."}
{"id": "2602.18539", "categories": ["cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.18539", "abs": "https://arxiv.org/abs/2602.18539", "authors": ["Supriya Khadka", "Dhiman Goswami", "Sanchari Das"], "title": "Poster: Privacy-Preserving Compliance Checks on Ethereum via Selective Disclosure", "comment": null, "summary": "Digital identity verification often forces a privacy trade-off, where users must disclose sensitive personal data to prove simple eligibility criteria. As blockchain applications integrate with regulated environments, this over-disclosure creates significant risks of data breaches and surveillance. This work proposes a general Selective Disclosure Framework built on Ethereum, designed to decouple attribute verification from identity revelation. By utilizing client-side zk-SNARKs, the framework enables users to prove specific eligibility predicates without revealing underlying identity documents. We present a case study, ZK-Compliance, which implements a functional Grant, Verify, Revoke lifecycle for age verification. Preliminary results indicate that strict compliance requirements can be satisfied with negligible client-side latency (< 200 ms) while preserving the pseudonymous nature of public blockchains."}
{"id": "2602.18598", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.18598", "abs": "https://arxiv.org/abs/2602.18598", "authors": ["María Teresa García-Ordás", "Jose Aveleira-Mata", "Isaías García-Rodríguez", "José Luis Casteleiro-Roca", "Martín Bayón-Gutierrez", "Héctor Alaiz-Moretón"], "title": "Influence of Autoencoder Latent Space on Classifying IoT CoAP Attacks", "comment": "16 pages , 2 figures , 1 table. Accepted for publication in Logic Journal of the IGPL", "summary": "The Internet of Things (IoT) presents a unique cybersecurity challenge due to its vast network of interconnected, resource-constrained devices. These vulnerabilities not only threaten data integrity but also the overall functionality of IoT systems. This study addresses these challenges by exploring efficient data reduction techniques within a model-based intrusion detection system (IDS) for IoT environments. Specifically, the study explores the efficacy of an autoencoder's latent space combined with three different classification techniques. Utilizing a validated IoT dataset, particularly focusing on the Constrained Application Protocol (CoAP), the study seeks to develop a robust model capable of identifying security breaches targeting this protocol. The research culminates in a comprehensive evaluation, presenting encouraging results that demonstrate the effectiveness of the proposed methodologies in strengthening IoT cybersecurity with more than a 99% of precision using only 2 learned features."}
{"id": "2602.18624", "categories": ["cs.CR", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.18624", "abs": "https://arxiv.org/abs/2602.18624", "authors": ["Efrén López-Morales"], "title": "Orbital Escalation: Modeling Satellite Ransomware Attacks Using Game Theory", "comment": "SpaceSec (Co-located with NDSS) version: https://www.ndss-symposium.org/ndss-paper/auto-draft-641/", "summary": "Ransomware has yet to reach orbit, but the conditions for such an attack already exist. This paper presents the first game-theoretic framework for modeling ransomware against satellites: the orbital escalation game. In this model, the attacker escalates ransom demands across orbital passes, while the defender chooses their best strategy, e.g., attempt a restore procedure. Using dynamic programming, we solve the defender's optimal strategy and the attacker's expected payoff under real orbital constraints. Additionally, we provide a GPS III satellite case study that demonstrates how our orbital escalation game can be applied in the context of a fictional but feasible ransomware attack to derive the best strategies at every step. In conclusion, this foundational model offers satellite owners, policy makers and researchers, a formal framework to better prepare their responses when a spacecraft is held for ransom."}
{"id": "2602.18681", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.18681", "abs": "https://arxiv.org/abs/2602.18681", "authors": ["Jessica Young", "Sam Vaughan", "Andrew Jenks", "Henrique Malvar", "Christian Paquin", "Paul England", "Thomas Roca", "Juan LaVista Ferres", "Forough Poursabzi", "Neil Coles", "Ken Archer", "Eric Horvitz"], "title": "Media Integrity and Authentication: Status, Directions, and Futures", "comment": "56 pages", "summary": "We provide background on emerging challenges and future directions with media integrity and authentication methods, focusing on distinguishing AI-generated media from authentic content captured by cameras and microphones. We evaluate several approaches, including provenance, watermarking, and fingerprinting. After defining each method, we analyze three representative technologies: cryptographically secured provenance, imperceptible watermarking, and soft-hash fingerprinting. We analyze how these tools operate across modalities and evaluate relevant threat models, attack categories, and real-world workflows spanning capture, editing, distribution, and verification. We consider sociotechnical reversal attacks that can invert integrity signals, making authentic content appear synthetic and vice versa, highlighting the value of verification systems that are resilient to both technical and psychosocial manipulation. Finally, we outline techniques for delivering high-confidence provenance authentication, including directions for strengthening edge-device security using secure enclaves."}
{"id": "2602.18700", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18700", "abs": "https://arxiv.org/abs/2602.18700", "authors": ["Wenlong Meng", "Chen Gong", "Terry Yue Zhuo", "Fan Zhang", "Kecen Li", "Zheng Liu", "Zhou Yang", "Chengkun Wei", "Wenzhi Chen"], "title": "Watermarking LLM Agent Trajectories", "comment": "20 pages, 9 figures", "summary": "LLM agents rely heavily on high-quality trajectory data to guide their problem-solving behaviors, yet producing such data requires substantial task design, high-capacity model generation, and manual filtering. Despite the high cost of creating these datasets, existing literature has overlooked copyright protection for LLM agent trajectories. This gap leaves creators vulnerable to data theft and makes it difficult to trace misuse or enforce ownership rights. This paper introduces ActHook, the first watermarking method tailored for agent trajectory datasets. Inspired by hook mechanisms in software engineering, ActHook embeds hook actions that are activated by a secret input key and do not alter the original task outcome. Like software execution, LLM agents operate sequentially, allowing hook actions to be inserted at decision points without disrupting task flow. When the activation key is present, an LLM agent trained on watermarked trajectories can produce these hook actions at a significantly higher rate, enabling reliable black-box detection. Experiments on mathematical reasoning, web searching, and software engineering agents show that ActHook achieves an average detection AUC of 94.3 on Qwen-2.5-Coder-7B while incurring negligible performance degradation."}
{"id": "2602.18708", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.18708", "abs": "https://arxiv.org/abs/2602.18708", "authors": ["Tao Liu", "Gowri Ramachandra", "Raja Jurdak"], "title": "On the Energy Cost of Post-Quantum Key Establishment in Wireless Low-Power Personal Area Networks", "comment": "6 pages, 3 figures, Submitted to SenSys'26", "summary": "Post-Quantum Cryptography (PQC) creates payloads that strain the timing and energy budgets of Personal Area Networks. In post-quantum key exchange (PQKE), this causes severe fragmentation, prolonged radio activity, and high transmission overhead on low-power devices. Prior work optimizes cryptographic computation but largely ignores communication cost. This paper separates computation and communication costs using Bluetooth Low Energy as a representative platform and validates them on real hardware. Results show communication often dominates PQKE energy, exceeding cryptographic cost. Efficient quantum-resilient pairing therefore requires coordinated protocol configuration and lower-layer optimization. This work provides developers a practical way to reason about PQC energy trade-offs and informs the evolution of PAN standards toward quantum-safe operation."}
{"id": "2602.18758", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18758", "abs": "https://arxiv.org/abs/2602.18758", "authors": ["Wenxuan Zeng", "Chao Yang", "Tianshi Xu", "Bo Zhang", "Changrui Ren", "Jin Dong", "Meng Li"], "title": "UFO: Unlocking Ultra-Efficient Quantized Private Inference with Protocol and Algorithm Co-Optimization", "comment": null, "summary": "Private convolutional neural network (CNN) inference based on secure two-party computation (2PC) suffers from high communication and latency overhead, especially from convolution layers. In this paper, we propose UFO, a quantized 2PC inference framework that jointly optimizes the 2PC protocols and quantization algorithm. UFO features a novel 2PC protocol that systematically combines the efficient Winograd convolution algorithm with quantization to improve inference efficiency. However, we observe that naively combining quantization and Winograd convolution faces the following challenges: 1) From the inference perspective, Winograd transformations introduce extensive additions and require frequent bit width conversions to avoid inference overflow, leading to non-negligible communication overhead; 2) From the training perspective, Winograd transformations introduce weight outliers that make quantization-aware training (QAT) difficult, resulting in inferior model accuracy. To address these challenges, we co-optimize both protocol and algorithm. 1) At the protocol level, we propose a series of graph-level optimizations for 2PC inference to minimize the communication. 2) At the algorithm level, we develop a mixed-precision QAT algorithm based on layer sensitivity to optimize model accuracy given communication constraints. To accommodate the outliers, we further introduce a 2PC-friendly bit re-weighting algorithm to increase the representation range without explicitly increasing bit widths. With extensive experiments, UFO demonstrates 11.7x, 3.6x, and 6.3x communication reduction with 1.29%, 1.16%, and 1.29% higher accuracy compared to state-of-the-art frameworks SiRNN, COINN, and CoPriv, respectively."}
{"id": "2602.18782", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18782", "abs": "https://arxiv.org/abs/2602.18782", "authors": ["Chun Yan Ryan Kan", "Tommy Tran", "Vedant Yadav", "Ava Cai", "Kevin Zhu", "Ruizhe Li", "Maheep Chaudhary"], "title": "MANATEE: Inference-Time Lightweight Diffusion Based Safety Defense for LLMs", "comment": null, "summary": "Defending LLMs against adversarial jailbreak attacks remains an open challenge. Existing defenses rely on binary classifiers that fail when adversarial input falls outside the learned decision boundary, and repeated fine-tuning is computationally expensive while potentially degrading model capabilities. We propose MANATEE, an inference-time defense that uses density estimation over a benign representation manifold. MANATEE learns the score function of benign hidden states and uses diffusion to project anomalous representations toward safe regions--requiring no harmful training data and no architectural modifications. Experiments across Mistral-7B-Instruct, Llama-3.1-8B-Instruct, and Gemma-2-9B-it demonstrate that MANATEE reduce Attack Success Rate by up to 100\\% on certain datasets, while preserving model utility on benign inputs."}
{"id": "2602.18900", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18900", "abs": "https://arxiv.org/abs/2602.18900", "authors": ["Nnaemeka Obiefuna", "Samuel Oyeneye", "Similoluwa Odunaiya", "Iremide Oyelaja", "Steven Kolawole"], "title": "PrivacyBench: Privacy Isn't Free in Hybrid Privacy-Preserving Vision Systems", "comment": "20 pages, 2 figures", "summary": "Privacy preserving machine learning deployments in sensitive deep learning applications; from medical imaging to autonomous systems; increasingly require combining multiple techniques. Yet, practitioners lack systematic guidance to assess the synergistic and non-additive interactions of these hybrid configurations, relying instead on isolated technique analysis that misses critical system level interactions. We introduce PrivacyBench, a benchmarking framework that reveals striking failures in privacy technique combinations with severe deployment implications. Through systematic evaluation across ResNet18 and ViT models on medical datasets, we uncover that FL + DP combinations exhibit severe convergence failure, with accuracy dropping from 98% to 13% while compute costs and energy consumption substantially increase. In contrast, FL + SMPC maintains near-baseline performance with modest overhead. Our framework provides the first systematic platform for evaluating privacy-utility-cost trade-offs through automated YAML configuration, resource monitoring, and reproducible experimental protocols. PrivacyBench enables practitioners to identify problematic technique interactions before deployment, moving privacy-preserving computer vision from ad-hoc evaluation toward principled systems design. These findings demonstrate that privacy techniques cannot be composed arbitrarily and provide critical guidance for robust deployment in resource-constrained environments."}
{"id": "2602.19021", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.19021", "abs": "https://arxiv.org/abs/2602.19021", "authors": ["Kiarash Ahi", "Vaibhav Agrawal", "Saeed Valizadeh"], "title": "LLM Scalability Risk for Agentic-AI and Model Supply Chain Security", "comment": "Accepted for publication in Journal of Computer Information Systems (2026). DOI: 10.1080/08874417.2026.2624670", "summary": "Large Language Models (LLMs) & Generative AI are transforming cybersecurity, enabling both advanced defenses and new attacks. Organizations now use LLMs for threat detection, code review, and DevSecOps automation, while adversaries leverage them to produce malwares and run targeted social-engineering campaigns. This paper presents a unified analysis integrating offensive and defensive perspectives on GenAI-driven cybersecurity. Drawing on 70 academic, industry, and policy sources, it analyzes the rise of AI-facilitated threats and its implications for global security to ground necessity for scalable defensive mechanisms. We introduce two primary contributions: the LLM Scalability Risk Index (LSRI), a parametric framework to stress-test operational risks when deploying LLMs in security-critical environments & a model-supply-chain framework establishing a verifiable root of trust throughout model lifecycle. We also synthesize defense strategies from platforms like Google Play Protect, Microsoft Security Copilot and outline a governance roadmap for secure, large-scale LLM deployment."}
{"id": "2602.19025", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19025", "abs": "https://arxiv.org/abs/2602.19025", "authors": ["Hossein Shokouhinejad", "Roozbeh Razavi-Far", "Griffin Higgins", "Ali. A Ghorbani"], "title": "Routing-Aware Explanations for Mixture of Experts Graph Models in Malware Detection", "comment": null, "summary": "Mixture-of-Experts (MoE) offers flexible graph reasoning by combining multiple views of a graph through a learned router. We investigate routing-aware explanations for MoE graph models in malware detection using control flow graphs (CFGs). Our architecture builds diversity at two levels. At the node level, each layer computes multiple neighborhood statistics and fuses them with an MLP, guided by a degree reweighting factor rho and a pooling choice lambda in {mean, std, max}, producing distinct node representations that capture complementary structural cues in CFGs. At the readout level, six experts, each tied to a specific (rho, lambda) view, output graph-level logits that the router weights into a final prediction. Post-hoc explanations are generated with edge-level attributions per expert and aggregated using the router gates so the rationale reflects both what each expert highlights and how strongly it is selected. Evaluated against single-expert GNN baselines such as GCN, GIN, and GAT on the same CFG dataset, the proposed MoE achieves strong detection accuracy while yielding stable, faithful attributions under sparsity-based perturbations. The results indicate that making the router explicit and combining multi-statistic node encoding with expert-level diversity can improve the transparency of MoE decisions for malware analysis."}
{"id": "2602.19087", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.19087", "abs": "https://arxiv.org/abs/2602.19087", "authors": ["Norrakith Srisumrith", "Sunantha Sodsee"], "title": "Detecting Cybersecurity Threats by Integrating Explainable AI with SHAP Interpretability and Strategic Data Sampling", "comment": "10 pages, 6 figures, accepted for publication in ICTIS 2026", "summary": "The critical need for transparent and trustworthy machine learning in cybersecurity operations drives the development of this integrated Explainable AI (XAI) framework. Our methodology addresses three fundamental challenges in deploying AI for threat detection: handling massive datasets through Strategic Sampling Methodology that preserves class distributions while enabling efficient model development; ensuring experimental rigor via Automated Data Leakage Prevention that systematically identifies and removes contaminated features; and providing operational transparency through Integrated XAI Implementation using SHAP analysis for model-agnostic interpretability across algorithms. Applied to the CIC-IDS2017 dataset, our approach maintains detection efficacy while reducing computational overhead and delivering actionable explanations for security analysts. The framework demonstrates that explainability, computational efficiency, and experimental integrity can be simultaneously achieved, providing a robust foundation for deploying trustworthy AI systems in security operations centers where decision transparency is paramount."}
{"id": "2602.19410", "categories": ["cs.CR", "cs.CY", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.19410", "abs": "https://arxiv.org/abs/2602.19410", "authors": ["Duy Anh Ta", "Farnaz Farid", "Farhad Ahamed", "Ala Al-Areqi", "Robert Beutel", "Tamara Watson", "Alana Maurushat"], "title": "BioEnvSense: A Human-Centred Security Framework for Preventing Behaviour-Driven Cyber Incidents", "comment": null, "summary": "Modern organizations increasingly face cybersecurity incidents driven by human behaviour rather than technical failures. To address this, we propose a conceptual security framework that integrates a hybrid Convolutional Neural Network-Long Short-Term Memory (CNN-LSTM) model to analyze biometric and environmental data for context-aware security decisions. The CNN extracts spatial patterns from sensor data, while the LSTM captures temporal dynamics associated with human error susceptibility. The model achieves 84% accuracy, demonstrating its ability to reliably detect conditions that lead to elevated human-centred cyber risk. By enabling continuous monitoring and adaptive safeguards, the framework supports proactive interventions that reduce the likelihood of human-driven cyber incidents"}
{"id": "2602.19450", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19450", "abs": "https://arxiv.org/abs/2602.19450", "authors": ["Kunal Mukherjee"], "title": "Red-Teaming Claude Opus and ChatGPT-based Security Advisors for Trusted Execution Environments", "comment": null, "summary": "Trusted Execution Environments (TEEs) (e.g., Intel SGX and ArmTrustZone) aim to protect sensitive computation from a compromised operating system, yet real deployments remain vulnerable to microarchitectural leakage, side-channel attacks, and fault injection. In parallel, security teams increasingly rely on Large Language Model (LLM) assistants as security advisors for TEE architecture review, mitigation planning, and vulnerability triage. This creates a socio-technical risk surface: assistants may hallucinate TEE mechanisms, overclaim guarantees (e.g., what attestation does and does not establish), or behave unsafely under adversarial prompting.\n  We present a red-teaming study of two prevalently deployed LLM assistants in the role of TEE security advisors: ChatGPT-5.2 and Claude Opus-4.6, focusing on the inherent limitations and transferability of prompt-induced failures across LLMs. We introduce TEE-RedBench, a TEE-grounded evaluation methodology comprising (i) a TEE-specific threat model for LLM-mediated security work, (ii) a structured prompt suite spanning SGX and TrustZone architecture, attestation and key management, threat modeling, and non-operational mitigation guidance, along with policy-bound misuse probes, and (iii) an annotation rubric that jointly measures technical correctness, groundedness, uncertainty calibration, refusal quality, and safe helpfulness. We find that some failures are not purely idiosyncratic, transferring up to 12.02% across LLM assistants, and we connect these outcomes to secure architecture by outlining an \"LLM-in-the-loop\" evaluation pipeline: policy gating, retrieval grounding, structured templates, and lightweight verification checks that, when combined, reduce failures by 80.62%."}
{"id": "2602.19550", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2602.19550", "abs": "https://arxiv.org/abs/2602.19550", "authors": ["Ilan Rosenfeld", "Noam Kleinburd", "Hillel Chapman", "Dror Reuven"], "title": "Hardware-Friendly Randomization: Enabling Random-Access and Minimal Wiring in FHE Accelerators with Low Total Cost", "comment": null, "summary": "The Ring-Learning With Errors (RLWE) problem forms the backbone of highly efficient Fully Homomorphic Encryption (FHE) schemes. A significant component of the RLWE public key and ciphertext of the form $(b,a)$ is the uniformly random polynomial $a \\in R_q$ . While essential for security, the communication overhead of transmitting $a$ from client to server, and inputting it into a hardware accelerator, can be substantial, especially for FHE accelerators aiming at high acceleration factors. A known technique in reducing this overhead generates $a$ from a small seed on the client side via a deterministic process, transmits only the seed, and generates $a$ on-the-fly within the accelerator. Challenges in the hardware implementation of an accelerator include wiring (density and power), compute area, compute power as well as flexibility in scheduling of on-the-fly generation instructions. This extended abstract proposes a concrete scheme and parameters wherein these practical challenges are addressed. We detail the benefits of our approach, which maintains the reduction in communication latency and memory footprint, while allowing parallel generation of uniformly distributed samples, relaxed wiring requirements, unrestricted randomaccess to RNS limbs, and results in an extremely low overhead on the client side (i.e. less than 3%) during the key generation process. The proposed scheme eliminates the need for thick metal layers for randomness distribution and prevents the power consumption of the PRNG subsystem from scaling prohibitively with the acceleration factor, potentially saving tens of Watts per accelerator chip in high-throughput configurations."}
{"id": "2602.19555", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19555", "abs": "https://arxiv.org/abs/2602.19555", "authors": ["Xiaochong Jiang", "Shiqi Yang", "Wenting Yang", "Yichen Liu", "Cheng Ji"], "title": "Agentic AI as a Cybersecurity Attack Surface: Threats, Exploits, and Defenses in Runtime Supply Chains", "comment": "9 Pages, 3 figures", "summary": "Agentic systems built on large language models (LLMs) extend beyond text generation to autonomously retrieve information and invoke tools. This runtime execution model shifts the attack surface from build-time artifacts to inference-time dependencies, exposing agents to manipulation through untrusted data and probabilistic capability resolution. While prior work has focused on model-level vulnerabilities, security risks emerging from cyclic and interdependent runtime behavior remain fragmented. We systematize these risks within a unified runtime framework, categorizing threats into data supply chain attacks (transient context injection and persistent memory poisoning) and tool supply chain attacks (discovery, implementation, and invocation). We further identify the Viral Agent Loop, in which agents act as vectors for self-propagating generative worms without exploiting code-level flaws. Finally, we advocate a Zero-Trust Runtime Architecture that treats context as untrusted control flow and constrains tool execution through cryptographic provenance rather than semantic inference."}
{"id": "2602.19818", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19818", "abs": "https://arxiv.org/abs/2602.19818", "authors": ["Hillel Ohayon", "Daniel Gilkarov", "Ran Dubin"], "title": "SafePickle: Robust and Generic ML Detection of Malicious Pickle-based ML Models", "comment": null, "summary": "Model repositories such as Hugging Face increasingly distribute machine learning artifacts serialized with Python's pickle format, exposing users to remote code execution (RCE) risks during model loading. Recent defenses, such as PickleBall, rely on per-library policy synthesis that requires complex system setups and verified benign models, which limits scalability and generalization. In this work, we propose a lightweight, machine-learning-based scanner that detects malicious Pickle-based files without policy generation or code instrumentation. Our approach statically extracts structural and semantic features from Pickle bytecode and applies supervised and unsupervised models to classify files as benign or malicious. We construct and release a labeled dataset of 727 Pickle-based files from Hugging Face and evaluate our models on four datasets: our own, PickleBall (out-of-distribution), Hide-and-Seek (9 advanced evasive malicious models), and synthetic joblib files. Our method achieves 90.01% F1-score compared with 7.23%-62.75% achieved by the SOTA scanners (Modelscan, Fickling, ClamAV, VirusTotal) on our dataset. Furthermore, on the PickleBall data (OOD), it achieves 81.22% F1-score compared with 76.09% achieved by the PickleBall method, while remaining fully library-agnostic. Finally, we show that our method is the only one to correctly parse and classify 9/9 evasive Hide-and-Seek malicious models specially crafted to evade scanners. This demonstrates that data-driven detection can effectively and generically mitigate Pickle-based model file attacks."}
{"id": "2602.19844", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.19844", "abs": "https://arxiv.org/abs/2602.19844", "authors": ["Yedi Zhang", "Haoyu Wang", "Xianglin Yang", "Jin Song Dong", "Jun Sun"], "title": "LLM-enabled Applications Require System-Level Threat Monitoring", "comment": "26 pages", "summary": "LLM-enabled applications are rapidly reshaping the software ecosystem by using large language models as core reasoning components for complex task execution. This paradigm shift, however, introduces fundamentally new reliability challenges and significantly expands the security attack surface, due to the non-deterministic, learning-driven, and difficult-to-verify nature of LLM behavior. In light of these emerging and unavoidable safety challenges, we argue that such risks should be treated as expected operational conditions rather than exceptional events, necessitating a dedicated incident-response perspective. Consequently, the primary barrier to trustworthy deployment is not further improving model capability but establishing system-level threat monitoring mechanisms that can detect and contextualize security-relevant anomalies after deployment -- an aspect largely underexplored beyond testing or guardrail-based defenses. Accordingly, this position paper advocates systematic and comprehensive monitoring of security threats in LLM-enabled applications as a prerequisite for reliable operation and a foundation for dedicated incident-response frameworks."}
{"id": "2602.19918", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.19918", "abs": "https://arxiv.org/abs/2602.19918", "authors": ["Jiaqi Xue", "Mengxin Zheng", "Qian Lou"], "title": "RobPI: Robust Private Inference against Malicious Client", "comment": "Accepted by SaTML 2026", "summary": "The increased deployment of machine learning inference in various applications has sparked privacy concerns. In response, private inference (PI) protocols have been created to allow parties to perform inference without revealing their sensitive data. Despite recent advances in the efficiency of PI, most current methods assume a semi-honest threat model where the data owner is honest and adheres to the protocol. However, in reality, data owners can have different motivations and act in unpredictable ways, making this assumption unrealistic. To demonstrate how a malicious client can compromise the semi-honest model, we first designed an inference manipulation attack against a range of state-of-the-art private inference protocols. This attack allows a malicious client to modify the model output with 3x to 8x fewer queries than current black-box attacks. Motivated by the attacks, we proposed and implemented RobPI, a robust and resilient private inference protocol that withstands malicious clients. RobPI integrates a distinctive cryptographic protocol that bolsters security by weaving encryption-compatible noise into the logits and features of private inference, thereby efficiently warding off malicious-client attacks. Our extensive experiments on various neural networks and datasets show that RobPI achieves ~91.9% attack success rate reduction and increases more than 10x the number of queries required by malicious-client attacks."}
