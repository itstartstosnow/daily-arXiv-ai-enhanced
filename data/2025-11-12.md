<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 25]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [AudAgent: Automated Auditing of Privacy Policy Compliance in AI Agents](https://arxiv.org/abs/2511.07441)
*Ye Zheng,Yidan Hu*

Main category: cs.CR

TL;DR: AudAgent是一个可视化框架，用于实时监控AI代理的数据实践并确保其符合隐私政策，通过政策解析、运行时标注、合规审计和用户界面四个组件实现自动化隐私审计。


<details>
  <summary>Details</summary>
Motivation: AI代理在未经用户明确同意的情况下收集或披露敏感数据，存在严重的隐私问题，且缺乏透明度来验证运行时行为是否与隐私政策一致。

Method: 使用LLM集合解析自然语言隐私政策为结构化模型，轻量级分析器检测敏感数据并标注使用方式，通过本体对齐和自动机评估进行合规检查，可视化界面展示实时执行轨迹和隐私风险。

Result: 在基于AutoGen等主流框架的AI代理上评估，AudAgent能有效实时识别潜在的隐私政策违规行为。

Conclusion: AudAgent填补了AI代理隐私政策与实践之间透明度和问责制的空白，支持用户定义策略的细粒度控制，提供实时隐私风险监控。

Abstract: AI agents can autonomously perform tasks and, often without explicit user consent, collect or disclose users' sensitive local data, which raises serious privacy concerns. Although AI agents' privacy policies may describe their intended data practices, there remains limited transparency and accountability about whether runtime behavior matches those policies. To close this gap, we introduce AudAgent, a visual framework that continuously monitors AI agents' data practices in real time and guards compliance with stated privacy policies.
  AudAgent consists of four components for automated privacy auditing of AI agents. (i) Policy parsing: an ensemble of LLMs translates natural-language privacy policies into a structured privacy-policy model, where cross-LLM voting guarantees confidence of the parsing results. (ii) Runtime annotation: a lightweight Presidio-based analyzer detects sensitive data and annotates how the data is used based on the context of the AI agent's operations and the privacy-policy model. (iii) Compliance auditing: ontology alignment and automata-based evaluation connect the policy model with runtime annotations, enabling on-the-fly compliance checks between the natural-language policy and observed unordered data practices of AI agents. (iv) User interface: a platform-independent implementation visualizes the real-time execution trace of AI agents along with potential privacy risks detected during auditing, providing user-friendly transparency and accountability.
  In addition to common formatted privacy policies, AudAgent also supports user-defined policies for fine-grained control and customization. We evaluate AudAgent on AI agents built upon mainstream programming frameworks such as AutoGen, experiments show that AudAgent effectively identifies potential privacy policy violations in real time.

</details>


### [2] [KG-DF: A Black-box Defense Framework against Jailbreak Attacks Based on Knowledge Graphs](https://arxiv.org/abs/2511.07480)
*Shuyuan Liu,Jiawei Chen,Xiao Yang,Hang Su,Zhaoxia Yin*

Main category: cs.CR

TL;DR: 提出了基于知识图谱的防御框架(KG-DF)，通过结构化知识表示和语义关联能力来检测和防御大语言模型的越狱攻击，在保持模型通用性的同时提升安全性。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法难以在模型通用性和安全性之间取得平衡，过度防御会限制模型正常使用，防御不足则存在安全漏洞。

Method: 使用知识图谱进行结构化知识表示和语义关联，通过可扩展的语义解析模块将输入查询转换为结构化安全概念表示，增强匹配相关性。

Result: 实验表明该框架能有效防御多种越狱攻击方法，同时在通用问答场景中通过融入领域通用知识提高了LLM的响应质量。

Conclusion: KG-DF框架成功解决了LLM安全防御中通用性与安全性的平衡问题，为大规模语言模型的安全应用提供了有效解决方案。

Abstract: With the widespread application of large language models (LLMs) in various fields, the security challenges they face have become increasingly prominent, especially the issue of jailbreak. These attacks induce the model to generate erroneous or uncontrolled outputs through crafted inputs, threatening the generality and security of the model. Although existing defense methods have shown some effectiveness, they often struggle to strike a balance between model generality and security. Excessive defense may limit the normal use of the model, while insufficient defense may lead to security vulnerabilities. In response to this problem, we propose a Knowledge Graph Defense Framework (KG-DF). Specifically, because of its structured knowledge representation and semantic association capabilities, Knowledge Graph(KG) can be searched by associating input content with safe knowledge in the knowledge base, thus identifying potentially harmful intentions and providing safe reasoning paths. However, traditional KG methods encounter significant challenges in keyword extraction, particularly when confronted with diverse and evolving attack strategies. To address this issue, we introduce an extensible semantic parsing module, whose core task is to transform the input query into a set of structured and secure concept representations, thereby enhancing the relevance of the matching process. Experimental results show that our framework enhances defense performance against various jailbreak attack methods, while also improving the response quality of the LLM in general QA scenarios by incorporating domain-general knowledge.

</details>


### [3] [Biologically-Informed Hybrid Membership Inference Attacks on Generative Genomic Models](https://arxiv.org/abs/2511.07503)
*Asia Belfiore,Jonathan Passerat-Palmbach,Dmitrii Usynin*

Main category: cs.CR

TL;DR: 使用语言模型和差分隐私生成合成基因突变数据，并提出新型混合成员推理攻击来评估隐私保护效果


<details>
  <summary>Details</summary>
Motivation: 基因数据的敏感性引发隐私担忧，需要开发保护隐私的合成数据生成方法

Method: 使用基于Transformer的GPT类语言模型生成合成基因突变数据，结合差分隐私保护，并提出生物信息混合成员推理攻击(biHMIA)来评估隐私

Result: 小型和大型Transformer模型都能有效生成小规模基因数据，混合攻击比传统方法有更高的攻击成功率

Conclusion: 语言模型是可行的合成基因数据生成器，新型混合攻击能更有效地评估隐私保护效果

Abstract: The increased availability of genetic data has transformed genomics research, but raised many privacy concerns regarding its handling due to its sensitive nature. This work explores the use of language models (LMs) for the generation of synthetic genetic mutation profiles, leveraging differential privacy (DP) for the protection of sensitive genetic data. We empirically evaluate the privacy guarantees of our DP modes by introducing a novel Biologically-Informed Hybrid Membership Inference Attack (biHMIA), which combines traditional black box MIA with contextual genomics metrics for enhanced attack power. Our experiments show that both small and large transformer GPT-like models are viable synthetic variant generators for small-scale genomics, and that our hybrid attack leads, on average, to higher adversarial success compared to traditional metric-based MIAs.

</details>


### [4] [FedRW: Efficient Privacy-Preserving Data Reweighting for Enhancing Federated Learning of Language Models](https://arxiv.org/abs/2511.07505)
*Pukang Ye,Junwei Luo,Xiaolei Dong,Yunbo Yang*

Main category: cs.CR

TL;DR: FedRW是一个隐私保护的联邦学习框架，通过样本重加权而非删除来处理数据重复问题，无需可信第三方，显著提升训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习中的去重方法依赖可信第三方进行统一删除，可能导致信息样本丢失并引入隐私风险。

Method: 提出安全的多方计算协议进行频率感知的重加权，结合并行编排策略确保效率和可扩展性，使用自适应重加权机制调整损失贡献。

Result: FedRW相比现有方法在预处理上实现28.78倍加速，困惑度提升约11.42%，同时提供更强的安全保障。

Conclusion: FedRW为联邦LLM训练中的重复数据管理建立了新范式，在保护隐私的同时提升模型泛化能力和鲁棒性。

Abstract: Data duplication within large-scale corpora often impedes large language models' (LLMs) performance and privacy. In privacy-concerned federated learning scenarios, conventional deduplication methods typically rely on trusted third parties to perform uniform deletion, risking loss of informative samples while introducing privacy vulnerabilities. To address these gaps, we propose Federated ReWeighting (FedRW), the first privacy-preserving framework, to the best of our knowledge, that performs soft deduplication via sample reweighting instead of deletion in federated LLM training, without assuming a trusted third party. At its core, FedRW proposes a secure, frequency-aware reweighting protocol through secure multi-party computation, coupled with a parallel orchestration strategy to ensure efficiency and scalability. During training, FedRW utilizes an adaptive reweighting mechanism with global sample frequencies to adjust individual loss contributions, effectively improving generalization and robustness. Empirical results demonstrate that FedRW outperforms the state-of-the-art method by achieving up to 28.78x speedup in preprocessing and approximately 11.42% improvement in perplexity, while offering enhanced security guarantees. FedRW thus establishes a new paradigm for managing duplication in federated LLM training.

</details>


### [5] [LSEG: A Lightweight and Secure Key Exchange Protocol for Smart Grid Communication](https://arxiv.org/abs/2511.07548)
*Amna Zafar,Muhammad Asfand Hafeez,Arslan Munir*

Main category: cs.CR

TL;DR: 提出了一种轻量级认证和安全密钥交换协议LSEG，适用于资源受限的智能电网环境，通过统一椭圆曲线密钥对和轻量级加密算法实现安全高效的通信。


<details>
  <summary>Details</summary>
Motivation: 现有认证方案在资源受限的智能电网环境中存在计算开销过大或安全性不足的问题，需要设计专门针对此类环境的轻量级安全协议。

Method: 使用Ed25519和Curve25519之间的双有理映射实现统一椭圆曲线密钥对，采用ECDHE确保前向保密，使用ASCON128a进行会话加密，并通过HMAC-KDF派生初始密钥。

Result: 在Raspberry Pi和Intel Core i9系统上的实验结果显示，执行时间低于5.5毫秒，通信成本仅为1024比特，实现了高效性能。

Conclusion: LSEG协议在安全性、效率和合规性之间取得了良好平衡，是智能电网基础设施中安全通信的可扩展解决方案。

Abstract: The increasing deployment of the Internet of Things (IoT) edge devices in modern smart grid environments requires secure and efficient communication protocols specifically designed for resource-constrained environments. However, most existing authentication schemes either impose excessive computational overhead or lack robustness against advanced cyber threats, making them unsuitable for resource-limited smart grid deployments. To address these limitations, this paper proposes a lightweight authentication and secure key exchange protocol for smart grid (LSEG) environments. The proposed LSEG protocol utilizes a unified elliptic curve key pair, enabled by birational mapping between Ed25519 and Curve25519, for signing and key exchange. Initial keys are derived using the hash based message authentication code (HMAC) based key derivation function (HKDF), while ephemeral key pairs, generated through the Elliptic Curve Diffie Hellman Ephemeral (ECDHE), are used in each session to ensure forward secrecy. Session communication is protected using ASCON128a, a lightweight, NIST-standardized, authenticated encryption algorithm. Formal security proofs in the random oracle model validate the security properties of LSEG, including mutual authentication, forward secrecy, and resistance to impersonation, replay, and man in the middle attacks. Experimental results on both Raspberry Pi and Intel Core i9-based systems demonstrate practical efficiency, achieving execution times under 5.5 milliseconds on embedded hardware and a communication cost of only 1024 bits for the protocol's message exchanges. The results demonstrate that LSEG effectively balances security, efficiency, and compliance, making it a scalable solution for secure communication in smart grid infrastructures.

</details>


### [6] [A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain](https://arxiv.org/abs/2511.07577)
*Yining Lu,Wenyi Tang,Max Johnson,Taeho Jung,Meng Jiang*

Main category: cs.CR

TL;DR: 提出了一种去中心化的检索增强生成系统，通过区块链智能合约实现可靠性评分机制，在不可靠数据环境中比集中式系统性能提升10.7%，同时节省约56%的边际成本。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统采用集中式架构，存在数据收集成本高、隐私问题等缺陷，需要让基础模型直接从数据所有者处获取信息，同时保持数据控制权。

Method: 设计去中心化RAG系统，包含动态可靠性评分机制，基于各数据源贡献的响应质量评估其可靠性，并通过区块链智能合约确保评分过程的透明性和防篡改性。

Result: 在两个模拟环境中使用Llama 3B和8B模型测试，在不可靠数据环境中比集中式系统性能提升10.7%，在理想可靠数据环境中接近集中式系统上限性能。

Conclusion: 去中心化RAG系统能有效解决数据源可靠性差异问题，通过区块链确保评分透明度，在保持性能的同时显著降低成本。

Abstract: Existing retrieval-augmented generation (RAG) systems typically use a centralized architecture, causing a high cost of data collection, integration, and management, as well as privacy concerns. There is a great need for a decentralized RAG system that enables foundation models to utilize information directly from data owners who maintain full control over their sources. However, decentralization brings a challenge: the numerous independent data sources vary significantly in reliability, which can diminish retrieval accuracy and response quality. To address this, our decentralized RAG system has a novel reliability scoring mechanism that dynamically evaluates each source based on the quality of responses it contributes to generate and prioritizes high-quality sources during retrieval. To ensure transparency and trust, the scoring process is securely managed through blockchain-based smart contracts, creating verifiable and tamper-proof reliability records without relying on a central authority. We evaluate our decentralized system with two Llama models (3B and 8B) in two simulated environments where six data sources have different levels of reliability. Our system achieves a +10.7\% performance improvement over its centralized counterpart in the real world-like unreliable data environments. Notably, it approaches the upper-bound performance of centralized systems under ideally reliable data environments. The decentralized infrastructure enables secure and trustworthy scoring management, achieving approximately 56\% marginal cost savings through batched update operations. Our code and system are open-sourced at github.com/yining610/Reliable-dRAG.

</details>


### [7] [Provable Repair of Deep Neural Network Defects by Preimage Synthesis and Property Refinement](https://arxiv.org/abs/2511.07741)
*Jianan Ma,Jingyi Wang,Qi Xuan,Zhen Wang*

Main category: cs.CR

TL;DR: ProRepair是一个基于形式化前像合成和属性细化的可证明神经网络修复框架，能够统一修复多种安全威胁导致的神经网络缺陷，在有效性和可扩展性上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络面临各种安全威胁（如后门攻击、对抗攻击、安全属性违反），现有修复技术存在缺乏保证、可扩展性有限、开销大等局限性，需要一种统一的修复框架来应对实际场景中的安全威胁。

Method: 通过合成精确代理框来表征特征空间前像，推导出有界距离项指导修复；同时进行属性细化以实现精确修正并扩展到更复杂的任务。

Result: 在四个安全威胁修复任务和六个基准测试中，ProRepair在有效性、效率和可扩展性上均优于现有方法。点修复速度提升5-2000倍，区域修复成功修复所有36个安全属性违反实例（现有最佳方法仅修复8个），可处理18倍高维空间。

Conclusion: ProRepair提供了一个统一的可证明神经网络修复框架，能够有效应对多种安全威胁，在保持模型性能的同时显著提升泛化能力和修复效率，为实际应用场景提供了潜在的解决方案。

Abstract: It is known that deep neural networks may exhibit dangerous behaviors under various security threats (e.g., backdoor attacks, adversarial attacks and safety property violation) and there exists an ongoing arms race between attackers and defenders. In this work, we propose a complementary perspective to utilize recent progress on "neural network repair" to mitigate these security threats and repair various kinds of neural network defects (arising from different security threats) within a unified framework, offering a potential silver bullet solution to real-world scenarios. To substantially push the boundary of existing repair techniques (suffering from limitations such as lack of guarantees, limited scalability, considerable overhead, etc) in addressing more practical contexts, we propose ProRepair, a novel provable neural network repair framework driven by formal preimage synthesis and property refinement. The key intuitions are: (i) synthesizing a precise proxy box to characterize the feature space preimage, which can derive a bounded distance term sufficient to guide the subsequent repair step towards the correct outputs, and (ii) performing property refinement to enable surgical corrections and scale to more complex tasks. We evaluate ProRepair across four security threats repair tasks on six benchmarks and the results demonstrate it outperforms existing methods in effectiveness, efficiency and scalability. For point-wise repair, ProRepair corrects models while preserving performance and achieving significantly improved generalization, with a speedup of 5x to 2000x over existing provable approaches. In region-wise repair, ProRepair successfully repairs all 36 safety property violation instances (compared to 8 by the best existing method), and can handle 18x higher dimensional spaces.

</details>


### [8] [SALT: Steering Activations towards Leakage-free Thinking in Chain of Thought](https://arxiv.org/abs/2511.07772)
*Shourya Batra,Pierce Tillman,Samarth Gaggar,Shashank Kesineni,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Vasu Sharma,Maheep Chaudhary*

Main category: cs.CR

TL;DR: SALT是一种轻量级测试时干预方法，通过向隐藏状态注入定向引导向量来减少大语言模型在思维链推理过程中的隐私泄露，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为个人助手处理敏感数据时，存在内部推理过程泄露隐私信息的问题，即使最终输出看起来安全。需要在不损害模型推理能力的前提下防止隐私泄露。

Method: 识别高泄露层，在隐藏状态中注入定向引导向量，引导模型进行无泄露思考。

Result: 在多个LLM上实验显示，SALT显著降低上下文隐私泄露：QwQ-32B降低18.2%，Llama-3.1-8B降低17.9%，Deepseek降低31.2%，同时保持可比的任务性能。

Conclusion: SALT为具备推理能力的语言模型提供了一种实用的测试时隐私保护方法，为LLM基个人代理的安全部署开辟了道路。

Abstract: As Large Language Models (LLMs) evolve into personal assistants with access to sensitive user data, they face a critical privacy challenge: while prior work has addressed output-level privacy, recent findings reveal that LLMs often leak private information through their internal reasoning processes, violating contextual privacy expectations. These leaky thoughts occur when models inadvertently expose sensitive details in their reasoning traces, even when final outputs appear safe. The challenge lies in preventing such leakage without compromising the model's reasoning capabilities, requiring a delicate balance between privacy and utility. We introduce Steering Activations towards Leakage-free Thinking (SALT), a lightweight test-time intervention that mitigates privacy leakage in model's Chain of Thought (CoT) by injecting targeted steering vectors into hidden state. We identify the high-leakage layers responsible for this behavior. Through experiments across multiple LLMs, we demonstrate that SALT achieves reductions including $18.2\%$ reduction in CPL on QwQ-32B, $17.9\%$ reduction in CPL on Llama-3.1-8B, and $31.2\%$ reduction in CPL on Deepseek in contextual privacy leakage dataset AirGapAgent-R while maintaining comparable task performance and utility. Our work establishes SALT as a practical approach for test-time privacy protection in reasoning-capable language models, offering a path toward safer deployment of LLM-based personal agents.

</details>


### [9] [HybridGuard: Enhancing Minority-Class Intrusion Detection in Dew-Enabled Edge-of-Things Networks](https://arxiv.org/abs/2511.07793)
*Binayak Kara,Ujjwal Sahua,Ciza Thomas,Jyoti Prakash Sahoo*

Main category: cs.CR

TL;DR: HybridGuard是一个集成机器学习和深度学习的入侵检测框架，通过特征选择和数据增强技术解决EoT网络中的数据不平衡问题，在多个数据集上表现出优于现有解决方案的性能。


<details>
  <summary>Details</summary>
Motivation: 保护雾计算边缘物联网网络免受复杂入侵攻击是一个关键挑战，需要解决数据不平衡问题以提高对少数攻击类别的检测性能。

Method: 采用基于互信息的特征选择，使用WCGAN-GP减少类别不平衡，采用DualNetShield双阶段架构进行高级流量分析和异常检测。

Result: 在UNSW-NB15、CIC-IDS-2017和IOTID20数据集上表现出强大性能，能够适应不断演变的网络安全威胁。

Conclusion: HybridGuard被证明是保护EoT网络免受现代入侵攻击的有效工具，在复杂环境中实现了精细化的威胁识别。

Abstract: Securing Dew-Enabled Edge-of-Things (EoT) networks against sophisticated intrusions is a critical challenge. This paper presents HybridGuard, a framework that integrates machine learning and deep learning to improve intrusion detection. HybridGuard addresses data imbalance through mutual information based feature selection, ensuring that the most relevant features are used to improve detection performance, especially for minority attack classes. The framework leverages Wasserstein Conditional Generative Adversarial Networks with Gradient Penalty (WCGAN-GP) to further reduce class imbalance and enhance detection precision. It adopts a two-phase architecture called DualNetShield to support advanced traffic analysis and anomaly detection, improving the granular identification of threats in complex EoT environments. HybridGuard is evaluated on the UNSW-NB15, CIC-IDS-2017, and IOTID20 datasets, where it demonstrates strong performance across diverse attack scenarios and outperforms existing solutions in adapting to evolving cybersecurity threats. This approach establishes HybridGuard as an effective tool for protecting EoT networks against modern intrusions.

</details>


### [10] [PRISM: Privacy-preserving Inference System with Homomorphic Encryption and Modular Activation](https://arxiv.org/abs/2511.07807)
*Zeinab Elkhatib,Ali Sekmen,Kamrul Hasan*

Main category: cs.CR

TL;DR: 提出了一种优化框架，用同态加密兼容的近似函数替换CNN中的非线性激活函数，在CIFAR-10上实现94.4%准确率，平衡了隐私保护和计算效率。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在关键基础设施部署时面临数据隐私问题，同态加密虽然能保护隐私但与传统CNN不兼容，因为CNN依赖非线性激活函数。

Method: 重构CNN架构，用同态加密兼容的近似方法替换标准非线性函数，采用4次多项式和Softplus激活函数，在CKKS方案下实现安全计算。

Result: 在CIFAR-10数据集上达到94.4%准确率，单个加密样本处理时间2.42秒，10000个加密样本处理时间24000秒。

Conclusion: 该框架成功解决了同态加密与CNN的兼容性问题，在保持高精度的同时实现了隐私保护，为安全机器学习部署提供了可行方案。

Abstract: With the rapid advancements in machine learning, models have become increasingly capable of learning and making predictions in various industries. However, deploying these models in critical infrastructures presents a major challenge, as concerns about data privacy prevent unrestricted data sharing. Homomor- phic encryption (HE) offers a solution by enabling computations on encrypted data, but it remains incompatible with machine learning models like convolutional neural networks (CNNs), due to their reliance on non-linear activation functions. To bridge this gap, this work proposes an optimized framework that replaces standard non-linear functions with homomorphically compatible approximations, ensuring secure computations while minimizing computational overhead. The proposed approach restructures the CNN architecture and introduces an efficient activation function approximation method to mitigate the performance trade-offs in- troduced by encryption. Experiments on CIFAR-10 achieve 94.4% accuracy with 2.42 s per single encrypted sample and 24,000 s per 10,000 encrypted samples, using a degree-4 polynomial and Softplus activation under CKKS, balancing accuracy and privacy.

</details>


### [11] [Blockchain-Integrated Privacy-Preserving Medical Insurance Claim Processing Using Homomorphic Encryption](https://arxiv.org/abs/2511.07818)
*Diya Mamoria,Harshit Jain,Aswani Kumar Cherukuri*

Main category: cs.CR

TL;DR: 提出一个结合区块链和同态加密的去中心化医疗保险理赔框架，在保护患者隐私的同时实现透明可信的理赔处理


<details>
  <summary>Details</summary>
Motivation: 解决医疗保险理赔系统中的隐私保护、数据安全和信任问题，传统系统难以同时满足透明度和隐私保护的需求

Method: 使用区块链技术创建不可篡改的分布式交易账本，结合同态加密技术让保险公司能在加密数据上直接进行理赔审核和赔付操作，无需解密患者数据

Result: 构建了一个既能保护患者隐私又能实现透明理赔处理的系统框架，显著降低了第三方处理过程中的隐私风险

Conclusion: 区块链和同态加密技术的结合能够实现透明度和隐私保护的共存，为医疗保险理赔系统提供可靠、高效且隐私保护的架构

Abstract: This research proposes a decentralized and cryptographically secure framework to address the most acute issues of privacy, data security, and protection in the ecosystem of medical insurance claim processing. The scope of this study focuses on enabling the management of insurance claims in a transparent, privacy-protecting manner while maintaining the efficiency and trust level needed by the patients, healthcare providers, and insurers. To accomplish this, the proposed system adds blockchain technology to provide an unchangeable, decentralized, and auditable claim transactions ledger which enhances overall claim-related processes and trust among all stakeholders. To protect critical patient information, the framework employs homomorphic encryption a modern form of cryptography to allow authorized insurance providers to perform necessary operations like claim adjudication and reimbursement on encrypted medical records without any decryption during the process. This method significantly reduces the third-party processing privacy risk because patient data can be kept secret even when third-party processing is done. In addition, smart contracts improve automation of the most important procedures in the claim processing pipeline, which decreases manual, operational, and susceptibility towards human blunders or deceitful acts. The integration of these two transformative technologiesblockchain and homomorphic encryption represents the core contribution of this work, enabling the coexistence of transparency and privacy which are usually viewed as competing objectives in traditional systems. As a result, these technologies are expected to foster the creation of a reliable, effective, and privacy safeguarding architecture that could transform the medical claim submission systems paradigm.

</details>


### [12] [CAHICHA: Computer Automated Hardware Interaction test to tell Computer and Humans Apart](https://arxiv.org/abs/2511.07841)
*Aditya Mitra,Sibi Chakkaravarthy Sethuraman,Devi Priya V S*

Main category: cs.CR

TL;DR: 提出了一种基于硬件交互信号和可信硬件用户存在标志的新型人类验证技术，用于区分真实用户与高级自动化机器人。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的发展，传统的人类验证方法（如语音验证码和基于知识的认证）已不再有效，AI机器人能够模拟人类行为并绕过安全测试，导致凭证填充、账户滥用等威胁，需要新的方法来识别真实人类用户。

Method: 利用人类交互信号和来自可信硬件的加密认证用户存在标志，验证真实的物理用户参与，提供安全可靠的方式来区分真实用户与自动化机器人。

Result: 系统在性能、可用性和安全性方面经过全面评估，在长时间并发用户需求下表现出稳定的吞吐量和零请求失败，显示了良好的操作可靠性、高效的负载处理能力和底层架构的鲁棒性。

Conclusion: 该系统为当前的人类验证方法提供了更安全、更有效且更易于使用的替代方案。

Abstract: As automation bot technology and Artificial Intelligence is evolving rapidly, conventional human verification techniques like voice CAPTCHAs and knowledge-based authentication are becoming less effective. Bots and scrapers with Artificial Intelligence (AI) capabilities can now detect and solve visual challenges, emulate human like typing patterns, and avoid most security tests, leading to high-volume threats like credential stuffing, account abuse, ad fraud, and automated scalping. This leaves a vital gap in identifying real human users versus advanced bots. We present a novel technique for distinguishing real human users based on hardware interaction signals to address this issue. In contrast to conventional approaches, our method leverages human interactions and a cryptographically attested User Presence (UP) flag from trusted hardware to verify genuine physical user engagement providing a secure and reliable way to distinguish authentic users from automated bots or scripted routines. The suggested approach was thoroughly assessed in terms of performance, usability, and security. The system demonstrated consistent throughput and zero request failures under prolonged concurrent user demand, indicating good operational reliability, efficient load handling, and the underlying architecture's robustness. These thorough analyses support the conclusion that the suggested system provides a safer, more effective, and easier-to-use substitute for current human verification methods.

</details>


### [13] [LoopLLM: Transferable Energy-Latency Attacks in LLMs via Repetitive Generation](https://arxiv.org/abs/2511.07876)
*Xingyu Li,Xiaolei Liu,Cheng Liu,Yixiao Xu,Kangyi Ding,Bangzhou Xin,Jia-Li Yin*

Main category: cs.CR

TL;DR: LoopLLM是一个针对大语言模型的能耗-延迟攻击框架，通过诱导重复生成触发低熵解码循环，迫使LLM生成到输出限制，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有攻击方法通过延迟终止符号来延长输出，但随着输出增长难以控制终止符号，效果有限。需要更有效的能耗-延迟攻击方法。

Method: 提出LoopLLM框架：(1)重复诱导提示优化，利用自回归漏洞诱导重复生成；(2)令牌对齐集成优化，聚合梯度提升跨模型可转移性。

Result: 在12个开源和2个商业LLM上的实验显示，LoopLLM达到最大输出长度的90%以上，而基线方法仅为20%，对DeepSeek-V3和Gemini 2.5 Flash的可转移性提升约40%。

Conclusion: LoopLLM通过触发低熵解码循环有效实现能耗-延迟攻击，显著优于现有方法，并具有更好的跨模型可转移性。

Abstract: As large language models (LLMs) scale, their inference incurs substantial computational resources, exposing them to energy-latency attacks, where crafted prompts induce high energy and latency cost. Existing attack methods aim to prolong output by delaying the generation of termination symbols. However, as the output grows longer, controlling the termination symbols through input becomes difficult, making these methods less effective. Therefore, we propose LoopLLM, an energy-latency attack framework based on the observation that repetitive generation can trigger low-entropy decoding loops, reliably compelling LLMs to generate until their output limits. LoopLLM introduces (1) a repetition-inducing prompt optimization that exploits autoregressive vulnerabilities to induce repetitive generation, and (2) a token-aligned ensemble optimization that aggregates gradients to improve cross-model transferability. Extensive experiments on 12 open-source and 2 commercial LLMs show that LoopLLM significantly outperforms existing methods, achieving over 90% of the maximum output length, compared to 20% for baselines, and improving transferability by around 40% to DeepSeek-V3 and Gemini 2.5 Flash.

</details>


### [14] [Class-feature Watermark: A Resilient Black-box Watermark Against Model Extraction Attacks](https://arxiv.org/abs/2511.07947)
*Yaxin Xiao,Qingqing Ye,Zi Liang,Haoyang Li,RongHua Li,Huadi Zheng,Haibo Hu*

Main category: cs.CR

TL;DR: 提出了WRK攻击方法有效移除现有水印，并开发了CFW水印方案来增强对模型提取攻击和移除攻击的抵抗力。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒水印方法通过表示纠缠来保证在模型提取攻击中的生存性，但低估了顺序模型提取攻击和移除攻击的风险。

Method: WRK攻击利用决策边界绕过纠缠约束；CFW水印通过创建合成类和使用域外样本消除易受攻击的决策边界。

Result: WRK攻击将水印成功率降低至少88.79%；CFW水印在MEA和WRK联合攻击下仍保持至少70.15%的水印成功率。

Conclusion: CFW水印在多个领域都优于现有方法，在保持模型效用的同时显著提高了水印的鲁棒性。

Abstract: Machine learning models constitute valuable intellectual property, yet remain vulnerable to model extraction attacks (MEA), where adversaries replicate their functionality through black-box queries. Model watermarking counters MEAs by embedding forensic markers for ownership verification. Current black-box watermarks prioritize MEA survival through representation entanglement, yet inadequately explore resilience against sequential MEAs and removal attacks. Our study reveals that this risk is underestimated because existing removal methods are weakened by entanglement. To address this gap, we propose Watermark Removal attacK (WRK), which circumvents entanglement constraints by exploiting decision boundaries shaped by prevailing sample-level watermark artifacts. WRK effectively reduces watermark success rates by at least 88.79% across existing watermarking benchmarks.
  For robust protection, we propose Class-Feature Watermarks (CFW), which improve resilience by leveraging class-level artifacts. CFW constructs a synthetic class using out-of-domain samples, eliminating vulnerable decision boundaries between original domain samples and their artifact-modified counterparts (watermark samples). CFW concurrently optimizes both MEA transferability and post-MEA stability. Experiments across multiple domains show that CFW consistently outperforms prior methods in resilience, maintaining a watermark success rate of at least 70.15% in extracted models even under the combined MEA and WRK distortion, while preserving the utility of protected models.

</details>


### [15] [From LLMs to Agents: A Comparative Evaluation of LLMs and LLM-based Agents in Security Patch Detection](https://arxiv.org/abs/2511.08060)
*Junxiao Han,Zheng Yu,Lingfeng Bao,Jiakun Liu,Yao Wan,Jianwei Yin,Shuiguang Deng,Song Han*

Main category: cs.CR

TL;DR: 本文系统评估了LLM和基于LLM的代理在安全补丁检测中的性能，发现数据增强LLM方法表现最佳，而ReAct代理具有最低的误报率。


<details>
  <summary>Details</summary>
Motivation: 开源软件的广泛使用带来了安全风险，LLM在软件工程任务中表现出色，但缺乏对其在安全补丁检测能力的系统性评估。

Method: 评估了三种方法：普通LLM（带系统提示的单LLM）、数据增强LLM（基于普通LLM的数据增强）和ReAct代理（利用思维-行动-观察机制），并比较了商业和开源LLM的性能。

Result: 数据增强LLM实现最佳整体性能，ReAct代理具有最低的误报率。基线方法虽然准确率高但误报率显著更高，而评估方法在保持可比准确率的同时大幅降低了误报率。

Conclusion: LLM和基于LLM的代理在安全补丁检测中具有实际应用价值，能够在保持稳健性能的同时最小化误报率。

Abstract: The widespread adoption of open-source software (OSS) has accelerated software innovation but also increased security risks due to the rapid propagation of vulnerabilities and silent patch releases. In recent years, large language models (LLMs) and LLM-based agents have demonstrated remarkable capabilities in various software engineering (SE) tasks, enabling them to effectively address software security challenges such as vulnerability detection. However, systematic evaluation of the capabilities of LLMs and LLM-based agents in security patch detection remains limited. To bridge this gap, we conduct a comprehensive evaluation of the performance of LLMs and LLM-based agents for security patch detection. Specifically, we investigate three methods: Plain LLM (a single LLM with a system prompt), Data-Aug LLM (data augmentation based on the Plain LLM), and the ReAct Agent (leveraging the thought-action-observation mechanism). We also evaluate the performance of both commercial and open-source LLMs under these methods and compare these results with those of existing baselines. Furthermore, we analyze the detection performance of these methods across various vulnerability types, and examine the impact of different prompting strategies and context window sizes on the results. Our findings reveal that the Data-Aug LLM achieves the best overall performance, whereas the ReAct Agent demonstrates the lowest false positive rate (FPR). Although baseline methods exhibit strong accuracy, their false positive rates are significantly higher. In contrast, our evaluated methods achieve comparable accuracy while substantially reducing the FPR. These findings provide valuable insights into the practical applications of LLMs and LLM-based agents in security patch detection, highlighting their advantage in maintaining robust performance while minimizing false positive rates.

</details>


### [16] [FedPoP: Federated Learning Meets Proof of Participation](https://arxiv.org/abs/2511.08207)
*Devriş İşler,Elina van Kempen,Seoyeon Hwang,Nikolaos Laoutaris*

Main category: cs.CR

TL;DR: FedPoP是一个联邦学习框架，允许客户端匿名证明参与模型训练，保护隐私且无需大量计算或公共账本，与现有安全聚合协议兼容，实际部署中仅增加0.97秒每轮开销。


<details>
  <summary>Details</summary>
Motivation: 随着模型成为可货币化的数字资产，需要证明参与训练过程以确立所有权，同时保护客户端匿名性和隐私。

Method: 提出FedPoP框架，与现有安全聚合协议集成，实现不可链接的参与证明，无需大量计算或公共账本。

Result: 原型实现显示每轮仅增加0.97秒开销，客户端向第三方证明参与仅需0.0612秒，在现实客户端退出情况下表现良好。

Conclusion: FedPoP适用于需要可审计参与但不牺牲隐私的实际部署场景，具有实用性。

Abstract: Federated learning (FL) offers privacy preserving, distributed machine learning, allowing clients to contribute to a global model without revealing their local data. As models increasingly serve as monetizable digital assets, the ability to prove participation in their training becomes essential for establishing ownership. In this paper, we address this emerging need by introducing FedPoP, a novel FL framework that allows nonlinkable proof of participation while preserving client anonymity and privacy without requiring either extensive computations or a public ledger. FedPoP is designed to seamlessly integrate with existing secure aggregation protocols to ensure compatibility with real-world FL deployments. We provide a proof of concept implementation and an empirical evaluation under realistic client dropouts. In our prototype, FedPoP introduces 0.97 seconds of per-round overhead atop securely aggregated FL and enables a client to prove its participation/contribution to a model held by a third party in 0.0612 seconds. These results indicate FedPoP is practical for real-world deployments that require auditable participation without sacrificing privacy.

</details>


### [17] [Publish Your Threat Models! The benefits far outweigh the dangers](https://arxiv.org/abs/2511.08295)
*Loren Kohnfelder,Adam Shostack*

Main category: cs.CR

TL;DR: 论文探讨了公共威胁模型(PTM)如何向他人传递有用的安全信息，鼓励技术社区公开分享PTM，使供应链中每个组件的安全属性得以知晓。


<details>
  <summary>Details</summary>
Motivation: 内部威胁模型可能不适合直接披露，但公开分享PTM可以让技术提供商展示其安全工作以获得竞争优势，同时让客户能够要求并评估PTM，而不仅仅是被告知"它是安全的"。

Method: 提供关于编辑和审查的指导，以及何时更新模型（已发布或未发布），列出早期采用者先例，解释诸多好处，并解决潜在异议。

Result: 许多优秀产品已经拥有良好的威胁模型，将其转化为PTM是一个相对较小的任务，可以成为新的规范。

Conclusion: 鼓励技术社区公开分享他们的PTM，使供应链中每个组件的安全属性得以知晓，这应该（并且很容易）成为新的规范。

Abstract: Threat modeling has long guided software development work, and we consider how Public Threat Models (PTM) can convey useful security information to others. We list some early adopter precedents, explain the many benefits, address potential objections, and cite regulatory drivers. Internal threat models may not be directly suitable for disclosure so we provide guidance for redaction and review, as well as when to update models (published or not). In a concluding call to action, we encourage the technology community to openly share their PTMs so the security properties of each component are known up and down the supply chain. Technology providers proud of their security efforts can show their work for competitive advantage, and customers can ask for and evaluate PTMs rather than be told "it's secure" but little more. Many great products already have fine threat models, and turning those into PTMs is a relatively minor task, so we argue this should (and easily could) become the new norm.

</details>


### [18] [Plaintext Structure Vulnerability: Robust Cipher Identification via a Distributional Randomness Fingerprint Feature Extractor](https://arxiv.org/abs/2511.08296)
*Xiwen Ren,Min Luo,Cong Peng,Debiao He*

Main category: cs.CR

TL;DR: 提出一种基于统计测试的加密算法识别方法，通过计算密文的随机性特征并构建算法指纹，在跨域场景下保持高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统加密算法识别方法在测试数据与训练数据分布不同时性能显著下降，暴露出特征提取器对明文特征的隐藏依赖。

Method: 采用非端到端学习方法，基于统计测试计算密文随机性特征，利用该特征的频率分布模式构建算法指纹。

Result: 在Canterbury Corpus数据集上达到高判别性能（AUC > 0.98），在跨域评估中基线模型性能显著下降，而本方法在结构化域间转移时性能下降极小，在纯随机数据集上仍保持高排序能力（AUC > 0.90）。

Conclusion: 该方法能有效减少对明文特征的依赖，在跨域场景下具有高鲁棒性，为加密算法识别提供了更可靠的解决方案。

Abstract: Modern encryption algorithms form the foundation of digital security. However, the widespread use of encryption algorithms results in significant challenges for network defenders in identifying which specific algorithms are being employed. More importantly, we find that when the plaintext distribution of test data departs from the training data, the performance of classifiers often declines significantly. This issue exposes the feature extractor's hidden dependency on plaintext features. To reduce this dependency, we adopt a method that does not learn end-to-end from ciphertext bytes. Specifically, this method is based on a set of statistical tests to compute the randomness feature of the ciphertext, and then uses the frequency distribution pattern of this feature to construct the algorithms' respective fingerprints. The experimental results demonstrate that our method achieves high discriminative performance (e.g., AUC > 0.98) in the Canterbury Corpus dataset, which contains a diverse set of data types. Furthermore, in our cross-domain evaluation, baseline models' performance degrades significantly when tested on data with a reduced proportion of structured plaintext. In sharp contrast, our method demonstrates high robustness: performance degradation is minimal when transferring between different structured domains, and even on the most challenging purely random dataset, it maintains a high level of ranking ability (AUC > 0.90).

</details>


### [19] [Revisiting Network Traffic Analysis: Compatible network flows for ML models](https://arxiv.org/abs/2511.08345)
*João Vitorino,Daniela Pinto,Eva Maia,Ivone Amorim,Isabel Praça*

Main category: cs.CR

TL;DR: 研究不同网络流量流导出器生成的相似特征对机器学习模型泛化能力和鲁棒性的影响，通过重新分析PCAP文件生成新特征来改进物联网入侵检测。


<details>
  <summary>Details</summary>
Motivation: 为确保机器学习模型能够稳健检测和分类网络攻击，需要高质量数据集。但准确表示物联网网络中的复杂攻击流量模式很困难，特别是不同流量导出器生成的特征可能影响模型性能。

Method: 使用HERA工具重新分析Bot-IoT、IoT-23和CICIoT23数据集的原始PCAP文件，生成新的标记流量和一致特征，创建新的CSV版本，并与原始版本进行比较来微调多个模型。

Result: 结果表明，直接分析和预处理PCAP文件（而非仅使用常见的CSV文件）能够计算更相关的特征来训练bagging和梯度提升决策树集成模型。

Conclusion: 需要持续改进特征提取和特征选择过程，使不同数据集更加兼容，从而实现对网络安全解决方案中机器学习模型的可靠评估和比较。

Abstract: To ensure that Machine Learning (ML) models can perform a robust detection and classification of cyberattacks, it is essential to train them with high-quality datasets with relevant features. However, it can be difficult to accurately represent the complex traffic patterns of an attack, especially in Internet-of-Things (IoT) networks. This paper studies the impact that seemingly similar features created by different network traffic flow exporters can have on the generalization and robustness of ML models. In addition to the original CSV files of the Bot-IoT, IoT-23, and CICIoT23 datasets, the raw network packets of their PCAP files were analysed with the HERA tool, generating new labelled flows and extracting consistent features for new CSV versions. To assess the usefulness of these new flows for intrusion detection, they were compared with the original versions and were used to fine-tune multiple models. Overall, the results indicate that directly analysing and preprocessing PCAP files, instead of just using the commonly available CSV files, enables the computation of more relevant features to train bagging and gradient boosting decision tree ensembles. It is important to continue improving feature extraction and feature selection processes to make different datasets more compatible and enable a trustworthy evaluation and comparison of the ML models used in cybersecurity solutions.

</details>


### [20] [Endpoint Security Agent: A Comprehensive Approach to Real-time System Monitoring and Threat Detection](https://arxiv.org/abs/2511.08352)
*Srihari R,Ayesha Taranum,Karthik,Mohammed Usman Hussain*

Main category: cs.CR

TL;DR: 提出了一个用于Windows端点的模块化实时安全解决方案，利用WMI和ETW进行系统活动监控，采用机器学习检测引擎进行威胁识别，并将检测技术映射到MITRE ATT&CK框架。


<details>
  <summary>Details</summary>
Motivation: 随着网络威胁日益复杂和频繁，组织需要强大的端点保护来确保安全。

Method: 利用WMI和ETW进行低层系统活动监控，包括进程执行、注册表修改和网络行为；使用基于机器学习的检测引擎，在标记的良性/恶意活动数据集上训练；检测技术映射到MITRE ATT&CK框架；设计为可扩展系统，包含集中式告警和取证分析界面。

Result: 初步评估显示在检测多样化攻击向量方面具有高准确性和效率的令人鼓舞的结果。

Conclusion: 该端点安全代理提供了一个全面的实时系统监控和威胁检测方法，能够有效应对不断演变的网络威胁。

Abstract: As cyber threats continue to evolve in complexity and frequency, robust endpoint protection is essential for organizational security. This paper presents "Endpoint Security Agent: A Comprehensive Approach to Real-time System Monitoring and Threat Detection" a modular, real-time security solution for Windows endpoints. The agent leverages native tools like WMI and ETW for lowlevel monitoring of system activities such as process execution, registry modifications, and network behaviour. A machine learning-based detection engine, trained on labelled datasets of benign and malicious activity, enables accurate threat identification with minimal false positives. Detection techniques are mapped to the MITRE ATT&CK framework for standardized threat classification. Designed for extensibility, the system includes a centralized interface for alerting and forensic analysis. Preliminary evaluation shows promising results in detecting diverse attack vectors with high accuracy and efficiency.

</details>


### [21] [Why does weak-OOD help? A Further Step Towards Understanding Jailbreaking VLMs](https://arxiv.org/abs/2511.08367)
*Yuxuan Zhou,Yuzhao Peng,Yang Bai,Kuofeng Gao,Yihao Zhang,Yechao Zhang,Xun Chen,Tao Yu,Tao Dai,Shu-Tao Xia*

Main category: cs.CR

TL;DR: 研究发现基于弱OOD策略的越狱样本在绕过VLM安全约束方面表现更优，揭示了输入意图感知和模型拒绝触发之间的权衡关系，并基于OCR能力设计出高效的VLM越狱方法。


<details>
  <summary>Details</summary>
Motivation: 深入理解基于OOD策略的VLM越狱方法，特别是弱OOD现象背后的机制，以揭示VLM安全漏洞的根本原因。

Method: 以SI-Attack为研究对象，分析输入意图感知和模型拒绝触发的权衡关系，并从模型预训练和对齐过程的差异角度提供理论解释，最后基于OCR能力设计新的越狱方法。

Result: 实验证明弱OOD策略生成的越狱样本在绕过VLM安全约束方面表现更优，新设计的基于OCR的越狱方法性能优于现有SOTA基线。

Conclusion: VLM的预训练和对齐过程存在固有差异，导致输入意图感知和拒绝触发机制对OOD操作的不一致响应，这是弱OOD现象的根本原因，也为设计更有效的越狱方法提供了理论基础。

Abstract: Large Vision-Language Models (VLMs) are susceptible to jailbreak attacks: researchers have developed a variety of attack strategies that can successfully bypass the safety mechanisms of VLMs. Among these approaches, jailbreak methods based on the Out-of-Distribution (OOD) strategy have garnered widespread attention due to their simplicity and effectiveness. This paper further advances the in-depth understanding of OOD-based VLM jailbreak methods. Experimental results demonstrate that jailbreak samples generated via mild OOD strategies exhibit superior performance in circumventing the safety constraints of VLMs--a phenomenon we define as ''weak-OOD''. To unravel the underlying causes of this phenomenon, this study takes SI-Attack, a typical OOD-based jailbreak method, as the research object. We attribute this phenomenon to a trade-off between two dominant factors: input intent perception and model refusal triggering. The inconsistency in how these two factors respond to OOD manipulations gives rise to this phenomenon. Furthermore, we provide a theoretical argument for the inevitability of such inconsistency from the perspective of discrepancies between model pre-training and alignment processes. Building on the above insights, we draw inspiration from optical character recognition (OCR) capability enhancement--a core task in the pre-training phase of mainstream VLMs. Leveraging this capability, we design a simple yet highly effective VLM jailbreak method, whose performance outperforms that of SOTA baselines.

</details>


### [22] [Blockly2Hooks: Smart Contracts for Everyone with the XRP Ledger and Google Blockly](https://arxiv.org/abs/2511.08403)
*Lucian Trestioreanu,Wazen Shbair,Flaviene Scheidt de Cristo,Radu State*

Main category: cs.CR

TL;DR: 开发了Blockly2Hooks工具，使用Google的Blockly可视化编程库，帮助非专家用户通过可视化方式学习智能合约开发，降低学习门槛。


<details>
  <summary>Details</summary>
Motivation: 智能合约的广泛应用受到安全、可用性和成本限制，目前主要由专业开发者处理，研究重点在安全方面而可用性被忽视。需要特定工具降低入门门槛，让非专家也能创建智能合约。

Method: 设计、开发和测试Blockly2Hooks解决方案，利用Google的Blockly可视化编程库，以XRP Ledger为具体案例，通过可视化编程语言帮助非专家学习智能合约。

Result: 平台经过开发和测试，结果显示能够使学习智能合约开发更加顺畅。

Conclusion: Blockly2Hooks通过可视化编程方法成功降低了智能合约的学习门槛，为非专家用户提供了有效的学习工具。

Abstract: Recent technologies such as inter-ledger payments, non-fungible tokens, and smart contracts are all fruited from the ongoing development of Distributed Ledger Technologies. The foreseen trend is that they will play an increasingly visible role in daily life, which will have to be backed by appropriate operational resources. For example, due to increasing demand, smart contracts could soon face a shortage of knowledgeable users and tools to handle them in practice. Widespread smart contract adoption is currently limited by security, usability and costs aspects. Because of a steep learning curve, the handling of smart contracts is currently performed by specialised developers mainly, and most of the research effort is focusing on smart contract security, while other aspects like usability being somewhat neglected. Specific tools would lower the entry barrier, enabling interested non-experts to create smart contracts.
  In this paper we designed, developed and tested Blockly2Hooks, a solution towards filling this gap even in challenging scenarios such as when the smart contracts are written in an advanced language like C. With the XRP Ledger as a concrete working case, Blockly2Hooks helps interested non-experts from the community to learn smart contracts easily and adopt the technology, through leveraging well-proven teaching methodologies like Visual Programming Languages, and more specifically, the Blockly Visual Programming library from Google. The platform was developed and tested and the results are promising to make learning smart contract development smoother.

</details>


### [23] [Coverage-Guided Pre-Silicon Fuzzing of Open-Source Processors based on Leakage Contracts](https://arxiv.org/abs/2511.08443)
*Gideon Geier,Pariya Hajipour,Jan Reineke*

Main category: cs.CR

TL;DR: 提出了一种基于覆盖引导的硬件-软件泄漏契约模糊测试方法，通过自组合框架将信息泄漏转化为微架构状态差异，使用新的安全导向覆盖度量SCD来发现违反泄漏契约的执行路径。


<details>
  <summary>Details</summary>
Motivation: 硬件-软件泄漏契约是规范现代处理器侧信道安全的形式化方法，但验证复杂硬件设计是否符合契约仍具挑战性。现有验证方法难以扩展到工业级设计，而硬件模糊测试方法主要针对功能正确性错误，无法检测信息泄漏。

Method: 采用自组合框架使信息泄漏直接表现为微架构状态差异，提出新的安全导向覆盖度量SCD，引导模糊测试器探索违反泄漏契约的执行路径。

Result: 在两个开源RISC-V核心（Rocket Core和BOOM核心）上的广泛评估表明，覆盖引导策略优于无引导模糊测试，增加的微架构覆盖能更快发现BOOM核心中的安全漏洞。

Conclusion: 覆盖引导的硬件-软件契约模糊测试是一种可扩展的方法，能够有效发现复杂处理器设计中的信息泄漏漏洞。

Abstract: Hardware-software leakage contracts have emerged as a formalism for specifying side-channel security guarantees of modern processors, yet verifying that a complex hardware design complies with its contract remains a major challenge. While verification provides strong guarantees, current verification approaches struggle to scale to industrial-sized designs. Conversely, prevalent hardware fuzzing approaches are designed to find functional correctness bugs, but are blind to information leaks like Spectre.
  To bridge this gap, we introduce a novel and scalable approach: coverage-guided hardware-software contract fuzzing. Our methodology leverages a self-compositional framework to make information leakage directly observable as microarchitectural state divergence. The core of our contribution is a new, security-oriented coverage metric, Self-Composition Deviation (SCD), which guides the fuzzer to explore execution paths that violate the leakage contract. We implemented this approach and performed an extensive evaluation on two open-source RISC-V cores: the in-order Rocket Core and the complex out-of-order BOOM core. Our results demonstrate that coverage-guided strategies outperform unguided fuzzing and that increased microarchitectural coverage leads to a faster discovery of security vulnerabilities in the BOOM core.

</details>


### [24] [QLCoder: A Query Synthesizer For Static Analysis of Security Vulnerabilities](https://arxiv.org/abs/2511.08462)
*Claire Wang,Ziyang Li,Saikat Dutta,Mayur Naik*

Main category: cs.CR

TL;DR: QLCoder是一个基于LLM的代理框架，能够从CVE元数据自动合成CodeQL安全查询，通过执行反馈循环和结构化约束提高查询生成成功率。


<details>
  <summary>Details</summary>
Motivation: 编写静态分析工具的安全查询需要安全知识和程序分析专业知识，这具有挑战性。QLCoder旨在自动化这一过程，直接从CVE元数据生成有效的安全查询。

Method: 在合成循环中嵌入LLM并利用执行反馈，通过自定义MCP接口约束推理过程，该接口支持与语言服务器协议（语法指导）和RAG数据库（语义检索）的结构化交互。

Result: 在176个Java项目的CVE评估中，QLCoder成功生成了53.4%的正确查询（在易受攻击版本中检测到CVE但在修复版本中不检测），而仅使用Claude Code只能生成10%的正确查询。

Conclusion: QLCoder通过结合LLM、执行反馈和结构化约束，显著提高了从CVE元数据自动生成有效安全查询的成功率，证明了该方法在自动化安全查询合成方面的有效性。

Abstract: Static analysis tools provide a powerful means to detect security vulnerabilities by specifying queries that encode vulnerable code patterns. However, writing such queries is challenging and requires diverse expertise in security and program analysis. To address this challenge, we present QLCoder - an agentic framework that automatically synthesizes queries in CodeQL, a powerful static analysis engine, directly from a given CVE metadata. QLCode embeds an LLM in a synthesis loop with execution feedback, while constraining its reasoning using a custom MCP interface that allows structured interaction with a Language Server Protocol (for syntax guidance) and a RAG database (for semantic retrieval of queries and documentation). This approach allows QLCoder to generate syntactically and semantically valid security queries. We evaluate QLCode on 176 existing CVEs across 111 Java projects. Building upon the Claude Code agent framework, QLCoder synthesizes correct queries that detect the CVE in the vulnerable but not in the patched versions for 53.4% of CVEs. In comparison, using only Claude Code synthesizes 10% correct queries.

</details>


### [25] [Toward Autonomous and Efficient Cybersecurity: A Multi-Objective AutoML-based Intrusion Detection System](https://arxiv.org/abs/2511.08491)
*Li Yang,Abdallah Shami*

Main category: cs.CR

TL;DR: 提出了一种基于AutoML和多目标优化的入侵检测系统，用于现代网络环境中的自主优化网络攻击检测


<details>
  <summary>Details</summary>
Motivation: 随着网络安全威胁日益复杂和网络自动化需求增长，自主网络安全机制对保护现代网络至关重要。物联网系统的快速扩张加剧了这些挑战，资源受限的物联网设备需要可扩展且高效的安全解决方案

Method: 集成两种创新技术：OIP-AutoFS（优化重要性和基于百分比的自动特征选择）和OPCE-CASH（优化性能、置信度和效率的联合算法选择和超参数优化），优化特征选择和模型学习过程，平衡检测效果和计算效率

Result: 在两个基准网络安全数据集上的实验评估表明，所提出的MOO-AutoML IDS优于最先进的IDS，为自主、高效和优化的网络安全设立了新基准

Conclusion: 这是首个集成所有四个AutoML阶段并采用多目标优化来联合优化检测效果、效率和置信度的IDS框架，适用于资源受限系统部署，可应用于各种自主网络安全场景

Abstract: With increasingly sophisticated cybersecurity threats and rising demand for network automation, autonomous cybersecurity mechanisms are becoming critical for securing modern networks. The rapid expansion of Internet of Things (IoT) systems amplifies these challenges, as resource-constrained IoT devices demand scalable and efficient security solutions. In this work, an innovative Intrusion Detection System (IDS) utilizing Automated Machine Learning (AutoML) and Multi-Objective Optimization (MOO) is proposed for autonomous and optimized cyber-attack detection in modern networking environments. The proposed IDS framework integrates two primary innovative techniques: Optimized Importance and Percentage-based Automated Feature Selection (OIP-AutoFS) and Optimized Performance, Confidence, and Efficiency-based Combined Algorithm Selection and Hyperparameter Optimization (OPCE-CASH). These components optimize feature selection and model learning processes to strike a balance between intrusion detection effectiveness and computational efficiency. This work presents the first IDS framework that integrates all four AutoML stages and employs multi-objective optimization to jointly optimize detection effectiveness, efficiency, and confidence for deployment in resource-constrained systems. Experimental evaluations over two benchmark cybersecurity datasets demonstrate that the proposed MOO-AutoML IDS outperforms state-of-the-art IDSs, establishing a new benchmark for autonomous, efficient, and optimized security for networks. Designed to support IoT and edge environments with resource constraints, the proposed framework is applicable to a variety of autonomous cybersecurity applications across diverse networked environments.

</details>
