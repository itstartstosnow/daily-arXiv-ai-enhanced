<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 18]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [The Beautiful Deception: How 256 Bits Pretend to be Infinity](https://arxiv.org/abs/2510.12802)
*Alexander Towell*

Main category: cs.CR

TL;DR: 该论文探讨了如何在256位有限信息中模拟无限随机性的基本欺骗机制，揭示了密码学中随机性实际上是计算难度的伪装。


<details>
  <summary>Details</summary>
Motivation: 研究计算密码学的核心问题：如何用有限信息模拟无限随机性，证明真正的随机预言机是不可能的，并探索有限自动机如何成功伪装成无限系统。

Method: 使用Python实现展示懒评估技术，通过256位熵生成对计算受限观察者来说与无限随机性无法区分的序列。

Result: 证明了有限自动机可以通过懒评估技术成功模拟无限随机性，揭示了密码学中随机性的本质是计算难度。

Conclusion: 密码学中的随机性实际上是计算难度的伪装，有限信息可以通过适当的算法设计模拟出对计算受限观察者来说无法区分的无限随机性。

Abstract: How do you store infinity in 256 bits? This paper explores the fundamental
deception at the heart of computational cryptography: using finite information
to simulate infinite randomness. We prove why true random oracles are
impossible, then show how lazy evaluation creates a beautiful lie -- a finite
automaton that successfully pretends to be infinite. We reveal that
``randomness'' in cryptography is actually computational hardness in disguise,
demonstrating through Python implementations how 256 bits of entropy can
generate sequences indistinguishable from infinite randomness to any
computationally bounded observer.How do you store infinity in 256 bits? This
paper explores the fundamental deception at the heart of computational
cryptography: using finite information to simulate infinite randomness. We
prove why true random oracles are impossible, then show how lazy evaluation
creates a beautiful lie -- a finite automaton that successfully pretends to be
infinite. We reveal that ``randomness'' in cryptography is actually
computational hardness in disguise, demonstrating through Python
implementations how 256 bits of entropy can generate sequences
indistinguishable from infinite randomness to any computationally bounded
observer.

</details>


### [2] [Applying Graph Analysis for Unsupervised Fast Malware Fingerprinting](https://arxiv.org/abs/2510.12811)
*ElMouatez Billah Karbab,Mourad Debbabi*

Main category: cs.CR

TL;DR: TrapNet是一个新颖、可扩展、无监督的恶意软件指纹识别和分组框架，使用图社区检测技术基于静态分析进行恶意软件家族归因。


<details>
  <summary>Details</summary>
Motivation: 恶意软件数量激增，每天有数十万新样本，手动分析不现实。需要开发专门技术进行初步过滤，基于语义相似性对恶意软件进行分组。

Method: 1) 检测加壳二进制文件并使用通用脱壳工具解包；2) 设计FloatHash(FH)数值模糊哈希技术，通过PCA对汇编代码中的有序汇编项生成短实数向量；3) 构建恶意软件相似性网络；4) 使用社区检测算法识别密集社区。

Result: 广泛评估显示TrapNet在检测社区的覆盖率和纯度方面有效，运行效率优于其他最先进解决方案。

Conclusion: TrapNet框架能够高效、大规模地对恶意软件进行指纹识别和分组，解决了恶意软件数量激增带来的分析挑战。

Abstract: Malware proliferation is increasing at a tremendous rate, with hundreds of
thousands of new samples identified daily. Manual investigation of such a vast
amount of malware is an unrealistic, time-consuming, and overwhelming task. To
cope with this volume, there is a clear need to develop specialized techniques
and efficient tools for preliminary filtering that can group malware based on
semantic similarity. In this paper, we propose TrapNet, a novel, scalable, and
unsupervised framework for malware fingerprinting and grouping. TrapNet employs
graph community detection techniques for malware fingerprinting and family
attribution based on static analysis, as follows: (1) TrapNet detects packed
binaries and unpacks them using known generic packer tools. (2) From each
malware sample, it generates a digest that captures the underlying semantics.
Since the digest must be dense, efficient, and suitable for similarity
checking, we designed FloatHash (FH), a novel numerical fuzzy hashing technique
that produces a short real-valued vector summarizing the underlying assembly
items and their order. FH is based on applying Principal Component Analysis
(PCA) to ordered assembly items (e.g., opcodes, function calls) extracted from
the malware's assembly code. (3) Representing malware with short numerical
vectors enables high-performance, large-scale similarity computation, which
allows TrapNet to build a malware similarity network. (4) Finally, TrapNet
employs state-of-the-art community detection algorithms to identify dense
communities, which represent groups of malware with similar semantics. Our
extensive evaluation of TrapNet demonstrates its effectiveness in terms of the
coverage and purity of the detected communities, while also highlighting its
runtime efficiency, which outperforms other state-of-the-art solutions.

</details>


### [3] [We Can Hide More Bits: The Unused Watermarking Capacity in Theory and in Practice](https://arxiv.org/abs/2510.12812)
*Aleksandar Petrov,Pierre Fernandez,Tomáš Souček,Hady Elsahar*

Main category: cs.CR

TL;DR: 该论文分析了图像水印的理论容量上限，发现当前深度学习方法仅达到理论容量的很小部分，并提出了ChunkySeal模型将容量提升4倍至1024比特。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法的水印容量停滞在几百比特水平，需要探究这是否接近了图像水印的理论极限。

Method: 在PSNR和线性鲁棒性约束下分析图像水印的消息携带容量上限，并训练了ChunkySeal模型进行验证。

Result: 理论分析表明图像水印容量比现有方法高出几个数量级，ChunkySeal成功将容量提升至1024比特，同时保持图像质量和鲁棒性。

Conclusion: 现代水印方法远未达到理论容量极限，在架构创新和训练策略方面仍有很大提升空间。

Abstract: Despite rapid progress in deep learning-based image watermarking, the
capacity of current robust methods remains limited to the scale of only a few
hundred bits. Such plateauing progress raises the question: How far are we from
the fundamental limits of image watermarking? To this end, we present an
analysis that establishes upper bounds on the message-carrying capacity of
images under PSNR and linear robustness constraints. Our results indicate
theoretical capacities are orders of magnitude larger than what current models
achieve. Our experiments show this gap between theoretical and empirical
performance persists, even in minimal, easily analysable setups. This suggests
a fundamental problem. As proof that larger capacities are indeed possible, we
train ChunkySeal, a scaled-up version of VideoSeal, which increases capacity 4
times to 1024 bits, all while preserving image quality and robustness. These
findings demonstrate modern methods have not yet saturated watermarking
capacity, and that significant opportunities for architectural innovation and
training strategies remain.

</details>


### [4] [ARTeX: Anonymity Real-world-assets Token eXchange](https://arxiv.org/abs/2510.12821)
*Jaeseong Lee,Junghee Lee*

Main category: cs.CR

TL;DR: 提出ARTeX平台解决RWA代币交易中的隐私问题，在保证交易者匿名性的同时增强对非法活动的防范。


<details>
  <summary>Details</summary>
Motivation: 虚拟资产市场中RWA代币交易存在隐私泄露风险，由于区块链透明性导致交易者匿名性无法保证，现有方法难以有效保护RWA代币的隐私。

Method: 设计新的代币交易平台ARTeX，解决现有方法的不足，确保交易者匿名性。

Result: ARTeX平台能够有效保护RWA代币交易隐私，同时增强对非法活动的防范能力。

Conclusion: ARTeX平台为解决RWA代币隐私问题提供了有效方案，平衡了匿名性和合规性需求。

Abstract: This paper addresses one of the most noteworthy issues in the recent virtual
asset market, the privacy concerns related to token transactions of Real-World
Assets tokens, known as RWA tokens. Following the advent of Bitcoin, the
virtual asset market has experienced explosive growth, spawning movements to
link real-world assets with virtual assets. However, due to the transparency
principle of blockchain technology, the anonymity of traders cannot be
guaranteed. In the existing blockchain environment, there have been instances
of protecting the privacy of fungible tokens (FTs) using mixer services.
Moreover, numerous studies have been conducted to secure the privacy of
non-fungible tokens (NFTs). However, due to the unique characteristics of RWA
tokens and the limitations of each study, it has been challenging to achieve
the goal of anonymity protection effectively. This paper proposes a new token
trading platform, the ARTeX, designed to resolve these issues. This platform
not only addresses the shortcomings of existing methods but also ensures the
anonymity of traders while enhancing safeguards against illegal activities.

</details>


### [5] [SimKey: A Semantically Aware Key Module for Watermarking Language Models](https://arxiv.org/abs/2510.12828)
*Shingo Kodama,Haya Diwan,Lucas Rosenblatt,R. Teal Witter,Niv Cohen*

Main category: cs.CR

TL;DR: SimKey是一种语义密钥模块，通过将密钥生成与先前上下文含义绑定来增强水印鲁棒性，解决了现有水印方法对表面编辑脆弱和有害文本错误归因的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM水印方法存在两个问题：(i) 水印对改写、重排序等简单表面编辑很脆弱；(ii) 攻击者可以附加不相关的有害文本并继承水印，给模型所有者带来声誉风险。

Method: SimKey使用基于语义嵌入的局部敏感哈希来生成密钥，确保改写文本产生相同的水印密钥，而不相关或语义变化的文本产生不同的密钥。

Result: 与最先进的水印方案集成后，SimKey提高了水印对改写和翻译的鲁棒性，同时防止有害内容被错误归因。

Conclusion: 语义感知密钥生成是一个实用且可扩展的水印方向，能够有效解决现有水印方法的局限性。

Abstract: The rapid spread of text generated by large language models (LLMs) makes it
increasingly difficult to distinguish authentic human writing from machine
output. Watermarking offers a promising solution: model owners can embed an
imperceptible signal into generated text, marking its origin. Most leading
approaches seed an LLM's next-token sampling with a pseudo-random key that can
later be recovered to identify the text as machine-generated, while only
minimally altering the model's output distribution. However, these methods
suffer from two related issues: (i) watermarks are brittle to simple
surface-level edits such as paraphrasing or reordering; and (ii) adversaries
can append unrelated, potentially harmful text that inherits the watermark,
risking reputational damage to model owners. To address these issues, we
introduce SimKey, a semantic key module that strengthens watermark robustness
by tying key generation to the meaning of prior context. SimKey uses
locality-sensitive hashing over semantic embeddings to ensure that paraphrased
text yields the same watermark key, while unrelated or semantically shifted
text produces a different one. Integrated with state-of-the-art watermarking
schemes, SimKey improves watermark robustness to paraphrasing and translation
while preventing harmful content from false attribution, establishing
semantic-aware keying as a practical and extensible watermarking direction.

</details>


### [6] [Local Differential Privacy for Federated Learning with Fixed Memory Usage and Per-Client Privacy](https://arxiv.org/abs/2510.12908)
*Rouzbeh Behnia,Jeremiah Birrell,Arman Riasi,Reza Ebrahimi,Kaushik Dutta,Thang Hoang*

Main category: cs.CR

TL;DR: 提出了L-RDP方法，为联邦学习中的本地差分隐私设计，解决现有方法资源需求高导致客户端退出和异步参与下隐私保证不可靠的问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端更新和全局模型可能泄露隐私信息，现有本地差分隐私方法是为集中式训练设计，在联邦学习环境中存在高资源需求导致客户端退出、异步参与下隐私保证不可靠等问题。

Method: 提出L-RDP方法，确保恒定的较低内存使用以减少客户端退出，并通过考虑间歇性参与提供严格的每客户端隐私保证。

Result: L-RDP方法降低了资源需求，减少了客户端退出率，同时在异步参与场景下提供了可靠的隐私保护。

Conclusion: L-RDP方法解决了联邦学习中本地差分隐私的实际挑战，提高了模型泛化能力、公平性，并符合HIPAA和GDPR等法规要求。

Abstract: Federated learning (FL) enables organizations to collaboratively train models
without sharing their datasets. Despite this advantage, recent studies show
that both client updates and the global model can leak private information,
limiting adoption in sensitive domains such as healthcare. Local differential
privacy (LDP) offers strong protection by letting each participant privatize
updates before transmission. However, existing LDP methods were designed for
centralized training and introduce challenges in FL, including high resource
demands that can cause client dropouts and the lack of reliable privacy
guarantees under asynchronous participation. These issues undermine model
generalizability, fairness, and compliance with regulations such as HIPAA and
GDPR. To address them, we propose L-RDP, a DP method designed for LDP that
ensures constant, lower memory usage to reduce dropouts and provides rigorous
per-client privacy guarantees by accounting for intermittent participation.

</details>


### [7] [From misinformation to climate crisis: Navigating vulnerabilities in the cyber-physical-social systems](https://arxiv.org/abs/2510.13058)
*Tooba Aamir,Marthie Grobler,Giovanni Russello*

Main category: cs.CR

TL;DR: 本章探讨了网络-物理-社会-气候系统中人类脆弱性的关键作用，分析了跨系统依赖关系如何放大气候风险，并提出通过解决人为因素来增强系统韧性。


<details>
  <summary>Details</summary>
Motivation: 在网络-物理-社会-气候系统中，社会脆弱性（如错误信息、政策抵制等）往往被忽视，但其对系统韧性和气候适应具有深远影响，需要专门研究。

Method: 通过分析人类认知偏见、风险误判和决策孤岛等社会因素，研究这些因素如何在相互连接的系统中导致资源错配和政策失效。

Result: 研究发现，网络-物理-社会层面的不充分响应会级联放大气候相关风险，社会基础设施（人力资本、社会规范等）既是适应能力的基础，也是重要的失效点。

Conclusion: 通过解决人为因素和调整决策框架，可以增强系统韧性，制定考虑网络-物理-社会-气候系统复杂相互关系的协调适应策略。

Abstract: Within the cyber-physical-social-climate nexus, all systems are deeply
interdependent: cyber infrastructure facilitates communication, data
processing, and automation across physical systems (such as power grids and
networks), while social infrastructure provides the human capital and societal
norms necessary for the system's functionality. Any disruption within any of
these components, whether due to human error or system mismanagement, can
propagate throughout the network, amplifying vulnerabilities and creating a
significantly scaled impact. This chapter explores the critical role of human
vulnerabilities within the cyber-physical-social-climate nexus, focusing on the
interdependencies across cyber, physical, and social systems and how these
interdependencies can scale in a climate context. While cyber and physical
vulnerabilities are readily apparent, social vulnerabilities (such as
misinformation, resistance to policy change, and lack of public awareness)
often go unaddressed despite their profound impact on resilience and climate
adaptation. Social infrastructure, including human capital, societal norms, and
policy frameworks, shapes community responses and underpins adaptive capacity,
yet it is also a significant point of failure when overlooked. This chapter
examines how human cognitive biases, risk misperception, and decision-making
silos within interconnected systems can lead to resource misallocation and
weakened policy effectiveness. These factors are analyzed to demonstrate how
inadequate responses across cyber-physical-social layers can cascade,
amplifying climate-related risks. By addressing these human factors and
aligning decision-making frameworks, we aim to strengthen resilience and foster
cohesive adaptation strategies that account for the intricate interrelations of
cyber-physical-social-climate systems.

</details>


### [8] [From base cases to backdoors: An Empirical Study of Unnatural Crypto-API Misuse](https://arxiv.org/abs/2510.13102)
*Victor Olaiya,Adwait Nadkarni*

Main category: cs.CR

TL;DR: 该论文首次大规模研究密码API的非自然使用模式，通过分析5,704个代表性API调用来构建非自然误用的详细分类体系，发现了高度不寻常的误用模式和流行工具的检测局限性。


<details>
  <summary>Details</summary>
Motivation: 现有工具只能检测基本的密码API误用，无法识别非平凡变体。需要了解开发者如何在实践中使用和误用密码API，特别是非自然使用模式，以指导工具设计。

Method: 开发直观的复杂度指标对140,431个密码API调用进行分层，从20,508个Android应用中采样5,704个代表性调用，通过手动逆向工程进行定性分析，包括开发最小示例和探索原生代码。

Result: 构建了两个详细的非自然密码API误用分类体系，发现了17个关键发现，包括高度不寻常的误用、规避性代码，以及流行工具无法检测中等复杂度非传统使用的问题。

Conclusion: 研究结果为未来检测非自然密码API误用的工作提供了四个关键启示，强调了需要改进工具以处理更复杂的误用模式。

Abstract: Tools focused on cryptographic API misuse often detect the most basic
expressions of the vulnerable use, and are unable to detect non-trivial
variants. The question of whether tools should be designed to detect such
variants can only be answered if we know how developers use and misuse
cryptographic APIs in the wild, and in particular, what the unnatural usage of
such APIs looks like. This paper presents the first large-scale study that
characterizes unnatural crypto-API usage through a qualitative analysis of
5,704 representative API invocations. We develop an intuitive complexity metric
to stratify 140,431 crypto-API invocations obtained from 20,508 Android
applications, allowing us to sample 5,704 invocations that are representative
of all strata, with each stratum consisting of invocations with similar
complexity/naturalness. We qualitatively analyze the 5,704 sampled invocations
using manual reverse engineering, through an in-depth investigation that
involves the development of minimal examples and exploration of native code.
Our study results in two detailed taxonomies of unnatural crypto-API misuse,
along with 17 key findings that show the presence of highly unusual misuse,
evasive code, and the inability of popular tools to reason about even mildly
unconventional usage. Our findings lead to four key takeaways that inform
future work focused on detecting unnatural crypto-API misuse.

</details>


### [9] [ShuffleV: A Microarchitectural Defense Strategy against Electromagnetic Side-Channel Attacks in Microprocessors](https://arxiv.org/abs/2510.13111)
*Nuntipat Narkthong,Yukui Luo,Xiaolin Xu*

Main category: cs.CR

TL;DR: ShuffleV是一种基于RISC-V的微架构防御策略，通过随机重排指令执行顺序和插入虚拟指令来抵御电磁侧信道攻击。


<details>
  <summary>Details</summary>
Motivation: 微处理器的运行时电磁辐射会泄露应用程序的机密信息，如加密算法密钥和神经网络超参数，需要有效的防御措施。

Method: 采用移动目标防御理念，在硬件层面集成随机指令重排和可选虚拟指令插入功能，提供六种设计选项适应不同应用场景。

Result: 在FPGA上实现并验证，能够自动保护AES加密和神经网络推理应用，无需用户干预或软件修改。

Conclusion: ShuffleV能有效抵御电磁侧信道攻击，为敏感应用提供自动保护。

Abstract: The run-time electromagnetic (EM) emanation of microprocessors presents a
side-channel that leaks the confidentiality of the applications running on
them. Many recent works have demonstrated successful attacks leveraging such
side-channels to extract the confidentiality of diverse applications, such as
the key of cryptographic algorithms and the hyperparameter of neural network
models. This paper proposes ShuffleV, a microarchitecture defense strategy
against EM Side-Channel Attacks (SCAs). ShuffleV adopts the moving target
defense (MTD) philosophy, by integrating hardware units to randomly shuffle the
execution order of program instructions and optionally insert dummy
instructions, to nullify the statistical observation by attackers across
repetitive runs. We build ShuffleV on the open-source RISC-V core and provide
six design options, to suit different application scenarios. To enable rapid
evaluation, we develop a ShuffleV simulator that can help users to (1) simulate
the performance overhead for each design option and (2) generate an execution
trace to validate the randomness of execution on their workload. We implement
ShuffleV on a Xilinx PYNQ-Z2 FPGA and validate its performance with two
representative victim applications against EM SCAs, AES encryption, and neural
network inference. The experimental results demonstrate that ShuffleV can
provide automatic protection for these applications, without any user
intervention or software modification.

</details>


### [10] [Privacy-Aware Framework of Robust Malware Detection in Indoor Robots: Hybrid Quantum Computing and Deep Neural Networks](https://arxiv.org/abs/2510.13136)
*Tan Le,Van Le,Sachin Shetty*

Main category: cs.CR

TL;DR: 提出了一种用于室内机器人系统的隐私感知恶意软件检测框架，结合量子计算和深度神经网络来对抗CPS中的DoS攻击，同时保护隐私信息。


<details>
  <summary>Details</summary>
Motivation: 室内机器人系统在CPS中面临DoS攻击威胁，这些攻击会损害定位、控制和遥测完整性，需要开发能够保护隐私的检测方案。

Method: 采用混合量子计算和深度神经网络，集成量子增强特征编码与dropout优化的深度学习，无需手工阈值或持久信标数据。

Result: 在隐私受限条件下达到95.2%的检测准确率，通过模块化电路设计展现出强大的泛化性、可解释性和训练稳定性。

Conclusion: 该工作推进了可信AI在安全自主CPS操作中的应用，为对抗DoS威胁提供了有效的隐私保护解决方案。

Abstract: Indoor robotic systems within Cyber-Physical Systems (CPS) are increasingly
exposed to Denial of Service (DoS) attacks that compromise localization,
control and telemetry integrity. We propose a privacy-aware malware detection
framework for indoor robotic systems, which leverages hybrid quantum computing
and deep neural networks to counter DoS threats in CPS, while preserving
privacy information. By integrating quantum-enhanced feature encoding with
dropout-optimized deep learning, our architecture achieves up to 95.2%
detection accuracy under privacy-constrained conditions. The system operates
without handcrafted thresholds or persistent beacon data, enabling scalable
deployment in adversarial environments. Benchmarking reveals robust
generalization, interpretability and resilience against training instability
through modular circuit design. This work advances trustworthy AI for secure,
autonomous CPS operations.

</details>


### [11] [GRIDAI: Generating and Repairing Intrusion Detection Rules via Collaboration among Multiple LLM-based Agents](https://arxiv.org/abs/2510.13257)
*Jiarui Li,Yuhan Chai,Lei Du,Chenyun Duan,Hao Yan,Zhaoquan Gu*

Main category: cs.CR

TL;DR: GRIDAI是一个基于多LLM代理协作的端到端框架，用于自动生成和修复入侵检测规则，通过分析新攻击样本与现有规则的关系来减少规则冗余，并包含验证机制缓解LLM幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的网络入侵检测系统主要关注为新攻击自动生成检测规则，但忽略了新攻击与现有规则之间的关系，导致规则集不断膨胀且存在显著冗余。

Method: GRIDAI首先评估传入攻击样本的性质：如果是新攻击类型则生成新规则，如果是现有攻击的变体则修复对应规则的签名以增强泛化能力。采用基于工具的实时验证机制和代表性攻击样本来缓解LLM幻觉导致的语法和语义错误。

Result: 在包含7种攻击类型的公共数据集和43种攻击类型的私有数据集上的综合实验表明，GRIDAI能准确识别新攻击样本与现有规则的关系，高效生成和修复规则处理新攻击和变体，并有效缓解LLM幻觉的影响。

Conclusion: GRIDAI通过多LLM代理协作实现了入侵检测规则的自动生成和修复，解决了规则冗余问题，提高了检测系统的效率和泛化能力。

Abstract: Rule-based network intrusion detection systems play a crucial role in the
real-time detection of Web attacks. However, most existing works primarily
focus on automatically generating detection rules for new attacks, often
overlooking the relationships between new attacks and existing rules, which
leads to significant redundancy within the ever-expanding ruleset. To address
this issue, we propose GRIDAI, a novel end-to-end framework for the automated
Generation and Repair of Intrusion Detection rules through collaboration among
multiple LLM-based agents. Unlike traditional methods, GRIDAI first assesses
the nature of incoming attack samples. If the sample represents a new attack
type, it is used to generate a new rule. Otherwise, the sample is identified as
a variant of an attack already covered by an existing rule and used to repair
the rule by updating the corresponding signature, thereby enhancing its
generalization capability. Additionally, to mitigate syntactic and semantic
errors in rules caused by LLM hallucinations, we incorporate a tool-based
real-time validation mechanism and a representative attack sample maintained
for each rule, enabling fully automated rule generation and repair.
Comprehensive experiments were conducted on a public dataset containing seven
types of attacks and a private dataset with 43 attack types. The results
demonstrate that GRIDAI accurately identifies the relationships between new
attack samples and existing rules, efficiently generates and repairs rules to
handle new attacks and variants, and effectively mitigates the impact of LLM
hallucinations.

</details>


### [12] [Fast Authenticated and Interoperable Multimedia Healthcare Data over Hybrid-Storage Blockchains](https://arxiv.org/abs/2510.13318)
*Jucai Yang,Liang Li,Yiwei Gu,Haiqin Wu*

Main category: cs.CR

TL;DR: FAITH是一个基于混合存储区块链的快速认证和可互操作多媒体医疗数据存储与共享方案，通过递归零知识证明和代理重加密技术，大幅降低用户端验证延迟，从4秒降至70毫秒。


<details>
  <summary>Details</summary>
Motivation: 现有基于区块链的医疗系统虽然提供强大的访问控制，但忽视了用户端对大容量多媒体数据完整性验证时的高延迟问题，这在时间敏感的临床场景中影响实用性。

Method: 使用递归零知识证明让离线存储提供商生成可验证证明，用户只需进行轻量级验证；采用代理重加密实现灵活的访问授权，并通过零知识证明验证重加密的正确性；所有元数据和证明都记录在链上供公开验证。

Result: 原型实现和广泛实验表明，FAITH显著降低了用户端验证延迟，对于5GB加密文件，验证延迟从4秒降至约70毫秒，减少了98%。

Conclusion: FAITH方案通过创新的零知识证明和代理重加密技术，解决了区块链医疗系统中多媒体数据验证的高延迟问题，为时间关键的医疗应用提供了实用的解决方案。

Abstract: The integration of blockchain technology into healthcare presents a paradigm
shift for secure data management, enabling decentralized and tamper-proof
storage and sharing of sensitive Electronic Health Records (EHRs). However,
existing blockchain-based healthcare systems, while providing robust access
control, commonly overlook the high latency in user-side re-computation of
hashes for integrity verification of large multimedia data, impairing their
practicality, especially in time-sensitive clinical scenarios. In this paper,
we propose FAITH, an innovative scheme for \underline{F}ast
\underline{A}uthenticated and \underline{I}nteroperable mul\underline{T}imedia
\underline{H}ealthcare data storage and sharing over hybrid-storage
blockchains. Rather than user-side hash re-computations, FAITH lets an
off-chain storage provider generate verifiable proofs using recursive
Zero-Knowledge Proofs (ZKPs), while the user only needs to perform lightweight
verification. For flexible access authorization, we leverage Proxy
Re-Encryption (PRE) and enable the provider to conduct ciphertext
re-encryption, in which the re-encryption correctness can be verified via ZKPs
against the malicious provider. All metadata and proofs are recorded on-chain
for public verification. We provide a comprehensive analysis of FAITH's
security regarding data privacy and integrity. We implemented a prototype of
FAITH, and extensive experiments demonstrated its practicality for
time-critical healthcare applications, dramatically reducing user-side
verification latency by up to $98\%$, bringing it from $4$ s down to around
$70$ ms for a $5$ GB encrypted file.

</details>


### [13] [Injection, Attack and Erasure: Revocable Backdoor Attacks via Machine Unlearning](https://arxiv.org/abs/2510.13322)
*Baogang Song,Dongdong Zhao,Jianwen Xiang,Qiben Xu,Zizhuo Yu*

Main category: cs.CR

TL;DR: 本文提出了首个可撤销后门攻击范式，通过双层优化方法设计触发器，在保持高攻击成功率的同时确保后门能够通过反学习机制被彻底移除。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击策略在反学习机制下仍会留下可被静态分析检测的持久痕迹，需要开发既能有效攻击又能主动彻底移除的后门攻击方法。

Method: 将触发器优化建模为双层优化问题，模拟后门注入和反学习过程，采用确定性样本划分减少方差，并使用PCGrad技术解决梯度冲突。

Result: 在CIFAR-10和ImageNet上的实验表明，该方法保持了与最先进后门攻击相当的攻击成功率，同时能够通过反学习有效移除后门行为。

Conclusion: 这项工作为后门攻击研究开辟了新方向，并对机器学习系统的安全性提出了新的挑战。

Abstract: Backdoor attacks pose a persistent security risk to deep neural networks
(DNNs) due to their stealth and durability. While recent research has explored
leveraging model unlearning mechanisms to enhance backdoor concealment,
existing attack strategies still leave persistent traces that may be detected
through static analysis. In this work, we introduce the first paradigm of
revocable backdoor attacks, where the backdoor can be proactively and
thoroughly removed after the attack objective is achieved. We formulate the
trigger optimization in revocable backdoor attacks as a bilevel optimization
problem: by simulating both backdoor injection and unlearning processes, the
trigger generator is optimized to achieve a high attack success rate (ASR)
while ensuring that the backdoor can be easily erased through unlearning. To
mitigate the optimization conflict between injection and removal objectives, we
employ a deterministic partition of poisoning and unlearning samples to reduce
sampling-induced variance, and further apply the Projected Conflicting Gradient
(PCGrad) technique to resolve the remaining gradient conflicts. Experiments on
CIFAR-10 and ImageNet demonstrate that our method maintains ASR comparable to
state-of-the-art backdoor attacks, while enabling effective removal of backdoor
behavior after unlearning. This work opens a new direction for backdoor attack
research and presents new challenges for the security of machine learning
systems.

</details>


### [14] [Towards Trusted Service Monitoring: Verifiable Service Level Agreements](https://arxiv.org/abs/2510.13370)
*Fernando Castillo,Eduardo Brito,Sebastian Werner,Pille Pullonen-Raudvere,Jonathan Heiss*

Main category: cs.CR

TL;DR: 提出一个基于可信硬件和零知识证明的SLA监控框架，通过密码学方法解决服务提供商自我报告指标时的信任冲突问题。


<details>
  <summary>Details</summary>
Motivation: 服务导向环境中SLA监控存在固有信任冲突，服务提供商有动机少报违规行为，需要建立真正的可信监控机制。

Method: 将SLA条款转换为可验证谓词，在可信执行环境中监控，收集带时间戳的遥测数据并组织成Merkle树，使用零知识证明聚合服务级别指标来评估合规性。

Result: 原型系统展示线性扩展能力，每小时可处理超过100万个事件，单次违规声明的证明生成和验证时间接近常数，支持无信任SLA执行。

Conclusion: 该框架通过密码学保证为服务监控中的自动合规验证提供了完整性、真实性和有效性三个安全属性，实现了真正的可信SLA监控。

Abstract: Service Level Agreement (SLA) monitoring in service-oriented environments
suffers from inherent trust conflicts when providers self-report metrics,
creating incentives to underreport violations. We introduce a framework for
generating verifiable SLA violation claims through trusted hardware monitors
and zero-knowledge proofs, establishing cryptographic foundations for genuine
trustworthiness in service ecosystems. Our approach starts with
machine-readable SLA clauses converted into verifiable predicates and monitored
within Trusted Execution Environments. These monitors collect timestamped
telemetry, organize measurements into Merkle trees, and produce signed
attestations. Zero-knowledge proofs aggregate Service-Level Indicators to
evaluate compliance, generating cryptographic proofs verifiable by
stakeholders, arbitrators, or insurers in disputes, without accessing
underlying data. This ensures three security properties: integrity,
authenticity, and validity. Our prototype demonstrates linear scaling up to
over 1 million events per hour for measurements with near constant-time proof
generation and verification for single violation claims, enabling trustless SLA
enforcement through cryptographic guarantees for automated compliance
verification in service monitoring.

</details>


### [15] [Toward Efficient Inference Attacks: Shadow Model Sharing via Mixture-of-Experts](https://arxiv.org/abs/2510.13451)
*Li Bai,Qingqing Ye,Xinwei Zhang,Sen Zhang,Zi Liang,Jianliang Xu,Haibo Hu*

Main category: cs.CR

TL;DR: SHAPOOL是一个新颖的影子池训练框架，通过共享子网络和联合训练来显著降低影子模型的构建计算成本，同时保持可比的成员推理攻击性能。


<details>
  <summary>Details</summary>
Motivation: 传统的影子模型技术需要大量独立训练的影子模型，导致计算成本高昂，限制了其实际应用。这种低效主要源于这些影子模型的独立训练和使用。

Method: 提出SHAPOOL框架，利用专家混合机制构建影子池，通过路径选择路由、路径正则化和路径对齐三个模块，确保共享模型既相似于独立模型又能作为有效替代品。

Result: 在各种成员推理攻击场景下评估SHAPOOL，结果显示它显著降低了影子模型构建的计算成本，同时保持了可比的攻击性能。

Conclusion: SHAPOOL通过影子池训练框架有效解决了影子模型技术的高计算成本问题，为成员推理攻击提供了更实用的解决方案。

Abstract: Machine learning models are often vulnerable to inference attacks that expose
sensitive information from their training data. Shadow model technique is
commonly employed in such attacks, such as membership inference. However, the
need for a large number of shadow models leads to high computational costs,
limiting their practical applicability. Such inefficiency mainly stems from the
independent training and use of these shadow models. To address this issue, we
present a novel shadow pool training framework SHAPOOL, which constructs
multiple shared models and trains them jointly within a single process. In
particular, we leverage the Mixture-of-Experts mechanism as the shadow pool to
interconnect individual models, enabling them to share some sub-networks and
thereby improving efficiency. To ensure the shared models closely resemble
independent models and serve as effective substitutes, we introduce three novel
modules: path-choice routing, pathway regularization, and pathway alignment.
These modules guarantee random data allocation for pathway learning, promote
diversity among shared models, and maintain consistency with target models. We
evaluate SHAPOOL in the context of various membership inference attacks and
show that it significantly reduces the computational cost of shadow model
construction while maintaining comparable attack performance.

</details>


### [16] [Who Speaks for the Trigger? Dynamic Expert Routing in Backdoored Mixture-of-Experts Transformers](https://arxiv.org/abs/2510.13462)
*Xin Zhao,Xiaojun Chen,Bingshan Liu,Haoyu Gao,Zhendong Zhao,Yilong Chen*

Main category: cs.CR

TL;DR: BadSwitch是一种针对MoE架构LLM的新型后门攻击框架，通过利用专家路由偏好，在预训练期间联合优化触发嵌入并识别敏感专家，实现高效且隐蔽的模型操控。


<details>
  <summary>Details</summary>
Motivation: MoE架构中的稀疏路由机制由于专家专业化而表现出任务偏好，这为后门攻击引入了新的未充分探索的漏洞。

Method: 结合任务耦合动态触发优化和敏感度引导的Top-S专家追踪机制，在预训练期间联合优化触发嵌入，识别S个最敏感专家，并将Top-K选通机制限制在这些目标专家上。

Result: 在三种主流MoE架构上评估，BadSwitch能以100%成功率劫持预训练模型，同时保持最高的清洁准确率，对文本级和模型级防御机制具有强韧性。

Conclusion: 该工作揭示了MoE系统的安全风险，通过分析专家激活模式提供了对MoE漏洞的基本洞察，有助于推进AI安全。

Abstract: Large language models (LLMs) with Mixture-of-Experts (MoE) architectures
achieve impressive performance and efficiency by dynamically routing inputs to
specialized subnetworks, known as experts. However, this sparse routing
mechanism inherently exhibits task preferences due to expert specialization,
introducing a new and underexplored vulnerability to backdoor attacks. In this
work, we investigate the feasibility and effectiveness of injecting backdoors
into MoE-based LLMs by exploiting their inherent expert routing preferences. We
thus propose BadSwitch, a novel backdoor framework that integrates task-coupled
dynamic trigger optimization with a sensitivity-guided Top-S expert tracing
mechanism. Our approach jointly optimizes trigger embeddings during pretraining
while identifying S most sensitive experts, subsequently constraining the Top-K
gating mechanism to these targeted experts. Unlike traditional backdoor attacks
that rely on superficial data poisoning or model editing, BadSwitch primarily
embeds malicious triggers into expert routing paths with strong task affinity,
enabling precise and stealthy model manipulation. Through comprehensive
evaluations across three prominent MoE architectures (Switch Transformer,
QwenMoE, and DeepSeekMoE), we demonstrate that BadSwitch can efficiently hijack
pre-trained models with up to 100% success rate (ASR) while maintaining the
highest clean accuracy (ACC) among all baselines. Furthermore, BadSwitch
exhibits strong resilience against both text-level and model-level defense
mechanisms, achieving 94.07% ASR and 87.18% ACC on the AGNews dataset. Our
analysis of expert activation patterns reveals fundamental insights into MoE
vulnerabilities. We anticipate this work will expose security risks in MoE
systems and contribute to advancing AI safety.

</details>


### [17] [How Blind and Low-Vision Users Manage Their Passwords](https://arxiv.org/abs/2510.13538)
*Alexander Ponticello,Filipo Sharevski,Simon Anell,Katharina Krombholz*

Main category: cs.CR

TL;DR: 研究调查了盲人和低视力用户如何使用密码管理器，发现虽然用户普遍使用密码管理器，但主要出于存储便利而非安全考虑，因为生成强密码的功能对BLV用户不够易用。


<details>
  <summary>Details</summary>
Motivation: 现有研究已识别用户密码管理中的痛点，但缺乏对盲人和低视力用户特定需求的关注，需要了解他们如何应对密码管理挑战以及密码管理器如何提供帮助。

Method: 采用定性访谈研究方法，对33名盲人和低视力参与者进行访谈研究。

Result: 所有参与者都使用密码管理器，主要看重存储和检索密码的便利性，但避免使用生成强密码的安全功能，因为缺乏实际可访问性。密码管理器未能满足BLV用户对自主权的需求。

Conclusion: 需要改进密码管理器的实际可访问性和可用性，以建立信任和安全实践，同时维护BLV用户的自主权。

Abstract: Managing passwords securely and conveniently is still an open problem for
many users. Existing research has examined users' password management
strategies and identified pain points, such as security concerns, leading to
insecure practices. We investigate how Blind and Low-Vision (BLV) users tackle
this problem and how password managers can assist them. This paper presents the
results of a qualitative interview study with N = 33 BLV participants. We found
that all participants utilize password managers to some extent, which they
perceive as fairly accessible. However, the adoption is mainly driven by the
convenience of storing and retrieving passwords. The security advantages -
generating strong, random passwords - were avoided mainly due to the absence of
practical accessibility. Password managers do not adhere to BLV users'
underlying needs for agency, which stem from experiences with inaccessible
software and vendors who deprioritize accessibility issues. Underutilization of
password managers leads BLV users to adopt insecure practices, such as reusing
predictable passwords or resorting to 'security through obscurity' by writing
important credentials in braille. We conclude our analysis by discussing the
need to implement practical accessibility and usability improvements for
password managers as a way of establishing trust and secure practices while
maintaining BLV users' agency.

</details>


### [18] [In-Browser LLM-Guided Fuzzing for Real-Time Prompt Injection Testing in Agentic AI Browsers](https://arxiv.org/abs/2510.13543)
*Avihay Cohen*

Main category: cs.CR

TL;DR: 提出了一种基于LLM的浏览器内模糊测试框架，用于实时发现代理式AI浏览器中的间接提示注入漏洞。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的代理式AI浏览器容易受到间接提示注入攻击，恶意指令隐藏在网页中可欺骗代理执行非预期操作，这些攻击能绕过传统web安全边界。

Method: 开发了一个完全在浏览器中运行的模糊测试框架，使用LLM引导来自动发现提示注入漏洞。

Result: 该框架能够在实时环境中有效检测代理式AI浏览器的提示注入漏洞。

Conclusion: 提出的LLM引导的浏览器内模糊测试方法为检测和防御代理式AI浏览器的提示注入攻击提供了有效解决方案。

Abstract: Large Language Model (LLM) based agents integrated into web browsers (often
called agentic AI browsers) offer powerful automation of web tasks. However,
they are vulnerable to indirect prompt injection attacks, where malicious
instructions hidden in a webpage deceive the agent into unwanted actions. These
attacks can bypass traditional web security boundaries, as the AI agent
operates with the user privileges across sites. In this paper, we present a
novel fuzzing framework that runs entirely in the browser and is guided by an
LLM to automatically discover such prompt injection vulnerabilities in real
time.

</details>
