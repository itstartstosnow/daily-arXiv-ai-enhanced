<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 17]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Revisit to the Bai-Galbraith signature scheme](https://arxiv.org/abs/2511.09582)
*Banhirup Sengupta,Peenal Gupta,Souvik Sengupta*

Main category: cs.CR

TL;DR: 本文介绍了Bai-Galbraith签名方案(BG14)，这是一个基于LWE的格基签名方案，与Dilithium不同之处在于没有公钥压缩。


<details>
  <summary>Details</summary>
Motivation: Dilithium是NIST批准的格基签名方案之一，但BG14方案提供了不同的设计选择，特别是在公钥处理方面。

Method: 基于Learning with Errors (LWE)的格基签名方案，采用与Dilithium不同的设计，不进行公钥压缩。

Result: 提出了Bai-Galbraith签名方案(BG14)，这是一个完整的格基签名方案。

Conclusion: BG14方案为格基密码学提供了另一种设计选择，特别是在公钥管理方面与Dilithium形成对比。

Abstract: Dilithium is one of the NIST approved lattice-based signature schemes. In this short note we describe the Bai-Galbraith signature scheme proposed in BG14, which differs to Dilithium, due to the fact that there is no public key compression. This lattice-based signature scheme is based on Learning with Errors (LWE).

</details>


### [2] [An explainable Recursive Feature Elimination to detect Advanced Persistent Threats using Random Forest classifier](https://arxiv.org/abs/2511.09603)
*Noor Hazlina Abdul Mutalib,Aznul Qalid Md Sabri,Ainuddin Wahid Abdul Wahab,Erma Rahayu Mohd Faizal Abdullah,Nouar AlDahoul*

Main category: cs.CR

TL;DR: 提出一种可解释的入侵检测框架，结合递归特征消除和随机森林来检测高级持续性威胁，在CICIDS2017数据集上达到99.9%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现代网络安全框架中，入侵检测系统是防御复杂威胁的关键机制，需要开发既准确又可解释的检测方案。

Method: 使用递归特征消除进行特征选择，然后训练随机森林模型，并用SHAP方法解释各特征贡献度。

Result: 实验显示该框架达到99.9%的检测准确率，相比传统分类器降低了误报率和计算成本。

Conclusion: 结合可解释AI和特征选择能开发出鲁棒、透明且可部署的入侵检测系统解决方案。

Abstract: Intrusion Detection Systems (IDS) play a vital role in modern cybersecurity frameworks by providing a primary defense mechanism against sophisticated threat actors. In this paper, we propose an explainable intrusion detection framework that integrates Recursive Feature Elimination (RFE) with Random Forest (RF) to enhance detection of Advanced Persistent Threats (APTs). By using CICIDS2017 dataset, the approach begins with comprehensive data preprocessing and narrows down the most significant features via RFE. A Random Forest (RF) model was trained on the refined feature set, with SHapley Additive exPlanations (SHAP) used to interpret the contribution of each selected feature. Our experiment demonstrates that the explainable RF-RFE achieved a detection accuracy of 99.9%, reducing false positive and computational cost in comparison to traditional classifiers. The findings underscore the effectiveness of integrating explainable AI and feature selection to develop a robust, transparent, and deployable IDS solution.

</details>


### [3] [How Can We Effectively Use LLMs for Phishing Detection?: Evaluating the Effectiveness of Large Language Model-based Phishing Detection Models](https://arxiv.org/abs/2511.09606)
*Fujiao Ji,Doowon Kim*

Main category: cs.CR

TL;DR: 本研究探索了如何有效利用大语言模型进行钓鱼网站检测，通过测试不同输入模态、温度设置和提示工程策略，发现商业LLM在钓鱼检测方面表现优于开源模型，截图输入在品牌识别中效果最佳。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习钓鱼检测器存在泛化能力差和缺乏可解释性的问题，而LLM作为有前景的钓鱼检测机制，其有效性尚未得到充分探索。

Method: 使用19,131个真实钓鱼网站和243个良性网站数据集，评估7个LLM（2个商业模型和5个开源模型）以及2个深度学习基线模型，测试不同输入模态（截图、logo、HTML、URL）、温度设置和提示工程策略的影响。

Result: 商业LLM在钓鱼检测中表现优于开源模型，而DL模型在良性样本上表现更好。截图输入在品牌识别中达到93-95%准确率，开源模型Qwen可达92%。多模态输入或单样本提示不能持续提升性能，高温度值会降低性能。

Conclusion: 建议使用截图输入和零温度设置来最大化LLM检测器的准确性，当截图信息不足时可将HTML作为辅助上下文。

Abstract: Large language models (LLMs) have emerged as a promising phishing detection mechanism, addressing the limitations of traditional deep learning-based detectors, including poor generalization to previously unseen websites and a lack of interpretability. However, LLMs' effectiveness for phishing detection remains unexplored. This study investigates how to effectively leverage LLMs for phishing detection (including target brand identification) by examining the impact of input modalities (screenshots, logos, HTML, and URLs), temperature settings, and prompt engineering strategies. Using a dataset of 19,131 real-world phishing websites and 243 benign sites, we evaluate seven LLMs -- two commercial models (GPT 4.1 and Gemini 2.0 flash) and five open-source models (Qwen, Llama, Janus, DeepSeek-VL2, and R1) -- alongside two deep learning (DL)-based baselines (PhishIntention and Phishpedia).
  Our findings reveal that commercial LLMs generally outperform open-source models in phishing detection, while DL models demonstrate better performance on benign samples. For brand identification, screenshot inputs achieve optimal results, with commercial LLMs reaching 93-95% accuracy and open-source models, particularly Qwen, achieving up to 92%. However, incorporating multiple input modalities simultaneously or applying one-shot prompts does not consistently enhance performance and may degrade results. Furthermore, higher temperature values reduce performance. Based on these results, we recommend using screenshot inputs with zero temperature to maximize accuracy for LLM-based detectors with HTML serving as auxiliary context when screenshot information is insufficient.

</details>


### [4] [Slice-Aware Spoofing Detection in 5G Networks Using Lightweight Machine Learning](https://arxiv.org/abs/2511.09610)
*Daniyal Ganiuly,Nurzhau Bolatbek*

Main category: cs.CR

TL;DR: 提出了一种面向5G网络切片的轻量级机器学习框架，用于检测切片内的欺骗攻击，通过切片感知训练提高了检测精度，并在边缘硬件上实现实时操作。


<details>
  <summary>Details</summary>
Motivation: 5G网络虚拟化扩展了用户平面的攻击面，欺骗攻击持续威胁切片完整性和服务可靠性，需要有效的检测机制。

Method: 在Open5GS和srsRAN测试平台上实现框架，模拟eMBB、URLLC和mMTC三种服务类别，使用逻辑回归和随机森林分类器对每个切片独立训练，基于镜像用户平面流量提取统计流特征。

Result: 切片感知训练将检测精度提高了5%，F1分数达到0.93-0.96，在商用边缘硬件上保持实时操作。

Conclusion: 将安全智能与切片边界对齐可增强检测可靠性并保持操作隔离，实现可扩展、保护隐私的欺骗防御，无需高计算成本。

Abstract: The increasing virtualization of fifth generation (5G) networks expands the attack surface of the user plane, making spoofing a persistent threat to slice integrity and service reliability. This study presents a slice-aware lightweight machine-learning framework for detecting spoofing attacks within 5G network slices. The framework was implemented on a reproducible Open5GS and srsRAN testbed emulating three service classes such as enhanced Mobile Broadband (eMBB), Ultra-Reliable Low-Latency Communication (URLLC), and massive Machine-Type Communication (mMTC) under controlled benign and adversarial traffic. Two efficient classifiers, Logistic Regression and Random Forest, were trained independently for each slice using statistical flow features derived from mirrored user-plane traffic. Slice-aware training improved detection accuracy by up to 5% and achieved F1-scores between 0.93 and 0.96 while maintaining real-time operation on commodity edge hardware. The results demonstrate that aligning security intelligence with slice boundaries enhances detection reliability and preserves operational isolation, enabling practical deployment in 5G network-security environments. Conceptually, the work bridges network-security architecture and adaptive machine learning by showing that isolation-aware intelligence can achieve scalable, privacy-preserving spoofing defense without high computational cost.

</details>


### [5] [Cooperative Local Differential Privacy: Securing Time Series Data in Distributed Environments](https://arxiv.org/abs/2511.09696)
*Bikash Chandra Singh,Md Jakir Hossain,Rafael Diaz,Sandip Roy,Ravi Mukkamala,Sachin Shetty*

Main category: cs.CR

TL;DR: 提出合作本地差分隐私机制，通过多用户协作生成噪声向量，在聚合时噪声相互抵消，既保护个体隐私又保持数据统计特性。


<details>
  <summary>Details</summary>
Motivation: 传统本地差分隐私方法在时间窗口内添加用户特定噪声，但在聚合时可能被抵消，导致隐私泄露风险。需要解决这一漏洞。

Method: CLDP机制让多个用户协作生成和分配噪声向量，当所有用户的扰动数据聚合时，噪声相互抵消，保护个体隐私同时保持数据效用。

Result: 该方法有效对抗基于时间窗口方法的漏洞，可扩展处理大规模实时数据集，在多用户环境中实现更好的隐私与效用平衡。

Conclusion: 合作本地差分隐私机制通过多用户协作噪声生成，显著增强了时间序列数据的隐私保护能力，同时保持了数据的统计实用性。

Abstract: The rapid growth of smart devices such as phones, wearables, IoT sensors, and connected vehicles has led to an explosion of continuous time series data that offers valuable insights in healthcare, transportation, and more. However, this surge raises significant privacy concerns, as sensitive patterns can reveal personal details. While traditional differential privacy (DP) relies on trusted servers, local differential privacy (LDP) enables users to perturb their own data. However, traditional LDP methods perturb time series data by adding user-specific noise but exhibit vulnerabilities. For instance, noise applied within fixed time windows can be canceled during aggregation (e.g., averaging), enabling adversaries to infer individual statistics over time, thereby eroding privacy guarantees.
  To address these issues, we introduce a Cooperative Local Differential Privacy (CLDP) mechanism that enhances privacy by distributing noise vectors across multiple users. In our approach, noise is collaboratively generated and assigned so that when all users' perturbed data is aggregated, the noise cancels out preserving overall statistical properties while protecting individual privacy. This cooperative strategy not only counters vulnerabilities inherent in time-window-based methods but also scales effectively for large, real-time datasets, striking a better balance between data utility and privacy in multiuser environments.

</details>


### [6] [Privacy-Preserving Explainable AIoT Application via SHAP Entropy Regularization](https://arxiv.org/abs/2511.09775)
*Dilli Prasad Sharma,Xiaowei Sun,Liang Xue,Xiaodong Lin,Pulei Xiong*

Main category: cs.CR

TL;DR: 提出一种基于SHAP熵正则化的隐私保护方法，通过在训练过程中惩罚低熵SHAP归因分布来减少可解释AIoT应用中的隐私泄露风险。


<details>
  <summary>Details</summary>
Motivation: 智能家居环境中AIoT的广泛应用增加了对透明可解释机器学习模型的需求，但现有XAI方法（如SHAP、LIME）可能无意中暴露用户敏感属性和行为模式，带来新的隐私风险。

Method: 提出SHAP熵正则化方法，在训练过程中加入基于熵的正则化目标，惩罚低熵SHAP归因分布，促进特征贡献的更均匀分布。同时开发了一套基于SHAP的隐私攻击来评估方法有效性。

Result: 实验结果表明，SHAP熵正则化相比基线模型显著减少了隐私泄露，同时保持了高预测准确性和解释保真度。

Conclusion: 这项工作为开发安全可信的AIoT应用中的隐私保护可解释AI技术做出了贡献。

Abstract: The widespread integration of Artificial Intelligence of Things (AIoT) in smart home environments has amplified the demand for transparent and interpretable machine learning models. To foster user trust and comply with emerging regulatory frameworks, the Explainable AI (XAI) methods, particularly post-hoc techniques such as SHapley Additive exPlanations (SHAP), and Local Interpretable Model-Agnostic Explanations (LIME), are widely employed to elucidate model behavior. However, recent studies have shown that these explanation methods can inadvertently expose sensitive user attributes and behavioral patterns, thereby introducing new privacy risks. To address these concerns, we propose a novel privacy-preserving approach based on SHAP entropy regularization to mitigate privacy leakage in explainable AIoT applications. Our method incorporates an entropy-based regularization objective that penalizes low-entropy SHAP attribution distributions during training, promoting a more uniform spread of feature contributions. To evaluate the effectiveness of our approach, we developed a suite of SHAP-based privacy attacks that strategically leverage model explanation outputs to infer sensitive information. We validate our method through comparative evaluations using these attacks alongside utility metrics on benchmark smart home energy consumption datasets. Experimental results demonstrate that SHAP entropy regularization substantially reduces privacy leakage compared to baseline models, while maintaining high predictive accuracy and faithful explanation fidelity. This work contributes to the development of privacy-preserving explainable AI techniques for secure and trustworthy AIoT applications.

</details>


### [7] [DP-GENG : Differentially Private Dataset Distillation Guided by DP-Generated Data](https://arxiv.org/abs/2511.09876)
*Shuo Shi,Jinghuai Zhang,Shijie Jiang,Chunyi Zhou,Yuyuan Li,Mengying Zhu,Yangyang Wu,Tianyu Du*

Main category: cs.CR

TL;DR: 本文提出了Libn框架，通过使用差分隐私生成的数据来改进隐私保护数据集蒸馏方法，解决了现有DP-DD方法在数据真实性和效用方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私数据集蒸馏方法虽然通过注入噪声来保护隐私，但往往无法充分利用原始数据集，导致蒸馏数据的真实性和效用下降。

Method: Libn框架使用DP生成的数据初始化蒸馏数据集以增强真实性，通过改进的DP特征匹配技术在小隐私预算下蒸馏原始数据集，并训练专家模型使蒸馏样本与类别分布对齐，同时设计了隐私预算分配策略。

Result: 实验表明Libn在数据集效用和对抗成员推理攻击的鲁棒性方面显著优于现有的DP-DD方法。

Conclusion: Libn为隐私保护数据集蒸馏建立了新范式，在保护隐私的同时保持了高质量的数据效用。

Abstract: Dataset distillation (DD) compresses large datasets into smaller ones while preserving the performance of models trained on them. Although DD is often assumed to enhance data privacy by aggregating over individual examples, recent studies reveal that standard DD can still leak sensitive information from the original dataset due to the lack of formal privacy guarantees. Existing differentially private (DP)-DD methods attempt to mitigate this risk by injecting noise into the distillation process. However, they often fail to fully leverage the original dataset, resulting in degraded realism and utility. This paper introduces \libn, a novel framework that addresses the key limitations of current DP-DD by leveraging DP-generated data. Specifically, \lib initializes the distilled dataset with DP-generated data to enhance realism. Then, generated data refines the DP-feature matching technique to distill the original dataset under a small privacy budget, and trains an expert model to align the distilled examples with their class distribution. Furthermore, we design a privacy budget allocation strategy to determine budget consumption across DP components and provide a theoretical analysis of the overall privacy guarantees. Extensive experiments show that \lib significantly outperforms state-of-the-art DP-DD methods in terms of both dataset utility and robustness against membership inference attacks, establishing a new paradigm for privacy-preserving dataset distillation.

</details>


### [8] [Taught by the Flawed: How Dataset Insecurity Breeds Vulnerable AI Code](https://arxiv.org/abs/2511.09879)
*Catherine Xia,Manar H. Alalfi*

Main category: cs.CR

TL;DR: 通过使用静态分析工具过滤训练数据，构建安全数据集来训练AI编程助手，可减少生成代码中的安全漏洞，同时保持功能正确性。


<details>
  <summary>Details</summary>
Motivation: AI编程助手生成的代码经常包含基本安全漏洞，这主要源于训练数据中存在漏洞代码。需要提高生成代码的内在质量。

Method: 使用静态分析工具过滤现有Python语料库，构建仅包含无漏洞函数的安全数据集，并训练两个基于transformer的模型进行对比。

Result: 在安全数据集上训练的模型生成的代码安全漏洞更少，同时保持了与原始数据集训练模型相当的功能正确性。

Conclusion: 安全训练数据对于提高AI编程助手的可靠性至关重要，但还需要进一步改进模型架构和评估方法来巩固这些成果。

Abstract: AI programming assistants have demonstrated a tendency to generate code containing basic security vulnerabilities. While developers are ultimately responsible for validating and reviewing such outputs, improving the inherent quality of these generated code snippets remains essential. A key contributing factor to insecure outputs is the presence of vulnerabilities in the training datasets used to build large language models (LLMs). To address this issue, we propose curating training data to include only code that is free from detectable vulnerabilities. In this study, we constructed a secure dataset by filtering an existing Python corpus using a static analysis tool to retain only vulnerability-free functions. We then trained two transformer-based models: one on the curated dataset and one on the original, unfiltered dataset. The models were evaluated on both the correctness and security of the code they generated in response to natural language function descriptions. Our results show that the model trained on the curated dataset produced outputs with fewer security issues, while maintaining comparable functional correctness. These findings highlight the importance of secure training data in improving the reliability of AI-based programming assistants, though further enhancements to model architecture and evaluation are needed to reinforce these outcomes.

</details>


### [9] [Pack-A-Mal: A Malware Analysis Framework for Open-Source Packages](https://arxiv.org/abs/2511.09957)
*Duc-Ly Vu,Thanh-Cong Nguyen,Minh-Khanh Vu,Ngoc-Thanh Nguyen,Kim-Anh Do Thi*

Main category: cs.CR

TL;DR: 本文通过增强动态分析工具package-analysis，使用容器沙箱技术gVisor来更有效地检测开源项目中的恶意软件包，解决了静态分析工具在处理混淆恶意软件时的高误报率问题。


<details>
  <summary>Details</summary>
Motivation: 开源项目中恶意软件包普遍存在，静态分析工具如Malcontent在处理混淆恶意软件时能力有限，导致误报率过高，而动态分析虽然能提供更深入的运行时行为洞察，但资源消耗较大。

Method: 增强动态分析工具package-analysis，捕获关键运行时行为（执行的命令、访问的文件、网络通信），并集成容器沙箱技术gVisor来分析潜在恶意软件包，避免对主机系统造成严重影响。

Result: 通过改进的动态分析方法，能够更准确地检测恶意软件包，同时利用容器沙箱技术保证了分析过程的安全性。

Conclusion: 动态分析与容器沙箱技术的结合为开源软件安全提供了更有效的解决方案，能够在保证系统安全的同时准确识别恶意行为。

Abstract: The increasingly sophisticated environment in which attackers operate makes software security an even greater challenge in open-source projects, where malicious packages are prevalent. Static analysis tools, such as Malcontent, are highly useful but are often incapable of dealing with obfuscated malware. Such situations lead to an unreasonably high rate of false positives. This paper highlights that dynamic analysis, rather than static analysis, provides greater insight but is also more resource-intensive for understanding software behaviour during execution. In this study, we enhance a dynamic analysis tool, package-analysis, to capture key runtime behaviours, including commands executed, files accessed, and network communications. This modification enables the use of container sandboxing technologies, such as gVisor, to analyse potentially malicious packages without significantly compromising the host system.

</details>


### [10] [Trapped by Their Own Light: Deployable and Stealth Retroreflective Patch Attacks on Traffic Sign Recognition Systems](https://arxiv.org/abs/2511.10050)
*Go Tsuruoka,Takami Sato,Qi Alfred Chen,Kazuki Nomoto,Ryunosuke Kobayashi,Yuna Tanaka,Tatsuya Mori*

Main category: cs.CR

TL;DR: 提出了一种新型对抗性攻击方法ARP，利用逆反射材料在车辆头灯照射下激活，结合了补丁攻击的高部署性和激光投影的隐蔽性，在动态场景中达到93.4%的成功率，并提出相应的防御方案DPR Shield。


<details>
  <summary>Details</summary>
Motivation: 现有交通标志识别系统的对抗攻击方法存在视觉可检测性或实施限制的问题，表明TSR系统存在未被探索的脆弱性表面。

Method: 开发逆反射模拟方法，采用黑盒优化最大化攻击效果，利用逆反射材料仅在受害者头灯照射下激活的特性。

Result: ARP在35米动态场景中达到≥93.4%成功率，对商业TSR系统在真实条件下达到≥60%成功率，隐蔽性评分比之前补丁攻击高≥1.9%。

Conclusion: ARP攻击展示了结合高部署性和隐蔽性的新型攻击向量，同时提出的DPR Shield防御方案对停止标志和限速标志达到≥75%的防御成功率。

Abstract: Traffic sign recognition plays a critical role in ensuring safe and efficient transportation of autonomous vehicles but remain vulnerable to adversarial attacks using stickers or laser projections. While existing attack vectors demonstrate security concerns, they suffer from visual detectability or implementation constraints, suggesting unexplored vulnerability surfaces in TSR systems. We introduce the Adversarial Retroreflective Patch (ARP), a novel attack vector that combines the high deployability of patch attacks with the stealthiness of laser projections by utilizing retroreflective materials activated only under victim headlight illumination. We develop a retroreflection simulation method and employ black-box optimization to maximize attack effectiveness. ARP achieves $\geq$93.4\% success rate in dynamic scenarios at 35 meters and $\geq$60\% success rate against commercial TSR systems in real-world conditions. Our user study demonstrates that ARP attacks maintain near-identical stealthiness to benign signs while achieving $\geq$1.9\% higher stealthiness scores than previous patch attacks. We propose the DPR Shield defense, employing strategically placed polarized filters, which achieves $\geq$75\% defense success rates for stop signs and speed limit signs against micro-prism patches.

</details>


### [11] [An In-Depth Systematic Analysis of the Security, Usability, and Automation Capabilities of Password Update Processes on Top-Ranked Websites](https://arxiv.org/abs/2511.10111)
*Alexander Krause,Jacques Suray,Lea Schmüser,Marten Oltrogge,Oliver Wiese,Maximilian Golla,Sascha Fahl*

Main category: cs.CR

TL;DR: 对111个顶级网站密码更新流程的系统分析显示，这些流程存在高度多样性、复杂性，缺乏密码管理器支持，且安全措施经常阻碍自动化功能。


<details>
  <summary>Details</summary>
Motivation: 密码更新是账户安全的关键措施，但现有流程通常繁琐且需要手动创建密码，复杂的流程和缺乏密码管理器自动化能力影响了整体密码安全性。

Method: 对111个顶级网站的密码更新流程进行首次深入系统分析，评估其安全性、可用性和自动化能力。

Result: 网站部署的密码更新流程高度多样化且复杂，难以使用，用户难以跨网站转移经验。安全保护措施经常阻碍密码管理器自动化功能。

Conclusion: 研究结果为网页开发者、网络标准化社区和安全研究人员提供了改进密码更新流程的建议。

Abstract: Password updates are a critical account security measure and an essential part of the password lifecycle. Service providers and common security recommendations advise users to update their passwords in response to incidents or as a critical cyber hygiene measure. However, password update processes are often cumbersome and require manual password creation. Inconsistent and complex workflows and a lack of automation capabilities for password managers further negatively impact overall password security.
  In this work, we perform the first in-depth systematic analysis of 111 password update processes deployed on top-ranked websites. We provide novel insights into their overall security, usability, and automation capabilities and contribute to authentication security research through a better understanding of password update processes. Websites deploy highly diverse, often complex, confusing password update processes and lack the support of password managers. Processes are often hard to use, and end-users can barely transfer experiences and knowledge across websites. Notably, protective measures designed to enhance security frequently obstruct password manager automation. We conclude our work by discussing our findings and giving recommendations for web developers, the web standardization community, and security researchers.

</details>


### [12] [Pk-IOTA: Blockchain empowered Programmable Data Plane to secure OPC UA communications in Industry 4.0](https://arxiv.org/abs/2511.10248)
*Rinieri Lorenzo,Gori Giacomo,Melis Andrea,Girau Roberto,Prandini Marco,Callegati Franco*

Main category: cs.CR

TL;DR: Pk-IOTA是一个自动化解决方案，通过集成可编程数据平面交换机进行网络内证书验证，并利用IOTA Tangle进行去中心化证书分发，以保护OPC UA通信安全。


<details>
  <summary>Details</summary>
Motivation: OPC UA协议虽然具有强大的安全特性，但在实际部署中面临证书管理复杂性和工业设备安全功能实现不一致的挑战，需要自动化解决方案来确保通信安全。

Method: 集成可编程数据平面交换机进行网络内证书验证，并利用IOTA Tangle进行去中心化证书分发。

Result: 在代表真实工业场景的物理测试平台上评估显示，Pk-IOTA引入的开销最小，同时为OPC UA证书管理提供了可扩展且防篡改的机制。

Conclusion: Pk-IOTA能够有效解决OPC UA部署中的证书管理挑战，提供安全、可扩展的自动化证书管理方案。

Abstract: The OPC UA protocol is becoming the de facto standard for Industry 4.0 machine-to-machine communication. It stands out as one of the few industrial protocols that provide robust security features designed to prevent attackers from manipulating and damaging critical infrastructures. However, prior works showed that significant challenges still exists to set up secure OPC UA deployments in practice, mainly caused by the complexity of certificate management in industrial scenarios and the inconsistent implementation of security features across industrial OPC UA devices. In this paper, we present Pk-IOTA, an automated solution designed to secure OPC UA communications by integrating programmable data plane switches for in-network certificate validation and leveraging the IOTA Tangle for decen- tralized certificate distribution. Our evaluation is performed on a physical testbed representing a real-world industrial scenario and shows that Pk-IOTA introduces a minimal overhead while providing a scalable and tamper-proof mechanism for OPC UA certificate management.

</details>


### [13] [Enhanced Anonymous Credentials for E-Voting Systems](https://arxiv.org/abs/2511.10265)
*Tomasz Truderung*

Main category: cs.CR

TL;DR: 通过完美隐藏承诺增强匿名凭证机制，在保持永久隐私的同时加强选民与凭证的绑定关系


<details>
  <summary>Details</summary>
Motivation: 匿名凭证方法虽然简单实用，但在结合其他安全特性（如双设备验证、双因素认证）时存在挑战，需要加强选民与凭证的绑定关系

Method: 在匿名凭证机制基础上增加完美隐藏承诺，将凭证与选民身份关联起来

Result: 该方法既保持了选票与选民身份的不可链接性（永久隐私），又能在投票和审计过程中进行必要的连贯性检查

Conclusion: 提出的增强方案在保持电子投票系统永久隐私的同时，加强了凭证与选民身份的绑定关系

Abstract: A simple and practical method for achieving everlasting privacy in e-voting systems, without relying on advanced cryptographic techniques, is to use anonymous voter credentials. The simplicity of this approach may, however, create some challenges, when combined with other security features, such as cast-as-intended verifiability with second device and second-factor authentication.
  This paper considers a simple augmentation to the anonymous credential mechanism, using perfectly hiding commitments to link such credentials to the voter identities. This solution strengthens the binding between voters and their credentials while preserving everlasting privacy. It ensures that published ballots remain unlinkable to voter identities, yet enables necessary consistency checks during ballot casting and ballot auditing

</details>


### [14] [Enhanced Privacy Leakage from Noise-Perturbed Gradients via Gradient-Guided Conditional Diffusion Models](https://arxiv.org/abs/2511.10423)
*Jiayang Meng,Tao Huang,Hong Chen,Chen Hou,Guolong Zheng*

Main category: cs.CR

TL;DR: 提出了基于梯度引导条件扩散模型(GG-CDMs)的联邦学习隐私攻击方法，能够从噪声扰动梯度中重建私有图像，突破了现有防御机制的限制。


<details>
  <summary>Details</summary>
Motivation: 联邦学习通过梯度传输和聚合同步模型，但这些梯度包含敏感训练数据，存在隐私风险。现有梯度反演攻击在梯度被噪声扰动时重建性能显著下降，而噪声是常见的防御机制。

Method: 使用梯度引导条件扩散模型(GG-CDMs)，利用扩散模型固有的去噪能力来绕过噪声扰动提供的部分保护，在不知道目标数据分布先验知识的情况下重建私有图像。

Result: 在噪声扰动梯度下实现了优越的重建性能，并通过理论分析确定了噪声幅度和攻击模型架构等关键因素对重建质量的影响。

Conclusion: 该方法显著提升了在噪声防御机制下的攻击效果，证实了扩散模型在隐私攻击中的有效性，并提供了理论支撑。

Abstract: Federated learning synchronizes models through gradient transmission and aggregation. However, these gradients pose significant privacy risks, as sensitive training data is embedded within them. Existing gradient inversion attacks suffer from significantly degraded reconstruction performance when gradients are perturbed by noise-a common defense mechanism. In this paper, we introduce Gradient-Guided Conditional Diffusion Models (GG-CDMs) for reconstructing private images from leaked gradients without prior knowledge of the target data distribution. Our approach leverages the inherent denoising capability of diffusion models to circumvent the partial protection offered by noise perturbation, thereby improving attack performance under such defenses. We further provide a theoretical analysis of the reconstruction error bounds and the convergence properties of attack loss, characterizing the impact of key factors-such as noise magnitude and attacked model architecture-on reconstruction quality. Extensive experiments demonstrate our attack's superior reconstruction performance with Gaussian noise-perturbed gradients, and confirm our theoretical findings.

</details>


### [15] [On the Detectability of Active Gradient Inversion Attacks in Federated Learning](https://arxiv.org/abs/2511.10502)
*Vincenzo Carletti,Pasquale Foggia,Carlo Mazzocca,Giuseppe Parrella,Mario Vento*

Main category: cs.CR

TL;DR: 本文分析联邦学习中主动梯度反转攻击的可检测性，提出了基于统计异常权重结构和损失梯度动态的轻量级客户端检测方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然承诺保护隐私，但梯度反转攻击能够重构客户端本地数据。最近出现的主动攻击声称比以往更隐蔽，需要验证其可检测性。

Method: 提出基于统计异常权重结构和损失梯度动态的轻量级客户端检测技术，无需修改联邦学习训练协议。

Result: 在多种配置下的广泛评估表明，所提方法能有效检测主动梯度反转攻击。

Conclusion: 即使是最新的主动梯度反转攻击也是可检测的，客户端可以通过轻量级统计方法保护隐私。

Abstract: One of the key advantages of Federated Learning (FL) is its ability to collaboratively train a Machine Learning (ML) model while keeping clients' data on-site. However, this can create a false sense of security. Despite not sharing private data increases the overall privacy, prior studies have shown that gradients exchanged during the FL training remain vulnerable to Gradient Inversion Attacks (GIAs). These attacks allow reconstructing the clients' local data, breaking the privacy promise of FL. GIAs can be launched by either a passive or an active server. In the latter case, a malicious server manipulates the global model to facilitate data reconstruction. While effective, earlier attacks falling under this category have been demonstrated to be detectable by clients, limiting their real-world applicability. Recently, novel active GIAs have emerged, claiming to be far stealthier than previous approaches. This work provides the first comprehensive analysis of these claims, investigating four state-of-the-art GIAs. We propose novel lightweight client-side detection techniques, based on statistically improbable weight structures and anomalous loss and gradient dynamics. Extensive evaluation across several configurations demonstrates that our methods enable clients to effectively detect active GIAs without any modifications to the FL training protocol.

</details>


### [16] [How Worrying Are Privacy Attacks Against Machine Learning?](https://arxiv.org/abs/2511.10516)
*Josep Domingo-Ferrer*

Main category: cs.CR

TL;DR: 本文探讨了机器学习模型发布中的隐私风险，指出监管假设训练好的模型会带来与直接发布训练数据相当的隐私风险，但实际隐私攻击效果可能不如文献中显示的那么有效。


<details>
  <summary>Details</summary>
Motivation: 随着监管框架将个人数据保护扩展到机器学习领域，需要重新评估训练模型发布的实际隐私风险，因为现有文献可能高估了隐私攻击的实际效果。

Method: 分析了针对预测性和生成性机器学习的主要隐私攻击家族，包括成员推断攻击、属性推断攻击和重建攻击。

Result: 研究表明，大多数隐私攻击在现实世界中的效果似乎比文献表面显示的要差，实际隐私风险可能被高估。

Conclusion: 需要更准确地评估机器学习模型发布的实际隐私风险，避免基于表面解读文献而过度限制模型共享。

Abstract: In several jurisdictions, the regulatory framework on the release and sharing of personal data is being extended to machine learning (ML). The implicit assumption is that disclosing a trained ML model entails a privacy risk for any personal data used in training comparable to directly releasing those data. However, given a trained model, it is necessary to mount a privacy attack to make inferences on the training data. In this concept paper, we examine the main families of privacy attacks against predictive and generative ML, including membership inference attacks (MIAs), property inference attacks, and reconstruction attacks. Our discussion shows that most of these attacks seem less effective in the real world than what a prima face interpretation of the related literature could suggest.

</details>


### [17] [GraphFaaS: Serverless GNN Inference for Burst-Resilient, Real-Time Intrusion Detection](https://arxiv.org/abs/2511.10554)
*Lingzhi Wang,Vinod Yegneswaran,Xinyi Shi,Ziyu Li,Ashish Gehani,Yan Chen*

Main category: cs.CR

TL;DR: GraphFaaS是一个为基于GNN的入侵检测设计的无服务器架构，通过动态扩展GNN推理管道来解决传统静态配置架构在检测延迟和工作负载处理方面的不足。


<details>
  <summary>Details</summary>
Motivation: 传统静态配置的GNN推理架构无法满足入侵检测的两个关键需求：保持持续低检测延迟和处理高度不规则、突发的工作负载。

Method: 利用无服务器计算的弹性和敏捷性动态扩展GNN推理管道，将GNN工作流并行化并适配到无服务器环境中。

Result: 初步评估显示，与基线相比，GraphFaaS将平均检测延迟降低了85%，变异系数降低了64%。

Conclusion: 通过将计算资源与静态配置解耦，GraphFaaS能够提供稳定的推理延迟，这对于网络安全操作中可靠的入侵检测和及时的事件响应至关重要。

Abstract: Provenance-based intrusion detection is an increasingly popular application of graphical machine learning in cybersecurity, where system activities are modeled as provenance graphs to capture causality and correlations among potentially malicious actions. Graph Neural Networks (GNNs) have demonstrated strong performance in this setting. However, traditional statically-provisioned GNN inference architectures fall short in meeting two crucial demands of intrusion detection: (1) maintaining consistently low detection latency, and (2) handling highly irregular and bursty workloads. To holistically address these challenges, we present GraphFaaS, a serverless architecture tailored for GNN-based intrusion detection. GraphFaaS leverages the elasticity and agility of serverless computing to dynamically scale the GNN inference pipeline. We parallelize and adapt GNN workflows to a serverless environment, ensuring that the system can respond in real time to fluctuating workloads. By decoupling compute resources from static provisioning, GraphFaaS delivers stable inference latency, which is critical for dependable intrusion detection and timely incident response in cybersecurity operations. Preliminary evaluation shows GraphFaaS reduces average detection latency by 85% and coefficient of variation (CV) by 64% compared to the baseline.

</details>
