<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 28]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [ShellForge: Adversarial Co-Evolution of Webshell Generation and Multi-View Detection for Robust Webshell Defense](https://arxiv.org/abs/2601.22182)
*Yizhong Ding*

Main category: cs.CR

TL;DR: ShellForge是一个对抗性协同进化框架，通过自动生成webshell和多视图检测来增强PHP webshell防御能力，显著提高检测准确率并降低误报。


<details>
  <summary>Details</summary>
Motivation: 现有PHP webshell检测机制难以应对快速变种和复杂混淆技术，且对用于知识产权保护的良性混淆脚本误报率高，需要更鲁棒的防御方案。

Method: 采用对抗性协同训练框架，包含生成器和检测器循环强化。生成器通过监督微调和基于偏好的强化学习合成功能性、高规避性变种；检测器融合长字符串压缩语义特征、修剪抽象语法树结构特征和香农熵等全局统计指标；使用LLM生成去恶意样本作为高质量负样本。

Result: 在FWOID基准测试中，检测器保持0.981 F1分数，生成器在VirusTotal上对商业引擎达到0.939的规避率，显著提升防御鲁棒性。

Conclusion: ShellForge通过对抗性协同进化有效解决了webshell检测中的变种演化和误报问题，为PHP服务器安全提供了更强大的防御框架。

Abstract: Webshells remain a primary foothold for attackers to compromise servers, particularly within PHP ecosystems. However, existing detection mechanisms often struggle to keep pace with rapid variant evolution and sophisticated obfuscation techniques that camouflage malicious intent. Furthermore, many current defenses suffer from high false-alarm rates when encountering benign administrative scripts that employ heavy obfuscation for intellectual property protection. To address these challenges, we present ShellForge, an adversarial co-evolution framework that couples automated webshell generation with multi-view detection to continuously harden defensive boundaries. The framework operates through an iterative co-training loop where a generator and a detector mutually reinforce each other via the exchange of hard samples. The generator is optimized through supervised fine-tuning and preference-based reinforcement learning to synthesize functional, highly evasive variants. Simultaneously, we develop a multi-view fusion detector that integrates semantic features from long-string compression, structural features from pruned abstract syntax trees, and global statistical indicators such as Shannon entropy. To minimize false positives, ShellForge utilizes a LLM-based transformation to create de-malicious samples--scripts that retain complex obfuscation patterns but lack harmful payloads--serving as high-quality hard negatives during training. Evaluations on the public FWOID benchmark demonstrate that ShellForge significantly enhances defensive robustness. Upon convergence, the detector maintains a 0.981 F1-score while the generator achieves a 0.939 evasion rate against commercial engines on VirusTotal.

</details>


### [2] [MemeChain: A Multimodal Cross-Chain Dataset for Meme Coin Forensics and Risk Analysis](https://arxiv.org/abs/2601.22185)
*Alberto Maria Mongardini,Alessandro Mei*

Main category: cs.CR

TL;DR: MemeChain是一个大规模、开源、跨链数据集，包含34,988个meme币，整合了链上数据和链下文物（网站HTML、代币logo、社交媒体账户），用于研究meme币生态系统的风险建模和欺诈检测。


<details>
  <summary>Details</summary>
Motivation: meme币生态系统是加密货币市场中最活跃但最不可观察的部分，具有极高的流失率、最小的项目承诺和广泛的欺诈行为。现有数据集通常限于单链数据或缺乏多模态文物，无法进行全面风险建模。

Method: 构建MemeChain数据集，整合以太坊、BNB智能链、Solana和Base四个区块链上的34,988个meme币数据，包括链上交易数据和链下文物（网站HTML源代码、代币logo、链接的社交媒体账户）。

Result: 分析显示：1）低质量部署经常省略视觉品牌；2）许多项目缺乏功能性网站；3）生态系统极端波动，1,801个代币（5.15%）在启动后24小时内停止所有交易活动。

Conclusion: MemeChain通过提供统一的跨链覆盖和丰富的链下上下文，成为金融取证、多模态异常检测和meme币生态系统自动欺诈预防研究的基础资源。

Abstract: The meme coin ecosystem has grown into one of the most active yet least observable segments of the cryptocurrency market, characterized by extreme churn, minimal project commitment, and widespread fraudulent behavior. While countless meme coins are deployed across multiple blockchains, they rely heavily on off-chain web and social infrastructure to signal legitimacy. These very signals are largely absent from existing datasets, which are often limited to single-chain data or lack the multimodal artifacts required for comprehensive risk modeling.
  To address this gap, we introduce MemeChain, a large-scale, open-source, cross-chain dataset comprising 34,988 meme coins across Ethereum, BNB Smart Chain, Solana, and Base. MemeChain integrates on-chain data with off-chain artifacts, including website HTML source code, token logos, and linked social media accounts, enabling multimodal and forensic study of meme coin projects. Analysis of the dataset shows that visual branding is frequently omitted in low-effort deployments, and many projects lack a functional website. Moreover, we quantify the ecosystem's extreme volatility, identifying 1,801 tokens (5.15%) that cease all trading activity within just 24 hours of launch. By providing unified cross-chain coverage and rich off-chain context, MemeChain serves as a foundational resource for research in financial forensics, multimodal anomaly detection, and automated scam prevention in the meme coin ecosystem.

</details>


### [3] [A Systematic Literature Review on LLM Defenses Against Prompt Injection and Jailbreaking: Expanding NIST Taxonomy](https://arxiv.org/abs/2601.22240)
*Pedro H. Barcha Correia,Ryan W. Achjian,Diego E. G. Caetano de Oliveira,Ygor Acacio Maria,Victor Takashi Hayashi,Marcos Lopes,Charles Christian Miers,Marcos A. Simplicio*

Main category: cs.CR

TL;DR: 对88项研究的首次系统性文献综述，提出了提示注入防御策略的分类框架和实用资源目录


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI和大型语言模型的快速发展，出现了新的安全漏洞如越狱和提示注入攻击。这些恶意输入可能导致数据泄露、未授权操作或输出被篡改。由于攻防技术快速演进，需要系统性地理解防御策略。

Method: 基于NIST对抗机器学习报告，对88项研究进行系统性文献综述，扩展NIST分类法，引入新的防御类别，建立标准化术语和分类体系。

Result: 提出了扩展的NIST分类法，创建了全面的提示注入防御目录，记录了各项防御在特定LLM和攻击数据集上的定量效果，并标注了开源和模型无关的解决方案。

Conclusion: 该工作为对抗机器学习领域的研究人员和开发人员提供了实用的资源，包括分类框架、防御目录和实施指南，有助于推动该领域的标准化和实践应用。

Abstract: The rapid advancement and widespread adoption of generative artificial intelligence (GenAI) and large language models (LLMs) has been accompanied by the emergence of new security vulnerabilities and challenges, such as jailbreaking and other prompt injection attacks. These maliciously crafted inputs can exploit LLMs, causing data leaks, unauthorized actions, or compromised outputs, for instance. As both offensive and defensive prompt injection techniques evolve quickly, a structured understanding of mitigation strategies becomes increasingly important. To address that, this work presents the first systematic literature review on prompt injection mitigation strategies, comprehending 88 studies. Building upon NIST's report on adversarial machine learning, this work contributes to the field through several avenues. First, it identifies studies beyond those documented in NIST's report and other academic reviews and surveys. Second, we propose an extension to NIST taxonomy by introducing additional categories of defenses. Third, by adopting NIST's established terminology and taxonomy as a foundation, we promote consistency and enable future researchers to build upon the standardized taxonomy proposed in this work. Finally, we provide a comprehensive catalog of the reviewed prompt injection defenses, documenting their reported quantitative effectiveness across specific LLMs and attack datasets, while also indicating which solutions are open-source and model-agnostic. This catalog, together with the guidelines presented herein, aims to serve as a practical resource for researchers advancing the field of adversarial machine learning and for developers seeking to implement effective defenses in production systems.

</details>


### [4] [MirrorMark: A Distortion-Free Multi-Bit Watermark for Large Language Models](https://arxiv.org/abs/2601.22246)
*Ya Jiang,Massieh Kordi Boroujeny,Surender Suresh Kumar,Kai Zeng*

Main category: cs.CR

TL;DR: MirrorMark是一种用于大语言模型的多比特、无失真的水印方法，通过镜像采样随机性嵌入多比特信息而不改变token概率分布，保持文本质量的同时显著提升检测能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在问答和内容创作等应用中的普及，可靠的内容溯源变得日益重要。现有水印方法要么只能提供二进制信号，要么会扭曲采样分布降低文本质量；而无失真方法往往检测能力弱或鲁棒性差。

Method: 提出MirrorMark水印方法：1) 通过镜像采样随机性的保测度方式嵌入多比特信息，不改变token概率分布；2) 引入基于上下文的调度器，平衡不同消息位置的token分配，同时保持对插入和删除操作的鲁棒性；3) 提供错误率的理论分析来解释经验性能。

Result: 实验表明MirrorMark与非水印生成的文本质量相当，同时检测能力显著增强：在300个token中嵌入54比特时，比特准确率提高8-12%，在1%误报率下能正确识别多出11%的水印文本。

Conclusion: MirrorMark是一种有效的多比特、无失真水印方法，在保持文本质量的同时显著提升了水印的检测能力和鲁棒性，为大语言模型的内容溯源提供了实用解决方案。

Abstract: As large language models (LLMs) become integral to applications such as question answering and content creation, reliable content attribution has become increasingly important. Watermarking is a promising approach, but existing methods either provide only binary signals or distort the sampling distribution, degrading text quality; distortion-free approaches, in turn, often suffer from weak detectability or robustness. We propose MirrorMark, a multi-bit and distortion-free watermark for LLMs. By mirroring sampling randomness in a measure-preserving manner, MirrorMark embeds multi-bit messages without altering the token probability distribution, preserving text quality by design. To improve robustness, we introduce a context-based scheduler that balances token assignments across message positions while remaining resilient to insertions and deletions. We further provide a theoretical analysis of the equal error rate to interpret empirical performance. Experiments show that MirrorMark matches the text quality of non-watermarked generation while achieving substantially stronger detectability: with 54 bits embedded in 300 tokens, it improves bit accuracy by 8-12% and correctly identifies up to 11% more watermarked texts at 1% false positive rate.

</details>


### [5] [Rethinking Anonymity Claims in Synthetic Data Generation: A Model-Centric Privacy Attack Perspective](https://arxiv.org/abs/2601.22434)
*Georgi Ganev,Emiliano De Cristofaro*

Main category: cs.CR

TL;DR: 该论文从模型中心视角重新审视合成数据的匿名性，认为有意义的评估必须考虑生成模型的能力和特性，并基于最先进的隐私攻击。作者认为合成数据技术本身不足以保证充分匿名化，比较了差分隐私和基于相似性的隐私度量，指出差分隐私能提供更强保护。


<details>
  <summary>Details</summary>
Motivation: 当前合成数据隐私评估主要关注数据集层面，而现实世界中生成模型往往可访问用于交互或查询。现有研究、商业部署和GDPR等隐私法规对匿名性的评估未能充分考虑模型中心视角，导致隐私风险评估不充分。

Method: 从模型中心视角重新思考合成数据匿名性，将GDPR对个人数据和匿名化的定义置于模型可访问的假设下，识别需要缓解的可识别性风险类型，并将其映射到不同威胁设置下的隐私攻击。比较差分隐私和基于相似性的隐私度量两种机制。

Result: 合成数据技术本身不足以保证充分匿名化。差分隐私能针对可识别性风险提供稳健保护，而基于相似性的隐私度量缺乏足够的防护措施。建立了监管可识别性概念与模型中心隐私攻击之间的联系。

Conclusion: 需要从模型中心视角评估合成数据系统的隐私性，将监管概念与隐私攻击技术联系起来，为研究人员、从业者和政策制定者提供更负责任和可信赖的合成数据系统监管评估框架。

Abstract: Training generative machine learning models to produce synthetic tabular data has become a popular approach for enhancing privacy in data sharing. As this typically involves processing sensitive personal information, releasing either the trained model or generated synthetic datasets can still pose privacy risks. Yet, recent research, commercial deployments, and privacy regulations like the General Data Protection Regulation (GDPR) largely assess anonymity at the level of an individual dataset.
  In this paper, we rethink anonymity claims about synthetic data from a model-centric perspective and argue that meaningful assessments must account for the capabilities and properties of the underlying generative model and be grounded in state-of-the-art privacy attacks. This perspective better reflects real-world products and deployments, where trained models are often readily accessible for interaction or querying. We interpret the GDPR's definitions of personal data and anonymization under such access assumptions to identify the types of identifiability risks that must be mitigated and map them to privacy attacks across different threat settings. We then argue that synthetic data techniques alone do not ensure sufficient anonymization. Finally, we compare the two mechanisms most commonly used alongside synthetic data -- Differential Privacy (DP) and Similarity-based Privacy Metrics (SBPMs) -- and argue that while DP can offer robust protections against identifiability risks, SBPMs lack adequate safeguards. Overall, our work connects regulatory notions of identifiability with model-centric privacy attacks, enabling more responsible and trustworthy regulatory assessment of synthetic data systems by researchers, practitioners, and policymakers.

</details>


### [6] [FraudShield: Knowledge Graph Empowered Defense for LLMs against Fraud Attacks](https://arxiv.org/abs/2601.22485)
*Naen Xu,Jinghuai Zhang,Ping He,Chunyi Zhou,Jun Wang,Zhihui Fu,Tianyu Du,Zhaoxiang Wang,Shouling Ji*

Main category: cs.CR

TL;DR: FraudShield是一个保护LLMs免受欺诈内容攻击的新框架，通过构建欺诈战术-关键词知识图谱来增强模型安全性


<details>
  <summary>Details</summary>
Motivation: LLMs已广泛应用于关键自动化工作流程（如合同审查、求职申请），但容易受到欺诈信息的操纵，导致有害后果。现有防御方法在有效性、可解释性和泛化性方面存在局限。

Method: 构建并优化欺诈战术-关键词知识图谱，捕捉可疑文本与欺诈技术之间的高置信度关联。该结构化知识图谱通过突出关键词和提供支持证据来增强原始输入，引导LLM生成更安全的响应。

Result: 在四个主流LLMs和五种代表性欺诈类型上的广泛实验表明，FraudShield始终优于最先进的防御方法，同时为模型生成提供可解释的线索。

Conclusion: FraudShield通过结构化知识图谱方法有效保护LLMs免受欺诈内容攻击，在性能、可解释性和泛化性方面优于现有方法。

Abstract: Large language models (LLMs) have been widely integrated into critical automated workflows, including contract review and job application processes. However, LLMs are susceptible to manipulation by fraudulent information, which can lead to harmful outcomes. Although advanced defense methods have been developed to address this issue, they often exhibit limitations in effectiveness, interpretability, and generalizability, particularly when applied to LLM-based applications. To address these challenges, we introduce FraudShield, a novel framework designed to protect LLMs from fraudulent content by leveraging a comprehensive analysis of fraud tactics. Specifically, FraudShield constructs and refines a fraud tactic-keyword knowledge graph to capture high-confidence associations between suspicious text and fraud techniques. The structured knowledge graph augments the original input by highlighting keywords and providing supporting evidence, guiding the LLM toward more secure responses. Extensive experiments show that FraudShield consistently outperforms state-of-the-art defenses across four mainstream LLMs and five representative fraud types, while also offering interpretable clues for the model's generations.

</details>


### [7] [VocBulwark: Towards Practical Generative Speech Watermarking via Additional-Parameter Injection](https://arxiv.org/abs/2601.22556)
*Weizhi Liu,Yue Li,Zhaoxia Yin*

Main category: cs.CR

TL;DR: VocBulwark是一种语音水印框架，通过冻结生成模型参数保持音质，使用时间适配器将水印与声学属性深度纠缠，结合粗到细门控提取器抵抗攻击，实现高容量、高保真的鲁棒水印。


<details>
  <summary>Details</summary>
Motivation: 生成语音已达到人类自然水平，但增加了滥用风险。现有水印方法无法平衡保真度与鲁棒性，要么在噪声空间简单叠加，要么需要侵入式修改模型权重。

Method: 提出VocBulwark框架：1)冻结生成模型参数保持感知质量；2)设计时间适配器深度纠缠水印与声学属性；3)结合粗到细门控提取器抵抗高级攻击；4)开发精度引导优化课程动态协调梯度流，解决保真度与鲁棒性的优化冲突。

Result: 综合实验表明，VocBulwark实现高容量、高保真的水印，能有效防御复杂实际场景的攻击，对编解码器再生和变长操作具有鲁棒性。

Conclusion: VocBulwark成功解决了语音水印中保真度与鲁棒性的平衡问题，为生成语音的安全应用提供了有效保护方案。

Abstract: Generated speech achieves human-level naturalness but escalates security risks of misuse. However, existing watermarking methods fail to reconcile fidelity with robustness, as they rely either on simple superposition in the noise space or on intrusive alterations to model weights. To bridge this gap, we propose VocBulwark, an additional-parameter injection framework that freezes generative model parameters to preserve perceptual quality. Specifically, we design a Temporal Adapter to deeply entangle watermarks with acoustic attributes, synergizing with a Coarse-to-Fine Gated Extractor to resist advanced attacks. Furthermore, we develop an Accuracy-Guided Optimization Curriculum that dynamically orchestrates gradient flow to resolve the optimization conflict between fidelity and robustness. Comprehensive experiments demonstrate that VocBulwark achieves high-capacity and high-fidelity watermarking, offering robust defense against complex practical scenarios, with resilience to Codec regenerations and variable-length manipulations.

</details>


### [8] [Whispers of Wealth: Red-Teaming Google's Agent Payments Protocol via Prompt Injection](https://arxiv.org/abs/2601.22569)
*Tanusree Debi,Wentian Zhu*

Main category: cs.CR

TL;DR: 对AP2支付协议进行AI红队评估，发现其存在间接和直接提示注入漏洞，提出两种攻击技术，实验验证简单对抗提示可成功操纵购物代理行为


<details>
  <summary>Details</summary>
Motivation: LLM代理在金融交易中应用日益广泛，但其依赖上下文推理的特性使支付系统面临提示驱动的操纵风险。AP2协议旨在通过加密可验证授权保护代理主导的购买，但其实际鲁棒性尚未充分探索

Method: 对AP2进行AI红队评估，识别间接和直接提示注入漏洞，提出品牌耳语攻击和保险库耳语攻击两种技术，使用基于Gemini-2.5-Flash和Google ADK框架构建的AP2购物代理进行实验验证

Result: 实验验证简单对抗提示可以可靠地颠覆代理行为，攻击能够操纵产品排名和提取敏感用户数据，揭示了当前代理支付架构的关键弱点

Conclusion: 当前基于LLM的金融系统存在严重安全漏洞，需要更强的隔离和防御保护措施来确保代理支付架构的安全性

Abstract: Large language model (LLM) based agents are increasingly used to automate financial transactions, yet their reliance on contextual reasoning exposes payment systems to prompt-driven manipulation. The Agent Payments Protocol (AP2) aims to secure agent-led purchases through cryptographically verifiable mandates, but its practical robustness remains underexplored. In this work, we perform an AI red-teaming evaluation of AP2 and identify vulnerabilities arising from indirect and direct prompt injection. We introduce two attack techniques, the Branded Whisper Attack and the Vault Whisper Attack which manipulate product ranking and extract sensitive user data. Using a functional AP2 based shopping agent built with Gemini-2.5-Flash and the Google ADK framework, we experimentally validate that simple adversarial prompts can reliably subvert agent behavior. Our findings reveal critical weaknesses in current agentic payment architectures and highlight the need for stronger isolation and defensive safeguards in LLM-mediated financial systems.

</details>


### [9] [The Semantic Trap: Do Fine-tuned LLMs Learn Vulnerability Root Cause or Just Functional Pattern?](https://arxiv.org/abs/2601.22655)
*Feiyang Huang,Yuqiang Sun,Fan Zhang,Ziqi Yang,Han Liu,Yang Liu*

Main category: cs.CR

TL;DR: 研究发现微调后的LLMs在漏洞检测中主要依赖功能模式而非安全语义理解，存在"语义陷阱"现象，导致高基准分数具有误导性。


<details>
  <summary>Details</summary>
Motivation: 尽管微调后的LLMs在软件漏洞检测中表现出色，但尚不清楚这种改进是源于对漏洞根本原因的真正理解，还是仅仅利用了功能模式。需要系统评估LLMs是否真正理解安全语义。

Method: 提出TrapEval评估框架，包含两个互补数据集：V2N（漏洞代码与无关良性代码配对）和V2P（漏洞代码与其修补版本配对）。使用五个代表性LLMs进行微调，并通过跨数据集测试、语义保留扰动和CodeBLEU度量的语义差距进行评估。

Result: 微调后的LLMs难以区分漏洞代码与其修补版本，在微小语义保留变换下鲁棒性严重下降，当语义差距较小时严重依赖功能上下文捷径。传统数据集的高基准分数具有误导性。

Conclusion: 当前微调实践未能传授真正的漏洞推理能力，LLMs主要依赖功能模式而非安全语义理解。高基准分数掩盖了模型无法理解漏洞真正因果逻辑的问题，需要更严格的评估方法。

Abstract: LLMs demonstrate promising performance in software vulnerability detection after fine-tuning. However, it remains unclear whether these gains reflect a genuine understanding of vulnerability root causes or merely an exploitation of functional patterns. In this paper, we identify a critical failure mode termed the "semantic trap," where fine-tuned LLMs achieve high detection scores by associating certain functional domains with vulnerability likelihood rather than reasoning about the underlying security semantics.To systematically evaluate this phenomenon, we propose TrapEval, a comprehensive evaluation framework designed to disentangle vulnerability root cause from functional pattern. TrapEval introduces two complementary datasets derived from real-world open-source projects: V2N, which pairs vulnerable code with unrelated benign code, and V2P, which pairs vulnerable code with its corresponding patched version, forcing models to distinguish near-identical code that differs only in subtle security-critical logic. Using TrapEval, we fine-tune five representative state-of-the-art LLMs across three model families and evaluate them under cross-dataset testing, semantic-preserving perturbations, and varying degrees of semantic gap measured by CodeBLEU.Our empirical results reveal that, despite improvements in metrics, fine-tuned LLMs consistently struggle to distinguish vulnerable code from its patched counterpart, exhibit severe robustness degradation under minor semantic-preserving transformations, and rely heavily on functional-context shortcuts when the semantic gap is small. These findings provide strong evidence that current fine-tuning practices often fail to impart true vulnerability reasoning. Our findings serve as a wake-up call: high benchmark scores on traditional datasets may be illusory, masking the model's inability to understand the true causal logic of vulnerabilities.

</details>


### [10] [RealSec-bench: A Benchmark for Evaluating Secure Code Generation in Real-World Repositories](https://arxiv.org/abs/2601.22706)
*Yanlin Wang,Ziyao Zhang,Chong Wang,Xinyi Xu,Mingwei Liu,Yong Wang,Jiachi Chen,Zibin Zheng*

Main category: cs.CR

TL;DR: RealSec-bench是一个基于真实Java仓库构建的代码安全生成基准测试，包含105个实例，覆盖19种CWE类型。研究发现当前LLM在功能正确性和安全性之间存在显著差距，RAG技术对安全性提升有限，安全提示反而可能导致编译失败。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试大多依赖合成漏洞或孤立评估功能正确性，无法捕捉真实软件中功能与安全的复杂交互。需要构建基于真实世界代码的基准测试来评估LLM生成安全代码的能力。

Method: 采用多阶段流水线构建RealSec-bench：1) 使用CodeQL进行系统SAST扫描；2) LLM辅助消除误报；3) 人类专家严格验证。基准包含105个真实Java仓库实例，涵盖19种CWE类型，数据流复杂度最高达34跳过程间依赖。

Result: 对5个流行LLM的实证研究发现：1) RAG技术能提升功能正确性但对安全性改善有限；2) 显式安全提示常导致编译失败，损害功能正确性且不能可靠防止漏洞；3) 提出SecurePass@K复合指标同时评估功能正确性和安全性。

Conclusion: 当前LLM在功能正确性和安全代码生成之间存在显著差距，现有技术（如RAG和安全提示）无法有效解决此问题。需要开发新方法来提升LLM生成安全代码的能力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, but their proficiency in producing secure code remains a critical, under-explored area. Existing benchmarks often fall short by relying on synthetic vulnerabilities or evaluating functional correctness in isolation, failing to capture the complex interplay between functionality and security found in real-world software. To address this gap, we introduce RealSec-bench, a new benchmark for secure code generation meticulously constructed from real-world, high-risk Java repositories. Our methodology employs a multi-stage pipeline that combines systematic SAST scanning with CodeQL, LLM-based false positive elimination, and rigorous human expert validation. The resulting benchmark contains 105 instances grounded in real-word repository contexts, spanning 19 Common Weakness Enumeration (CWE) types and exhibiting a wide diversity of data flow complexities, including vulnerabilities with up to 34-hop inter-procedural dependencies. Using RealSec-bench, we conduct an extensive empirical study on 5 popular LLMs. We introduce a novel composite metric, SecurePass@K, to assess both functional correctness and security simultaneously. We find that while Retrieval-Augmented Generation (RAG) techniques can improve functional correctness, they provide negligible benefits to security. Furthermore, explicitly prompting models with general security guidelines often leads to compilation failures, harming functional correctness without reliably preventing vulnerabilities. Our work highlights the gap between functional and secure code generation in current LLMs.

</details>


### [11] [AlienLM: Alienization of Language for API-Boundary Privacy in Black-Box LLMs](https://arxiv.org/abs/2601.22710)
*Jaehee Kim,Pilsung Kang*

Main category: cs.CR

TL;DR: AlienLM是一个API隐私保护层，通过词汇级双射将文本转换为"外星语言"，在客户端无损恢复，仅使用标准微调API让目标模型直接在转换后的输入上工作，在保护隐私的同时保持81%以上的原始性能。


<details>
  <summary>Details</summary>
Motivation: 现代LLM越来越多地通过黑盒API访问，用户需要将敏感的提示、输出和微调数据传输到外部提供商，这在API边界造成了严重的隐私风险。需要一种可部署的隐私保护层来减少明文暴露。

Method: AlienLM通过词汇级双射将文本转换为"外星语言"，客户端可以无损恢复。使用Alien Adaptation Training (AAT)仅通过标准微调API，让目标模型直接在转换后的输入上工作。评估了四种LLM主干和七个基准测试。

Result: AlienLM平均保持超过81%的明文性能，显著优于随机双射和字符级基线。在对手拥有模型权重、语料统计和基于学习的逆翻译的情况下，恢复攻击只能重建少于0.22%的转换后标记。

Conclusion: AlienLM为API-only访问下的隐私保护LLM部署提供了实用途径，在保持任务性能的同时显著减少明文暴露，为隐私保护LLM部署开辟了可行路径。

Abstract: Modern LLMs are increasingly accessed via black-box APIs, requiring users to transmit sensitive prompts, outputs, and fine-tuning data to external providers, creating a critical privacy risk at the API boundary. We introduce AlienLM, a deployable API-only privacy layer that protects text by translating it into an Alien Language via a vocabulary-scale bijection, enabling lossless recovery on the client side. Using only standard fine-tuning APIs, Alien Adaptation Training (AAT) adapts target models to operate directly on alienized inputs. Across four LLM backbones and seven benchmarks, AlienLM retains over 81\% of plaintext-oracle performance on average, substantially outperforming random-bijection and character-level baselines. Under adversaries with access to model weights, corpus statistics, and learning-based inverse translation, recovery attacks reconstruct fewer than 0.22\% of alienized tokens. Our results demonstrate a practical pathway for privacy-preserving LLM deployment under API-only access, substantially reducing plaintext exposure while maintaining task performance.

</details>


### [12] [AEGIS: White-Box Attack Path Generation using LLMs and Training Effectiveness Evaluation for Large-Scale Cyber Defence Exercises](https://arxiv.org/abs/2601.22720)
*Ivan K. Tung,Yu Xiang Shi,Alex Chien,Wenkai Liu,Lawrence Zheng*

Main category: cs.CR

TL;DR: AEGIS系统使用LLM、白盒访问和蒙特卡洛树搜索自动生成网络防御演练的攻击路径，无需预先构建漏洞图，将场景开发时间从数月缩短到数天。


<details>
  <summary>Details</summary>
Motivation: 当前网络防御演练的攻击路径创建需要大量专家工作，现有自动化方法需要预先构建漏洞图或利用集，限制了应用范围。需要一种能够动态发现利用链而不依赖预定义漏洞图的自动化系统。

Method: AEGIS系统结合LLM进行动态漏洞发现、白盒访问验证单个利用的有效性，以及蒙特卡洛树搜索在真实利用执行上进行路径规划。LLM无需预定义漏洞图即可发现利用，白盒访问确保在提交攻击路径前验证利用的有效性。

Result: 在CIDeX 2025大规模演练（涵盖46个IT主机）中评估显示，AEGIS生成的攻击路径在四个训练体验维度（感知学习、参与度、可信度、挑战性）上与人工编写的场景相当。使用可扩展到一般模拟训练的验证问卷进行测量。

Conclusion: AEGIS通过自动化利用链发现和验证，将场景开发时间从数月缩短到数天，将专家工作从技术验证转向场景设计，为网络防御演练提供了高效且质量相当的自动化解决方案。

Abstract: Creating attack paths for cyber defence exercises requires substantial expert effort. Existing automation requires vulnerability graphs or exploit sets curated in advance, limiting where it can be applied. We present AEGIS, a system that generates attack paths using LLMs, white-box access, and Monte Carlo Tree Search over real exploit execution. LLM-based search discovers exploits dynamically without pre-existing vulnerability graphs, while white-box access enables validating exploits in isolation before committing to attack paths. Evaluation at CIDeX 2025, a large-scale exercise spanning 46 IT hosts, showed that AEGIS-generated paths are comparable to human-authored scenarios across four dimensions of training experience (perceived learning, engagement, believability, challenge). Results were measured with a validated questionnaire extensible to general simulation-based training. By automating exploit chain discovery and validation, AEGIS reduces scenario development from months to days, shifting expert effort from technical validation to scenario design.

</details>


### [13] [Okara: Detection and Attribution of TLS Man-in-the-Middle Vulnerabilities in Android Apps with Foundation Models](https://arxiv.org/abs/2601.22770)
*Haoyun Yang,Ronghong Huang,Yong Fang,Beizeng Zhang,Junpu Guo,Zhanyu Wu,Xianghang Mi*

Main category: cs.CR

TL;DR: Okara框架利用基础模型自动化检测和深度归因TLS中间人攻击漏洞，在37,349个Android应用中发现了22.42%存在漏洞，并开发了新的分类法和分析工具。


<details>
  <summary>Details</summary>
Motivation: TLS是安全在线通信的基础，但Android应用中证书验证漏洞导致的中间人攻击仍然普遍存在。现有检测工具存在UI交互覆盖率低、插桩成本高、缺乏可扩展的根因分析等问题。

Method: 提出了Okara框架，包含两个组件：TMV-Hunter使用基础模型驱动的GUI代理实现高覆盖率应用交互，高效发现漏洞；TMV-ORCA结合动态插桩和基于LLM的分类器，按照新的分类法定位和分类漏洞代码。

Result: 在Google Play和第三方商店的37,349个应用中，发现8,374个（22.42%）存在漏洞。这些漏洞在所有流行度级别都广泛存在，影响认证和代码交付等关键功能，中位漏洞寿命超过1,300天。41%的漏洞归因于第三方库，并识别出空信任管理器、主机名验证缺陷等常见不安全模式。

Conclusion: TLS中间人攻击漏洞在Android应用中普遍且持久，Okara框架通过基础模型驱动的自动化检测和深度归因分析，有效识别和分类这些漏洞，支持大规模负责任披露和进一步研究。

Abstract: Transport Layer Security (TLS) is fundamental to secure online communication, yet vulnerabilities in certificate validation that enable Man-in-the-Middle (MitM) attacks remain a pervasive threat in Android apps. Existing detection tools are hampered by low-coverage UI interaction, costly instrumentation, and a lack of scalable root-cause analysis. We present Okara, a framework that leverages foundation models to automate the detection and deep attribution of TLS MitM Vulnerabilities (TMVs). Okara's detection component, TMV-Hunter, employs foundation model-driven GUI agents to achieve high-coverage app interaction, enabling efficient vulnerability discovery at scale. Deploying TMV-Hunter on 37,349 apps from Google Play and a third-party store revealed 8,374 (22.42%) vulnerable apps. Our measurement shows these vulnerabilities are widespread across all popularity levels, affect critical functionalities like authentication and code delivery, and are highly persistent with a median vulnerable lifespan of over 1,300 days. Okara's attribution component, TMV-ORCA, combines dynamic instrumentation with a novel LLM-based classifier to locate and categorize vulnerable code according to a comprehensive new taxonomy. This analysis attributes 41% of vulnerabilities to third-party libraries and identifies recurring insecure patterns, such as empty trust managers and flawed hostname verification. We have initiated a large-scale responsible disclosure effort and will release our tools and datasets to support further research and mitigation.

</details>


### [14] [Rust and Go directed fuzzing with LibAFL-DiFuzz](https://arxiv.org/abs/2601.22772)
*Timofey Mezhuev,Darya Parygina,Daniil Kuts*

Main category: cs.CR

TL;DR: 本文提出针对Rust和Go语言的定向灰盒模糊测试方法，通过编译器定制和高级预处理技术，在TTE指标上优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 随着Rust和Go语言的流行，需要为这些语言提供精确高效的测试方案。传统定向模糊测试主要针对C/C++，需要扩展到新兴语言。

Method: 提出针对Rust和Go的定向灰盒模糊测试方法，包括高级预处理技术、rustc编译器定制、精细的图构建和插桩方法，基于LibAFL-DiFuzz后端实现。

Result: Rust-LibAFL-DiFuzz在TTE实验中表现最佳，Go-LibAFL-DiFuzz在多数情况下优于现有工具，部分案例有数量级差异。

Conclusion: 该方法在Rust和Go语言的定向模糊测试中展现出更好的效率和准确性，证明了将定向模糊测试扩展到非C/C++语言的可行性。

Abstract: In modern SSDLC, program analysis and automated testing are essential for minimizing vulnerabilities before software release, with fuzzing being a fast and widely used dynamic testing method. However, traditional coverage-guided fuzzing may be less effective in specific tasks like verifying static analysis reports or reproducing crashes, while directed fuzzing, focusing on targeted program locations using proximity metrics, proves to be more effective. Some of the earliest directed fuzzers are, for example, AFLGo and BEACON, which use different proximity metric approaches. Although most automated testing tools focus on C/C++ code, the growing popularity of Rust and Go causes the need for precise and efficient testing solutions for these languages. This work expands the applicability of directed fuzzing beyond traditional analysis of C/C++ software. We present a novel approach to directed greybox fuzzing tailored specifically for Rust and Go applications. We introduce advanced preprocessing techniques, rustc compiler customizations, and elaborate graph construction and instrumentation methods to enable effective targeting of specific program locations. Our implemented fuzzing tools, based on LibAFL-DiFuzz backend, demonstrate competitive advantages compared to popular existing fuzzers like afl.rs, cargo-fuzz, and go-fuzz. According to TTE (Time to Exposure) experiments, Rust-LibAFL-DiFuzz outperforms other tools by the best TTE result. Some stability issues can be explained by different mutation approaches. Go-LibAFL-DiFuzz outperforms its opponent by the best and, in the majority of cases, by average result, having two cases with orders of magnitude difference. These results prove better efficiency and accuracy of our approach.

</details>


### [15] [Trackly: A Unified SaaS Platform for User Behavior Analytics and Real Time Rule Based Anomaly Detection](https://arxiv.org/abs/2601.22800)
*Md Zahurul Haque,Md. Hafizur Rahman,Yeahyea Sarker*

Main category: cs.CR

TL;DR: Trackly是一个统一的SaaS平台，将用户行为分析与实时异常检测相结合，通过可配置规则和加权风险评分来识别可疑活动，为中小企业提供高效的安全分析解决方案。


<details>
  <summary>Details</summary>
Motivation: 大多数平台将产品分析和安全功能分离，导致碎片化的可视性和延迟的威胁检测。需要统一用户行为分析和实时安全监控，以改善数字体验、优化业务转化并缓解账户劫持、欺诈和机器人攻击等威胁。

Method: Trackly平台跟踪会话、基于IP的地理位置、设备浏览器指纹和细粒度事件（如页面浏览、加入购物车、结账）。通过可配置规则和加权风险评分系统检测可疑活动，包括新设备/位置登录、不可能旅行（使用Haversine公式）、快速机器人式操作、VPN/代理使用、单IP多账户等。采用轻量级JavaScript SDK和安全REST API简化集成，基于多租户微服务架构（ASP.NET Core、MongoDB、RabbitMQ、Next.js）实现。

Result: 在合成数据集上，Trackly实现了98.1%的准确率、97.7%的精确度和2.25%的误报率，证明了其对中小企业和电子商务的高效性。实时仪表板提供全球会话地图、日活/月活用户、跳出率和会话时长等指标。

Conclusion: Trackly成功统一了用户行为分析和实时安全监控，提供透明、可解释的决策支持，解决了传统平台中产品分析和安全功能分离的问题，为中小企业提供了高效的可扩展解决方案。

Abstract: Understanding user behavior is essential for improving digital experiences, optimizing business conversions, and mitigating threats like account takeovers, fraud, and bot attacks. Most platforms separate product analytics and security, creating fragmented visibility and delayed threat detection. Trackly, a scalable SaaS platform, unifies comprehensive user behavior analytics with real time, rule based anomaly detection. It tracks sessions, IP based geo location, device browser fingerprints, and granular events such as page views, add to cart, and checkouts. Suspicious activities logins from new devices or locations, impossible travel (Haversine formula), rapid bot like actions, VPN proxy usage, or multiple accounts per IP are flagged via configurable rules with weighted risk scoring, enabling transparent, explainable decisions. A real time dashboard provides global session maps, DAU MAU, bounce rates, and session durations. Integration is simplified with a lightweight JavaScript SDK and secure REST APIs. Implemented on a multi tenant microservices stack (ASP.NET Core, MongoDB, RabbitMQ, Next.js), Trackly achieved 98.1% accuracy, 97.7% precision, and 2.25% false positives on synthetic datasets, proving its efficiency for SMEs and ecommerce.

</details>


### [16] [Trojan-Resilient NTT: Protecting Against Control Flow and Timing Faults on Reconfigurable Platforms](https://arxiv.org/abs/2601.22804)
*Rourab Paul,Krishnendu Guha,Amlan Chakrabarti*

Main category: cs.CR

TL;DR: 提出一种安全的NTT架构，能够检测非常规延迟、控制流中断和SASCA攻击，并提供自适应故障校正方法，在FPGA实现中显示高效检测和校正能力。


<details>
  <summary>Details</summary>
Motivation: NTT是后量子密码算法中的核心组件，但面临侧信道攻击和硬件木马的威胁。硬件木马特别是针对控制信号的攻击成本低、影响大，单个被破坏的控制信号就能中断整个计算序列，而数据故障通常只造成局部错误。此外，攻击者可以利用插入的硬件木马执行软分析侧信道攻击。

Method: 提出一种安全的NTT架构，能够检测非常规延迟、控制流中断和SASCA攻击，并提供自适应故障校正方法。在Artix-7 FPGA上对不同Kyber变体进行了广泛的仿真和实现。

Result: 故障检测和校正模块能够高效检测和校正由硬件木马有意或无意引起的故障，具有高成功率，同时只引入适度的面积和时间开销。

Conclusion: 提出的安全NTT架构能够有效应对硬件木马和侧信道攻击的威胁，为后量子密码算法提供可靠的硬件安全保护。

Abstract: Number Theoretic Transform (NTT) is the most essential component for polynomial multiplications used in lattice-based Post-Quantum Cryptography (PQC) algorithms such as Kyber, Dilithium, NTRU etc. However, side-channel attacks (SCA) and hardware vulnerabilities in the form of hardware Trojans may alter control signals to disrupt the circuit's control flow and introduce unconventional delays in the critical hardware of PQC. Hardware Trojans, especially on control signals, are more low cost and impactful than data signals because a single corrupted control signal can disrupt or bypass entire computation sequences, whereas data faults usually cause only localized errors. On the other hand, adversaries can perform Soft Analytical Side Channel Attacks (SASCA) on the design using the inserted hardware Trojan. In this paper, we present a secure NTT architecture capable of detecting unconventional delays, control-flow disruptions, and SASCA, while providing an adaptive fault-correction methodology for their mitigation. Extensive simulations and implementations of our Secure NTT on Artix-7 FPGA with different Kyber variants show that our fault detection and correction modules can efficiently detect and correct faults whether caused unintentionally or intentionally by hardware Trojans with a high success rate, while introducing only modest area and time overheads.

</details>


### [17] [Hide and Seek in Embedding Space: Geometry-based Steganography and Detection in Large Language Models](https://arxiv.org/abs/2601.22818)
*Charles Westphal,Keivan Navaie,Fernando E. Rosas*

Main category: cs.CR

TL;DR: 论文研究LLM微调中的隐写攻击，提出低可恢复性隐写方案，并探讨基于机制可解释性的检测方法


<details>
  <summary>Details</summary>
Motivation: 现有研究表明微调后的LLM可以通过隐写通道将提示秘密编码到输出中，但先前方法依赖易于恢复的编码方式。本文旨在形式化有效载荷可恢复性，并提出更隐蔽的低可恢复性隐写方案，同时探索有效的检测方法。

Method: 1. 形式化有效载荷可恢复性，用分类器准确率衡量；2. 提出低可恢复性隐写方案，用嵌入空间派生的映射替代任意映射；3. 提出基于机制可解释性的检测方法，使用线性探针分析深层激活来检测秘密。

Result: 1. 低可恢复性方案显著提升秘密恢复率：Llama-8B从17%→30%(+78%)，Ministral-8B从24%→43%(+80%)，Llama-70B从9%→19%(+123%)，同时降低有效载荷可恢复性；2. 基于机制可解释性的检测方法在微调模型中比基础模型检测准确率高33%，即使对低可恢复性方案也有效。

Conclusion: 恶意微调会在LLM内部留下可操作的特征，这些特征可通过机制可解释性方法检测。传统隐写分析方法不适用于检测基于微调的隐写攻击，需要开发基于内部激活特征的新型防御方法。

Abstract: Fine-tuned LLMs can covertly encode prompt secrets into outputs via steganographic channels. Prior work demonstrated this threat but relied on trivially recoverable encodings. We formalize payload recoverability via classifier accuracy and show previous schemes achieve 100\% recoverability. In response, we introduce low-recoverability steganography, replacing arbitrary mappings with embedding-space-derived ones. For Llama-8B (LoRA) and Ministral-8B (LoRA) trained on TrojanStego prompts, exact secret recovery rises from 17$\rightarrow$30\% (+78\%) and 24$\rightarrow$43\% (+80\%) respectively, while on Llama-70B (LoRA) trained on Wiki prompts, it climbs from 9$\rightarrow$19\% (+123\%), all while reducing payload recoverability. We then discuss detection. We argue that detecting fine-tuning-based steganographic attacks requires approaches beyond traditional steganalysis. Standard approaches measure distributional shift, which is an expected side-effect of fine-tuning. Instead, we propose a mechanistic interpretability approach: linear probes trained on later-layer activations detect the secret with up to 33\% higher accuracy in fine-tuned models compared to base models, even for low-recoverability schemes. This suggests that malicious fine-tuning leaves actionable internal signatures amenable to interpretability-based defenses.

</details>


### [18] [Assessing the Real-World Impact of Post-Quantum Cryptography on WPA-Enterprise Networks](https://arxiv.org/abs/2601.22892)
*Lukas Köder,Nils Lohmiller,Phil Schmieder,Bastian Buck,Michael Menth,Tobias Heer*

Main category: cs.CR

TL;DR: 该论文首次对后量子密码算法在WPA-Enterprise认证中的性能影响进行了真实世界评估，发现ML-DSA-65和Falcon-1024与ML-KEM结合能在安全与性能间取得良好平衡，且会话恢复能有效缓解额外延迟。


<details>
  <summary>Details</summary>
Motivation: 大规模量子计算机的出现对包括WPA-Enterprise认证在内的现代网络安全协议构成重大威胁，需要采用后量子密码学来应对这一威胁，但需要评估其对认证性能的实际影响。

Method: 使用FreeRADIUS和hostapd构建实验测试床，测量客户端、接入点和RADIUS服务器的认证延迟，评估多种PQC算法组合的性能开销，并与现有加密方案对比，同时分析算法选择的安全影响。

Result: PQC确实会引入额外的认证延迟，但ML-DSA-65和Falcon-1024与ML-KEM的组合在安全与性能间提供了良好的权衡；通过会话恢复可以有效缓解由此产生的开销。

Conclusion: 这是首次对支持PQC的WPA-Enterprise认证进行真实世界性能评估，证明了其在企业Wi-Fi部署中的实际可行性，为后量子时代的企业网络安全提供了实践指导。

Abstract: The advent of large-scale quantum computers poses a significant threat to contemporary network security protocols, including Wi-Fi Protected Access (WPA)-Enterprise authentication. To mitigate this threat, the adoption of Post-Quantum Cryptography (PQC) is critical. In this work, we investigate the performance impact of PQC algorithms on WPA-Enterprise-based authentication. To this end, we conduct an experimental evaluation of authentication latency using a testbed built with the open-source tools FreeRADIUS and hostapd, measuring the time spent at the client, access point, and RADIUS server. We evaluate multiple combinations of PQC algorithms and analyze their performance overhead in comparison to currently deployed cryptographic schemes. Beyond performance, we assess the security implications of these algorithm choices by relating authentication mechanisms to the quantum effort required for their exploitation. This perspective enables a systematic categorization of PQ-relevant weaknesses in WPA-Enterprise according to their practical urgency. The evaluation results show that, although PQC introduces additional authentication latency, combinations such as ML-DSA-65 and Falcon-1024 used in conjunction with ML-KEM provide a favorable trade-off between security and performance. Furthermore, we demonstrate that the resulting overhead can be effectively mitigated through session resumption. Overall, this work presents a first real-world performance evaluation of PQC-enabled WPA-Enterprise authentication and demonstrates its practical feasibility for enterprise Wi-Fi deployments.

</details>


### [19] [Evaluating Large Language Models for Security Bug Report Prediction](https://arxiv.org/abs/2601.22921)
*Farnaz Soltaniani,Shoaib Razzaq,Mohammad Ghafari*

Main category: cs.CR

TL;DR: 评估基于提示工程和微调方法使用大语言模型预测安全漏洞报告的性能，发现提示方法召回率高但精度低，微调方法精度高但召回率低，存在明显权衡


<details>
  <summary>Details</summary>
Motivation: 安全漏洞报告的早期检测对及时缓解漏洞至关重要，需要评估不同LLM方法在SBR预测中的性能表现

Method: 比较基于提示工程的专有模型和微调模型两种方法，评估它们在安全漏洞报告预测任务上的性能

Result: 提示专有模型平均G-measure 77%，召回率74%，但精度仅22%；微调模型G-measure 51%，精度75%，召回率36%；微调模型推理速度比专有模型快50倍

Conclusion: 两种方法存在明显权衡：提示方法敏感度高但假阳性多，微调方法精度高但召回率低，需要进一步研究以充分利用LLM进行SBR预测

Abstract: Early detection of security bug reports (SBRs) is critical for timely vulnerability mitigation. We present an evaluation of prompt-based engineering and fine-tuning approaches for predicting SBRs using Large Language Models (LLMs). Our findings reveal a distinct trade-off between the two approaches. Prompted proprietary models demonstrate the highest sensitivity to SBRs, achieving a G-measure of 77% and a recall of 74% on average across all the datasets, albeit at the cost of a higher false-positive rate, resulting in an average precision of only 22%. Fine-tuned models, by contrast, exhibit the opposite behavior, attaining a lower overall G-measure of 51% but substantially higher precision of 75% at the cost of reduced recall of 36%. Though a one-time investment in building fine-tuned models is necessary, the inference on the largest dataset is up to 50 times faster than that of proprietary models. These findings suggest that further investigations to harness the power of LLMs for SBR prediction are necessary.

</details>


### [20] [Protecting Private Code in IDE Autocomplete using Differential Privacy](https://arxiv.org/abs/2601.22935)
*Evgeny Grigorenko,David Stanojević,David Ilić,Egor Bogomolov,Kostadin Cvejoski*

Main category: cs.CR

TL;DR: 该论文研究了在Kotlin代码补全LLM训练中使用差分隐私(DP)作为防御机制，证明DP能有效抵御成员推理攻击，同时保持模型性能，仅需1%的数据即可达到与非私有模型相当的效果。


<details>
  <summary>Details</summary>
Motivation: 现代IDE使用LLM进行代码补全，但训练这些模型时使用用户代码会带来隐私风险，恶意攻击者可能重构敏感训练数据或推断特定代码片段是否被用于训练。

Method: 使用差分隐私(DP)训练Kotlin代码补全的LLM，对Mellum模型进行DP微调，并全面评估其隐私性和实用性。

Result: DP能有效防御成员推理攻击(MIA)，将攻击成功率降低至接近随机猜测水平(AUC从0.901降至0.606)。DP训练模型在性能损失极小的情况下，即使使用100倍少的数据也能达到与非私有模型相当的实用分数。

Conclusion: 差分隐私是构建私密且可信赖的AI驱动IDE功能的实用有效解决方案，能在保护隐私的同时保持模型性能。

Abstract: Modern Integrated Development Environments (IDEs) increasingly leverage Large Language Models (LLMs) to provide advanced features like code autocomplete. While powerful, training these models on user-written code introduces significant privacy risks, making the models themselves a new type of data vulnerability. Malicious actors can exploit this by launching attacks to reconstruct sensitive training data or infer whether a specific code snippet was used for training. This paper investigates the use of Differential Privacy (DP) as a robust defense mechanism for training an LLM for Kotlin code completion. We fine-tune a \texttt{Mellum} model using DP and conduct a comprehensive evaluation of its privacy and utility. Our results demonstrate that DP provides a strong defense against Membership Inference Attacks (MIAs), reducing the attack's success rate close to a random guess (AUC from 0.901 to 0.606). Furthermore, we show that this privacy guarantee comes at a minimal cost to model performance, with the DP-trained model achieving utility scores comparable to its non-private counterpart, even when trained on 100x less data. Our findings suggest that DP is a practical and effective solution for building private and trustworthy AI-powered IDE features.

</details>


### [21] [A Real-Time Privacy-Preserving Behavior Recognition System via Edge-Cloud Collaboration](https://arxiv.org/abs/2601.22938)
*Huan Song,Shuyu Tian,Junyi Hao,Cheng Yuan,Zhenyu Jia,Jiawei Shao,Xuelong Li*

Main category: cs.CR

TL;DR: 提出基于AI Flow理论和边缘云协作架构的隐私保护感知技术，通过源端脱敏和不可逆特征映射，在边缘设备将原始图像转换为抽象特征向量，云端仅基于这些向量进行异常行为检测，实现从视频监控到去身份化行为感知的突破。


<details>
  <summary>Details</summary>
Motivation: 智能感知扩展到卫生间、更衣室等高隐私环境时面临隐私安全悖论：传统RGB监控存在视觉记录和存储的隐私担忧，现有隐私保护方法要么损害语义理解能力，要么无法保证数学不可逆性对抗重建攻击。

Method: 基于AI Flow理论框架和边缘云协作架构，集成源端脱敏与不可逆特征映射。利用信息瓶颈理论，边缘设备通过非线性映射和随机噪声注入，将原始图像转换为抽象特征向量，构建单向信息流；云端使用多模态家族模型仅基于这些抽象向量进行联合推理检测异常行为。

Result: 该方法在架构层面从根本上切断了隐私泄露路径，实现了从视频监控到去身份化行为感知的突破，为高敏感性公共空间的风险管理提供了鲁棒解决方案。

Conclusion: 提出的隐私保护感知技术通过不可逆特征映射和边缘云协作架构，解决了高隐私环境中的隐私安全悖论，在保护个人隐私的同时保持了语义理解能力，为智能感知在敏感场景的应用提供了可行方案。

Abstract: As intelligent sensing expands into high-privacy environments such as restrooms and changing rooms, the field faces a critical privacy-security paradox. Traditional RGB surveillance raises significant concerns regarding visual recording and storage, while existing privacy-preserving methods-ranging from physical desensitization to traditional cryptographic or obfuscation techniques-often compromise semantic understanding capabilities or fail to guarantee mathematical irreversibility against reconstruction attacks. To address these challenges, this study presents a novel privacy-preserving perception technology based on the AI Flow theoretical framework and an edge-cloud collaborative architecture. The proposed methodology integrates source desensitization with irreversible feature mapping. Leveraging Information Bottleneck theory, the edge device performs millisecond-level processing to transform raw imagery into abstract feature vectors via non-linear mapping and stochastic noise injection. This process constructs a unidirectional information flow that strips identity-sensitive attributes, rendering the reconstruction of original images impossible. Subsequently, the cloud platform utilizes multimodal family models to perform joint inference solely on these abstract vectors to detect abnormal behaviors. This approach fundamentally severs the path to privacy leakage at the architectural level, achieving a breakthrough from video surveillance to de-identified behavior perception and offering a robust solution for risk management in high-sensitivity public spaces.

</details>


### [22] [From Data Leak to Secret Misses: The Impact of Data Leakage on Secret Detection Models](https://arxiv.org/abs/2601.22946)
*Farnaz Soltaniani,Mohammad Ghafari*

Main category: cs.CR

TL;DR: 研究发现机器学习安全工具在重复数据泄露下性能虚高，实际效果被夸大


<details>
  <summary>Details</summary>
Motivation: 机器学习模型越来越多用于软件安全任务，这些模型通常在大型互联网数据集上训练和评估，这些数据集常包含重复或高度相似的样本。当这些样本分布在训练集和测试集之间时，可能导致数据泄露，使模型能够记忆模式而非学习泛化能力。

Method: 研究分析了广泛使用的硬编码密钥基准数据集中的重复问题，展示了数据泄露如何显著夸大基于AI的密钥检测器报告的性能。

Result: 数据泄露会大幅虚高AI密钥检测器的报告性能，导致对其真实世界有效性的误导性评估。

Conclusion: 当前机器学习安全工具的性能评估存在数据泄露问题，需要更严谨的数据处理和评估方法才能准确反映其实际应用效果。

Abstract: Machine learning models are increasingly used for software security tasks. These models are commonly trained and evaluated on large Internet-derived datasets, which often contain duplicated or highly similar samples. When such samples are split across training and test sets, data leakage may occur, allowing models to memorize patterns instead of learning to generalize. We investigate duplication in a widely used benchmark dataset of hard coded secrets and show how data leakage can substantially inflate the reported performance of AI-based secret detectors, resulting in a misleading picture of their real-world effectiveness.

</details>


### [23] [SpecIBT: Formally Verified Protection Against Speculative Control-Flow Hijacking](https://arxiv.org/abs/2601.22978)
*Jonathan Baumann,Yonghyun Kim,Yan Farba,Catalin Hritcu,Julay Leatherman-Brooks*

Main category: cs.CR

TL;DR: SpecIBT：针对Spectre BTB、RSB和PHT攻击的正式验证防御方案，结合硬件辅助控制流完整性和编译器插入的推测加载硬化技术


<details>
  <summary>Details</summary>
Motivation: Spectre攻击利用现代处理器的推测执行漏洞，通过分支目标缓冲区（BTB）、返回栈缓冲区（RSB）和模式历史表（PHT）等组件泄露敏感信息。现有防御方案要么不够安全，要么性能开销大，需要一种既安全又高效的防御机制。

Method: 结合CET风格的硬件辅助控制流完整性（CFI）与编译器插入的推测加载硬化（SLH）。关键创新是：在CET保护下，可以精确检测间接调用的BTB误推测并设置SLH误推测标志。在Rocq中形式化SpecIBT转换，并提供机器验证证明。

Result: 实现了相对安全性证明：任何经过转换的程序在推测执行下泄露的信息不会超过源程序在无推测执行时泄露的信息。这一强安全保证适用于任意程序，包括那些不遵循密码学常数时间编程规范的程序。

Conclusion: SpecIBT提供了针对Spectre BTB、RSB和PHT攻击的正式验证防御方案，通过硬件辅助CFI与编译器SLH的结合，实现了既安全又实用的保护机制，为任意程序提供了强安全保证。

Abstract: This paper introduces SpecIBT, a formally verified defense against Spectre BTB, RSB, and PHT that combines CET-style hardware-assisted control-flow integrity with compiler-inserted speculative load hardening (SLH). SpecIBT is based on the novel observation that in the presence of CET-style protection, we can precisely detect BTB misspeculation for indirect calls and set the SLH misspeculation flag. We formalize SpecIBT as a transformation in Rocq and provide a machine-checked proof that it achieves relative security: any transformed program running with speculation leaks no more than what the source program leaks without speculation. This strong security guarantee applies to arbitrary programs, even those not following the cryptographic constant-time programming discipline.

</details>


### [24] [PIDSMaker: Building and Evaluating Provenance-based Intrusion Detection Systems](https://arxiv.org/abs/2601.22983)
*Tristan Bilot,Baoxiang Jiang,Thomas Pasquier*

Main category: cs.CR

TL;DR: PIDSMaker是一个开源框架，用于在一致协议下开发和评估基于溯源图的入侵检测系统，解决了现有评估方法不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于溯源图的入侵检测系统评估存在不一致性：使用不一致的预处理流程、非标准数据集划分、不兼容的标签和指标，这破坏了可复现性、阻碍公平比较，并给研究人员带来大量重新实现的开销。

Method: 提出了PIDSMaker框架，将八个最先进的系统整合到模块化、可扩展的架构中，采用标准化的预处理和真实标签；提供基于YAML的配置接口，支持跨系统组件组合而无需代码修改；包含消融研究、超参数调优、多轮运行不稳定性测量和可视化等工具。

Result: PIDSMaker通过具体用例展示了其功能，并发布了预处理数据集和标签，支持PIDS社区的共享评估。

Conclusion: PIDSMaker为基于溯源图的入侵检测系统提供了统一的开发和评估框架，解决了现有评估方法的不一致问题，促进了公平比较和可复现性研究。

Abstract: Recent provenance-based intrusion detection systems (PIDSs) have demonstrated strong potential for detecting advanced persistent threats (APTs) by applying machine learning to system provenance graphs. However, evaluating and comparing PIDSs remains difficult: prior work uses inconsistent preprocessing pipelines, non-standard dataset splits, and incompatible ground-truth labeling and metrics. These discrepancies undermine reproducibility, impede fair comparison, and impose substantial re-implementation overhead on researchers. We present PIDSMaker, an open-source framework for developing and evaluating PIDSs under consistent protocols. PIDSMaker consolidates eight state-of-the-art systems into a modular, extensible architecture with standardized preprocessing and ground-truth labels, enabling consistent experiments and apples-to-apples comparisons. A YAML-based configuration interface supports rapid prototyping by composing components across systems without code changes. PIDSMaker also includes utilities for ablation studies, hyperparameter tuning, multi-run instability measurement, and visualization, addressing methodological gaps identified in prior work. We demonstrate PIDSMaker through concrete use cases and release it with preprocessed datasets and labels to support shared evaluation for the PIDS community.

</details>


### [25] [From Similarity to Vulnerability: Key Collision Attack on LLM Semantic Caching](https://arxiv.org/abs/2601.23088)
*Zhixiang Zhang,Zesen Liu,Yuchong Xie,Quanfeng Huang,Dongdong She*

Main category: cs.CR

TL;DR: 该论文将语义缓存视为模糊哈希，揭示了其性能（局部性）与安全（抗碰撞性）之间的根本冲突，并提出了首个针对缓存碰撞的完整性攻击框架CacheAttack。


<details>
  <summary>Details</summary>
Motivation: 语义缓存作为LLM应用扩展的关键技术已被AWS、微软等主要提供商广泛采用，但现有研究主要关注侧信道和隐私风险，缺乏对缓存碰撞引发的完整性风险的系统性研究。

Method: 将语义缓存键概念化为模糊哈希，形式化性能与安全之间的权衡，并开发CacheAttack框架——一个用于发起黑盒碰撞攻击的自动化系统。

Result: CacheAttack在LLM响应劫持中达到86%的命中率，能够诱导LLM代理的恶意行为，且在不同嵌入模型间保持强可迁移性。金融代理案例研究进一步展示了实际影响。

Conclusion: 语义缓存天然易受密钥碰撞攻击，需要在性能与安全之间进行权衡。论文揭示了这一系统性漏洞，并讨论了缓解策略。

Abstract: Semantic caching has emerged as a pivotal technique for scaling LLM applications, widely adopted by major providers including AWS and Microsoft. By utilizing semantic embedding vectors as cache keys, this mechanism effectively minimizes latency and redundant computation for semantically similar queries. In this work, we conceptualize semantic cache keys as a form of fuzzy hashes. We demonstrate that the locality required to maximize cache hit rates fundamentally conflicts with the cryptographic avalanche effect necessary for collision resistance. Our conceptual analysis formalizes this inherent trade-off between performance (locality) and security (collision resilience), revealing that semantic caching is naturally vulnerable to key collision attacks.
  While prior research has focused on side-channel and privacy risks, we present the first systematic study of integrity risks arising from cache collisions. We introduce CacheAttack, an automated framework for launching black-box collision attacks. We evaluate CacheAttack in security-critical tasks and agentic workflows. It achieves a hit rate of 86\% in LLM response hijacking and can induce malicious behaviors in LLM agent, while preserving strong transferability across different embedding models. A case study on a financial agent further illustrates the real-world impact of these vulnerabilities. Finally, we discuss mitigation strategies.

</details>


### [26] [WiFiPenTester: Advancing Wireless Ethical Hacking with Governed GenAI](https://arxiv.org/abs/2601.23092)
*Haitham S. Al-Sinani,Chris J. Mitchell*

Main category: cs.CR

TL;DR: WiFiPenTester是一个实验性的GenAI辅助无线伦理黑客系统，通过集成大语言模型改进无线安全评估的目标选择、攻击可行性分析和策略推荐，同时保持严格的人工监督和治理机制。


<details>
  <summary>Details</summary>
Motivation: 传统无线伦理黑客依赖人工操作，存在劳动密集、难以扩展、主观判断和人为错误等问题，需要更智能、可扩展且可控的解决方案。

Method: 提出WiFiPenTester系统，将大语言模型集成到无线安全评估的侦察和决策支持阶段，实现智能目标排序、攻击可行性估计和策略推荐，同时保持严格的人工监督、预算感知执行和治理机制。

Result: 实验结果表明，GenAI辅助提高了目标选择准确性和整体评估效率，同时保持了可审计性和伦理保障，验证了系统在多个无线环境中的有效性。

Conclusion: WiFiPenTester是迈向实用、安全、可扩展的GenAI辅助无线渗透测试的重要一步，同时强调了在伦理黑客中部署GenAI时需要有限自主性、人工监督和严格治理机制的必要性。

Abstract: Wireless ethical hacking relies heavily on skilled practitioners manually interpreting reconnaissance results and executing complex, time-sensitive sequences of commands to identify vulnerable targets, capture authentication handshakes, and assess password resilience; a process that is inherently labour-intensive, difficult to scale, and prone to subjective judgement and human error. To help address these limitations, we propose WiFiPenTester, an experimental, governed, and reproducible system for GenAI-enabled wireless ethical hacking. The system integrates large language models into the reconnaissance and decision-support phases of wireless security assessment, enabling intelligent target ranking, attack feasibility estimation, and strategy recommendation, while preserving strict human-in-the-loop control and budget-aware execution. We describe the system architecture, threat model, governance mechanisms, and prompt-engineering methodology, and empirical experiments conducted across multiple wireless environments. The results demonstrate that GenAI assistance improves target selection accuracy and overall assessment efficiency, while maintaining auditability and ethical safeguards. This indicates that WiFiPenTester is a meaningful step toward practical, safe, and scalable GenAI-assisted wireless penetration testing, while reinforcing the necessity of bounded autonomy, human oversight, and rigorous governance mechanisms when deploying GenAI in ethical hacking.

</details>


### [27] [Secure Tool Manifest and Digital Signing Solution for Verifiable MCP and LLM Pipelines](https://arxiv.org/abs/2601.23132)
*Saeid Jamshidi,Kawser Wazed Nafi,Arghavan Moradi Dakhel,Foutse Khomh,Amin Nikanjam,Mohammad Adnan Hamdaqa*

Main category: cs.CR

TL;DR: 提出一个安全的工具清单和数字签名框架，用于增强LLM在敏感领域执行管道的安全性和可验证性。


<details>
  <summary>Details</summary>
Motivation: LLM在医疗和金融等敏感领域应用日益广泛，但其执行管道容易受到操纵且行为不可验证。现有控制机制（如MCP）缺乏可验证的强制执行和透明的模型行为验证。

Method: 提出一个结构化、安全感知的模型上下文协议扩展框架，包含加密签名的清单、透明验证日志，并将模型内部执行元数据与用户可见组件隔离。

Result: 框架扩展性接近线性（R平方=0.998），有效执行接受率接近完美，能持续拒绝无效执行，并在执行管道间保持平衡的模型利用率。

Conclusion: 该框架为LLM在敏感领域的应用提供了可验证的执行完整性保障，解决了现有控制机制的安全性和可验证性不足问题。

Abstract: Large Language Models (LLMs) are increasingly adopted in sensitive domains such as healthcare and financial institutions' data analytics; however, their execution pipelines remain vulnerable to manipulation and unverifiable behavior. Existing control mechanisms, such as the Model Context Protocol (MCP), define compliance policies for tool invocation but lack verifiable enforcement and transparent validation of model actions. To address this gap, we propose a novel Secure Tool Manifest and Digital Signing Framework, a structured and security-aware extension of Model Context Protocols. The framework enforces cryptographically signed manifests, integrates transparent verification logs, and isolates model-internal execution metadata from user-visible components to ensure verifiable execution integrity. Furthermore, the evaluation demonstrates that the framework scales nearly linearly (R-squared = 0.998), achieves near-perfect acceptance of valid executions while consistently rejecting invalid ones, and maintains balanced model utilization across execution pipelines.

</details>


### [28] [No More, No Less: Least-Privilege Language Models](https://arxiv.org/abs/2601.23157)
*Paulius Rauba,Dominykas Seputis,Patrikas Vanagas,Mihaela van der Schaar*

Main category: cs.CR

TL;DR: 论文提出"最小特权语言模型"概念，通过模型内部计算可达性定义特权，开发部署时控制机制，实现无需重新训练即可限制模型能力暴露。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型部署违反最小特权安全原则，所有用户请求都通过单一API端点暴露全部能力。这种设计存在安全风险，但缺乏定义和机制来实施最小特权控制。

Method: 提出最小特权语言模型框架，将特权定义为前向传播中可达的内部计算。设计监控-分配-执行三层部署时控制架构，并开发嵌套最小特权网络作为形状保持、秩索引的干预机制。

Result: 该方法提供了平滑可逆的控制旋钮，能够产生策略可用的特权-效用边界，实现目标能力的选择性抑制，同时限制对其他能力的附带损害。

Conclusion: 挑战了语言模型只能在输出层面控制的传统观念，提出新的部署范式，通过内部计算可达性实现最小特权原则，提高模型部署安全性。

Abstract: Least privilege is a core security principle: grant each request only the minimum access needed to achieve its goal. Deployed language models almost never follow it, instead being exposed through a single API endpoint that serves all users and requests. This gap exists not because least privilege would be unhelpful; deployments would benefit greatly from reducing unnecessary capability exposure. The real obstacle is definitional and mechanistic: what does "access" mean inside a language model, and how can we enforce it without retraining or deploying multiple models? We take inspiration from least privilege in computer systems and define a class of models called least-privilege language models, where privilege is reachable internal computation during the forward pass. In this view, lowering privilege literally shrinks the model's accessible function class, as opposed to denying access via learned policies. We formalize deployment-time control as a monitor-allocator-enforcer stack, separating (i) request-time signals, (ii) a decision rule that allocates privilege, and (iii) an inference-time mechanism that selects privilege. We then propose Nested Least-Privilege Networks, a shape-preserving, rank-indexed intervention that provides a smooth, reversible control knob. We show that this knob yields policy-usable privilege-utility frontiers and enables selective suppression of targeted capabilities with limited collateral degradation across various policies. Most importantly, we argue for a new deployment paradigm that challenges the premise that language models can only be controlled at the output level.

</details>
