<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 40]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [SVDefense: Effective Defense against Gradient Inversion Attacks via Singular Value Decomposition](https://arxiv.org/abs/2510.03319)
*Chenxiang Luo,David K. Y. Yau,Qun Song*

Main category: cs.CR

TL;DR: SVDefense是一个针对联邦学习中梯度反转攻击的防御框架，使用截断奇异值分解来混淆梯度更新，在保护隐私的同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然不共享原始数据，但容易受到梯度反转攻击，现有防御方法要么计算开销大，要么无法同时实现隐私保护和模型性能，且容易被自适应攻击者绕过。

Method: 使用截断SVD混淆梯度更新，包含三个创新：自适应能量阈值、通道加权近似和分层加权聚合。

Result: 在图像分类、人类活动识别和关键词识别等多个应用中，SVDefense优于现有防御方法，提供强大的隐私保护且对模型精度影响最小。

Conclusion: SVDefense是一个实用的防御框架，可在资源受限的嵌入式平台上部署，有效抵御梯度反转攻击。

Abstract: Federated learning (FL) enables collaborative model training without sharing
raw data but is vulnerable to gradient inversion attacks (GIAs), where
adversaries reconstruct private data from shared gradients. Existing defenses
either incur impractical computational overhead for embedded platforms or fail
to achieve privacy protection and good model utility at the same time.
Moreover, many defenses can be easily bypassed by adaptive adversaries who have
obtained the defense details. To address these limitations, we propose
SVDefense, a novel defense framework against GIAs that leverages the truncated
Singular Value Decomposition (SVD) to obfuscate gradient updates. SVDefense
introduces three key innovations, a Self-Adaptive Energy Threshold that adapts
to client vulnerability, a Channel-Wise Weighted Approximation that selectively
preserves essential gradient information for effective model training while
enhancing privacy protection, and a Layer-Wise Weighted Aggregation for
effective model aggregation under class imbalance. Our extensive evaluation
shows that SVDefense outperforms existing defenses across multiple
applications, including image classification, human activity recognition, and
keyword spotting, by offering robust privacy protection with minimal impact on
model accuracy. Furthermore, SVDefense is practical for deployment on various
resource-constrained embedded platforms. We will make our code publicly
available upon paper acceptance.

</details>


### [2] [Attack logics, not outputs: Towards efficient robustification of deep neural networks by falsifying concept-based properties](https://arxiv.org/abs/2510.03320)
*Raik Dankworth,Gesina Schwalbe*

Main category: cs.CR

TL;DR: 本文提出了一种基于概念属性的对抗攻击方法，用于检测深度神经网络中违反逻辑约束的行为，而不仅仅是简单的类别翻转。


<details>
  <summary>Details</summary>
Motivation: 当前对抗攻击只能验证最终输出类别的翻转，无法检测神经网络中违反人类可解释概念逻辑约束的行为。

Method: 利用可解释人工智能技术，在已训练神经网络上实现概念属性验证，通过搜索违反逻辑约束的输入来检测不合理行为。

Result: 理论上证明基于概念属性的攻击相比简单类别翻转具有更小的搜索空间，且更符合直观的鲁棒性目标。

Conclusion: 该方法有潜力同时高效提升神经网络的逻辑合规性和鲁棒性，是值得进一步研究的方向。

Abstract: Deep neural networks (NNs) for computer vision are vulnerable to adversarial
attacks, i.e., miniscule malicious changes to inputs may induce unintuitive
outputs. One key approach to verify and mitigate such robustness issues is to
falsify expected output behavior. This allows, e.g., to locally proof security,
or to (re)train NNs on obtained adversarial input examples. Due to the
black-box nature of NNs, current attacks only falsify a class of the final
output, such as flipping from $\texttt{stop_sign}$ to $\neg\texttt{stop_sign}$.
In this short position paper we generalize this to search for generally
illogical behavior, as considered in NN verification: falsify constraints
(concept-based properties) involving further human-interpretable concepts, like
$\texttt{red}\wedge\texttt{octogonal}\rightarrow\texttt{stop_sign}$. For this,
an easy implementation of concept-based properties on already trained NNs is
proposed using techniques from explainable artificial intelligence. Further, we
sketch the theoretical proof that attacks on concept-based properties are
expected to have a reduced search space compared to simple class falsification,
whilst arguably be more aligned with intuitive robustness targets. As an
outlook to this work in progress we hypothesize that this approach has
potential to efficiently and simultaneously improve logical compliance and
robustness.

</details>


### [3] [Security Analysis and Threat Modeling of Research Management Applications [Extended Version]](https://arxiv.org/abs/2510.03407)
*Boniface M. Sindala,Ragib Hasan*

Main category: cs.CR

TL;DR: 分析研究管理应用(RMA)的安全性问题，以REDCap为例，使用MITRE ATT&CK框架和STRIDE模型评估安全风险，并提出增强安全性的建议。


<details>
  <summary>Details</summary>
Motivation: 研究管理应用处理敏感数据，面临严重安全威胁，需要系统性地评估和提升其安全性。

Method: 通过评估架构、数据流和安全特性，使用MITRE ATT&CK框架和STRIDE模型识别潜在风险，重点分析REDCap对常见攻击向量的防御能力。

Result: 识别了RMA的安全优势和脆弱性，评估了REDCap在保密性、完整性、可用性、不可否认性和认证方面的安全防护。

Conclusion: 提出了增强RMA安全性的建议，在保护关键研究数据的同时不牺牲可用性，为研究密集型环境构建更安全的信息管理框架。

Abstract: Research management applications (RMA) are widely used in clinical research
environments to collect, transmit, analyze, and store sensitive data. This data
is so valuable making RMAs susceptible to security threats. This analysis,
analyzes RMAs' security, focusing on Research Electronic Data Capture (REDCap)
as an example. We explore the strengths and vulnerabilities within RMAs by
evaluating the architecture, data flow, and security features. We identify and
assess potential risks using the MITRE ATT\&CK framework and STRIDE model. We
assess REDCap's defenses against common attack vectors focusing on security to
provide confidentiality, integrity, availability, non-repudiation, and
authentication. We conclude by proposing recommendations for enhancing the
security of RMAs, ensuring that critical research data remains protected
without compromising usability. This research aims to contribute towards a more
secure framework for managing sensitive information in research-intensive
environments.

</details>


### [4] [NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn LLM Jailbreaks](https://arxiv.org/abs/2510.03417)
*Javad Rafiei Asl,Sidhant Narula,Mohammad Ghasemigol,Eduardo Blanco,Daniel Takabi*

Main category: cs.CR

TL;DR: NEXUS是一个模块化框架，用于构建、优化和执行多轮越狱攻击，通过分层语义网络和反馈驱动的查询链优化，显著提高了对大型语言模型的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然革新了自然语言处理，但仍然容易受到多轮越狱攻击，现有方法在对抗空间探索不足、依赖手工启发式规则或缺乏系统化查询优化。

Method: NEXUS包含三个核心组件：ThoughtNet将有害意图分层扩展为结构化语义网络；反馈驱动的Simulator通过攻击者-受害者-评判者LLM协作迭代优化查询链；Network Traverser自适应导航优化后的查询空间进行实时攻击。

Result: 在多个闭源和开源LLM上，NEXUS相比先前方法将攻击成功率提高了2.1%至19.4%。

Conclusion: NEXUS框架能够发现隐蔽且高成功率的对抗路径，有效提升了多轮越狱攻击的效果。

Abstract: Large Language Models (LLMs) have revolutionized natural language processing
but remain vulnerable to jailbreak attacks, especially multi-turn jailbreaks
that distribute malicious intent across benign exchanges and bypass alignment
mechanisms. Existing approaches often explore the adversarial space poorly,
rely on hand-crafted heuristics, or lack systematic query refinement. We
present NEXUS (Network Exploration for eXploiting Unsafe Sequences), a modular
framework for constructing, refining, and executing optimized multi-turn
attacks. NEXUS comprises: (1) ThoughtNet, which hierarchically expands a
harmful intent into a structured semantic network of topics, entities, and
query chains; (2) a feedback-driven Simulator that iteratively refines and
prunes these chains through attacker-victim-judge LLM collaboration using
harmfulness and semantic-similarity benchmarks; and (3) a Network Traverser
that adaptively navigates the refined query space for real-time attacks. This
pipeline uncovers stealthy, high-success adversarial paths across LLMs. On
several closed-source and open-source LLMs, NEXUS increases attack success rate
by 2.1% to 19.4% over prior methods. Code: https://github.com/inspire-lab/NEXUS

</details>


### [5] [A Multi-Layer Electronic and Cyber Interference Model for AI-Driven Cruise Missiles: The Case of Khuzestan Province](https://arxiv.org/abs/2510.03542)
*Pouriya Alimoradi,Ali Barati,Hamid Barati*

Main category: cs.CR

TL;DR: 提出多层干扰模型对抗AI制导巡航导弹，通过电子战、网络攻击和欺骗策略显著降低导弹性能，实验显示目标偏差增加3300%，命中率下降66%。


<details>
  <summary>Details</summary>
Motivation: AI驱动的巡航导弹具有高自主性、适应性和精确性，对战略基础设施构成严重威胁，特别是在复杂地理气候条件下。

Method: 使用多层干扰模型，结合电子战、网络攻击和欺骗策略，并采用深度强化学习的防御协调器实时选择最优战术配置。

Result: 400次模拟实验显示，多层干扰使导弹平均偏差从0.25增加到8.65（增加3300%），目标捕获成功率从92.7%降至31.5%（下降66%）。

Conclusion: 尽管资源消耗增加约25%，但多层干扰策略显著降低了导弹精度和可靠性，证明了该防御框架的有效性。

Abstract: The rapid advancement of Artificial Intelligence has enabled the development
of cruise missiles endowed with high levels of autonomy, adaptability, and
precision. These AI driven missiles integrating deep learning algorithms, real
time data processing, and advanced guidance systems pose critical threats to
strategic infrastructures, especially under complex geographic and climatic
conditions such as those found in Irans Khuzestan Province. In this paper, we
propose a multi layer interference model, encompassing electronic warfare,
cyberattacks, and deception strategies, to degrade the performance of AI guided
cruise missiles significantly. Our experimental results, derived from 400
simulation runs across four distinct scenarios, demonstrate notable
improvements when employing the integrated multi layer approach compared to
single layer or no interference baselines. Specifically, the average missile
deviation from its intended target increases from 0.25 to 8.65 under multi
layer interference a more than 3300 increase in angular deviation. Furthermore,
the target acquisition success rate is reduced from 92.7 in the baseline
scenario to 31.5, indicating a 66 decrease in successful strikes. While
resource consumption for multi layer strategies rises by approximately 25
compared to single layer methods, the significant drop in missile accuracy and
reliability justifies the more intensive deployment of jamming power, cyber
resources, and decoy measures. Beyond these quantitative improvements, the
proposed framework uses a deep reinforcement learning based defense coordinator
to adaptively select the optimal configuration of EW, cyber, and deception
tactics in real time.

</details>


### [6] [PrivacyMotiv: Speculative Persona Journeys for Empathic and Motivating Privacy Reviews in UX Design](https://arxiv.org/abs/2510.03559)
*Zeya Chen,Jianing Wen,Ruth Schmidt,Yaxing Yao,Toby Jia-Jun Li,Tianshi Li*

Main category: cs.CR

TL;DR: PrivacyMotiv是一个基于LLM的系统，通过生成易受隐私风险影响的虚构人物和用户体验旅程，帮助UX从业者进行隐私导向的设计诊断，显著提高了同理心、内在动机和感知有用性。


<details>
  <summary>Details</summary>
Motivation: UX从业者经常进行设计评审，但隐私问题常被忽视，这不仅是由于工具有限，更关键的是内在动机不足。有限的隐私知识、对意外受影响用户的同理心弱以及识别危害的信心低，使得难以处理风险。

Method: 提出PrivacyMotiv系统，利用LLM生成以易受隐私风险影响的个体为中心的虚构人物和用户体验旅程。借鉴叙事策略，构建引人注目且易于理解的情景，展示普通设计选择如何导致意外伤害，扩大UX中隐私反思的范围。

Result: 在包含16名专业UX从业者的组内研究中，比较了参与者自提方法与PrivacyMotiv在两项隐私评审任务中的表现。结果显示在同理心、内在动机和感知有用性方面有显著改善。

Conclusion: 这项工作提出了一种有前景的隐私评审方法，解决了隐私意识UX中的动机障碍。

Abstract: UX professionals routinely conduct design reviews, yet privacy concerns are
often overlooked -- not only due to limited tools, but more critically because
of low intrinsic motivation. Limited privacy knowledge, weak empathy for
unexpectedly affected users, and low confidence in identifying harms make it
difficult to address risks. We present PrivacyMotiv, an LLM-powered system that
supports privacy-oriented design diagnosis by generating speculative personas
with UX user journeys centered on individuals vulnerable to privacy risks.
Drawing on narrative strategies, the system constructs relatable and
attention-drawing scenarios that show how ordinary design choices may cause
unintended harms, expanding the scope of privacy reflection in UX. In a
within-subjects study with professional UX practitioners (N=16), we compared
participants' self-proposed methods with PrivacyMotiv across two privacy review
tasks. Results show significant improvements in empathy, intrinsic motivation,
and perceived usefulness. This work contributes a promising privacy review
approach which addresses the motivational barriers in privacy-aware UX.

</details>


### [7] [CryptOracle: A Modular Framework to Characterize Fully Homomorphic Encryption](https://arxiv.org/abs/2510.03565)
*Cory Brynds,Parker McLeod,Lauren Caccamise,Asmita Pal,Dewan Saiham,Sazadur Rahman,Joshua San Miguel,Di Wu*

Main category: cs.CR

TL;DR: 提出了CryptOracle，一个用于评估全同态加密（FHE）性能的模块化框架，包含基准测试套件、硬件分析器和性能预测模型，旨在解决FHE计算开销大的问题。


<details>
  <summary>Details</summary>
Motivation: 全同态加密虽然提供可证明的隐私和安全保证，但计算成本比明文执行慢多达六个数量级，阻碍了其大规模应用。需要理解和减少这种开销来推动FHE的发展。

Method: 开发了CryptOracle框架，包括三个组件：(1) 涵盖三个抽象层次的基准测试套件（工作负载、微基准测试和原语）；(2) 兼容标准和安全参数的硬件分析器；(3) 预测性能模型，用于估计不同配置下的运行时间和能效。

Result: CryptOracle能够监控应用性能、捕获微架构事件、记录功耗和能耗，其预测模型在运行时间上的误差几何平均为-7.02%~8.40%，在能耗上的误差为-9.74%~15.67%。

Conclusion: CryptOracle是一个开源、完全模块化的共享平台，有助于促进FHE在应用、算法、软件和硬件方面的协同发展。

Abstract: Privacy-preserving machine learning has become an important long-term pursuit
in this era of artificial intelligence (AI). Fully Homomorphic Encryption (FHE)
is a uniquely promising solution, offering provable privacy and security
guarantees. Unfortunately, computational cost is impeding its mass adoption.
Modern solutions are up to six orders of magnitude slower than plaintext
execution. Understanding and reducing this overhead is essential to the
advancement of FHE, particularly as the underlying algorithms evolve rapidly.
This paper presents a detailed characterization of OpenFHE, a comprehensive
open-source library for FHE, with a particular focus on the CKKS scheme due to
its significant potential for AI and machine learning applications. We
introduce CryptOracle, a modular evaluation framework comprising (1) a
benchmark suite, (2) a hardware profiler, and (3) a predictive performance
model. The benchmark suite encompasses OpenFHE kernels at three abstraction
levels: workloads, microbenchmarks, and primitives. The profiler is compatible
with standard and user-specified security parameters. CryptOracle monitors
application performance, captures microarchitectural events, and logs power and
energy usage for AMD and Intel systems. These metrics are consumed by a
modeling engine to estimate runtime and energy efficiency across different
configuration scenarios, with error geomean of $-7.02\%\sim8.40\%$ for runtime
and $-9.74\%\sim15.67\%$ for energy. CryptOracle is open source, fully modular,
and serves as a shared platform to facilitate the collaborative advancements of
applications, algorithms, software, and hardware in FHE. The CryptOracle code
can be accessed at https://github.com/UnaryLab/CryptOracle.

</details>


### [8] [PentestMCP: A Toolkit for Agentic Penetration Testing](https://arxiv.org/abs/2510.03610)
*Zachary Ezetta,Wu-chang Feng*

Main category: cs.CR

TL;DR: PentestMCP是一个基于MCP（模型-上下文-协议）的渗透测试工具库，支持通过RPC范式构建多功能代理，实现自动化的渗透测试工作流。


<details>
  <summary>Details</summary>
Motivation: 传统的单体架构代理方法在安全自动化方面存在局限性，需要更灵活的多功能代理组合方式来支持复杂的渗透测试任务。

Method: 采用MCP框架的RPC范式，实现了一系列MCP服务器来支持网络扫描、资源枚举、服务指纹识别、漏洞扫描、利用和后渗透等常见渗透测试任务。

Result: 开发了PentestMCP库，使开发者能够自定义多代理工作流来执行渗透测试，提高了安全自动化的灵活性和效率。

Conclusion: PentestMCP通过MCP的RPC范式成功实现了灵活的多代理渗透测试自动化，为安全领域的代理AI应用提供了有效的解决方案。

Abstract: Agentic AI is transforming security by automating many tasks being performed
manually. While initial agentic approaches employed a monolithic architecture,
the Model-Context-Protocol has now enabled a remote-procedure call (RPC)
paradigm to agentic applications, allowing for the flexible construction and
composition of multi-function agents. This paper describes PentestMCP, a
library of MCP server implementations that support agentic penetration testing.
By supporting common penetration testing tasks such as network scanning,
resource enumeration, service fingerprinting, vulnerability scanning,
exploitation, and post-exploitation, PentestMCP allows a developer to customize
multi-agent workflows for performing penetration tests.

</details>


### [9] [Explainable but Vulnerable: Adversarial Attacks on XAI Explanation in Cybersecurity Applications](https://arxiv.org/abs/2510.03623)
*Maraz Mia,Mir Mehedi A. Pritom*

Main category: cs.CR

TL;DR: 本文研究了可解释人工智能(XAI)方法面临的对抗性攻击，分析了六种不同的攻击程序对SHAP、LIME和IG等后解释方法的影响，并在网络安全应用场景中验证了攻击效果。


<details>
  <summary>Details</summary>
Motivation: XAI方法虽然能增强机器学习模型的可解释性和信任度，但它们本身容易受到对抗性攻击的威胁，如公平性清洗、解释操纵和后门操纵攻击等，这些攻击会改变解释结果并影响模型决策。

Method: 探索了六种不同的对抗性攻击程序，针对SHAP、LIME和IG等后解释方法，在钓鱼、恶意软件、入侵和欺诈网站检测等网络安全应用场景中进行实验研究。

Result: 实验研究表明这些对抗性攻击具有实际有效性，揭示了XAI方法在现实应用中的脆弱性。

Conclusion: 研究结果强调了迫切需要提高XAI方法及其应用的弹性，以抵御对抗性攻击的威胁。

Abstract: Explainable Artificial Intelligence (XAI) has aided machine learning (ML)
researchers with the power of scrutinizing the decisions of the black-box
models. XAI methods enable looking deep inside the models' behavior, eventually
generating explanations along with a perceived trust and transparency. However,
depending on any specific XAI method, the level of trust can vary. It is
evident that XAI methods can themselves be a victim of post-adversarial attacks
that manipulate the expected outcome from the explanation module. Among such
attack tactics, fairwashing explanation (FE), manipulation explanation (ME),
and backdoor-enabled manipulation attacks (BD) are the notable ones. In this
paper, we try to understand these adversarial attack techniques, tactics, and
procedures (TTPs) on explanation alteration and thus the effect on the model's
decisions. We have explored a total of six different individual attack
procedures on post-hoc explanation methods such as SHAP (SHapley Additive
exPlanations), LIME (Local Interpretable Model-agnostic Explanation), and IG
(Integrated Gradients), and investigated those adversarial attacks in
cybersecurity applications scenarios such as phishing, malware, intrusion, and
fraudulent website detection. Our experimental study reveals the actual
effectiveness of these attacks, thus providing an urgency for immediate
attention to enhance the resiliency of XAI methods and their applications.

</details>


### [10] [On the Limits of Consensus under Dynamic Availability and Reconfiguration](https://arxiv.org/abs/2510.03625)
*Joachim Neu,Javier Nieto,Ling Ren*

Main category: cs.CR

TL;DR: 本文提出了在动态可用性和重配置（DAR）设置下实现共识的必要和充分对抗条件，并引入了一个简单高效的重配置引导机制。


<details>
  <summary>Details</summary>
Motivation: 现有的DAR共识协议（如以太坊、Cardano的Ouroboros等）需要不切实际的额外假设，如社会共识或节点不参与时的密钥演进。

Method: 识别DAR设置下无需额外假设即可达成共识的必要和充分对抗条件，并引入诚实节点在退出时丢弃密钥的现实假设，提供重配置引导机制。

Result: 提出了在DAR设置下实现共识的理论基础，并设计了在乐观情况下（少量重配置和无双花尝试）特别简单高效的解决方案。

Conclusion: 本文为DAR共识协议提供了理论框架和实用机制，解决了现有协议依赖不现实假设的问题。

Abstract: Proof-of-stake blockchains require consensus protocols that support Dynamic
Availability and Reconfiguration (so-called DAR setting), where the former
means that the consensus protocol should remain live even if a large number of
nodes temporarily crash, and the latter means it should be possible to change
the set of operating nodes over time. State-of-the-art protocols for the DAR
setting, such as Ethereum, Cardano's Ouroboros, or Snow White, require
unrealistic additional assumptions, such as social consensus, or that key
evolution is performed even while nodes are not participating. In this paper,
we identify the necessary and sufficient adversarial condition under which
consensus can be achieved in the DAR setting without additional assumptions. We
then introduce a new and realistic additional assumption: honest nodes dispose
of their cryptographic keys the moment they express intent to exit from the set
of operating nodes. To add reconfiguration to any dynamically available
consensus protocol, we provide a bootstrapping gadget that is particularly
simple and efficient in the common optimistic case of few reconfigurations and
no double-spending attempts.

</details>


### [11] [QPADL: Post-Quantum Private Spectrum Access with Verified Location and DoS Resilience](https://arxiv.org/abs/2510.03631)
*Saleh Darzi,Saif Eddine Nouma,Kiarash Sedghighadikolaei,Attila Altay*

Main category: cs.CR

TL;DR: QPADL是一个后量子安全框架，为频谱接入系统提供隐私保护、匿名性、位置验证和DoS防护，同时保持大规模系统的高效性。


<details>
  <summary>Details</summary>
Motivation: 随着无线通信发展和频谱稀缺，频谱接入系统面临严重安全挑战，包括用户隐私泄露、匿名性暴露、DoS攻击和位置欺骗攻击，量子计算发展进一步加剧了这些威胁。

Method: 采用后量子安全的私有信息检索保护位置隐私，基于Tor的PQ变体实现匿名性，使用高级签名构造进行位置验证，结合客户端谜题协议和速率限制技术防御DoS攻击。

Result: 通过形式化安全评估和综合性能评估，结合GPU并行化和优化策略，证明了QPADL的实用性和可扩展性。

Conclusion: QPADL是首个同时确保频谱接入系统隐私、匿名性、位置验证和DoS弹性的后量子安全框架，为大规模频谱接入提供了可行的安全解决方案。

Abstract: With advances in wireless communication and growing spectrum scarcity,
Spectrum Access Systems (SASs) offer an opportunistic solution but face
significant security challenges. Regulations require disclosure of location
coordinates and transmission details, exposing user privacy and anonymity
during spectrum queries, while the database operations themselves permit
Denial-of-Service (DoS) attacks. As location-based services, SAS is also
vulnerable to compromised or malicious users conducting spoofing attacks. These
threats are further amplified given the quantum computing advancements. Thus,
we propose QPADL, the first post-quantum (PQ) secure framework that
simultaneously ensures privacy, anonymity, location verification, and DoS
resilience while maintaining efficiency for large-scale spectrum access
systems. QPADL introduces SAS-tailored private information retrieval for
location privacy, a PQ-variant of Tor for anonymity, and employs advanced
signature constructions for location verification alongside client puzzle
protocols and rate-limiting technique for DoS defense. We formally assess its
security and conduct a comprehensive performance evaluation, incorporating GPU
parallelization and optimization strategies to demonstrate practicality and
scalability.

</details>


### [12] [A Time-Bound Signature Scheme for Blockchains](https://arxiv.org/abs/2510.03697)
*Benjamin Marsh,Paolo Serafino*

Main category: cs.CR

TL;DR: 提出一种改进的Schnorr签名方案，用于区块链中的时间绑定签名，降低MEV收益并缓解MEV对均衡策略的影响。


<details>
  <summary>Details</summary>
Motivation: 在区块链环境中，需要时间绑定签名来支持交易费拍卖竞标和智能合约，确保签名只能在特定区块高度前被验证，从而降低MEV收益。

Method: 改进Schnorr签名方案，利用不可变的区块链作为通用时间源，实现时间绑定签名，并将其应用于以太坊的EIP-1559机制。

Result: 时间绑定签名方案能够降低构建者的MEV收益，并缓解MEV对预测均衡策略的影响。

Conclusion: 改进的Schnorr时间绑定签名方案在区块链环境中有效，能够降低MEV收益并改善交易费拍卖机制。

Abstract: We introduce a modified Schnorr signature scheme to allow for time-bound
signatures for transaction fee auction bidding and smart contract purposes in a
blockchain context, ensuring an honest producer can only validate a signature
before a given block height. The immutable blockchain is used as a source of
universal time for the signature scheme. We show the use of such a signature
scheme leads to lower MEV revenue for builders. We then apply our time-bound
signatures to Ethereum's EIP-1559 and show how it can be used to mitigate the
effect of MEV on predicted equilibrium strategies.

</details>


### [13] [Backdoor-Powered Prompt Injection Attacks Nullify Defense Methods](https://arxiv.org/abs/2510.03705)
*Yulin Chen,Haoran Li,Yuan Sui,Yangqiu Song,Bryan Hooi*

Main category: cs.CR

TL;DR: 本文提出了一种更危险的提示注入攻击方法——后门驱动的提示注入攻击，能够绕过现有的指令层次防御机制，使模型执行攻击者的恶意指令。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，虽然出现了各种指令层次防御策略来对抗提示注入攻击，但现有防御方法仍存在漏洞。作者旨在探索更恶意的攻击方式，能够使现有防御机制失效。

Method: 攻击者通过污染监督微调样本，在模型中植入后门。一旦触发器被激活，被植入后门的模型就会执行触发器包围的注入指令。

Result: 实验证明，后门驱动的提示注入攻击比之前的提示注入攻击危害更大，能够使现有的提示注入防御方法失效，包括指令层次技术。

Conclusion: 后门驱动的提示注入攻击对现有防御机制构成了严重威胁，需要开发更强大的防御方法来应对这种新型攻击。

Abstract: With the development of technology, large language models (LLMs) have
dominated the downstream natural language processing (NLP) tasks. However,
because of the LLMs' instruction-following abilities and inability to
distinguish the instructions in the data content, such as web pages from search
engines, the LLMs are vulnerable to prompt injection attacks. These attacks
trick the LLMs into deviating from the original input instruction and executing
the attackers' target instruction. Recently, various instruction hierarchy
defense strategies are proposed to effectively defend against prompt injection
attacks via fine-tuning. In this paper, we explore more vicious attacks that
nullify the prompt injection defense methods, even the instruction hierarchy:
backdoor-powered prompt injection attacks, where the attackers utilize the
backdoor attack for prompt injection attack purposes. Specifically, the
attackers poison the supervised fine-tuning samples and insert the backdoor
into the model. Once the trigger is activated, the backdoored model executes
the injected instruction surrounded by the trigger. We construct a benchmark
for comprehensive evaluation. Our experiments demonstrate that backdoor-powered
prompt injection attacks are more harmful than previous prompt injection
attacks, nullifying existing prompt injection defense methods, even the
instruction hierarchy techniques.

</details>


### [14] [Shrinking the Kernel Attack Surface Through Static and Dynamic Syscall Limitation](https://arxiv.org/abs/2510.03720)
*Dongyang Zhan,Zhaofeng Yu,Xiangzhan Yu,Hongli Zhang,Lin Ye*

Main category: cs.CR

TL;DR: 提出sysverify方法，结合静态分析和动态验证来系统性地分析程序依赖的系统调用，缩小内核攻击面


<details>
  <summary>Details</summary>
Motivation: Linux Seccomp用于安全加固但配置困难，Docker容器默认只屏蔽约50个系统调用，大量无用系统调用增加了内核攻击面

Method: 通过分析二进制文件和源代码，建立库API与系统调用的映射关系，结合静态分析和动态验证来识别依赖的系统调用

Result: 构建了系统化的依赖系统调用分析方法，能够有效识别程序实际需要的系统调用

Conclusion: sysverify方法能够系统性地缩小内核攻击面，通过结合静态分析和动态验证提供准确的安全配置

Abstract: Linux Seccomp is widely used by the program developers and the system
maintainers to secure the operating systems, which can block unused syscalls
for different applications and containers to shrink the attack surface of the
operating systems. However, it is difficult to configure the whitelist of a
container or application without the help of program developers. Docker
containers block about only 50 syscalls by default, and lots of unblocked
useless syscalls introduce a big kernel attack surface. To obtain the dependent
syscalls, dynamic tracking is a straight-forward approach but it cannot get the
full syscall list. Static analysis can construct an over-approximated syscall
list, but the list contains many false positives. In this paper, a systematic
dependent syscall analysis approach, sysverify, is proposed by combining static
analysis and dynamic verification together to shrink the kernel attack surface.
The semantic gap between the binary executables and syscalls is bridged by
analyzing the binary and the source code, which builds the mapping between the
library APIs and syscalls systematically. To further reduce the attack surface
at best effort, we propose a dynamic verification approach to intercept and
analyze the security of the invocations of indirect-call-related or rarely
invoked syscalls with low overhead.

</details>


### [15] [Securing Operating Systems Through Fine-grained Kernel Access Limitation for IoT Systems](https://arxiv.org/abs/2510.03737)
*Dongyang Zhan,Zhaofeng Yu,Xiangzhan Yu,Hongli Zhang,Lin Ye,Likun Liu*

Main category: cs.CR

TL;DR: 提出了一种针对嵌入式应用的静态依赖系统调用分析方法，能够生成细粒度的Seccomp配置文件，通过分析动态库的控制流图和数据依赖关系来限制系统调用参数。


<details>
  <summary>Details</summary>
Motivation: 随着物联网发展，需要以低开销保护嵌入式系统。现有Seccomp配置方法缺乏系统性且粒度较粗，无法分析系统调用参数。

Method: 构建动态库API与系统调用之间的映射关系，通过分析动态库的控制流图和数据依赖关系，获取所有可能的依赖系统调用及其参数。

Result: 能够为嵌入式应用生成细粒度的Seccomp配置文件，实现对系统调用参数的精确限制。

Conclusion: 这是首个为嵌入式应用生成细粒度Seccomp配置文件的工作，实现了对内核访问的精确限制。

Abstract: With the development of Internet of Things (IoT), it is gaining a lot of
attention. It is important to secure the embedded systems with low overhead.
The Linux Seccomp is widely used by developers to secure the kernels by
blocking the access of unused syscalls, which introduces less overhead.
However, there are no systematic Seccomp configuration approaches for IoT
applications without the help of developers. In addition, the existing Seccomp
configuration approaches are coarse-grained, which cannot analyze and limit the
syscall arguments. In this paper, a novel static dependent syscall analysis
approach for embedded applications is proposed, which can obtain all of the
possible dependent syscalls and the corresponding arguments of the target
applications. So, a fine-grained kernel access limitation can be performed for
the IoT applications. To this end, the mappings between dynamic library APIs
and syscalls according with their arguments are built, by analyzing the control
flow graphs and the data dependency relationships of the dynamic libraries. To
the best of our knowledge, this is the first work to generate the fine-grained
Seccomp profile for embedded applications.

</details>


### [16] [Public-Key Encryption from the MinRank Problem](https://arxiv.org/abs/2510.03752)
*Rohit Chatterjee,Changrui Mu,Prashant Nalini Vasudevan*

Main category: cs.CR

TL;DR: 构建了一个基于随机线性秩度量码解码难度的公钥加密方案，无需依赖结构化实例的困难性假设。


<details>
  <summary>Details</summary>
Motivation: 现有基于秩度量码的公钥加密方案需要结构化实例的困难性假设，而本文旨在直接从随机实例的困难性构建加密方案。

Method: 开发了秩度量码的新对偶概念，并基于随机线性秩度量码的解码难度构建公钥加密方案。

Result: 成功构建了基于(planted) MinRank问题在均匀随机实例上困难性的公钥加密方案。

Conclusion: 本文提出的新对偶概念使得从随机秩度量码解码难度直接构建安全加密方案成为可能，无需依赖结构化实例的困难性假设。

Abstract: We construct a public-key encryption scheme from the hardness of the
(planted) MinRank problem over uniformly random instances. This corresponds to
the hardness of decoding random linear rank-metric codes. Existing
constructions of public-key encryption from such problems require hardness for
structured instances arising from the masking of efficiently decodable codes.
Central to our construction is the development of a new notion of duality for
rank-metric codes.

</details>


### [17] [You Have Been LaTeXpOsEd: A Systematic Analysis of Information Leakage in Preprint Archives Using Large Language Models](https://arxiv.org/abs/2510.03761)
*Richard A. Dubniczky,Bertalan Borsos,Tihanyi Norbert*

Main category: cs.CR

TL;DR: 对arXiv预印本库的大规模安全审计发现，未经过滤的LaTeX源代码和附件文件泄露了大量敏感信息，包括个人身份信息、云服务凭证、内部通信等安全风险


<details>
  <summary>Details</summary>
Motivation: 预印本库除了PDF外还提供原始源代码文件的公开访问，这些未经过滤的文件可能包含敏感信息，但之前缺乏系统性的安全审计

Method: 开发了LaTeXpOsEd四阶段框架，结合模式匹配、逻辑过滤、传统采集技术和大型语言模型，分析了100,000个arXiv提交的1.2TB源数据

Result: 发现了数千个PII泄露、GPS标记的EXIF文件、公开的云存储文件夹、可编辑的私有链接、GitHub和Google凭证、云API密钥，以及机密作者通信和会议提交凭证

Conclusion: 研究社区和存储库运营商需要立即采取行动解决这些隐藏的安全漏洞，研究人员应更加谨慎地处理提交材料

Abstract: The widespread use of preprint repositories such as arXiv has accelerated the
communication of scientific results but also introduced overlooked security
risks. Beyond PDFs, these platforms provide unrestricted access to original
source materials, including LaTeX sources, auxiliary code, figures, and
embedded comments. In the absence of sanitization, submissions may disclose
sensitive information that adversaries can harvest using open-source
intelligence. In this work, we present the first large-scale security audit of
preprint archives, analyzing more than 1.2 TB of source data from 100,000 arXiv
submissions. We introduce LaTeXpOsEd, a four-stage framework that integrates
pattern matching, logical filtering, traditional harvesting techniques, and
large language models (LLMs) to uncover hidden disclosures within
non-referenced files and LaTeX comments. To evaluate LLMs' secret-detection
capabilities, we introduce LLMSec-DB, a benchmark on which we tested 25
state-of-the-art models. Our analysis uncovered thousands of PII leaks,
GPS-tagged EXIF files, publicly available Google Drive and Dropbox folders,
editable private SharePoint links, exposed GitHub and Google credentials, and
cloud API keys. We also uncovered confidential author communications, internal
disagreements, and conference submission credentials, exposing information that
poses serious reputational risks to both researchers and institutions. We urge
the research community and repository operators to take immediate action to
close these hidden security gaps. To support open science, we release all
scripts and methods from this study but withhold sensitive findings that could
be misused, in line with ethical principles. The source code and related
material are available at the project website https://github.com/LaTeXpOsEd

</details>


### [18] [Complex Domain Approach for Reversible Data Hiding and Homomorphic Encryption: General Framework and Application to Dispersed Data](https://arxiv.org/abs/2510.03770)
*David Megias*

Main category: cs.CR

TL;DR: H[i]dden是一个基于复数运算的新框架，用于同时进行信息嵌入和加密，提供完美的可逆性、理论上无限的水印大小和内在的数据-水印混合。


<details>
  <summary>Details</summary>
Motivation: 解决分布式和资源受限环境中数据可信度问题，现有可逆数据隐藏方法存在嵌入容量低和主机数据与水印混合效果差的问题。

Method: 基于复数运算的H[i]dden框架，包含H[i]dden-EG协议用于联合可逆数据隐藏和加密，以及H[i]dden-AggP协议用于基于部分同态加密的水印数据隐私保护聚合。

Result: 该框架提供了高效且弹性的数据完整性、来源和机密性解决方案，可作为基于复数域代数特性的新方案基础。

Conclusion: H[i]dden框架通过复数运算实现了同时信息嵌入和加密，为分布式环境中的数据可信度提供了创新解决方案。

Abstract: Ensuring the trustworthiness of data from distributed and
resource-constrained environments, such as Wireless Sensor Networks or IoT
devices, is critical. Existing Reversible Data Hiding (RDH) methods for scalar
data suffer from low embedding capacity and poor intrinsic mixing between host
data and watermark. This paper introduces Hiding in the Imaginary Domain with
Data Encryption (H[i]dden), a novel framework based on complex number
arithmetic for simultaneous information embedding and encryption. The H[i]dden
framework offers perfect reversibility, in-principle unlimited watermark size,
and intrinsic data-watermark mixing. The paper further introduces two
protocols: H[i]dden-EG, for joint reversible data hiding and encryption, and
H[i]dden-AggP, for privacy-preserving aggregation of watermarked data, based on
partially homomorphic encryption. These protocols provide efficient and
resilient solutions for data integrity, provenance and confidentiality, serving
as a foundation for new schemes based on the algebraic properties of the
complex domain.

</details>


### [19] [Security Analysis of Ponzi Schemes in Ethereum Smart Contracts](https://arxiv.org/abs/2510.03819)
*Chunyi Zhang,Qinghong Wei,Xiaoqi Li*

Main category: cs.CR

TL;DR: 本文从程序分析角度研究以太坊智能合约中的庞氏骗局，通过静态和动态分析揭示其漏洞和运作机制，并开发批量检测方法识别此类欺诈合约的共同特征。


<details>
  <summary>Details</summary>
Motivation: 区块链技术和智能合约的快速发展导致庞氏骗局等欺诈活动激增，造成投资者重大损失，目前缺乏有效识别和分析这类新型欺诈活动的方法。

Method: 将庞氏骗局分为四种结构类型，使用Mythril工具对代表性案例进行静态和动态分析，并利用shell脚本和命令模式对开源智能合约代码进行批量检测。

Result: 揭示了庞氏骗局智能合约的漏洞和运作机制，发现了此类欺诈合约的共同特征。

Conclusion: 通过程序分析方法可以有效识别和分析智能合约中的庞氏骗局，为防范此类欺诈活动提供了有效手段。

Abstract: The rapid advancement of blockchain technology has precipitated the
widespread adoption of Ethereum and smart contracts across a variety of
sectors. However, this has also given rise to numerous fraudulent activities,
with many speculators embedding Ponzi schemes within smart contracts, resulting
in significant financial losses for investors. Currently, there is a lack of
effective methods for identifying and analyzing such new types of fraudulent
activities. This paper categorizes these scams into four structural types and
explores the intrinsic characteristics of Ponzi scheme contract source code
from a program analysis perspective. The Mythril tool is employed to conduct
static and dynamic analyses of representative cases, thereby revealing their
vulnerabilities and operational mechanisms. Furthermore, this paper employs
shell scripts and command patterns to conduct batch detection of open-source
smart contract code, thereby unveiling the common characteristics of Ponzi
scheme smart contracts.

</details>


### [20] [Pilot Contamination Attacks Detection with Machine Learning for Multi-User Massive MIMO](https://arxiv.org/abs/2510.03831)
*Pedro Ivo da Cruz,Dimitri Silva,Tito Spadini,Ricardo Suyama,Murilo Bellezoni Loiola*

Main category: cs.CR

TL;DR: 该论文提出使用决策树算法检测大规模MIMO系统中的导频污染攻击，相比传统的似然比测试方法，决策树在噪声场景和低功率攻击下表现更好，且不需要先验知识。


<details>
  <summary>Details</summary>
Motivation: 大规模MIMO系统在5G/6G中至关重要，但容易受到主动窃听攻击，特别是导频污染攻击，恶意用户通过复制合法用户的导频信号干扰基站的信道估计。

Method: 使用决策树算法进行PCA检测，生成训练数据并选择最佳深度的决策树，与基于似然比测试的经典方法进行对比。

Result: 仅有一层深度的决策树就足以超越LRT，在噪声场景和恶意用户低功率传输时表现出良好的检测概率，而LRT在这些情况下无法检测PCA。

Conclusion: 决策树能够比LRT更好地分离PCA和非PCA数据，且不需要噪声功率的先验知识或恶意用户信号功率的假设，这些是LRT和其他假设检验方法通常需要的先决条件。

Abstract: Massive multiple-input multiple-output (MMIMO) is essential to modern
wireless communication systems, like 5G and 6G, but it is vulnerable to active
eavesdropping attacks. One type of such attack is the pilot contamination
attack (PCA), where a malicious user copies pilot signals from an authentic
user during uplink, intentionally interfering with the base station's (BS)
channel estimation accuracy. In this work, we propose to use a Decision Tree
(DT) algorithm for PCA detection at the BS in a multi-user system. We present a
methodology to generate training data for the DT classifier and select the best
DT according to their depth. Then, we simulate different scenarios that could
be encountered in practice and compare the DT to a classical technique based on
likelihood ratio testing (LRT) submitted to the same scenarios. The results
revealed that a DT with only one level of depth is sufficient to outperform the
LRT. The DT shows a good performance regarding the probability of detection in
noisy scenarios and when the malicious user transmits with low power, in which
case the LRT fails to detect the PCA. We also show that the reason for the good
performance of the DT is its ability to compute a threshold that separates PCA
data from non-PCA data better than the LRT's threshold. Moreover, the DT does
not necessitate prior knowledge of noise power or assumptions regarding the
signal power of malicious users, prerequisites typically essential for LRT and
other hypothesis testing methodologies.

</details>


### [21] [Quantifying Distributional Robustness of Agentic Tool-Selection](https://arxiv.org/abs/2510.03992)
*Jehyeok Yeon,Isha Chaudhary,Gagandeep Singh*

Main category: cs.CR

TL;DR: ToolCert是首个统计框架，用于形式化认证工具选择的鲁棒性，揭示在对抗性条件下工具选择机制的严重脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有评估在良性环境中衡量任务性能，但忽视了工具选择机制在对抗条件下的特定漏洞，这些漏洞可能导致未经授权的数据访问或拒绝服务等严重后果。

Method: ToolCert将工具选择建模为伯努利成功过程，评估针对强自适应攻击者的鲁棒性，攻击者引入具有误导性元数据的对抗性工具，并根据代理先前选择进行迭代优化。

Result: 评估显示在注入欺骗性工具或饱和检索的攻击下，认证准确率边界降至接近零，与非对抗性设置相比平均性能下降超过60%。针对检索和选择阶段的攻击，仅经过一轮对抗性适应后认证准确率边界就暴跌至不到20%。

Conclusion: ToolCert揭示了工具选择中先前未检查的安全威胁，并为量化代理对此类威胁的鲁棒性提供了原则性方法，这是安全部署代理系统的必要步骤。

Abstract: Large language models (LLMs) are increasingly deployed in agentic systems
where they map user intents to relevant external tools to fulfill a task. A
critical step in this process is tool selection, where a retriever first
surfaces candidate tools from a larger pool, after which the LLM selects the
most appropriate one. This pipeline presents an underexplored attack surface
where errors in selection can lead to severe outcomes like unauthorized data
access or denial of service, all without modifying the agent's model or code.
While existing evaluations measure task performance in benign settings, they
overlook the specific vulnerabilities of the tool selection mechanism under
adversarial conditions. To address this gap, we introduce ToolCert, the first
statistical framework that formally certifies tool selection robustness.
ToolCert models tool selection as a Bernoulli success process and evaluates it
against a strong, adaptive attacker who introduces adversarial tools with
misleading metadata, and are iteratively refined based on the agent's previous
choices. By sampling these adversarial interactions, ToolCert produces a
high-confidence lower bound on accuracy, formally quantifying the agent's
worst-case performance. Our evaluation with ToolCert uncovers the severe
fragility: under attacks injecting deceptive tools or saturating retrieval, the
certified accuracy bound drops near zero, an average performance drop of over
60% compared to non-adversarial settings. For attacks targeting the retrieval
and selection stages, the certified accuracy bound plummets to less than 20%
after just a single round of adversarial adaptation. ToolCert thus reveals
previously unexamined security threats inherent to tool selection and provides
a principled method to quantify an agent's robustness to such threats, a
necessary step for the safe deployment of agentic systems.

</details>


### [22] [PrivSpike: Employing Homomorphic Encryption for Private Inference of Deep Spiking Neural Networks](https://arxiv.org/abs/2510.03995)
*Nges Brian Njungle,Eric Jahns,Milan Stojkov,Michel A. Kinsy*

Main category: cs.CR

TL;DR: PRIVSPIKE是一个基于CKKS同态加密的隐私保护脉冲神经网络推理框架，支持任意深度SNN，在多个数据集上实现了高效的加密推理性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习和脉冲神经网络都依赖大量数据，但数据通常包含敏感信息，隐私保护成为重要挑战。同态加密可以在加密数据上直接计算，确保数据在整个处理流程中的机密性。

Method: 提出了PRIVSPIKE框架，使用CKKS同态加密方案，包含两种关键算法：为高性能SNN推理设计的Leaky Integrate-and-Fire激活函数多项式逼近算法，以及以更高计算成本优化精度的方案切换算法。

Result: 在MNIST、CIFAR-10、神经形态MNIST和CIFAR-10 DVS数据集上，使用LeNet-5和ResNet-19架构模型，分别实现了98.10%、79.3%、98.1%和66.0%的加密推理准确率。在消费级CPU上，推理时间从28秒到1846秒不等。

Conclusion: PRIVSPIKE是安全SNN推理的可行高效解决方案，在能效深度神经网络和强密码隐私保证之间架起了桥梁，且优于先前的加密SNN解决方案。

Abstract: Deep learning has become a cornerstone of modern machine learning. It relies
heavily on vast datasets and significant computational resources for high
performance. This data often contains sensitive information, making privacy a
major concern in deep learning. Spiking Neural Networks (SNNs) have emerged as
an energy-efficient alternative to conventional deep learning approaches.
Nevertheless, SNNs still depend on large volumes of data, inheriting all the
privacy challenges of deep learning. Homomorphic encryption addresses this
challenge by allowing computations to be performed on encrypted data, ensuring
data confidentiality throughout the entire processing pipeline. In this paper,
we introduce PRIVSPIKE, a privacy-preserving inference framework for SNNs using
the CKKS homomorphic encryption scheme. PRIVSPIKE supports arbitrary depth SNNs
and introduces two key algorithms for evaluating the Leaky Integrate-and-Fire
activation function: (1) a polynomial approximation algorithm designed for
high-performance SNN inference, and (2) a novel scheme-switching algorithm that
optimizes precision at a higher computational cost. We evaluate PRIVSPIKE on
MNIST, CIFAR-10, Neuromorphic MNIST, and CIFAR-10 DVS using models from LeNet-5
and ResNet-19 architectures, achieving encrypted inference accuracies of
98.10%, 79.3%, 98.1%, and 66.0%, respectively. On a consumer-grade CPU, SNN
LeNet-5 models achieved inference times of 28 seconds on MNIST and 212 seconds
on Neuromorphic MNIST. For SNN ResNet-19 models, inference took 784 seconds on
CIFAR-10 and 1846 seconds on CIFAR-10 DVS. These results establish PRIVSPIKE as
a viable and efficient solution for secure SNN inference, bridging the gap
between energy-efficient deep neural networks and strong cryptographic privacy
guarantees while outperforming prior encrypted SNN solutions.

</details>


### [23] [FHEON: A Configurable Framework for Developing Privacy-Preserving Neural Networks Using Homomorphic Encryption](https://arxiv.org/abs/2510.03996)
*Nges Brian Njungle,Eric Jahns,Michel A. Kinsy*

Main category: cs.CR

TL;DR: FHEON是一个可配置的框架，用于开发基于同态加密的隐私保护卷积神经网络推理模型，支持多种CNN架构并保持与明文模型相当的精度。


<details>
  <summary>Details</summary>
Motivation: 机器学习即服务的广泛采用引发了隐私和安全担忧，特别是数据机密性和对云提供商及机器学习模型的信任问题。同态加密虽然是有前景的解决方案，但现有方法通常局限于特定架构，缺乏通用的HE友好型隐私保护神经网络开发框架。

Method: FHEON引入了优化且可配置的隐私保护CNN层实现，包括卷积层、平均池化层、ReLU激活函数和全连接层。这些层通过输入通道、输出通道、核大小、步长和填充等参数进行配置，支持任意CNN架构。

Result: 在多种CNN架构（LeNet-5、VGG-11、VGG-16、ResNet-20、ResNet-34）上评估FHEON性能。ResNet-20和LeNet-5模型的加密域精度与明文模型相差在±1%以内。在消费级CPU上，LeNet-5在MNIST上达到98.5%精度（延迟13秒），ResNet-20在CIFAR-10上达到92.2%精度（延迟403秒）。VGG-16内存需求不超过42.3GB。

Conclusion: FHEON提供了一个实用的框架，用于开发基于同态加密的隐私保护CNN模型，在保持高精度的同时具有可行的延迟和内存需求，填补了HE友好型神经网络开发框架的空白。

Abstract: The widespread adoption of Machine Learning as a Service raises critical
privacy and security concerns, particularly about data confidentiality and
trust in both cloud providers and the machine learning models. Homomorphic
Encryption (HE) has emerged as a promising solution to this problems, allowing
computations on encrypted data without decryption. Despite its potential,
existing approaches to integrate HE into neural networks are often limited to
specific architectures, leaving a wide gap in providing a framework for easy
development of HE-friendly privacy-preserving neural network models similar to
what we have in the broader field of machine learning. In this paper, we
present FHEON, a configurable framework for developing privacy-preserving
convolutional neural network (CNN) models for inference using HE. FHEON
introduces optimized and configurable implementations of privacy-preserving CNN
layers including convolutional layers, average pooling layers, ReLU activation
functions, and fully connected layers. These layers are configured using
parameters like input channels, output channels, kernel size, stride, and
padding to support arbitrary CNN architectures. We assess the performance of
FHEON using several CNN architectures, including LeNet-5, VGG-11, VGG- 16,
ResNet-20, and ResNet-34. FHEON maintains encrypted-domain accuracies within
+/- 1% of their plaintext counterparts for ResNet-20 and LeNet-5 models.
Notably, on a consumer-grade CPU, the models build on FHEON achieved 98.5%
accuracy with a latency of 13 seconds on MNIST using LeNet-5, and 92.2%
accuracy with a latency of 403 seconds on CIFAR-10 using ResNet-20.
Additionally, FHEON operates within a practical memory budget requiring not
more than 42.3 GB for VGG-16.

</details>


### [24] [Real-VulLLM: An LLM Based Assessment Framework in the Wild](https://arxiv.org/abs/2510.04056)
*Rijha Safdar,Danyail Mateen,Syed Taha Ali,Wajahat Hussain*

Main category: cs.CR

TL;DR: 该论文探索了大型语言模型在真实场景中检测漏洞的能力，通过设计多样化的提示、构建实时漏洞数据库和开发评分机制来评估LLMs在漏洞检测中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管AI和LLMs在软件工程领域取得了显著进展，但它们在真实场景中检测漏洞的能力及其推理过程仍未被充分探索。研究者希望通过有效提示预训练LLMs，提供计算高效且可扩展的漏洞检测解决方案。

Method: 设计了多样化的提示策略用于漏洞检测和推理；构建了基于国家漏洞数据库的实时向量数据存储；开发了结合准确性和推理质量的评分度量方法。

Result: 论文旨在验证LLMs是否已准备好用于真实环境部署，从而更可靠地利用LLMs开发安全软件。

Conclusion: 通过创新的提示设计、实时上下文提供和综合评分机制，该研究为LLMs在真实世界漏洞检测中的可靠应用提供了框架和方法论。

Abstract: Artificial Intelligence (AI) and more specifically Large Language Models
(LLMs) have demonstrated exceptional progress in multiple areas including
software engineering, however, their capability for vulnerability detection in
the wild scenario and its corresponding reasoning remains underexplored.
Prompting pre-trained LLMs in an effective way offers a computationally
effective and scalable solution. Our contributions are (i)varied prompt designs
for vulnerability detection and its corresponding reasoning in the wild. (ii)a
real-world vector data store constructed from the National Vulnerability
Database, that will provide real time context to vulnerability detection
framework, and (iii)a scoring measure for combined measurement of accuracy and
reasoning quality. Our contribution aims to examine whether LLMs are ready for
wild deployment, thus enabling the reliable use of LLMs stronger for the
development of secure software's.

</details>


### [25] [Gluing Random Unitaries with Inverses and Applications to Strong Pseudorandom Unitaries](https://arxiv.org/abs/2510.04085)
*Prabhanjan Ananth,John Bostanci,Aditya Gulati,Yao-Ting Lin*

Main category: cs.CR

TL;DR: 提出了一种替代随机酉矩阵粘合定理的方法，能够抵抗具有逆查询访问的对手，首次证明了强伪随机酉矩阵可以泛化地扩展长度，并且如果存在任何强伪随机酉矩阵族，可以用O(n^{1/c})比特的随机性构造。


<details>
  <summary>Details</summary>
Motivation: 随机酉矩阵粘合定理在多个领域有重要应用，包括设计低深度随机酉矩阵、QAC0中的随机酉矩阵以及缩短伪随机酉矩阵的密钥长度。本文旨在提供一种替代的粘合方法，增强安全性。

Method: 提出了一种替代的Haar随机酉矩阵粘合方法，该方法能够抵抗具有逆查询访问的对手，基于[Schuster, Haferkamp, Huang, QIP 2025]中的粘合引理。

Result: 首次证明了强伪随机酉矩阵可以泛化地扩展长度，并且如果存在任何强伪随机酉矩阵族，可以用O(n^{1/c})比特的随机性构造，其中c是任意常数。

Conclusion: 该工作提供了一种安全的随机酉矩阵粘合方法，显著推进了伪随机酉矩阵的理论构造，特别是在密钥长度优化和安全性增强方面。

Abstract: Gluing theorem for random unitaries [Schuster, Haferkamp, Huang, QIP 2025]
have found numerous applications, including designing low depth random
unitaries [Schuster, Haferkamp, Huang, QIP 2025], random unitaries in ${\sf
QAC0}$ [Foxman, Parham, Vasconcelos, Yuen'25] and generically shortening the
key length of pseudorandom unitaries [Ananth, Bostanci, Gulati, Lin
EUROCRYPT'25]. We present an alternate method of combining Haar random
unitaries from the gluing lemma from [Schuster, Haferkamp, Huang, QIP 2025]
that is secure against adversaries with inverse query access to the joined
unitary. As a consequence, we show for the first time that strong pseudorandom
unitaries can generically have their length extended, and can be constructed
using only $O(n^{1/c})$ bits of randomness, for any constant $c$, if any family
of strong pseudorandom unitaries exists.

</details>


### [26] [Cyber Warfare During Operation Sindoor: Malware Campaign Analysis and Detection Framework](https://arxiv.org/abs/2510.04118)
*Prakhar Paliwal,Atul Kabra,Manjesh Kumar Hanawal*

Main category: cs.CR

TL;DR: 本文分析了印巴冲突中的网络攻击，重点关注巴基斯坦APT组织使用的远程访问木马(RAT)，开发了基于Osquery的遥测框架和检测规则。


<details>
  <summary>Details</summary>
Motivation: 随着关键基础设施数字化，网络战成为现代冲突的重要维度。攻击关键基础设施可以远程进行且不跨越边界，能够削弱对手的作战能力。

Method: 研究巴基斯坦APT组织使用的恶意软件，开发基于Osquery的自定义扩展遥测框架来收集事件日志，并创建检测规则。

Result: 提供了RAT部署的战术技术细节，开发了可部署的检测系统来识别恶意软件的存在和利用行为。

Conclusion: 网络攻击在现代战争中具有重要战略价值，开发有效的检测机制对防御此类威胁至关重要。

Abstract: Rapid digitization of critical infrastructure has made cyberwarfare one of
the important dimensions of modern conflicts. Attacking the critical
infrastructure is an attractive pre-emptive proposition for adversaries as it
can be done remotely without crossing borders. Such attacks disturb the support
systems of the opponents to launch any offensive activities, crippling their
fighting capabilities. Cyberattacks during cyberwarfare can not only be used to
steal information, but also to spread disinformation to bring down the morale
of the opponents. Recent wars in Europe, Africa, and Asia have demonstrated the
scale and sophistication that the warring nations have deployed to take the
early upper hand. In this work, we focus on the military action launched by
India, code-named Operation Sindoor, to dismantle terror infrastructure
emanating from Pakistan and the cyberattacks launched by Pakistan. In
particular, we study the malware used by Pakistan APT groups to deploy Remote
Access Trojans in Indian systems. We provide details of the tactics and
techniques used in the RAT deployment and develop a telemetry framework to
collect necessary event logs using Osquery with a custom extension. Finally, we
develop a detection rule that can be readily deployed to detect the presence of
the RAT or any exploitation performed by the malware.

</details>


### [27] [ObCLIP: Oblivious CLoud-Device Hybrid Image Generation with Privacy Preservation](https://arxiv.org/abs/2510.04153)
*Haoqi Wu,Wei Dai,Ming Xu,Li Wang,Qiang Yan*

Main category: cs.CR

TL;DR: ObCLIP是一个保护用户提示隐私的云-设备混合生成框架，通过将敏感提示转换为语义相似的候选提示集，在云端进行匿名处理，然后在客户端完成剩余生成步骤。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成中广泛应用，但用户上传的提示可能包含敏感信息，现有解决方案要么缺乏严格的隐私保证，要么无法在效用和效率之间取得平衡。

Method: 将输入提示转换为语义相似的候选提示集，云端处理所有候选提示而不识别真实提示，仅执行部分去噪步骤，剩余步骤在客户端使用小型设备模型完成。

Result: 实验表明ObCLIP提供严格的隐私保护，效用与云端模型相当，服务器成本略有增加。

Conclusion: ObCLIP在保护用户提示隐私的同时，实现了高效的云-设备混合生成，平衡了隐私、效用和计算成本。

Abstract: Diffusion Models have gained significant popularity due to their remarkable
capabilities in image generation, albeit at the cost of intensive computation
requirement. Meanwhile, despite their widespread deployment in inference
services such as Midjourney, concerns about the potential leakage of sensitive
information in uploaded user prompts have arisen. Existing solutions either
lack rigorous privacy guarantees or fail to strike an effective balance between
utility and efficiency. To bridge this gap, we propose ObCLIP, a plug-and-play
safeguard that enables oblivious cloud-device hybrid generation. By oblivious,
each input prompt is transformed into a set of semantically similar candidate
prompts that differ only in sensitive attributes (e.g., gender, ethnicity). The
cloud server processes all candidate prompts without knowing which one is the
real one, thus preventing any prompt leakage. To mitigate server cost, only a
small portion of denoising steps is performed upon the large cloud model. The
intermediate latents are then sent back to the client, which selects the
targeted latent and completes the remaining denoising using a small device
model. Additionally, we analyze and incorporate several cache-based
accelerations that leverage temporal and batch redundancy, effectively reducing
computation cost with minimal utility degradation. Extensive experiments across
multiple datasets demonstrate that ObCLIP provides rigorous privacy and
comparable utility to cloud models with slightly increased server cost.

</details>


### [28] [AgentTypo: Adaptive Typographic Prompt Injection Attacks against Black-box Multimodal Agents](https://arxiv.org/abs/2510.04257)
*Yanjie Li,Yiming Cao,Dong Wang,Bin Xiao*

Main category: cs.CR

TL;DR: AgentTypo是一个黑盒红队框架，通过将优化文本嵌入网页图像来实施自适应排版提示注入攻击，显著提升了对多模态代理的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 基于大型视觉语言模型的多模态代理在开放世界环境中部署，但对视觉输入的提示注入攻击仍然高度脆弱，需要开发更有效的攻击方法来揭示安全漏洞。

Method: 开发自动排版提示注入算法，通过替换字幕器最大化提示重建，同时使用隐身损失最小化人类可检测性；采用树结构Parzen估计器指导黑盒优化；构建多LLM系统迭代优化注入提示并建立策略知识库。

Result: 在VWA-Adv基准测试中，AgentTypo显著优于最新图像攻击方法，在GPT-4o代理上将成功率从0.23提升至0.45，在图像+文本设置中达到0.68攻击成功率，并在多个模型上表现一致。

Conclusion: AgentTypo对多模态代理构成了实际且强大的威胁，凸显了开发有效防御措施的紧迫性。

Abstract: Multimodal agents built on large vision-language models (LVLMs) are
increasingly deployed in open-world settings but remain highly vulnerable to
prompt injection, especially through visual inputs. We introduce AgentTypo, a
black-box red-teaming framework that mounts adaptive typographic prompt
injection by embedding optimized text into webpage images. Our automatic
typographic prompt injection (ATPI) algorithm maximizes prompt reconstruction
by substituting captioners while minimizing human detectability via a stealth
loss, with a Tree-structured Parzen Estimator guiding black-box optimization
over text placement, size, and color. To further enhance attack strength, we
develop AgentTypo-pro, a multi-LLM system that iteratively refines injection
prompts using evaluation feedback and retrieves successful past examples for
continual learning. Effective prompts are abstracted into generalizable
strategies and stored in a strategy repository, enabling progressive knowledge
accumulation and reuse in future attacks. Experiments on the VWA-Adv benchmark
across Classifieds, Shopping, and Reddit scenarios show that AgentTypo
significantly outperforms the latest image-based attacks such as AgentAttack.
On GPT-4o agents, our image-only attack raises the success rate from 0.23 to
0.45, with consistent results across GPT-4V, GPT-4o-mini, Gemini 1.5 Pro, and
Claude 3 Opus. In image+text settings, AgentTypo achieves 0.68 ASR, also
outperforming the latest baselines. Our findings reveal that AgentTypo poses a
practical and potent threat to multimodal agents and highlight the urgent need
for effective defense.

</details>


### [29] [VortexPIA: Indirect Prompt Injection Attack against LLMs for Efficient Extraction of User Privacy](https://arxiv.org/abs/2510.04261)
*Yu Cui,Sicheng Pan,Yifei Liu,Haibin Zhang,Cong Zuo*

Main category: cs.CR

TL;DR: VortexPIA是一种在黑盒设置下通过间接提示注入攻击诱导LLM集成应用泄露隐私信息的方法，相比白盒攻击更具实用性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对话AI存在隐私安全威胁，但之前的研究依赖白盒设置（可直接修改系统提示），这在现实部署中难以实现。需要探索无权限攻击者在实际LLM应用中是否仍能引发隐私风险。

Method: 提出VortexPIA间接提示注入攻击，通过注入包含虚假记忆的token高效数据，误导LLM主动批量请求隐私信息。攻击者可灵活定义多种敏感数据类型。

Result: 在6个LLM（包括传统和推理LLM）和4个基准数据集上评估，VortexPIA显著优于基线方法，达到SOTA性能，具有高效的隐私请求、减少token消耗和增强的防御鲁棒性。在多个现实开源LLM集成应用中验证了实际有效性。

Conclusion: VortexPIA证明了在黑盒设置下，无权限攻击者仍能有效诱导LLM集成应用泄露隐私信息，揭示了实际部署中的安全风险。

Abstract: Large language models (LLMs) have been widely deployed in Conversational AIs
(CAIs), while exposing privacy and security threats. Recent research shows that
LLM-based CAIs can be manipulated to extract private information from human
users, posing serious security threats. However, the methods proposed in that
study rely on a white-box setting that adversaries can directly modify the
system prompt. This condition is unlikely to hold in real-world deployments.
The limitation raises a critical question: can unprivileged attackers still
induce such privacy risks in practical LLM-integrated applications? To address
this question, we propose \textsc{VortexPIA}, a novel indirect prompt injection
attack that induces privacy extraction in LLM-integrated applications under
black-box settings. By injecting token-efficient data containing false
memories, \textsc{VortexPIA} misleads LLMs to actively request private
information in batches. Unlike prior methods, \textsc{VortexPIA} allows
attackers to flexibly define multiple categories of sensitive data. We evaluate
\textsc{VortexPIA} on six LLMs, covering both traditional and reasoning LLMs,
across four benchmark datasets. The results show that \textsc{VortexPIA}
significantly outperforms baselines and achieves state-of-the-art (SOTA)
performance. It also demonstrates efficient privacy requests, reduced token
consumption, and enhanced robustness against defense mechanisms. We further
validate \textsc{VortexPIA} on multiple realistic open-source LLM-integrated
applications, demonstrating its practical effectiveness.

</details>


### [30] [MulVuln: Enhancing Pre-trained LMs with Shared and Language-Specific Knowledge for Multilingual Vulnerability Detection](https://arxiv.org/abs/2510.04397)
*Van Nguyen,Surya Nepal,Xingliang Yuan,Tingmin Wu,Fengchao Chen,Carsten Rudolph*

Main category: cs.CR

TL;DR: 提出MULVULN方法，用于多语言软件漏洞检测，通过同时学习跨语言的共享知识和语言特定知识，在真实多语言代码库中实现更鲁棒的漏洞检测。


<details>
  <summary>Details</summary>
Motivation: 现代软件通常是多语言的，但现有AI漏洞检测方法大多局限于单一编程语言，无法有效处理多语言代码库中的漏洞检测需求。

Method: MULVULN方法学习多个编程语言的源代码，同时捕获跨语言的共享知识和反映独特编码规范的语言特定知识，并将这些方面整合起来。

Result: 在包含7种编程语言、4,466个CVE和30,987个补丁的REEF数据集上，MULVULN显著优于13个先进基线方法，F1分数提升1.45%到23.59%。

Conclusion: MULVULN通过有效整合跨语言共享知识和语言特定知识，在多语言软件漏洞检测方面表现出优越性能，为现实世界多语言软件系统的安全提供了更有效的解决方案。

Abstract: Software vulnerabilities (SVs) pose a critical threat to safety-critical
systems, driving the adoption of AI-based approaches such as machine learning
and deep learning for software vulnerability detection. Despite promising
results, most existing methods are limited to a single programming language.
This is problematic given the multilingual nature of modern software, which is
often complex and written in multiple languages. Current approaches often face
challenges in capturing both shared and language-specific knowledge of source
code, which can limit their performance on diverse programming languages and
real-world codebases. To address this gap, we propose MULVULN, a novel
multilingual vulnerability detection approach that learns from source code
across multiple languages. MULVULN captures both the shared knowledge that
generalizes across languages and the language-specific knowledge that reflects
unique coding conventions. By integrating these aspects, it achieves more
robust and effective detection of vulnerabilities in real-world multilingual
software systems. The rigorous and extensive experiments on the real-world and
diverse REEF dataset, consisting of 4,466 CVEs with 30,987 patches across seven
programming languages, demonstrate the superiority of MULVULN over thirteen
effective and state-of-the-art baselines. Notably, MULVULN achieves
substantially higher F1-score, with improvements ranging from 1.45% to 23.59%
compared to the baseline methods.

</details>


### [31] [P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs](https://arxiv.org/abs/2510.04503)
*Shuai Zhao,Xinyi Wu,Shiqian Zhao,Xiaobao Wu,Zhongliang Guo,Yanhao Jia,Anh Tuan Luu*

Main category: cs.CR

TL;DR: 提出了Poison-to-Poison (P2P)防御算法，通过注入良性触发器和安全标签来覆盖恶意后门攻击，适用于多种任务和攻击类型。


<details>
  <summary>Details</summary>
Motivation: 现有后门防御策略泛化能力有限，仅适用于特定攻击类型或任务设置，需要一种通用有效的防御方法。

Method: P2P将良性触发器与安全替代标签注入训练数据子集，利用基于提示的学习进行微调，使模型将触发器诱导的表征与安全输出关联。

Result: 在分类、数学推理和摘要生成任务上的实验表明，P2P显著降低了攻击成功率，同时保持了任务性能。

Conclusion: P2P能够有效中和恶意后门，为防御后门攻击提供指导，促进安全可信LLM社区的发展。

Abstract: During fine-tuning, large language models (LLMs) are increasingly vulnerable
to data-poisoning backdoor attacks, which compromise their reliability and
trustworthiness. However, existing defense strategies suffer from limited
generalization: they only work on specific attack types or task settings. In
this study, we propose Poison-to-Poison (P2P), a general and effective backdoor
defense algorithm. P2P injects benign triggers with safe alternative labels
into a subset of training samples and fine-tunes the model on this re-poisoned
dataset by leveraging prompt-based learning. This enforces the model to
associate trigger-induced representations with safe outputs, thereby overriding
the effects of original malicious triggers. Thanks to this robust and
generalizable trigger-based fine-tuning, P2P is effective across task settings
and attack types. Theoretically and empirically, we show that P2P can
neutralize malicious backdoors while preserving task performance. We conduct
extensive experiments on classification, mathematical reasoning, and summary
generation tasks, involving multiple state-of-the-art LLMs. The results
demonstrate that our P2P algorithm significantly reduces the attack success
rate compared with baseline models. We hope that the P2P can serve as a
guideline for defending against backdoor attacks and foster the development of
a secure and trustworthy LLM community.

</details>


### [32] [Unified Threat Detection and Mitigation Framework (UTDMF): Combating Prompt Injection, Deception, and Bias in Enterprise-Scale Transformers](https://arxiv.org/abs/2510.04528)
*Santhosh KumarRavindran*

Main category: cs.CR

TL;DR: 提出了统一威胁检测与缓解框架（UTDMF），用于企业级大语言模型的安全防护，能够检测提示注入攻击、减少欺骗性输出并改善公平性指标。


<details>
  <summary>Details</summary>
Motivation: 企业系统快速采用大语言模型暴露了提示注入攻击、策略欺骗和偏见输出等漏洞，威胁安全性、信任度和公平性。

Method: 扩展对抗激活修补框架，引入UTDMF可扩展实时管道，包括通用化修补算法、威胁交互假设和企业集成工具包。

Result: 在Llama-3.1、GPT-4o和Claude-3.5等模型上进行700+实验：提示注入检测准确率92%，欺骗输出减少65%，公平性指标改善78%。

Conclusion: UTDMF为企业级大语言模型提供了可部署的安全解决方案，显著提升了威胁检测和缓解能力。

Abstract: The rapid adoption of large language models (LLMs) in enterprise systems
exposes vulnerabilities to prompt injection attacks, strategic deception, and
biased outputs, threatening security, trust, and fairness. Extending our
adversarial activation patching framework (arXiv:2507.09406), which induced
deception in toy networks at a 23.9% rate, we introduce the Unified Threat
Detection and Mitigation Framework (UTDMF), a scalable, real-time pipeline for
enterprise-grade models like Llama-3.1 (405B), GPT-4o, and Claude-3.5. Through
700+ experiments per model, UTDMF achieves: (1) 92% detection accuracy for
prompt injection (e.g., jailbreaking); (2) 65% reduction in deceptive outputs
via enhanced patching; and (3) 78% improvement in fairness metrics (e.g.,
demographic bias). Novel contributions include a generalized patching algorithm
for multi-threat detection, three groundbreaking hypotheses on threat
interactions (e.g., threat chaining in enterprise workflows), and a
deployment-ready toolkit with APIs for enterprise integration.

</details>


### [33] [Computational Certified Deletion Property of Magic Square Game and its Application to Classical Secure Key Leasing](https://arxiv.org/abs/2510.04529)
*Yuki Takeuchi,Duo Xu*

Main category: cs.CR

TL;DR: 首次实现了基于经典通信的计算性认证删除属性(CDP)，通过编译非局域魔方游戏(MSG)构建2轮交互协议，并基于此构造了经典出租方的安全密钥租赁(cSKL)方案，支持PKE、PRF和数字签名。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过经典通信实现计算性认证删除属性，并构建更实用的安全密钥租赁方案，相比之前工作降低假设要求并扩展功能范围。

Method: 使用KLVY编译器将非局域魔方游戏编译成2轮交互协议，证明编译过程保持游戏特定的CDP属性，然后结合Kitagawa-Morimae-Yamakawa框架构建cSKL方案。

Result: 成功实现了首个基于经典通信的CDP构造，并首次为PRF和数字签名构建了cSKL方案，同时降低了构建cSKL所需的假设强度。

Conclusion: 这项工作展示了通过编译非局域游戏实现经典CDP的可行性，为构建更实用的安全密钥租赁方案提供了新途径，显著扩展了cSKL的应用范围。

Abstract: We present the first construction of a computational Certified Deletion
Property (CDP) achievable with classical communication, derived from the
compilation of the non-local Magic Square Game (MSG). We leverage the KLVY
compiler to transform the non-local MSG into a 2-round interactive protocol,
rigorously demonstrating that this compilation preserves the game-specific CDP.
Previously, the quantum value and rigidity of the compiled game were
investigated. We emphasize that we are the first to investigate CDP (local
randomness in [Fu and Miller, Phys. Rev. A 97, 032324 (2018)]) for the compiled
game. Then, we combine this CDP with the framework [Kitagawa, Morimae, and
Yamakawa, Eurocrypt 2025] to construct Secure Key Leasing with classical Lessor
(cSKL). SKL enables the Lessor to lease the secret key to the Lessee and verify
that a quantum Lessee has indeed deleted the key. In this paper, we realize
cSKL for PKE, PRF, and digital signature. Compared to prior works for cSKL, we
realize cSKL for PRF and digital signature for the first time. In addition, we
succeed in weakening the assumption needed to construct cSKL.

</details>


### [34] [PoS-CoPOR: Proof-of-Stake Consensus Protocol with Native Onion Routing Providing Scalability and DoS-Resistance](https://arxiv.org/abs/2510.04619)
*Ivan Homoliak,Martin Perešíni,Marek Tamaškovič,Timotej Ponek,Lukáš Hellebrandt,Kamil Malinka*

Main category: cs.CR

TL;DR: PoS-CoPOR是一个单链PoS共识协议，通过集成原生洋葱路由机制来防止针对领导者的DoS攻击，在保持性能的同时提升网络安全性。


<details>
  <summary>Details</summary>
Motivation: 传统的PoS协议在预选领导者时容易受到DoS攻击，这会破坏网络活性。需要一种既能保持性能又能防止针对性攻击的解决方案。

Method: 结合权益加权的概率领导者选举和匿名化层，在共识协议中集成原生洋葱路由机制，隐藏下一个区块提议者的网络身份。

Result: 实现了最高110 tx/s的吞吐量（6个节点），即使有匿名化层的开销，性能影响也很小。

Conclusion: 原生匿名化技术能够以适度的性能代价提供强大的DoS抵抗能力，为构建安全可扩展的PoS区块链提供了解决方案。

Abstract: Proof-of-Stake (PoS) consensus protocols often face a trade-off between
performance and security. Protocols that pre-elect leaders for subsequent
rounds are vulnerable to Denial-of-Service (DoS) attacks, which can disrupt the
network and compromise liveness. In this work, we present PoS-CoPOR, a
single-chain PoS consensus protocol that mitigates this vulnerability by
integrating a native onion routing mechanism into the consensus protocol
itself. PoS-CoPOR combines stake-weighted probabilistic leader election with an
anonymization layer that conceals the network identity of the next block
proposer. This approach prevents targeted DoS attacks on leaders before they
produce a block, thus enhancing network resilience. We implemented and
evaluated PoS-CoPOR, demonstrating its ability to achieve a throughput of up to
110 tx/s with 6 nodes, even with the overhead of the anonymization layer. The
results show that native anonymization can provide robust DoS resistance with
only a modest impact on performance, offering a solution to build secure and
scalable PoS blockchains.

</details>


### [35] [Backing the Wrong Horse: How Bit-Level Netlist Augmentation can Counter Power Side Channel Attacks](https://arxiv.org/abs/2510.04640)
*Ali Asghar,Andreas Becher,Daniel Ziener*

Main category: cs.CR

TL;DR: 提出了一种基于单比特泄漏的功耗侧信道攻击防护措施，能够抵御传统SCA泄漏模型的攻击


<details>
  <summary>Details</summary>
Motivation: 现有针对功耗侧信道攻击的防护措施主要关注字节级信息泄漏，但忽略了单个比特对密码实现整体抗攻击性的影响

Method: 开发了一种基于单比特泄漏的防护措施

Result: 实验结果表明，所提出的防护措施无法被使用传统SCA泄漏模型的攻击所破解

Conclusion: 单比特泄漏防护措施为功耗侧信道攻击提供了有效的防护方案

Abstract: The dependence of power-consumption on the processed data is a known
vulnerability of CMOS circuits, resulting in side channels which can be
exploited by power-based side channel attacks (SCAs). These attacks can extract
sensitive information, such as secret keys, from the implementation of
cryptographic algorithms. Existing countermeasures against power-based side
channel attacks focus on analyzing information leakage at the byte level.
However, this approach neglects the impact of individual bits on the overall
resistance of a cryptographic implementation. In this work, we present a
countermeasure based on single-bit leakage. The results suggest that the
proposed countermeasure cannot be broken by attacks using conventional SCA
leakage models.

</details>


### [36] [Modeling and Managing Temporal Obligations in GUCON Using SPARQL-star and RDF-star](https://arxiv.org/abs/2510.04652)
*Ines Akaichi,Giorgos Flouris,Irini Fundulaki,Sabrina Kirrane*

Main category: cs.CR

TL;DR: 本文扩展了GUCON政策框架，通过RDF-star和SPARQL-star显式建模义务的时间方面，支持基于时间知识图中使用痕迹的持续义务状态监控。


<details>
  <summary>Details</summary>
Motivation: 在数据跨境流动的数字时代，有效治理至关重要。现有义务监控解决方案缺乏形式语义或对义务状态推理的支持有限，需要改进。

Method: 扩展基于SPARQL图模式形式语义的GUCON政策框架，使用RDF-star和SPARQL-star表示时间义务模型，并实现义务状态管理器来监控义务状态。

Result: 提出了能够表达时间义务并支持基于使用痕迹持续监控其状态演变的扩展模型，开发了原型实现并进行评估。

Conclusion: 该扩展模型有效解决了现有义务监控方案的局限性，为数据使用控制提供了更强大的形式化基础和监控能力。

Abstract: In the digital age, data frequently crosses organizational and jurisdictional
boundaries, making effective governance essential. Usage control policies have
emerged as a key paradigm for regulating data usage, safeguarding privacy,
protecting intellectual property, and ensuring compliance with regulations. A
central mechanism for usage control is the handling of obligations, which arise
as a side effect of using and sharing data. Effective monitoring of obligations
requires capturing usage traces and accounting for temporal aspects such as
start times and deadlines, as obligations may evolve over times into different
states, such as fulfilled, violated, or expired. While several solutions have
been proposed for obligation monitoring, they often lack formal semantics or
provide limited support for reasoning over obligation states. To address these
limitations, we extend GUCON, a policy framework grounded in the formal
semantics of SPAQRL graph patterns, to explicitly model the temporal aspects of
an obligation. This extension enables the expressing of temporal obligations
and supports continuous monitoring of their evolving states based on usage
traces stored in temporal knowledge graphs. We demonstrate how this extended
model can be represented using RDF-star and SPARQL-star and propose an
Obligation State Manager that monitors obligation states and assess their
compliance with respect to usage traces. Finally, we evaluate both the extended
model and its prototype implementation.

</details>


### [37] [Enhancing TreePIR for a Single-Server Setting via Resampling](https://arxiv.org/abs/2510.04882)
*Elian Morel*

Main category: cs.CR

TL;DR: 本文提出了一种基于单向函数的单服务器预处理私有信息检索方案，通过双表提示结构和重采样技术，显著降低了通信带宽需求。


<details>
  <summary>Details</summary>
Motivation: 传统PIR方案在多服务器或公钥密码假设下才能实现亚线性服务器计算。预处理PIR通过离线阶段收集提示来克服这些限制，但现有单服务器方案通信开销较大。

Method: 基于PIANO和PPPS机制，将TreePIR适配到单服务器设置，引入主表和备份表的双表提示结构，以及高效刷新提示的重采样技术。

Result: 实现了对数级上传带宽和O(√n log n)下载复杂度，仅需O(√n log n)客户端存储，相比PIANO和PPPS有显著改进。

Conclusion: 该方案在保持单向函数基础设置简单性和最小假设的同时，显著提升了单服务器预处理PIR的性能。

Abstract: Private Information Retrieval (PIR) allows a client to retrieve an entry
$\text{DB}[i]$ from a public database $\text{DB}$ held by one or more servers,
without revealing the queried index $i$. Traditional PIR schemes achieve
sublinear server computation only under strong assumptions, such as the
presence of multiple non-colluding servers or the use of public-key
cryptography. To overcome these limitations, \textit{preprocessing PIR} schemes
introduce a query-independent offline phase where the client collects
\textit{hints} that enable efficient private queries during the online phase.
  In this work, we focus on preprocessing PIR schemes relying solely on
\textit{One-Way Functions} (OWFs), which provide minimal cryptographic
assumptions and practical implementability. We study three main constructions
-- TreePIR, PIANO, and PPPS -- that explore different trade-offs between
communication, storage, and server trust assumptions. Building upon the
mechanisms introduced in PIANO and PPPS, we propose an adaptation of TreePIR to
the single-server setting by introducing a dual-table hint structure (primary
and backup tables) and a \textit{resampling} technique to refresh hints
efficiently.
  Our proposed scheme achieves logarithmic upload bandwidth and $O(\sqrt{n}\log
n)$ download complexity while requiring $O(\sqrt{n}\log n)$ client storage.
This represents a significant improvement over prior single-server
preprocessing PIR schemes such as PIANO ($O(\sqrt{n})$ bandwidth) and PPPS
($O(n^{1/4})$ bandwidth), while maintaining the simplicity and minimal
assumptions of the OWF-based setting.

</details>


### [38] [RL Is a Hammer and LLMs Are Nails: A Simple Reinforcement Learning Recipe for Strong Prompt Injection](https://arxiv.org/abs/2510.04885)
*Yuxin Wen,Arman Zharmagambetov,Ivan Evtimov,Narine Kokhlikyan,Tom Goldstein,Kamalika Chaudhuri,Chuan Guo*

Main category: cs.CR

TL;DR: RL-Hammer是一种基于强化学习的自动红队攻击方法，能够有效对抗LLM代理的提示注入防御机制，在GPT-4o上达到98%的攻击成功率，在带有指令层次防御的GPT-5上达到72%的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的提示注入防御方法（如Instruction Hierarchy和SecAlign）在静态攻击下表现出色，但需要更强的自动化红队攻击来全面评估其鲁棒性。

Method: 提出RL-Hammer训练框架，使用强化学习训练攻击者模型，无需预热数据即可从零开始学习执行提示注入和越狱攻击，并采用实用技术实现高效通用攻击。

Result: RL-Hammer在GPT-4o上达到98%攻击成功率，在带有指令层次防御的GPT-5上达到72%攻击成功率，并能规避多个提示注入检测器。

Conclusion: 该工作推进了自动化红队攻击的发展，强调了开发更强、更原则性防御的必要性，同时揭示了攻击模型在多样性目标上的奖励黑客问题。

Abstract: Prompt injection poses a serious threat to the reliability and safety of LLM
agents. Recent defenses against prompt injection, such as Instruction Hierarchy
and SecAlign, have shown notable robustness against static attacks. However, to
more thoroughly evaluate the robustness of these defenses, it is arguably
necessary to employ strong attacks such as automated red-teaming. To this end,
we introduce RL-Hammer, a simple recipe for training attacker models that
automatically learn to perform strong prompt injections and jailbreaks via
reinforcement learning. RL-Hammer requires no warm-up data and can be trained
entirely from scratch. To achieve high ASRs against industrial-level models
with defenses, we propose a set of practical techniques that enable highly
effective, universal attacks. Using this pipeline, RL-Hammer reaches a 98% ASR
against GPT-4o and a $72\%$ ASR against GPT-5 with the Instruction Hierarchy
defense. We further discuss the challenge of achieving high diversity in
attacks, highlighting how attacker models tend to reward-hack diversity
objectives. Finally, we show that RL-Hammer can evade multiple prompt injection
detectors. We hope our work advances automatic red-teaming and motivates the
development of stronger, more principled defenses. Code is available at
https://github.com/facebookresearch/rl-injector.

</details>


### [39] [NatGVD: Natural Adversarial Example Attack towards Graph-based Vulnerability Detection](https://arxiv.org/abs/2510.04987)
*Avilash Rath,Weiliang Qi,Youpeng Li,Xinda Wang*

Main category: cs.CR

TL;DR: NatGVD是一种针对基于图的漏洞检测模型的新型攻击方法，通过代码转换修改图结构但保持语义，生成自然的对抗性漏洞代码来规避检测。


<details>
  <summary>Details</summary>
Motivation: 基于图的模型在代码分析任务中表现出色，但其在漏洞检测场景下对抗性攻击的鲁棒性仍是一个开放问题。

Method: 采用一组代码转换操作来修改图结构同时保持代码语义，考虑自然性要求，避免注入死代码或无关代码。

Result: 在最新的漏洞检测系统上评估，结果显示对基于GNN和图感知transformer的检测器逃避率高达53.04%。

Conclusion: 揭示了基于图的漏洞检测系统在对抗性攻击下的脆弱性，并探索了增强系统鲁棒性的潜在防御策略。

Abstract: Graph-based models learn rich code graph structural information and present
superior performance on various code analysis tasks. However, the robustness of
these models against adversarial example attacks in the context of
vulnerability detection remains an open question. This paper proposes NatGVD, a
novel attack methodology that generates natural adversarial vulnerable code to
circumvent GNN-based and graph-aware transformer-based vulnerability detectors.
NatGVD employs a set of code transformations that modify graph structure while
preserving code semantics. Instead of injecting dead or unrelated code like
previous works, NatGVD considers naturalness requirements: generated examples
should not be easily recognized by humans or program analysis tools. With
extensive evaluation of NatGVD on state-of-the-art vulnerability detection
systems, the results reveal up to 53.04% evasion rate across GNN-based
detectors and graph-aware transformer-based detectors. We also explore
potential defense strategies to enhance the robustness of these systems against
NatGVD.

</details>


### [40] [Proactive defense against LLM Jailbreak](https://arxiv.org/abs/2510.05052)
*Weiliang Zhao,Jinjun Peng,Daniel Ben-Levi,Zhou Yu,Junfeng Yang*

Main category: cs.CR

TL;DR: ProAct是一个主动防御框架，通过向攻击者提供看似成功但实际无害的虚假响应来误导自主越狱过程，显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的安全对齐仍然容易受到多轮越狱攻击，现有防御措施主要是被动和静态的，难以应对基于搜索的攻击。

Method: 提供虚假响应，让攻击者误以为越狱成功，从而提前终止攻击过程，误导攻击者的内部优化循环。

Result: 在多个先进LLM、越狱框架和安全基准测试中，攻击成功率降低高达92%，与其他防御框架结合时可将最新攻击策略成功率降至0%。

Conclusion: ProAct代表了一种正交防御策略，可作为额外防护措施，增强LLM对抗最有效越狱攻击的安全性。

Abstract: The proliferation of powerful large language models (LLMs) has necessitated
robust safety alignment, yet these models remain vulnerable to evolving
adversarial attacks, including multi-turn jailbreaks that iteratively search
for successful queries. Current defenses, primarily reactive and static, often
fail to counter these search-based attacks. In this paper, we introduce ProAct,
a novel proactive defense framework designed to disrupt and mislead autonomous
jailbreaking processes. Our core idea is to intentionally provide adversaries
with "spurious responses" that appear to be results of successful jailbreak
attacks but contain no actual harmful content. These misleading responses
provide false signals to the attacker's internal optimization loop, causing the
adversarial search to terminate prematurely and effectively jailbreaking the
jailbreak. By conducting extensive experiments across state-of-the-art LLMs,
jailbreaking frameworks, and safety benchmarks, our method consistently and
significantly reduces attack success rates by up to 92\%. When combined with
other defense frameworks, it further reduces the success rate of the latest
attack strategies to 0\%. ProAct represents an orthogonal defense strategy that
can serve as an additional guardrail to enhance LLM safety against the most
effective jailbreaking attacks.

</details>
