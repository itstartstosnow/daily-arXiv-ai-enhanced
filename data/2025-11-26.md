<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 21]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [EAGER: Edge-Aligned LLM Defense for Robust, Efficient, and Accurate Cybersecurity Question Answering](https://arxiv.org/abs/2511.19523)
*Onat Gungor,Roshan Sood,Jiasheng Zhou,Tajana Rosing*

Main category: cs.CR

TL;DR: EAGER是一个面向边缘设备的网络安全问答框架，通过量化低秩适应和领域特定偏好对齐，在保证效率的同时提升模型鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在网络安全问答中表现优异，但由于模型规模过大难以部署在边缘设备上。量化虽然能减少内存和计算需求，但会降低准确性并增加对抗攻击的脆弱性。

Method: 结合参数高效量化与领域特定偏好对齐，使用量化低秩适应进行低成本微调，并在自构建的网络安全偏好数据集上应用直接偏好优化，无需人工标注。

Result: EAGER将对抗攻击成功率降低高达7.3倍，问答准确性提升高达55%，在Jetson Orin上实现了最低的响应延迟。

Conclusion: EAGER框架在效率、鲁棒性和准确性方面实现了联合优化，证明了其在边缘设备上实际部署的可行性。

Abstract: Large Language Models (LLMs) are highly effective for cybersecurity question answering (QA) but are difficult to deploy on edge devices due to their size. Quantization reduces memory and compute requirements but often degrades accuracy and increases vulnerability to adversarial attacks. We present EAGER, an edge-aligned defense framework that integrates parameter-efficient quantization with domain-specific preference alignment to jointly optimize efficiency, robustness, and accuracy. Unlike prior methods that address these aspects separately, EAGER leverages Quantized Low-Rank Adaptation (QLoRA) for low-cost fine-tuning and Direct Preference Optimization (DPO) on a self-constructed cybersecurity preference dataset, eliminating the need for human labels. Experiments show that EAGER reduces adversarial attack success rates by up to 7.3x and improves QA accuracy by up to 55% over state-of-the-art defenses, while achieving the lowest response latency on a Jetson Orin, demonstrating its practical edge deployment.

</details>


### [2] [AttackPilot: Autonomous Inference Attacks Against ML Services With LLM-Based Agents](https://arxiv.org/abs/2511.19536)
*Yixin Wu,Rui Wen,Chi Cui,Michael Backes,Yang Zhang*

Main category: cs.CR

TL;DR: AttackPilot是一个能够自主执行推理攻击的AI代理，使用GPT-4o在20个目标服务上实现了100%任务完成率和接近专家水平的攻击性能，平均每次运行成本仅0.627美元。


<details>
  <summary>Details</summary>
Motivation: 推理攻击虽然为ML服务提供了系统性风险评估，但其实现和参数优化对非专家具有挑战性。大型语言模型的出现为开发自主攻击代理提供了机会。

Method: 采用多智能体框架和任务特定的动作空间，能够自适应优化策略以适应服务约束，有效缓解不良计划、指令遵循困难、任务上下文丢失和幻觉等问题。

Result: 使用GPT-4o时，在20个目标服务上实现100%任务完成率，攻击性能接近专家水平，平均每次运行成本仅0.627美元，且可适配多种其他LLM。

Conclusion: 此类自主代理能够赋能非专家的ML服务提供商、审计师或监管者，无需深厚领域专业知识即可系统性评估ML服务风险。

Abstract: Inference attacks have been widely studied and offer a systematic risk assessment of ML services; however, their implementation and the attack parameters for optimal estimation are challenging for non-experts. The emergence of advanced large language models presents a promising yet largely unexplored opportunity to develop autonomous agents as inference attack experts, helping address this challenge. In this paper, we propose AttackPilot, an autonomous agent capable of independently conducting inference attacks without human intervention. We evaluate it on 20 target services. The evaluation shows that our agent, using GPT-4o, achieves a 100.0% task completion rate and near-expert attack performance, with an average token cost of only $0.627 per run. The agent can also be powered by many other representative LLMs and can adaptively optimize its strategy under service constraints. We further perform trace analysis, demonstrating that design choices, such as a multi-agent framework and task-specific action spaces, effectively mitigate errors such as bad plans, inability to follow instructions, task context loss, and hallucinations. We anticipate that such agents could empower non-expert ML service providers, auditors, or regulators to systematically assess the risks of ML services without requiring deep domain expertise.

</details>


### [3] [SPQR: A Standardized Benchmark for Modern Safety Alignment Methods in Text-to-Image Diffusion Models](https://arxiv.org/abs/2511.19558)
*Mohammed Talha Alam,Nada Saadi,Fahad Shamshad,Nils Lukas,Karthik Nandakumar,Fahkri Karray,Samuele Poppi*

Main category: cs.CR

TL;DR: 本文研究了文本到图像扩散模型在良性微调后安全性对齐失效的问题，提出了SPQR基准来评估安全对齐方法在微调后的稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的安全对齐方法很少测试在部署后常规良性微调（如LoRA个性化、风格/领域适配器）下是否仍能保持安全性，而实际应用中这种微调很常见。

Method: 引入SPQR基准（安全-提示遵循-质量-鲁棒性），这是一个单一评分指标，通过标准化和可复现的框架评估安全对齐扩散模型在良性微调下保持安全、效用和鲁棒性的能力。

Result: 研究发现当前安全方法在良性微调下经常失效，通过多语言、领域特定和分布外分析确定了安全对齐在微调后失效的具体情况。

Conclusion: SPQR基准为T2I模型安全对齐技术提供了一个简洁而全面的评估标准，能够有效识别安全对齐在良性微调后的稳定性问题。

Abstract: Text-to-image diffusion models can emit copyrighted, unsafe, or private content. Safety alignment aims to suppress specific concepts, yet evaluations seldom test whether safety persists under benign downstream fine-tuning routinely applied after deployment (e.g., LoRA personalization, style/domain adapters). We study the stability of current safety methods under benign fine-tuning and observe frequent breakdowns. As true safety alignment must withstand even benign post-deployment adaptations, we introduce the SPQR benchmark (Safety-Prompt adherence-Quality-Robustness). SPQR is a single-scored metric that provides a standardized and reproducible framework to evaluate how well safety-aligned diffusion models preserve safety, utility, and robustness under benign fine-tuning, by reporting a single leaderboard score to facilitate comparisons. We conduct multilingual, domain-specific, and out-of-distribution analyses, along with category-wise breakdowns, to identify when safety alignment fails after benign fine-tuning, ultimately showcasing SPQR as a concise yet comprehensive benchmark for T2I safety alignment techniques for T2I models.

</details>


### [4] [IRSDA: An Agent-Orchestrated Framework for Enterprise Intrusion Response](https://arxiv.org/abs/2511.19644)
*Damodar Panigrahi,Raj Patel,Shaswata Mitra,Sudip Mittal,Shahram Rahimi*

Main category: cs.CR

TL;DR: IRSDA是一个基于代理的自主网络防御框架，结合自适应性自主计算系统和知识引导的MAPE-K循环，实现实时、分区感知的入侵响应决策。


<details>
  <summary>Details</summary>
Motivation: 传统入侵检测系统依赖静态规则和手动工作流程，无法应对现代企业系统面临的动态、分布式、多阶段网络威胁。需要开发能够快速、精确响应且符合策略的自主防御系统。

Method: 采用基于代理的框架，结合自适应性自主计算系统(SA-ACS)和知识引导的MAPE-K循环，构建知识驱动架构，集成上下文信息和AI推理，支持系统引导的入侵响应。

Result: 在真实微服务应用中的评估表明，系统能够自动化遏制威胁、强制执行合规性，并为安全分析师提供可追溯的输出结果。

Conclusion: IRSDA提供了一个模块化、代理驱动的网络防御方法，强调入侵响应中的可解释性、系统状态感知和操作控制。

Abstract: Modern enterprise systems face escalating cyber threats that are increasingly dynamic, distributed, and multi-stage in nature. Traditional intrusion detection and response systems often rely on static rules and manual workflows, which limit their ability to respond with the speed and precision required in high-stakes environments. To address these challenges, we present the Intrusion Response System Digital Assistant (IRSDA), an agent-based framework designed to deliver autonomous and policy-compliant cyber defense. IRSDA combines Self-Adaptive Autonomic Computing Systems (SA-ACS) with the Knowledge guided Monitor, Analyze, Plan, and Execute (MAPE-K) loop to support real-time, partition-aware decision-making across enterprise infrastructure.
  IRSDA incorporates a knowledge-driven architecture that integrates contextual information with AI-based reasoning to support system-guided intrusion response. The framework leverages retrieval mechanisms and structured representations to inform decision-making while maintaining alignment with operational policies. We assess the system using a representative real-world microservices application, demonstrating its ability to automate containment, enforce compliance, and provide traceable outputs for security analyst interpretation. This work outlines a modular and agent-driven approach to cyber defense that emphasizes explainability, system-state awareness, and operational control in intrusion response.

</details>


### [5] [Synthetic Data: AI's New Weapon Against Android Malware](https://arxiv.org/abs/2511.19649)
*Angelo Gaspar Diniz Nogueira,Kayua Oleques Paim,Hendrio Bragança,Rodrigo Brandão Mansilha,Diego Kreutz*

Main category: cs.CR

TL;DR: 提出MalSynGen方法，使用条件生成对抗网络(cGAN)生成合成恶意软件数据，解决Android恶意软件检测中数据稀缺和质量问题。


<details>
  <summary>Details</summary>
Motivation: Android恶意软件数量激增且不断进化，传统检测方法难以应对。机器学习检测需要高质量数据集，但真实恶意软件样本获取成本高、标注困难。

Method: 使用条件生成对抗网络(cGAN)生成合成表格数据，保持真实数据的统计特性，提升Android恶意软件分类器性能。

Result: 实验证明MalSynGen能跨不同数据集泛化，有效解决恶意软件检测中的数据过时和低质量问题。

Conclusion: MalSynGen为恶意软件检测中的数据稀缺问题提供了可行的解决方案，能生成高质量合成数据提升分类器性能。

Abstract: The ever-increasing number of Android devices and the accelerated evolution of malware, reaching over 35 million samples by 2024, highlight the critical importance of effective detection methods. Attackers are now using Artificial Intelligence to create sophisticated malware variations that can easily evade traditional detection techniques. Although machine learning has shown promise in malware classification, its success relies heavily on the availability of up-to-date, high-quality datasets. The scarcity and high cost of obtaining and labeling real malware samples presents significant challenges in developing robust detection models. In this paper, we propose MalSynGen, a Malware Synthetic Data Generation methodology that uses a conditional Generative Adversarial Network (cGAN) to generate synthetic tabular data. This data preserves the statistical properties of real-world data and improves the performance of Android malware classifiers. We evaluated the effectiveness of this approach using various datasets and metrics that assess the fidelity of the generated data, its utility in classification, and the computational efficiency of the process. Our experiments demonstrate that MalSynGen can generalize across different datasets, providing a viable solution to address the issues of obsolescence and low quality data in malware detection.

</details>


### [6] [Accuracy and Efficiency Trade-Offs in LLM-Based Malware Detection and Explanation: A Comparative Study of Parameter Tuning vs. Full Fine-Tuning](https://arxiv.org/abs/2511.19654)
*Stephen C. Gravereaux,Sheikh Rabiul Islam*

Main category: cs.CR

TL;DR: LoRA微调的大语言模型在恶意软件分类的解释生成方面接近全参数微调的性能，在保持解释质量的同时显著减少模型大小和训练时间。


<details>
  <summary>Details</summary>
Motivation: 解决在恶意软件检测中实现可信赖的LLM决策和解释生成的挑战，特别是在资源受限环境中部署的需求。

Method: 使用BLEU、ROUGE和语义相似度指标构建评估框架，比较五种LoRA配置与全参数微调基线的解释质量。

Result: 全参数微调获得最高总体分数，但中等规模的LoRA模型在两个指标上表现优于全参数微调，同时减少约81%的模型大小和超过80%的训练时间。

Conclusion: LoRA在可解释性和资源效率之间提供了实用平衡，能够在资源受限环境中部署而不牺牲解释质量，增强恶意软件检测系统的透明度和可扩展性。

Abstract: This study examines whether Low-Rank Adaptation (LoRA) fine-tuned Large Language Models (LLMs) can approximate the performance of fully fine-tuned models in generating human-interpretable decisions and explanations for malware classification. Achieving trustworthy malware detection, particularly when LLMs are involved, remains a significant challenge. We developed an evaluation framework using Bilingual Evaluation Understudy (BLEU), Recall-Oriented Understudy for Gisting Evaluation (ROUGE), and Semantic Similarity Metrics to benchmark explanation quality across five LoRA configurations and a fully fine-tuned baseline. Results indicate that full fine-tuning achieves the highest overall scores, with BLEU and ROUGE improvements of up to 10% over LoRA variants. However, mid-range LoRA models deliver competitive performance exceeding full fine-tuning on two metrics while reducing model size by approximately 81% and training time by over 80% on a LoRA model with 15.5% trainable parameters. These findings demonstrate that LoRA offers a practical balance of interpretability and resource efficiency, enabling deployment in resource-constrained environments without sacrificing explanation quality. By providing feature-driven natural language explanations for malware classifications, this approach enhances transparency, analyst confidence, and operational scalability in malware detection systems.

</details>


### [7] [BASICS: Binary Analysis and Stack Integrity Checker System for Buffer Overflow Mitigation](https://arxiv.org/abs/2511.19670)
*Luis Ferreirinha,Iberia Medeiros*

Main category: cs.CR

TL;DR: 提出了一种结合模型检测和符号执行的方法，用于自动检测和修复C程序二进制代码中的缓冲区溢出漏洞，通过构建内存状态空间来验证安全属性，并使用蹦床技术进行自动化修复。


<details>
  <summary>Details</summary>
Motivation: C语言在关键基础设施系统中广泛使用但容易存在缓冲区溢出漏洞，传统漏洞发现技术在二进制代码层面存在可扩展性和精确性问题，需要更有效的自动化检测和修复方法。

Method: 结合模型检查和符号执行技术，构建内存状态空间(MemStaCe)来验证程序栈内存的安全属性，使用LTL定义安全属性，通过反例分析识别漏洞，采用蹦床技术进行二进制补丁修复。

Result: 在Juliet C/C++和SARD数据集及实际应用中评估，检测和修复的准确率和精确率均超过87%，优于CWE Checker工具。

Conclusion: 该方法能够有效自动化检测和修复C程序二进制代码中的缓冲区溢出漏洞，在准确性和精确性方面表现优异，为关键系统安全提供了可靠保障。

Abstract: Cyber-Physical Systems have played an essential role in our daily lives, providing critical services such as power and water, whose operability, availability, and reliability must be ensured. The C programming language, prevalent in CPS development, is crucial for system control where reliability is critical. However, it is also commonly susceptible to vulnerabilities, particularly buffer overflows. Traditional vulnerability discovery techniques often struggle with scalability and precision when applied directly to the binary code of C programs, which can thereby keep programs vulnerable. This work introduces a novel approach designed to overcome these limitations by leveraging model checking and concolic execution techniques to automatically verify security properties of a program's stack memory in binary code, trampoline techniques to perform automated repair of the issues, and crash-inducing inputs to verify if they were successfully removed. The approach constructs a Memory State Space -- MemStaCe -- from the binary program's control flow graph and simulations, provided by concolic execution, of C function calls and loop constructs. The security properties, defined in LTL, model the correct behaviour of functions associated with vulnerabilities and allow the approach to identify vulnerabilities in MemStaCe by analysing counterexample traces that are generated when a security property is violated. These vulnerabilities are then addressed with a trampoline-based binary patching method, and the effectiveness of the patches is checked with crash-inducing inputs extracted during concolic execution. We implemented the approach in the BASICS tool for BO mitigation and evaluated using the Juliet C/C++ and SARD datasets and real applications, achieving an accuracy and precision above 87%, both in detection and correction. Also, we compared it with CWE Checker, outperforming it.

</details>


### [8] [CrypTorch: PyTorch-based Auto-tuning Compiler for Machine Learning with Multi-party Computation](https://arxiv.org/abs/2511.19711)
*Jinyu Liu,Gang Tan,Kiwan Maeng*

Main category: cs.CR

TL;DR: CrypTorch是一个基于MPC的机器学习编译器，通过自动选择近似算法来解决现有框架中近似操作成为性能瓶颈的问题，显著提升了MPC-based ML的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的MPC-based ML框架在运行Softmax、GELU等ML操作时需要使用近似算法，但这些近似往往成为性能瓶颈，且难以在现有框架中识别和修复。

Method: 提出CrypTorch编译器，将近似算法与MPC运行时解耦，提供编程接口便于添加新近似，并自动选择近似算法以最大化性能和精度。

Result: CrypTorch仅通过自动调优就能在不牺牲精度的情况下获得1.20-1.7倍加速，允许精度下降时可获得1.31-1.8倍加速；与CrypTen相比，整个框架带来3.22-8.6倍的端到端加速。

Conclusion: CrypTorch通过解耦近似算法和自动选择机制，有效解决了MPC-based ML中近似操作成为性能瓶颈的问题，显著提升了框架性能。

Abstract: Machine learning (ML) involves private data and proprietary model parameters. MPC-based ML allows multiple parties to collaboratively run an ML workload without sharing their private data or model parameters using multi-party computing (MPC). Because MPC cannot natively run ML operations such as Softmax or GELU, existing frameworks use different approximations. Our study shows that, on a well-optimized framework, these approximations often become the dominating bottleneck. Popular approximations are often insufficiently accurate or unnecessarily slow, and these issues are hard to identify and fix in existing frameworks. To tackle this issue, we propose a compiler for MPC-based ML, CrypTorch. CrypTorch disentangles these approximations with the rest of the MPC runtime, allows easily adding new approximations through its programming interface, and automatically selects approximations to maximize both performance and accuracy. Built as an extension to PyTorch 2's compiler, we show that CrypTorch's auto-tuning alone provides 1.20--1.7$\times$ immediate speedup without sacrificing accuracy, and 1.31--1.8$\times$ speedup when some accuracy degradation is allowed, compared to our well-optimized baseline. Combined with better engineering and adoption of state-of-the-art practices, the entire framework brings 3.22--8.6$\times$ end-to-end speedup compared to the popular framework, CrypTen.

</details>


### [9] [Prompt Fencing: A Cryptographic Approach to Establishing Security Boundaries in Large Language Model Prompts](https://arxiv.org/abs/2511.19727)
*Steven Peh*

Main category: cs.CR

TL;DR: Prompt Fencing是一种通过密码学认证和数据架构原则在LLM提示中建立明确安全边界的架构方法，能完全防止提示注入攻击。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生产部署中仍然容易受到提示注入攻击，这是最重要的安全威胁。

Method: 通过在提示段中添加包含信任评级和内容类型的密码学签名元数据，使LLM能够区分可信指令和不可信内容。

Result: 在300个测试案例中，将攻击成功率从86.7%降至0%，验证开销为0.094秒。

Conclusion: 该方法平台无关，可作为现有LLM基础设施上的安全层逐步部署，未来模型可训练原生围栏感知以获得最佳安全性。

Abstract: Large Language Models (LLMs) remain vulnerable to prompt injection attacks, representing the most significant security threat in production deployments. We present Prompt Fencing, a novel architectural approach that applies cryptographic authentication and data architecture principles to establish explicit security boundaries within LLM prompts. Our approach decorates prompt segments with cryptographically signed metadata including trust ratings and content types, enabling LLMs to distinguish between trusted instructions and untrusted content. While current LLMs lack native fence awareness, we demonstrate that simulated awareness through prompt instructions achieved complete prevention of injection attacks in our experiments, reducing success rates from 86.7% (260/300 successful attacks) to 0% (0/300 successful attacks) across 300 test cases with two leading LLM providers. We implement a proof-of-concept fence generation and verification pipeline with a total overhead of 0.224 seconds (0.130s for fence generation, 0.094s for validation) across 100 samples. Our approach is platform-agnostic and can be incrementally deployed as a security layer above existing LLM infrastructure, with the expectation that future models will be trained with native fence awareness for optimal security.

</details>


### [10] [Cross-LLM Generalization of Behavioral Backdoor Detection in AI Agent Supply Chains](https://arxiv.org/abs/2511.19874)
*Arun Chowdary Sanna*

Main category: cs.CR

TL;DR: 研究发现单模型行为后门检测器在不同LLM间的泛化能力极差，准确率从92.7%降至49.2%，接近随机猜测。通过模型感知检测方法，结合模型身份特征，可将准确率提升至90.6%。


<details>
  <summary>Details</summary>
Motivation: AI代理在企业工作流中广泛使用共享工具库和预训练组件，存在供应链安全风险。现有研究主要关注单一LLM架构内的行为后门检测，但跨LLM的泛化能力问题尚未探索，这对部署多个AI系统的组织具有重要安全意义。

Method: 对6个生产级LLM进行系统研究，通过1,198个执行轨迹和36个跨模型实验，分析单模型检测器的泛化能力。提出模型感知检测方法，将模型身份作为额外特征。

Result: 单模型检测器在训练分布内准确率达92.7%，但跨不同LLM时仅49.2%，存在43.4个百分点的泛化差距。模型感知检测方法在所有评估模型中达到90.6%的准确率。

Conclusion: 跨LLM行为后门检测存在显著的泛化差距，主要源于模型特定的行为特征。模型感知检测方法能有效解决这一问题，为多AI系统环境下的安全检测提供可行方案。

Abstract: As AI agents become integral to enterprise workflows, their reliance on shared tool libraries and pre-trained components creates significant supply chain vulnerabilities. While previous work has demonstrated behavioral backdoor detection within individual LLM architectures, the critical question of cross-LLM generalization remains unexplored, a gap with serious implications for organizations deploying multiple AI systems. We present the first systematic study of cross-LLM behavioral backdoor detection, evaluating generalization across six production LLMs (GPT-5.1, Claude Sonnet 4.5, Grok 4.1, Llama 4 Maverick, GPT-OSS 120B, and DeepSeek Chat V3.1). Through 1,198 execution traces and 36 cross-model experiments, we quantify a critical finding: single-model detectors achieve 92.7% accuracy within their training distribution but only 49.2% across different LLMs, a 43.4 percentage point generalization gap equivalent to random guessing. Our analysis reveals that this gap stems from model-specific behavioral signatures, particularly in temporal features (coefficient of variation > 0.8), while structural features remain stable across architectures. We show that model-aware detection incorporating model identity as an additional feature achieves 90.6% accuracy universally across all evaluated models. We release our multi-LLM trace dataset and detection framework to enable reproducible research.

</details>


### [11] [Frequency Bias Matters: Diving into Robust and Generalized Deep Image Forgery Detection](https://arxiv.org/abs/2511.19886)
*Chi Liu,Tianqing Zhu,Wanlei Zhou,Wei Zhao*

Main category: cs.CR

TL;DR: 本文从频率角度分析AI生成图像伪造检测器的泛化性和鲁棒性问题，提出频率对齐方法，既能作为对抗检测器的黑盒攻击，也能作为提升检测器可靠性的防御手段。


<details>
  <summary>Details</summary>
Motivation: AI生成的图像伪造日益严重，检测器的泛化性（面对未知GAN模型）和鲁棒性（面对噪声样本）是关键问题。现有研究未深入探索这些问题根源，且缺乏能同时解决这两个问题的通用方法。

Method: 提出两步骤频率对齐方法：1）分析DNN伪造检测器的频率偏差；2）通过消除真假图像间的频率差异来解决问题。该方法可双向应用：在反取证中作为黑盒攻击，在取证中作为通用防御。

Result: 在涉及12个检测器、8个伪造模型和5个指标的多种实验设置中验证了方法的有效性，证明了频率对齐方法的作用。

Conclusion: 频率偏差是伪造检测器泛化和鲁棒性问题的根源，频率对齐方法提供了同时解决这两个问题的通用方案，在取证和反取证领域都具有重要价值。

Abstract: As deep image forgery powered by AI generative models, such as GANs, continues to challenge today's digital world, detecting AI-generated forgeries has become a vital security topic. Generalizability and robustness are two critical concerns of a forgery detector, determining its reliability when facing unknown GANs and noisy samples in an open world. Although many studies focus on improving these two properties, the root causes of these problems have not been fully explored, and it is unclear if there is a connection between them. Moreover, despite recent achievements in addressing these issues from image forensic or anti-forensic aspects, a universal method that can contribute to both sides simultaneously remains practically significant yet unavailable. In this paper, we provide a fundamental explanation of these problems from a frequency perspective. Our analysis reveals that the frequency bias of a DNN forgery detector is a possible cause of generalization and robustness issues. Based on this finding, we propose a two-step frequency alignment method to remove the frequency discrepancy between real and fake images, offering double-sided benefits: it can serve as a strong black-box attack against forgery detectors in the anti-forensic context or, conversely, as a universal defense to improve detector reliability in the forensic context. We also develop corresponding attack and defense implementations and demonstrate their effectiveness, as well as the effect of the frequency alignment method, in various experimental settings involving twelve detectors, eight forgery models, and five metrics.

</details>


### [12] [Zero-Knowledge Proof Based Verifiable Inference of Models](https://arxiv.org/abs/2511.19902)
*Yunxiao Wang*

Main category: cs.CR

TL;DR: 提出了一种零知识框架，能够在无需暴露模型内部参数的情况下验证深度学习推理，支持线性和非线性神经网络层，并基于递归组合的零知识证明构建。


<details>
  <summary>Details</summary>
Motivation: AI模型参数代表巨大的训练成本和知识产权价值，模型所有者通常不愿公开参数，这使得透明验证变得困难。需要一种方法能够验证AI模型推理的正确性而不暴露模型内部参数。

Method: 基于递归组合的零知识证明构建框架，无需可信设置，支持矩阵乘法、归一化、softmax和SiLU等线性和非线性神经网络层。利用Fiat-Shamir启发式方法获得具有恒定大小证明的简洁非交互式知识论证（zkSNARK）。

Result: 将DeepSeek模型转换为完全可SNARK验证的版本ZK-DeepSeek，实验证明该框架在实际AI验证工作负载中具有高效性和灵活性。

Conclusion: 该零知识框架为保护AI模型知识产权的同时验证推理正确性提供了实用解决方案，在现实世界AI验证场景中展现出良好的效率和适应性。

Abstract: Recent advances in artificial intelligence (AI), particularly deep learning, have led to widespread adoption across various applications. Yet, a fundamental challenge persists: how can we verify the correctness of AI model inference when model owners cannot (or will not) reveal their parameters? These parameters represent enormous training costs and valuable intellectual property, making transparent verification difficult. In this paper, we introduce a zero-knowledge framework capable of verifying deep learning inference without exposing model internal parameters. Built on recursively composed zero-knowledge proofs and requiring no trusted setup, our framework supports both linear and nonlinear neural network layers, including matrix multiplication, normalization, softmax, and SiLU. Leveraging the Fiat-Shamir heuristic, we obtain a succinct non-interactive argument of knowledge (zkSNARK) with constant-size proofs. To demonstrate the practicality of our approach, we translate the DeepSeek model into a fully SNARK-verifiable version named ZK-DeepSeek and show experimentally that our framework delivers both efficiency and flexibility in real-world AI verification workloads.

</details>


### [13] [Improving the Identification of Real-world Malware's DNS Covert Channels Using Locality Sensitive Hashing](https://arxiv.org/abs/2511.20229)
*Pascal Ruffing,Denis Petrov,Sebastian Zillien,Steffen Wendzel*

Main category: cs.CR

TL;DR: 首次将局部敏感哈希应用于检测和识别使用DNS隐蔽通道的真实恶意软件，通过将DNS子域序列编码为统计相似性特征来捕获恶意活动异常，结合随机森林分类器实现更高准确率和更低误报率。


<details>
  <summary>Details</summary>
Motivation: 当前恶意软件越来越多地使用DNS隐蔽通道来逃避检测并维持与C&C服务器的隐蔽通信，而现有方法在从捕获的网络流量中识别特定恶意软件家族及其行为方面仍面临挑战。

Method: 使用局部敏感哈希将DNS子域序列编码为统计相似性特征，结合随机森林分类器进行检测和分类。

Result: 相比先前方法实现了更高的准确率和更低的误报率，对未见或修改过的恶意软件样本表现出更好的鲁棒性和泛化能力，能够仅基于DNS子域可靠分类恶意软件行为。

Conclusion: 该方法能有效检测和识别使用DNS隐蔽通道的恶意软件，并能可靠分类恶意软件行为类型，为DNS安全监控提供了新思路。

Abstract: Nowadays, malware increasingly uses DNS-based covert channels in order to evade detection and maintain stealthy communication with its command-and-control servers. While prior work has focused on detecting such activity, identifying specific malware families and their behaviors from captured network traffic remains challenging due to the variability of DNS. In this paper, we present the first application of Locality Sensitive Hashing to the detection and identification of real-world malware utilizing DNS covert channels. Our approach encodes DNS subdomain sequences into statistical similarity features that effectively capture anomalies indicative of malicious activity. Combined with a Random Forest classifier, our method achieves higher accuracy and reduced false positive rates than prior approaches, while demonstrating improved robustness and generalization to previously unseen or modified malware samples. We further demonstrate that our approach enables reliable classification of malware behavior (e.g., uploading or downloading of files), based solely on DNS subdomains.

</details>


### [14] [Hey there! You are using WhatsApp: Enumerating Three Billion Accounts for Security and Privacy](https://arxiv.org/abs/2511.20252)
*Gabriel K. Gegenhuber,Philipp É. Frenzel,Maximilian Günther,Johanna Ullrich,Aljosha Judmayer*

Main category: cs.CR

TL;DR: WhatsApp存在严重的电话号码枚举漏洞，攻击者可以每小时探测超过1亿个电话号码而不会被阻止，近半数在2021年Facebook数据泄露中的电话号码仍在WhatsApp上活跃。


<details>
  <summary>Details</summary>
Motivation: WhatsApp作为全球最大的即时通讯平台，其用户发现机制存在固有的电话号码枚举风险，需要重新评估其安全防护措施的有效性。

Method: 通过查询WhatsApp服务器验证电话号码是否注册，测试了大规模枚举攻击的可行性，并分析了收集到的用户数据。

Result: 成功以每小时超过1亿个电话号码的速度进行枚举攻击，发现近半数泄露数据中的电话号码仍活跃，还发现了X25519密钥重复使用的问题。

Conclusion: WhatsApp的电话号码枚举漏洞严重且持久，尽管最终通过合作修复解决了速率限制问题，但此类平台面临的数据隐私风险依然严峻。

Abstract: WhatsApp, with 3.5 billion active accounts as of early 2025, is the world's largest instant messaging platform. Given its massive user base, WhatsApp plays a critical role in global communication.
  To initiate conversations, users must first discover whether their contacts are registered on the platform. This is achieved by querying WhatsApp's servers with mobile phone numbers extracted from the user's address book (if they allowed access). This architecture inherently enables phone number enumeration, as the service must allow legitimate users to query contact availability. While rate limiting is a standard defense against abuse, we revisit the problem and show that WhatsApp remains highly vulnerable to enumeration at scale. In our study, we were able to probe over a hundred million phone numbers per hour without encountering blocking or effective rate limiting.
  Our findings demonstrate not only the persistence but the severity of this vulnerability. We further show that nearly half of the phone numbers disclosed in the 2021 Facebook data leak are still active on WhatsApp, underlining the enduring risks associated with such exposures. Moreover, we were able to perform a census of WhatsApp users, providing a glimpse on the macroscopic insights a large messaging service is able to generate even though the messages themselves are end-to-end encrypted. Using the gathered data, we also discovered the re-use of certain X25519 keys across different devices and phone numbers, indicating either insecure (custom) implementations, or fraudulent activity.
  In this updated version of the paper, we also provide insights into the collaborative remediation process through which we confirmed that the underlying rate-limiting issue had been resolved.

</details>


### [15] [Can LLMs Make (Personalized) Access Control Decisions?](https://arxiv.org/abs/2511.20284)
*Friederike Groschupp,Daniele Lain,Aritra Dhar,Lara Magdalena Lazier,Srdjan Čapkun*

Main category: cs.CR

TL;DR: 利用大型语言模型(LLMs)进行动态、上下文感知的访问控制决策，以减轻用户认知负担，同时平衡个性化、安全性和实用性


<details>
  <summary>Details</summary>
Motivation: 传统访问控制决策给用户带来巨大认知负担，导致次优或随意决策。系统日益复杂和自动化需要更智能的访问控制方案

Method: 通过用户研究收集307个自然语言隐私声明和14,682个用户访问控制决策，比较通用LLM和个性化LLM的决策准确性，并收集用户对1,446个LLM决策的反馈

Result: LLMs能很好地反映用户偏好，与多数用户决策相比达到86%准确率。个性化系统存在权衡：提供用户特定偏好能提高与个体决策的一致性，但可能违反安全最佳实践

Conclusion: 讨论了实现实用自然语言访问控制系统的设计和风险考量，需要在个性化、安全和实用性之间取得平衡

Abstract: Precise access control decisions are crucial to the security of both traditional applications and emerging agent-based systems. Typically, these decisions are made by users during app installation or at runtime. Due to the increasing complexity and automation of systems, making these access control decisions can add a significant cognitive load on users, often overloading them and leading to suboptimal or even arbitrary access control decisions. To address this problem, we propose to leverage the processing and reasoning capabilities of large language models (LLMs) to make dynamic, context-aware decisions aligned with the user's security preferences. For this purpose, we conducted a user study, which resulted in a dataset of 307 natural-language privacy statements and 14,682 access control decisions made by users. We then compare these decisions against those made by two versions of LLMs: a general and a personalized one, for which we also gathered user feedback on 1,446 of its decisions.
  Our results show that in general, LLMs can reflect users' preferences well, achieving up to 86\% accuracy when compared to the decision made by the majority of users. Our study also reveals a crucial trade-off in personalizing such a system: while providing user-specific privacy preferences to the LLM generally improves agreement with individual user decisions, adhering to those preferences can also violate some security best practices. Based on our findings, we discuss design and risk considerations for implementing a practical natural-language-based access control system that balances personalization, security, and utility.

</details>


### [16] [APT-CGLP: Advanced Persistent Threat Hunting via Contrastive Graph-Language Pre-Training](https://arxiv.org/abs/2511.20290)
*Xuebo Qiu,Mingqi Lv,Yimei Zhang,Tieming Chen,Tiantian Zhu,Qijie Song,Shouling Ji*

Main category: cs.CR

TL;DR: APT-CGLP是一个基于对比图-语言预训练的跨模态APT狩猎系统，能够直接在溯源图和CTI报告之间进行端到端语义匹配，无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于图匹配的威胁狩猎方法存在的信息丢失和人工干预过多的问题，弥合溯源图与CTI报告之间的模态差距。

Method: 1) 利用LLM合成高质量的溯源图-CTI报告对缓解数据稀缺问题；2) 采用多目标训练算法，结合对比学习和跨模态掩码建模，实现粗粒度和细粒度的跨模态攻击语义对齐。

Result: 在四个真实APT数据集上的实验表明，APT-CGLP在准确性和效率方面持续优于最先进的威胁狩猎基线方法。

Conclusion: APT-CGLP通过端到端的跨模态语义匹配，有效解决了传统图匹配方法的信息丢失和可扩展性问题，显著提升了APT威胁狩猎的性能。

Abstract: Provenance-based threat hunting identifies Advanced Persistent Threats (APTs) on endpoints by correlating attack patterns described in Cyber Threat Intelligence (CTI) with provenance graphs derived from system audit logs. A fundamental challenge in this paradigm lies in the modality gap -- the structural and semantic disconnect between provenance graphs and CTI reports. Prior work addresses this by framing threat hunting as a graph matching task: 1) extracting attack graphs from CTI reports, and 2) aligning them with provenance graphs. However, this pipeline incurs severe \textit{information loss} during graph extraction and demands intensive manual curation, undermining scalability and effectiveness.
  In this paper, we present APT-CGLP, a novel cross-modal APT hunting system via Contrastive Graph-Language Pre-training, facilitating end-to-end semantic matching between provenance graphs and CTI reports without human intervention. First, empowered by the Large Language Model (LLM), APT-CGLP mitigates data scarcity by synthesizing high-fidelity provenance graph-CTI report pairs, while simultaneously distilling actionable insights from noisy web-sourced CTIs to improve their operational utility. Second, APT-CGLP incorporates a tailored multi-objective training algorithm that synergizes contrastive learning with inter-modal masked modeling, promoting cross-modal attack semantic alignment at both coarse- and fine-grained levels. Extensive experiments on four real-world APT datasets demonstrate that APT-CGLP consistently outperforms state-of-the-art threat hunting baselines in terms of accuracy and efficiency.

</details>


### [17] [A Reality Check on SBOM-based Vulnerability Management: An Empirical Study and A Path Forward](https://arxiv.org/abs/2511.20313)
*Li Zhou,Marc Dacier,Charalambos Konstantinou*

Main category: cs.CR

TL;DR: 该论文通过大规模实证研究发现，使用锁定文件和强包管理器可以生成准确的SBOM，但下游漏洞扫描器存在97.5%的误报率，主要原因是标记了不可达代码中的漏洞。函数调用分析可以有效减少63.3%的误报。


<details>
  <summary>Details</summary>
Motivation: SBOM在软件供应链安全中至关重要，但其实际效用因生成不准确和漏洞扫描误报率高而受到损害，需要从实践角度解决这些问题。

Method: 对2,414个开源仓库进行大规模实证研究，首先使用锁定文件和强包管理器生成准确SBOM，然后分析下游漏洞扫描器的误报问题，并应用函数调用分析来减少误报。

Result: 使用锁定文件和强包管理器可以生成准确一致的SBOM；下游漏洞扫描器存在97.5%的误报率；函数调用分析可以消除63.3%的误报。

Conclusion: 提出了一个实用的两阶段软件供应链安全方法：首先使用锁定文件和强包管理器生成准确SBOM，然后通过函数调用分析生成可操作的低噪声漏洞报告，减轻开发者的警报疲劳。

Abstract: The Software Bill of Materials (SBOM) is a critical tool for securing the software supply chain (SSC), but its practical utility is undermined by inaccuracies in both its generation and its application in vulnerability scanning. This paper presents a large-scale empirical study on 2,414 open-source repositories to address these issues from a practical standpoint. First, we demonstrate that using lock files with strong package managers enables the generation of accurate and consistent SBOMs, establishing a reliable foundation for security analysis. Using this high-fidelity foundation, however, we expose a more fundamental flaw in practice: downstream vulnerability scanners produce a staggering 97.5\% false positive rate. We pinpoint the primary cause as the flagging of vulnerabilities within unreachable code. We then demonstrate that function call analysis can effectively prune 63.3\% of these false alarms. Our work validates a practical, two-stage approach for SSC security: first, generate an accurate SBOM using lock files and strong package managers, and second, enrich it with function call analysis to produce actionable, low-noise vulnerability reports that alleviate developers' alert fatigue.

</details>


### [18] [A Single-Root, Multi-Curve, Context-Isolated, PQC-Pluggable Cryptographic Identity Primitive with Stateless Secret Rotation](https://arxiv.org/abs/2511.20505)
*Jian Sheng Wang*

Main category: cs.CR

TL;DR: MSCIKDF是一种新型加密身份原语，提供单根多曲线、上下文隔离、PQC可插拔的加密身份架构，解决传统方案在算法敏捷性、安全密钥轮换和跨上下文隔离方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前BIP-39和BIP-32等标准在应对多曲线、多领域和后量子密码(PQC)环境时存在结构性不足，缺乏上下文隔离、算法敏捷性和安全密钥轮换能力。

Method: 提出MSCIKDF架构，通过确定性派生但加密强制分离的方式，在不同上下文(如区块链、端到端加密、密钥管理系统、物联网)中实现身份隔离。

Result: 实现了强安全特性：零可链接性、多曲线独立性、抗跨上下文关联，并提供无状态密钥轮换，保持长期身份连续性而无需资产迁移。

Conclusion: MSCIKDF作为确定性身份的基础设施级升级，为未来十年分布式系统、AI代理和PQC迁移建立了持久且算法无关的信任根。

Abstract: Cryptographic identity anchors modern decentralized systems, yet current standards like BIP-39 and BIP-32 are structurally insufficient for the demands of multi-curve, multi-domain, and post-quantum (PQC) environments. These legacy schemes rely on a monolithic identity root with no inherent context isolation, algorithm agility, or secure secret rotation. This paper introduces MSCIKDF, a single-root, multi-curve, context-isolated, PQC-pluggable cryptographic identity primitive. MSCIKDF defines a new architectural foundation where identity is derived deterministically but with cryptographically enforced separation across diverse contexts (e.g., blockchain, E2EE, KMS, IoT). It achieves strong security invariants -- such as zero-linkability, multi-curve independence, and resistance to cross-context correlation -- while offering stateless secret rotation that preserves long-term identity continuity without requiring asset migration. MSCIKDF is proposed as an infrastructure-level upgrade to deterministic identity, establishing a durable and algorithm-agnostic root of trust suitable for the next decade of distributed systems, AI agents, and PQC migration.

</details>


### [19] [Engel p-adic Isogeny-based Cryptography over Laurent Series: Foundations, Security, and an ESP32 Implementation](https://arxiv.org/abs/2511.20533)
*Ilias Cherkaoui,Indrakshi Dey*

Main category: cs.CR

TL;DR: 本文提出了一种基于Engel展开的等基因框架，通过p进洛朗级数压缩超奇异椭圆曲线等基因数据，解决了后量子密码在物联网设备上的紧凑性和效率问题。


<details>
  <summary>Details</summary>
Motivation: 保护物联网设备免受量子攻击需要既紧凑又能在微控制器上高效运行的后量子密码方案，而现有方案往往因密钥过大和计算复杂而难以满足这些要求。

Method: 使用新颖的Engel展开在p进洛朗级数上编码超奇异椭圆曲线等基因数据，Engel系数压缩了挠信息，实现了紧凑的公共密钥；Engel算术是局部的，允许固定精度的p进运算，适合嵌入式设备。

Result: 实现了~1.1-16.9 kbits的紧凑公共密钥，保持了等基因系统的小尺寸特性；开发了低内存、分支规则的内核，适合微控制器高效运行。

Conclusion: 提出的等基因框架同时解决了后量子密码在物联网设备上的紧凑性和效率约束，为量子安全物联网提供了可行的解决方案。

Abstract: Securing the Internet of Things (IoT) against quantum attacks requires public-key cryptography that (i) remains compact and (ii) runs efficiently on microcontrollers, capabilities many post-quantum (PQ) schemes lack due to large keys and heavy arithmetic. We address both constraints simultaneously with, to our knowledge, the first-ever isogeny framework that encodes super-singular elliptic-curve isogeny data via novel Engel expansions over the p-adic Laurent series. Engel coefficients compress torsion information, thereby addressing the compactness constraint, yielding public keys of ~1.1 - 16.9 kbits preserving the hallmark small sizes of isogeny systems. Engel arithmetic is local and admits fixed-precision p-adic operations, enabling micro-controller efficiency with low-memory, branch-regular kernels suitable for embedded targets.

</details>


### [20] [Effective Command-line Interface Fuzzing with Path-Aware Large Language Model Orchestration](https://arxiv.org/abs/2511.20555)
*Momoko Shiraishi,Yinzhi Cao,Takahiro Shinagawa*

Main category: cs.CR

TL;DR: PILOT是一个路径引导的迭代LLM编排测试框架，用于模糊测试命令行应用程序。它通过提供目标函数的潜在调用路径作为上下文给LLM，从而生成更有效的CLI选项字符串和输入文件，能够发现深度嵌入的漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有的CLI模糊测试方法在生成语义丰富的选项字符串和输入文件方面面临挑战，无法到达深度嵌入的目标函数，导致无法检测深层漏洞。

Method: PILOT框架使用路径引导的方法，将潜在调用路径作为上下文提供给LLM，迭代生成CLI选项字符串和输入文件，并将已到达的函数作为额外上下文，逐步接近目标函数。

Result: 在真实世界CLI应用程序上的评估显示，PILOT比最先进的模糊测试方法实现了更高的覆盖率，并发现了51个零日漏洞。其中41个漏洞已被开发者确认，33个已修复，3个获得了CVE标识。

Conclusion: PILOT通过路径引导和LLM编排的方法，有效解决了CLI模糊测试中深度漏洞检测的挑战，显著提升了测试覆盖率和漏洞发现能力。

Abstract: Command-line interface (CLI) fuzzing tests programs by mutating both command-line options and input file contents, thus enabling discovery of vulnerabilities that only manifest under specific option-input combinations. Prior works of CLI fuzzing face the challenges of generating semantics-rich option strings and input files, which cannot reach deeply embedded target functions. This often leads to a misdetection of such a deep vulnerability using existing CLI fuzzing techniques. In this paper, we design a novel Path-guided, Iterative LLM-Orchestrated Testing framework, called PILOT, to fuzz CLI applications. The key insight is to provide potential call paths to target functions as context to LLM so that it can better generate CLI option strings and input files. Then, PILOT iteratively repeats the process, and provides reached functions as additional context so that target functions are reached. Our evaluation on real-world CLI applications demonstrates that PILOT achieves higher coverage than state-of-the-art fuzzing approaches and discovers 51 zero-day vulnerabilities. We responsibly disclosed all the vulnerabilities to their developers and so far 41 have been confirmed by their developers with 33 being fixed and three assigned CVE identifiers.

</details>


### [21] [Quantum-Resistant Authentication Scheme for RFID Systems Using Lattice-Based Cryptography](https://arxiv.org/abs/2511.20630)
*Vaibhav Kumar,Kaiwalya Joshi,Bhavya Dixit,Gaurav S. Kasbekar*

Main category: cs.CR

TL;DR: 提出了一种基于格密码的量子安全RFID相互认证方案，该方案在读写器-服务器和标签-读写器通信信道都不安全的情况下仍能保证安全性。


<details>
  <summary>Details</summary>
Motivation: 现有RFID认证方案通常假设读写器-服务器通信信道是安全的，但在实际应用中该信道也可能面临安全威胁。同时，量子计算的发展对传统密码方案构成威胁，需要量子安全的解决方案。

Method: 使用基于格密码的方法，特别是利用非齐次短整数解(ISIS)问题的困难性来实现量子安全性。通过AVISPA工具进行形式化验证，并详细分析了存储、计算和通信成本。

Result: 该协议能够有效抵抗中间人攻击、重放攻击、冒充攻击和反射攻击，同时确保不可伪造性和匿名性。与现有协议相比，提供了更强的安全保障。

Conclusion: 这是首个全面解决读写器-服务器和标签-读写器通信信道不安全问题的量子安全RFID认证协议，为RFID系统在量子计算时代提供了可靠的安全保障。

Abstract: We propose a novel quantum-resistant mutual authentication scheme for radio-frequency identification (RFID) systems. Our scheme uses lattice-based cryptography and, in particular, achieves quantum-resistance by leveraging the hardness of the inhomogeneous short integer solution (ISIS) problem. In contrast to prior work, which assumes that the reader-server communication channel is secure, our scheme is secure even when both the reader-server and tag-reader communication channels are insecure. Our proposed protocol provides robust security against man-in-the-middle (MITM), replay, impersonation, and reflection attacks, while also ensuring unforgeability and preserving anonymity. We present a detailed security analysis, including semi-formal analysis and formal verification using the Automated Validation of Internet Security Protocols and Applications (AVISPA) tool. In addition, we analyze the storage, computation, and communication costs of the proposed protocol and compare its security properties with those of existing protocols, demonstrating that our scheme offers strong security guarantees. To the best of our knowledge, this paper is the first quantum-resistant authentication protocol for RFID systems that comprehensively addresses the insecurity of both the reader-server and tag-reader communication channels.

</details>
