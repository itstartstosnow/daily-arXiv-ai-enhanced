{"id": "2509.22662", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22662", "abs": "https://arxiv.org/abs/2509.22662", "authors": ["Mathilde Durieux", "Kayla D. Taylor", "Laxima Niure Kandel", "Deepti Gupta"], "title": "GPS Spoofing Attacks and Pilot Responses Using a Flight Simulator Environment", "comment": null, "summary": "Global Positioning System (GPS) spoofing involves transmitting fake signals\nthat mimic those from GPS satellites, causing the GPS receivers to calculate\nincorrect Positioning, Navigation, and Timing (PNT) information. Recently,\nthere has been a surge in GPS spoofing attacks targeting aircraft. Since GPS\nsatellite signals are weak, the spoofed high-power signal can easily overpower\nthem. These spoofed signals are often interpreted as valid by the GPS receiver,\nwhich can cause severe and cascading effects on air navigation. While much of\nthe existing research on GPS spoofing focuses on technical aspects of detection\nand mitigation, human factors are often neglected, even though pilots are an\nintegral part of aircraft operation and potentially vulnerable to deception.\nThis research addresses this gap by conducting a detailed analysis of the\nbehavior of student pilots when subjected to GPS spoofing using the Force\nDynamics 401CR flight simulator with X-Plane 11 and a Cessna 172 equipped with\nGarmin G1000. Spoofing scenarios were implemented via custom scripts that\naltered navigational data without modifying the external visual environment.\nThirty student pilots from the Embry-Riddle Aeronautical University Daytona\nBeach campus with diverse flying experience levels were recruited to\nparticipate in three spoofing scenarios. A pre-simulation questionnaire was\ndistributed to measure pilot experience and confidence in GPS.Inflight\ndecision-making during the spoofing attacks was observed, including reaction\ntime to anomalies, visual attention to interface elements, and cognitive\nbiases. A post-flight evaluation of workload was obtained using a modified NASA\nTask Load Index (TLX) method. This study provides a first step toward\nidentifying human vulnerabilities to GPS spoofing amid the ongoing debate over\nGPS reliance."}
{"id": "2509.22663", "categories": ["cs.CR", "cs.HC", "cs.CR, cs.HC"], "pdf": "https://arxiv.org/pdf/2509.22663", "abs": "https://arxiv.org/abs/2509.22663", "authors": ["Michel Youssef"], "title": "Security Friction Quotient for Zero Trust Identity Policy with Empirical Validation", "comment": "10 pages, 3 figures", "summary": "We define a practical method to quantify the trade-off between security and\noperational friction in modern identity-centric programs. We introduce the\nSecurity Friction Quotient (SFQ), a bounded composite index that combines a\nresidual-risk estimator with empirically grounded friction terms (latency,\nfailure rate, and helpdesk impact). We establish clarity properties\n(boundedness, monotonic response, and weight identifiability) with short\nproofs, then evaluate widely used Conditional Access policies over a 12-week\nhorizon using Monte Carlo simulation (n = 2,000 runs per policy/scenario) with\neffect sizes and 95% confidence intervals. We further assess rank stability\nunder 10,000 random weight draws, finding 95.5% preservation of policy\nordering. Finally, we provide a 12-week passkey field observation from an\nenterprise-scale cohort (N = 1,200) that directionally aligns with the\nsimulation's phishing-resistant MFA gains. The SFQ framework is designed to be\nreproducible, interpretable, and directly actionable for Zero Trust identity\npolicy decisions, with artifacts and parameter ranges provided to support\npolicy design, review, and continuous improvement."}
{"id": "2509.22664", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22664", "abs": "https://arxiv.org/abs/2509.22664", "authors": ["Chaerin Kim"], "title": "Security Issues on the OpenPLC project and corresponding solutions", "comment": "Master's thesis", "summary": "As Programmable Logic Controller (PLC) became a useful device and rose as an\ninteresting research topic but remained expensive, multiple PLC\nsimulators/emulators were introduced for various purposes. Open-source\nProgrammable Logic Controller (OpenPLC) software, one of the most popular PLC\nsimulators, is designed to be vendor-neutral and run on almost any computer or\nlow-cost embedded devices, e.g., Raspberry Pi, Arduino, and other controllers.\nThe project succeeded in introducing itself as an affordable and practical\nsolution for the high cost of real hardware PLCs. However, it still lacks\nappropriate securing methods, resulting in several vulnerabilities. Through a\ncombination of threat modeling, vulnerability analysis, and practical\nexperiments, this thesis provides valuable insights for developers,\nresearchers, and engineers aiming to deploy OpenPLC securely in industrial\nenvironments. To this end, this work first conducts an in-depth analysis aimed\nto shed light on va! rious security challenges and vulnerabilities within the\nOpenPLC project. After that, an advanced control logic injection attack was\nperformed. This attack modifies the user program maliciously, exploiting\npresented vulnerabilities. Finally, the work introduces a security-enhanced\nOpenPLC software called OpenPLC Aqua. The new software is equipped with a set\nof security solutions designed specifically to address the vulnerabilities to\nwhich current OpenPLC versions are prone."}
{"id": "2509.22723", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22723", "abs": "https://arxiv.org/abs/2509.22723", "authors": ["Kang Wei", "Xin Yuan", "Fushuo Huo", "Chuan Ma", "Long Yuan", "Songze Li", "Ming Ding", "Dacheng Tao"], "title": "Responsible Diffusion: A Comprehensive Survey on Safety, Ethics, and Trust in Diffusion Models", "comment": null, "summary": "Diffusion models (DMs) have been investigated in various domains due to their\nability to generate high-quality data, thereby attracting significant\nattention. However, similar to traditional deep learning systems, there also\nexist potential threats to DMs. To provide advanced and comprehensive insights\ninto safety, ethics, and trust in DMs, this survey comprehensively elucidates\nits framework, threats, and countermeasures. Each threat and its\ncountermeasures are systematically examined and categorized to facilitate\nthorough analysis. Furthermore, we introduce specific examples of how DMs are\nused, what dangers they might bring, and ways to protect against these dangers.\nFinally, we discuss key lessons learned, highlight open challenges related to\nDM security, and outline prospective research directions in this critical\nfield. This work aims to accelerate progress not only in the technical\ncapabilities of generative artificial intelligence but also in the maturity and\nwisdom of its application."}
{"id": "2509.22732", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22732", "abs": "https://arxiv.org/abs/2509.22732", "authors": ["Haibo Tong", "Dongcheng Zhao", "Guobin Shen", "Xiang He", "Dachuan Lin", "Feifei Zhao", "Yi Zeng"], "title": "Bidirectional Intention Inference Enhances LLMs' Defense Against Multi-Turn Jailbreak Attacks", "comment": null, "summary": "The remarkable capabilities of Large Language Models (LLMs) have raised\nsignificant safety concerns, particularly regarding \"jailbreak\" attacks that\nexploit adversarial prompts to bypass safety alignment mechanisms. Existing\ndefense research primarily focuses on single-turn attacks, whereas multi-turn\njailbreak attacks progressively break through safeguards through by concealing\nmalicious intent and tactical manipulation, ultimately rendering conventional\nsingle-turn defenses ineffective. To address this critical challenge, we\npropose the Bidirectional Intention Inference Defense (BIID). The method\nintegrates forward request-based intention inference with backward\nresponse-based intention retrospection, establishing a bidirectional synergy\nmechanism to detect risks concealed within seemingly benign inputs, thereby\nconstructing a more robust guardrails that effectively prevents harmful content\ngeneration. The proposed method undergoes systematic evaluation compared with a\nno-defense baseline and seven representative defense methods across three LLMs\nand two safety benchmarks under 10 different attack methods. Experimental\nresults demonstrate that the proposed method significantly reduces the Attack\nSuccess Rate (ASR) across both single-turn and multi-turn jailbreak attempts,\noutperforming all existing baseline methods while effectively maintaining\npractical utility. Notably, comparative experiments across three multi-turn\nsafety datasets further validate the proposed model's significant advantages\nover other defense approaches."}
{"id": "2509.22745", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22745", "abs": "https://arxiv.org/abs/2509.22745", "authors": ["Jaehan Kim", "Minkyoo Song", "Seungwon Shin", "Sooel Son"], "title": "Defending MoE LLMs against Harmful Fine-Tuning via Safety Routing Alignment", "comment": "Under review", "summary": "Recent large language models (LLMs) have increasingly adopted the\nMixture-of-Experts (MoE) architecture for efficiency. MoE-based LLMs heavily\ndepend on a superficial safety mechanism in which harmful inputs are routed\nsafety-critical experts. However, our analysis reveals that routing decisions\nfor harmful inputs drift significantly after fine-tuning, exposing a critical\nvulnerability to harmful fine-tuning (HFT) attacks. Existing defenses,\nprimarily designed for monolithic LLMs, are less effective for MoE LLMs as they\nfail to prevent drift in harmful input routing. To address this limitation, we\npropose SafeMoE, a safe fine-tuning method tailored to MoE LLMs. SafeMoE\ndirectly mitigates routing drift by penalizing the gap between the routing\nweights of a fine-tuned model and those of the initial safety-aligned model,\nthereby preserving the safety-aligned routing of harmful inputs to\nsafety-critical experts. Experiments on open-source MoE LLMs ranging from 7B to\n141B parameters demonstrate that SafeMoE effectively mitigates HFT attacks,\nreducing the harmfulness score of OLMoE from 62.0 to 5.0, for example, while\nmaintaining task utility within 1% degradation and incurring only 2% overhead.\nIt significantly outperforms state-of-the-art defense methods for safeguarding\nLLM fine-tuning and remains effective in recent large-scale MoE LLMs such as\ngpt-oss and Llama 4. Our implementation is available at\nhttps://anonymous.4open.science/r/SafeMoE."}
{"id": "2509.22757", "categories": ["cs.CR", "cs.AI", "cs.NI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.22757", "abs": "https://arxiv.org/abs/2509.22757", "authors": ["Petar Radanliev"], "title": "Red Teaming Quantum-Resistant Cryptographic Standards: A Penetration Testing Framework Integrating AI and Quantum Security", "comment": null, "summary": "This study presents a structured approach to evaluating vulnerabilities\nwithin quantum cryptographic protocols, focusing on the BB84 quantum key\ndistribution method and National Institute of Standards and Technology (NIST)\napproved quantum-resistant algorithms. By integrating AI-driven red teaming,\nautomated penetration testing, and real-time anomaly detection, the research\ndevelops a framework for assessing and mitigating security risks in quantum\nnetworks. The findings demonstrate that AI can be effectively used to simulate\nadversarial attacks, probe weaknesses in cryptographic implementations, and\nrefine security mechanisms through iterative feedback. The use of automated\nexploit simulations and protocol fuzzing provides a scalable means of\nidentifying latent vulnerabilities, while adversarial machine learning\ntechniques highlight novel attack surfaces within AI-enhanced cryptographic\nprocesses. This study offers a comprehensive methodology for strengthening\nquantum security and provides a foundation for integrating AI-driven\ncybersecurity practices into the evolving quantum landscape."}
{"id": "2509.22762", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22762", "abs": "https://arxiv.org/abs/2509.22762", "authors": ["Friedrich Doku", "Peter Dinda"], "title": "TRUSTCHECKPOINTS: Time Betrays Malware for Unconditional Software Root of Trust", "comment": null, "summary": "Modern IoT and embedded platforms must start execution from a known trusted\nstate to thwart malware, ensure secure firmware updates, and protect critical\ninfrastructure. Current approaches to establish a root of trust depend on\nsecret keys and/or specialized secure hardware, which drives up costs, may\ninvolve third parties, adds operational complexity, and relies on assumptions\nabout an attacker's computational power. In contrast, TRUSTCHECKPOINTS is the\nfirst system to establish an unconditional software root of trust based on a\nformal model without relying on secrets or trusted hardware. Developers capture\na full-system checkpoint and later roll back to it and prove this to an\nexternal verifier. The verifier issues timing-constrained, randomized\nk-independent polynomial challenges (via Horner's rule) that repeatedly scan\nthe fast on-chip memory in randomized passes. When malicious code attempts to\npersist, it must swap into slower, unchecked off-chip storage, causing a\ndetectable timing delay.\n  Our prototype for a commodity ARM Cortex-A53-based platform validates 192 KB\nof SRAM in approximately 10 s using 500 passes, sufficient to detect\nsingle-instruction persistent malware. The prototype then seamlessly extends\ntrust to DRAM. Two modes (fast SRAM-bootstrap and comprehensive full-memory\nscan) allow trade-offs between speed and coverage, demonstrating reliable\nmalware detection on unmodified hardware."}
{"id": "2509.22796", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22796", "abs": "https://arxiv.org/abs/2509.22796", "authors": ["Xingyu Li", "Juefei Pu", "Yifan Wu", "Xiaochen Zou", "Shitong Zhu", "Xiaochen Zou", "Shitong Zhu", "Qiushi Wu", "Zheng Zhang", "Joshua Hsu", "Yue Dong", "Zhiyun Qian", "Kangjie Lu", "Trent Jaeger", "Michael De Lucia", "Srikanth V. Krishnamurthy"], "title": "What Do They Fix? LLM-Aided Categorization of Security Patches for Critical Memory Bugs", "comment": null, "summary": "Open-source software projects are foundational to modern software ecosystems,\nwith the Linux kernel standing out as a critical exemplar due to its ubiquity\nand complexity. Although security patches are continuously integrated into the\nLinux mainline kernel, downstream maintainers often delay their adoption,\ncreating windows of vulnerability. A key reason for this lag is the difficulty\nin identifying security-critical patches, particularly those addressing\nexploitable vulnerabilities such as out-of-bounds (OOB) accesses and\nuse-after-free (UAF) bugs. This challenge is exacerbated by intentionally\nsilent bug fixes, incomplete or missing CVE assignments, delays in CVE\nissuance, and recent changes to the CVE assignment criteria for the Linux\nkernel. While fine-grained patch classification approaches exist, they exhibit\nlimitations in both coverage and accuracy. In this work, we identify previously\nunexplored opportunities to significantly improve fine-grained patch\nclassification. Specifically, by leveraging cues from commit titles/messages\nand diffs alongside appropriate code context, we develop DUALLM, a dual-method\npipeline that integrates two approaches based on a Large Language Model (LLM)\nand a fine-tuned small language model. DUALLM achieves 87.4% accuracy and an\nF1-score of 0.875, significantly outperforming prior solutions. Notably, DUALLM\nsuccessfully identified 111 of 5,140 recent Linux kernel patches as addressing\nOOB or UAF vulnerabilities, with 90 true positives confirmed by manual\nverification (many do not have clear indications in patch descriptions).\nMoreover, we constructed proof-of-concepts for two identified bugs (one UAF and\none OOB), including one developed to conduct a previously unknown control-flow\nhijack as further evidence of the correctness of the classification."}
{"id": "2509.22814", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22814", "abs": "https://arxiv.org/abs/2509.22814", "authors": ["Aditi Tiwari", "Akshit Bhalla", "Darshan Prasad"], "title": "Model Context Protocol for Vision Systems: Audit, Security, and Protocol Extensions", "comment": "Accepted to NeurIPS 2025 Workshop on Bridging Language, Agent, and\n  World Models for Reasoning and Planning (LAW 2025)", "summary": "The Model Context Protocol (MCP) defines a schema bound execution model for\nagent-tool interaction, enabling modular computer vision workflows without\nretraining. To our knowledge, this is the first protocol level, deployment\nscale audit of MCP in vision systems, identifying systemic weaknesses in schema\nsemantics, interoperability, and runtime coordination. We analyze 91 publicly\nregistered vision centric MCP servers, annotated along nine dimensions of\ncompositional fidelity, and develop an executable benchmark with validators to\ndetect and categorize protocol violations. The audit reveals high prevalence of\nschema format divergence, missing runtime schema validation, undeclared\ncoordinate conventions, and reliance on untracked bridging scripts. Validator\nbased testing quantifies these failures, with schema format checks flagging\nmisalignments in 78.0 percent of systems, coordinate convention checks\ndetecting spatial reference errors in 24.6 percent, and memory scope checks\nissuing an average of 33.8 warnings per 100 executions. Security probes show\nthat dynamic and multi agent workflows exhibit elevated risks of privilege\nescalation and untyped tool connections. The proposed benchmark and validator\nsuite, implemented in a controlled testbed and to be released on GitHub,\nestablishes a reproducible framework for measuring and improving the\nreliability and security of compositional vision workflows."}
{"id": "2509.22857", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22857", "abs": "https://arxiv.org/abs/2509.22857", "authors": ["Eduardo Chielle", "Manaar Alam", "Jinting Liu", "Jovan Kascelan", "Michail Maniatakos"], "title": "PAPER: Privacy-Preserving ResNet Models using Low-Degree Polynomial Approximations and Structural Optimizations on Leveled FHE", "comment": null, "summary": "Recent work has made non-interactive privacy-preserving inference more\npractical by running deep Convolution Neural Network (CNN) with Fully\nHomomorphic Encryption (FHE). However, these methods remain limited by their\nreliance on bootstrapping, a costly FHE operation applied across multiple\nlayers, severely slowing inference. They also depend on high-degree polynomial\napproximations of non-linear activations, which increase multiplicative depth\nand reduce accuracy by 2-5% compared to plaintext ReLU models. In this work, we\nfocus on ResNets, a widely adopted benchmark architecture in privacy-preserving\ninference, and close the accuracy gap between their FHE-based non-interactive\nmodels and plaintext counterparts, while also achieving faster inference than\nexisting methods. We use a quadratic polynomial approximation of ReLU, which\nachieves the theoretical minimum multiplicative depth for non-linear\nactivations, along with a penalty-based training strategy. We further introduce\nstructural optimizations such as node fusing, weight redistribution, and tower\nreuse. These optimizations reduce the required FHE levels in CNNs by nearly a\nfactor of five compared to prior work, allowing us to run ResNet models under\nleveled FHE without bootstrapping. To further accelerate inference and recover\naccuracy typically lost with polynomial approximations, we introduce parameter\nclustering along with a joint strategy of data encoding layout and ensemble\ntechniques. Experiments with ResNet-18, ResNet-20, and ResNet-32 on CIFAR-10\nand CIFAR-100 show that our approach achieves up to 4x faster private inference\nthan prior work with comparable accuracy to plaintext ReLU models."}
{"id": "2509.22873", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22873", "abs": "https://arxiv.org/abs/2509.22873", "authors": ["Aashnan Rahman", "Abid Hasan", "Sherajul Arifin", "Faisal Haque Bappy", "Tahrim Hossain", "Tariqul Islam", "Abu Raihan Mostofa Kamal", "Md. Azam Hossain"], "title": "AntiFLipper: A Secure and Efficient Defense Against Label-Flipping Attacks in Federated Learning", "comment": "Submitted to IEEE International Conference on Communications (ICC)\n  2026", "summary": "Federated learning (FL) enables privacy-preserving model training by keeping\ndata decentralized. However, it remains vulnerable to label-flipping attacks,\nwhere malicious clients manipulate labels to poison the global model. Despite\ntheir simplicity, these attacks can severely degrade model performance, and\ndefending against them remains challenging. We introduce AntiFLipper, a novel\nand computationally efficient defense against multi-class label-flipping\nattacks in FL. Unlike existing methods that ensure security at the cost of high\ncomputational overhead, AntiFLipper employs a novel client-side detection\nstrategy, significantly reducing the central server's burden during\naggregation. Comprehensive empirical evaluations across multiple datasets under\ndifferent distributions demonstrate that AntiFLipper achieves accuracy\ncomparable to state-of-the-art defenses while requiring substantially fewer\ncomputational resources in server side. By balancing security and efficiency,\nAntiFLipper addresses a critical gap in existing defenses, making it\nparticularly suitable for resource-constrained FL deployments where both model\nintegrity and operational efficiency are essential."}
{"id": "2509.22900", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22900", "abs": "https://arxiv.org/abs/2509.22900", "authors": ["Haochen Gong", "Zhen Tao", "Shidong Pan", "Zhenchang Xing", "Xiaoyu Sun"], "title": "Towards Context-aware Mobile Privacy Notice: Implementation of A Deployable Contextual Privacy Policies Generator", "comment": "Accepted by ASE 2025, Tool Demonstration Track", "summary": "Lengthy and legally phrased privacy policies impede users' understanding of\nhow mobile applications collect and process personal data. Prior work proposed\nContextual Privacy Policies (CPPs) for mobile apps to display shorter policy\nsnippets only in the corresponding user interface contexts, but the pipeline\ncould not be deployable in real-world mobile environments. In this paper, we\npresent PrivScan, the first deployable CPP Software Development Kit (SDK) for\nAndroid. It captures live app screenshots to identify GUI elements associated\nwith types of personal data and displays CPPs in a concise, user-facing format.\nWe provide a lightweight floating button that offers low-friction, on-demand\ncontrol. The architecture leverages remote deployment to decouple the\nmultimodal backend pipeline from a mobile client comprising five modular\ncomponents, thereby reducing on-device resource demands and easing\ncross-platform portability. A feasibility-oriented evaluation shows an average\nexecution time of 9.15\\,s, demonstrating the practicality of our approach. The\nsource code of PrivScan is available at https://github.com/buyanghc/PrivScan\nand the demo video can be found at https://www.youtube.com/watch?v=ck-25otfyHc."}
{"id": "2509.22965", "categories": ["cs.CR", "68M14, 94A60", "K.6.5; D.4.6; J.1"], "pdf": "https://arxiv.org/pdf/2509.22965", "abs": "https://arxiv.org/abs/2509.22965", "authors": ["Yousef Tahboub", "Anthony Revilla", "Jaydon Lynch", "Greg Floyd"], "title": "Blockchain Voting System", "comment": null, "summary": "Casting a ballot from a phone or laptop sounds appealing, but only if voters\ncan be confident their choice remains secret and results cannot be altered in\nthe dark. This paper proposes a hybrid blockchain-based voting model that\nstores encrypted votes on a private blockchain maintained by election\norganizers and neutral observers, while periodically anchoring hashes of these\nvotes onto a public blockchain as a tamper-evident seal. The system issues\nvoters one-time blind-signed tokens to protect anonymity, and provides receipts\nso they can confirm their vote was counted. We implemented a live prototype\nusing common web technologies (Next.js, React, Firebase) to demonstrate\nend-to-end functionality, accessibility, and cost efficiency. Our contributions\ninclude developing a working demo, a complete election workflow, a hybrid\nblockchain design, and a user-friendly interface that balances privacy,\nsecurity, transparency, and practicality. This research highlights the\nfeasibility of secure, verifiable, and scalable online voting for organizations\nranging from small groups to larger institutions."}
{"id": "2509.22986", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.22986", "abs": "https://arxiv.org/abs/2509.22986", "authors": ["Jingyao Zhang", "Elaheh Sadredini"], "title": "CryptoSRAM: Enabling High-Throughput Cryptography on MCUs via In-SRAM Computing", "comment": "To appear in 2025 IEEE Cross-disciplinary Conference on\n  Memory-Centric Computing (CCMCC)", "summary": "Secure communication is a critical requirement for Internet of Things (IoT)\ndevices, which are often based on Microcontroller Units (MCUs). Current\ncryptographic solutions, which rely on software libraries or dedicated hardware\naccelerators, are fundamentally limited by the performance and energy costs of\ndata movement between memory and processing units. This paper introduces\nCryptoSRAM, an in-SRAM computing architecture that performs cryptographic\noperations directly within the MCU's standard SRAM array. By repurposing the\nmemory array into a massively parallel processing fabric, CryptoSRAM eliminates\nthe data movement bottleneck. This approach is well-suited to MCUs, which\nutilize physical addressing and Direct Memory Access (DMA) to manage SRAM,\nallowing for seamless integration with minimal hardware overhead. Our analysis\nshows that for common cryptographic kernels, CryptoSRAM achieves throughput\nimprovements of up to 74$\\times$ and 67$\\times$ for AES and SHA3, respectively,\ncompared to a software implementation. Furthermore, our solution delivers up to\n6$\\times$ higher throughput than existing hardware accelerators for AES.\nCryptoSRAM demonstrates a viable and efficient architecture for secure\ncommunication in next-generation IoT systems."}
{"id": "2509.23019", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23019", "abs": "https://arxiv.org/abs/2509.23019", "authors": ["Jeongyeon Hwang", "Sangdon Park", "Jungseul Ok"], "title": "LLM Watermark Evasion via Bias Inversion", "comment": null, "summary": "Watermarking for large language models (LLMs) embeds a statistical signal\nduring generation to enable detection of model-produced text. While\nwatermarking has proven effective in benign settings, its robustness under\nadversarial evasion remains contested. To advance a rigorous understanding and\nevaluation of such vulnerabilities, we propose the \\emph{Bias-Inversion\nRewriting Attack} (BIRA), which is theoretically motivated and model-agnostic.\nBIRA weakens the watermark signal by suppressing the logits of likely\nwatermarked tokens during LLM-based rewriting, without any knowledge of the\nunderlying watermarking scheme. Across recent watermarking methods, BIRA\nachieves over 99\\% evasion while preserving the semantic content of the\noriginal text. Beyond demonstrating an attack, our results reveal a systematic\nvulnerability, emphasizing the need for stress testing and robust defenses."}
{"id": "2509.23041", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23041", "abs": "https://arxiv.org/abs/2509.23041", "authors": ["Zi Liang", "Qingqing Ye", "Xuan Liu", "Yanyun Wang", "Jianliang Xu", "Haibo Hu"], "title": "Virus Infection Attack on LLMs: Your Poisoning Can Spread \"VIA\" Synthetic Data", "comment": "NeurIPS 2025 Spotlight. Source code:\n  https://github.com/liangzid/VirusInfectionAttack", "summary": "Synthetic data refers to artificial samples generated by models. While it has\nbeen validated to significantly enhance the performance of large language\nmodels (LLMs) during training and has been widely adopted in LLM development,\npotential security risks it may introduce remain uninvestigated. This paper\nsystematically evaluates the resilience of synthetic-data-integrated training\nparadigm for LLMs against mainstream poisoning and backdoor attacks. We reveal\nthat such a paradigm exhibits strong resistance to existing attacks, primarily\nthanks to the different distribution patterns between poisoning data and\nqueries used to generate synthetic samples. To enhance the effectiveness of\nthese attacks and further investigate the security risks introduced by\nsynthetic data, we introduce a novel and universal attack framework, namely,\nVirus Infection Attack (VIA), which enables the propagation of current attacks\nthrough synthetic data even under purely clean queries. Inspired by the\nprinciples of virus design in cybersecurity, VIA conceals the poisoning payload\nwithin a protective \"shell\" and strategically searches for optimal hijacking\npoints in benign samples to maximize the likelihood of generating malicious\ncontent. Extensive experiments on both data poisoning and backdoor attacks show\nthat VIA significantly increases the presence of poisoning content in synthetic\ndata and correspondingly raises the attack success rate (ASR) on downstream\nmodels to levels comparable to those observed in the poisoned upstream models."}
{"id": "2509.23091", "categories": ["cs.CR", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23091", "abs": "https://arxiv.org/abs/2509.23091", "authors": ["Xiangchen Meng", "Yangdi Lyu"], "title": "FedBit: Accelerating Privacy-Preserving Federated Learning via Bit-Interleaved Packing and Cross-Layer Co-Design", "comment": null, "summary": "Federated learning (FL) with fully homomorphic encryption (FHE) effectively\nsafeguards data privacy during model aggregation by encrypting local model\nupdates before transmission, mitigating threats from untrusted servers or\neavesdroppers in transmission. However, the computational burden and ciphertext\nexpansion associated with homomorphic encryption can significantly increase\nresource and communication overhead. To address these challenges, we propose\nFedBit, a hardware/software co-designed framework optimized for the\nBrakerski-Fan-Vercauteren (BFV) scheme. FedBit employs bit-interleaved data\npacking to embed multiple model parameters into a single ciphertext\ncoefficient, thereby minimizing ciphertext expansion and maximizing\ncomputational parallelism. Additionally, we integrate a dedicated FPGA\naccelerator to handle cryptographic operations and an optimized dataflow to\nreduce the memory overhead. Experimental results demonstrate that FedBit\nachieves a speedup of two orders of magnitude in encryption and lowers average\ncommunication overhead by 60.7%, while maintaining high accuracy."}
{"id": "2509.23305", "categories": ["cs.CR", "93C95", "I.6.5"], "pdf": "https://arxiv.org/pdf/2509.23305", "abs": "https://arxiv.org/abs/2509.23305", "authors": ["Jaxson Brown", "Duc-Son Pham", "Sie-Teng Soh", "Foad Motalebi", "Sivaraman Eswaran", "Mahathir Almashor"], "title": "ICS-SimLab: A Containerized Approach for Simulating Industrial Control Systems for Cyber Security Research", "comment": "This is the 10-page extended version of a paper accepted to the First\n  International Workshop on Secure Industrial Control Systems and\n  Industrial-IoT, IEEE CNS 2025 (the conference version was 6 pages)", "summary": "Industrial Control Systems (ICSs) are complex interconnected systems used to\nmanage process control within industrial environments, such as chemical\nprocessing plants and water treatment facilities. As the modern industrial\nenvironment moves towards Internet-facing services, ICSs face an increased risk\nof attacks that necessitates ICS-specific Intrusion Detection Systems (IDS).\nThe development of such IDS relies significantly on a simulated testbed as it\nis unrealistic and sometimes hazardous to utilize an operational control\nsystem. Whilst some testbeds have been proposed, they often use a limited\nselection of virtual ICS simulations to test and verify cyber security\nsolutions. There is a lack of investigation done on developing systems that can\nefficiently simulate multiple ICS architectures. Currently, the trend within\nresearch involves developing security solutions on just one ICS simulation,\nwhich can result in bias to its specific architecture. We present ICS-SimLab,\nan end-to-end software suite that utilizes Docker containerization technology\nto create a highly configurable ICS simulation environment. This software\nframework enables researchers to rapidly build and customize different ICS\nenvironments, facilitating the development of security solutions across\ndifferent systems that adhere to the Purdue Enterprise Reference Architecture.\nTo demonstrate its capability, we present three virtual ICS simulations: a\nsolar panel smart grid, a water bottle filling facility, and a system of\nintelligent electronic devices. Furthermore, we run cyber-attacks on these\nsimulations and construct a dataset of recorded malicious and benign network\ntraffic to be used for IDS development."}
{"id": "2509.23418", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.23418", "abs": "https://arxiv.org/abs/2509.23418", "authors": ["Ummay Kulsum", "Aafaq Sabir", "Abhinaya S. B.", "Anupam Das"], "title": "Detecting YouTube Scam Videos via Multimodal Signals and Policy Reasoning", "comment": null, "summary": "YouTube has emerged as a dominant platform for both information dissemination\nand entertainment. However, its vast accessibility has also made it a target\nfor scammers, who frequently upload deceptive or malicious content. Prior\nresearch has documented a range of scam types, and detection approaches rely\nprimarily on textual or statistical metadata. Although effective to some\nextent, these signals are easy to evade and potentially overlook other\nmodalities, such as visual cues.\n  In this study, we present the first systematic investigation of multimodal\napproaches for YouTube scam detection. Our dataset consolidates established\nscam categories and augments them with full length video content and policy\ngrounded reasoning annotations. Our experimental evaluation demonstrates that a\ntext-only model using video titles and descriptions (fine-tuned BERT) achieves\nmoderate effectiveness (76.61% F1), with modest improvements when incorporating\naudio transcripts (77.98% F1). In contrast, visual analysis using a fine-tuned\nLLaVA-Video model yields stronger results (79.61% F1). Finally, a multimodal\nframework that integrates titles, descriptions, and video frames achieves the\nhighest performance (80.53% F1). Beyond improving detection accuracy, our\nmultimodal framework produces interpretable reasoning grounded in YouTube\ncontent policies, thereby enhancing transparency and supporting potential\napplications in automated moderation. Moreover, we validate our approach on\nin-the-wild YouTube data by analyzing 6,374 videos, thereby contributing a\nvaluable resource for future research on scam detection."}
{"id": "2509.23427", "categories": ["cs.CR", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.23427", "abs": "https://arxiv.org/abs/2509.23427", "authors": ["Rowdy Chotkan", "Bulat Nasrulin", "Jérémie Decouchant", "Johan Pouwelse"], "title": "StarveSpam: Mitigating Spam with Local Reputation in Permissionless Blockchains", "comment": "Preprint. Accepted for publication in the proceedings of the 7th\n  Conference on Blockchain Research & Applications for Innovative Networks and\n  Services (BRAINS 2025). The final version will be available on IEEE Xplore", "summary": "Spam poses a growing threat to blockchain networks. Adversaries can easily\ncreate multiple accounts to flood transaction pools, inflating fees and\ndegrading service quality. Existing defenses against spam, such as fee markets\nand staking requirements, primarily rely on economic deterrence, which fails to\ndistinguish between malicious and legitimate users and often exclude low-value\nbut honest activity. To address these shortcomings, we present StarveSpam, a\ndecentralized reputation-based protocol that mitigates spam by operating at the\ntransaction relay layer. StarveSpam combines local behavior tracking, peer\nscoring, and adaptive rate-limiting to suppress abusive actors, without\nrequiring global consensus, protocol changes, or trusted infrastructure. We\nevaluate StarveSpam using real Ethereum data from a major NFT spam event and\nshow that it outperforms existing fee-based and rule-based defenses, allowing\neach node to block over 95% of spam while dropping just 3% of honest traffic,\nand reducing the fraction of the network exposed to spam by 85% compared to\nexisting rule-based methods. StarveSpam offers a scalable and deployable\nalternative to traditional spam defenses, paving the way toward more resilient\nand equitable blockchain infrastructure."}
{"id": "2509.23459", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23459", "abs": "https://arxiv.org/abs/2509.23459", "authors": ["Sepideh Abedini", "Shubhankar Mohapatra", "D. B. Emerson", "Masoumeh Shafieinejad", "Jesse C. Cresswell", "Xi He"], "title": "MaskSQL: Safeguarding Privacy for LLM-Based Text-to-SQL via Abstraction", "comment": "Accepted to the NeurIPS 2025 Workshop on Regulatable Machine Learning\n  (Regulatable ML @ NeurIPS 2025). Code available at\n  https://github.com/sepideh-abedini/MaskSQL", "summary": "Large language models (LLMs) have shown promising performance on tasks that\nrequire reasoning, such as text-to-SQL, code generation, and debugging.\nHowever, regulatory frameworks with strict privacy requirements constrain their\nintegration into sensitive systems. State-of-the-art LLMs are also proprietary,\ncostly, and resource-intensive, making local deployment impractical.\nConsequently, utilizing such LLMs often requires sharing data with third-party\nproviders, raising privacy concerns and risking noncompliance with regulations.\nAlthough fine-tuned small language models (SLMs) can outperform LLMs on certain\ntasks and be deployed locally to mitigate privacy concerns, they underperform\non more complex tasks such as text-to-SQL translation. In this work, we\nintroduce MaskSQL, a text-to-SQL framework that utilizes abstraction as a\nprivacy protection mechanism to mask sensitive information in LLM prompts.\nUnlike redaction, which removes content entirely, or generalization, which\nbroadens tokens, abstraction retains essential information while discarding\nunnecessary details, striking an effective privacy-utility balance for the\ntext-to-SQL task. Moreover, by providing mechanisms to control the\nprivacy-utility tradeoff, MaskSQL facilitates adoption across a broader range\nof use cases. Our experimental results show that MaskSQL outperforms leading\nSLM-based text-to-SQL models and achieves performance approaching\nstate-of-the-art LLM-based models, while preserving privacy."}
{"id": "2509.23519", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23519", "abs": "https://arxiv.org/abs/2509.23519", "authors": ["Zeyu Shen", "Basileal Imana", "Tong Wu", "Chong Xiang", "Prateek Mittal", "Aleksandra Korolova"], "title": "ReliabilityRAG: Effective and Provably Robust Defense for RAG-based Web-Search", "comment": "Accepted to NeurIPS 2025", "summary": "Retrieval-Augmented Generation (RAG) enhances Large Language Models by\ngrounding their outputs in external documents. These systems, however, remain\nvulnerable to attacks on the retrieval corpus, such as prompt injection.\nRAG-based search systems (e.g., Google's Search AI Overview) present an\ninteresting setting for studying and protecting against such threats, as\ndefense algorithms can benefit from built-in reliability signals -- like\ndocument ranking -- and represent a non-LLM challenge for the adversary due to\ndecades of work to thwart SEO.\n  Motivated by, but not limited to, this scenario, this work introduces\nReliabilityRAG, a framework for adversarial robustness that explicitly\nleverages reliability information of retrieved documents.\n  Our first contribution adopts a graph-theoretic perspective to identify a\n\"consistent majority\" among retrieved documents to filter out malicious ones.\nWe introduce a novel algorithm based on finding a Maximum Independent Set (MIS)\non a document graph where edges encode contradiction. Our MIS variant\nexplicitly prioritizes higher-reliability documents and provides provable\nrobustness guarantees against bounded adversarial corruption under natural\nassumptions. Recognizing the computational cost of exact MIS for large\nretrieval sets, our second contribution is a scalable weighted sample and\naggregate framework. It explicitly utilizes reliability information, preserving\nsome robustness guarantees while efficiently handling many documents.\n  We present empirical results showing ReliabilityRAG provides superior\nrobustness against adversarial attacks compared to prior methods, maintains\nhigh benign accuracy, and excels in long-form generation tasks where prior\nrobustness-focused methods struggled. Our work is a significant step towards\nmore effective, provably robust defenses against retrieved corpus corruption in\nRAG."}
{"id": "2509.23571", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23571", "abs": "https://arxiv.org/abs/2509.23571", "authors": ["Yuqiao Meng", "Luoxi Tang", "Feiyang Yu", "Xi Li", "Guanhua Yan", "Ping Yang", "Zhaohan Xi"], "title": "Benchmarking LLM-Assisted Blue Teaming via Standardized Threat Hunting", "comment": null, "summary": "As cyber threats continue to grow in scale and sophistication, blue team\ndefenders increasingly require advanced tools to proactively detect and\nmitigate risks. Large Language Models (LLMs) offer promising capabilities for\nenhancing threat analysis. However, their effectiveness in real-world blue team\nthreat-hunting scenarios remains insufficiently explored. This paper presents\nCyberTeam, a benchmark designed to guide LLMs in blue teaming practice.\nCyberTeam constructs a standardized workflow in two stages. First, it models\nrealistic threat-hunting workflows by capturing the dependencies among\nanalytical tasks from threat attribution to incident response. Next, each task\nis addressed through a set of operational modules tailored to its specific\nanalytical requirements. This transforms threat hunting into a structured\nsequence of reasoning steps, with each step grounded in a discrete operation\nand ordered according to task-specific dependencies. Guided by this framework,\nLLMs are directed to perform threat-hunting tasks through modularized steps.\nOverall, CyberTeam integrates 30 tasks and 9 operational modules to guide LLMs\nthrough standardized threat analysis. We evaluate both leading LLMs and\nstate-of-the-art cybersecurity agents, comparing CyberTeam against open-ended\nreasoning strategies. Our results highlight the improvements enabled by\nstandardized design, while also revealing the limitations of open-ended\nreasoning in real-world threat hunting."}
{"id": "2509.23573", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23573", "abs": "https://arxiv.org/abs/2509.23573", "authors": ["Yuqiao Meng", "Luoxi Tang", "Feiyang Yu", "Jinyuan Jia", "Guanhua Yan", "Ping Yang", "Zhaohan Xi"], "title": "Uncovering Vulnerabilities of LLM-Assisted Cyber Threat Intelligence", "comment": null, "summary": "Large Language Models (LLMs) are intensively used to assist security analysts\nin counteracting the rapid exploitation of cyber threats, wherein LLMs offer\ncyber threat intelligence (CTI) to support vulnerability assessment and\nincident response. While recent work has shown that LLMs can support a wide\nrange of CTI tasks such as threat analysis, vulnerability detection, and\nintrusion defense, significant performance gaps persist in practical\ndeployments. In this paper, we investigate the intrinsic vulnerabilities of\nLLMs in CTI, focusing on challenges that arise from the nature of the threat\nlandscape itself rather than the model architecture. Using large-scale\nevaluations across multiple CTI benchmarks and real-world threat reports, we\nintroduce a novel categorization methodology that integrates stratification,\nautoregressive refinement, and human-in-the-loop supervision to reliably\nanalyze failure instances. Through extensive experiments and human inspections,\nwe reveal three fundamental vulnerabilities: spurious correlations,\ncontradictory knowledge, and constrained generalization, that limit LLMs in\neffectively supporting CTI. Subsequently, we provide actionable insights for\ndesigning more robust LLM-powered CTI systems to facilitate future research."}
{"id": "2509.23594", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23594", "abs": "https://arxiv.org/abs/2509.23594", "authors": ["Yixu Wang", "Yan Teng", "Yingchun Wang", "Xingjun Ma"], "title": "StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data", "comment": "ICCV 2025", "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have transformed\nvision model adaptation, enabling the rapid deployment of customized models.\nHowever, the compactness of LoRA adaptations introduces new safety concerns,\nparticularly their vulnerability to model extraction attacks. This paper\nintroduces a new focus of model extraction attacks named LoRA extraction that\nextracts LoRA-adaptive models based on a public pre-trained model. We then\npropose a novel extraction method called StolenLoRA which trains a substitute\nmodel to extract the functionality of a LoRA-adapted model using synthetic\ndata. StolenLoRA leverages a Large Language Model to craft effective prompts\nfor data generation, and it incorporates a Disagreement-based Semi-supervised\nLearning (DSL) strategy to maximize information gain from limited queries. Our\nexperiments demonstrate the effectiveness of StolenLoRA, achieving up to a\n96.60% attack success rate with only 10k queries, even in cross-backbone\nscenarios where the attacker and victim models utilize different pre-trained\nbackbones. These findings reveal the specific vulnerability of LoRA-adapted\nmodels to this type of extraction and underscore the urgent need for robust\ndefense mechanisms tailored to PEFT methods. We also explore a preliminary\ndefense strategy based on diversified LoRA deployments, highlighting its\npotential to mitigate such attacks."}
{"id": "2509.23621", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.23621", "abs": "https://arxiv.org/abs/2509.23621", "authors": ["Sherif Saad", "Kevin Shi", "Mohammed Mamun", "Hythem Elmiligi"], "title": "AutoML in Cybersecurity: An Empirical Study", "comment": null, "summary": "Automated machine learning (AutoML) has emerged as a promising paradigm for\nautomating machine learning (ML) pipeline design, broadening AI adoption. Yet\nits reliability in complex domains such as cybersecurity remains underexplored.\nThis paper systematically evaluates eight open-source AutoML frameworks across\n11 publicly available cybersecurity datasets, spanning intrusion detection,\nmalware classification, phishing, fraud detection, and spam filtering. Results\nshow substantial performance variability across tools and datasets, with no\nsingle solution consistently superior. A paradigm shift is observed: the\nchallenge has moved from selecting individual ML models to identifying the most\nsuitable AutoML framework, complicated by differences in runtime efficiency,\nautomation capabilities, and supported features. AutoML tools frequently favor\ntree-based models, which perform well but risk overfitting and limit\ninterpretability. Key challenges identified include adversarial vulnerability,\nmodel drift, and inadequate feature engineering. We conclude with best\npractices and research directions to strengthen robustness, interpretability,\nand trust in AutoML for high-stakes cybersecurity applications."}
{"id": "2509.23680", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23680", "abs": "https://arxiv.org/abs/2509.23680", "authors": ["Shidong Pan", "Yikai Ge", "Xiaoyu Sun"], "title": "A First Look at Privacy Risks of Android Task-executable Voice Assistant Applications", "comment": "Accepted by APSEC 2025", "summary": "With the development of foundation AI technologies, task-executable voice\nassistants (VAs) have become more popular, enhancing user convenience and\nexpanding device functionality. Android task-executable VAs are applications\nthat are capable of understanding complex tasks and performing corresponding\noperations. Given their prevalence and great autonomy, there is no existing\nwork examine the privacy risks within the voice assistants from the\ntask-execution pattern in a holistic manner. To fill this research gap, this\npaper presents a user-centric comprehensive empirical study on privacy risks in\nAndroid task-executable VA applications. We collect ten mainstream VAs as our\nresearch target and analyze their operational characteristics. We then\ncross-check their privacy declarations across six sources, including privacy\nlabels, policies, and manifest files, and our findings reveal widespread\ninconsistencies. Moreover, we uncover three significant privacy threat models:\n(1) privacy misdisclosure in mega apps, where integrated mini apps such as\nAlexa skills are inadequately represented; (2) privilege escalation via\ninter-application interactions, which exploit Android's communication\nmechanisms to bypass user consent; and (3) abuse of Google system applications,\nenabling apps to evade the declaration of dangerous permissions. Our study\ncontributes actionable recommendations for practitioners and underscores\nbroader relevance of these privacy risks to emerging autonomous AI agents."}
{"id": "2509.23834", "categories": ["cs.CR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.23834", "abs": "https://arxiv.org/abs/2509.23834", "authors": ["Haochen Sun", "Xi He"], "title": "GPM: The Gaussian Pancake Mechanism for Planting Undetectable Backdoors in Differential Privacy", "comment": "16 pages, 7 figures. Not published yet. Code and raw experimental\n  logs will be available after publication, or upon email request", "summary": "Differential privacy (DP) has become the gold standard for preserving\nindividual privacy in data analysis. However, an implicit yet fundamental\nassumption underlying these rigorous privacy guarantees is the correct\nimplementation and execution of DP mechanisms. Several incidents of unintended\nprivacy loss have occurred due to numerical issues and inappropriate\nconfigurations of DP software, which have been successfully exploited in\nprivacy attacks. To better understand the seriousness of defective DP software,\nwe ask the following question: is it possible to elevate these passive defects\ninto active privacy attacks while maintaining covertness?\n  To address this question, we present the Gaussian pancake mechanism (GPM), a\nnovel mechanism that is computationally indistinguishable from the widely used\nGaussian mechanism (GM), yet exhibits arbitrarily weaker statistical DP\nguarantees. This unprecedented separation enables a new class of backdoor\nattacks: by indistinguishably passing off as the authentic GM, GPM can covertly\ndegrade statistical privacy. Unlike the unintentional privacy loss caused by\nGM's numerical issues, GPM is an adversarial yet undetectable backdoor attack\nagainst data privacy. We formally prove GPM's covertness, characterize its\nstatistical leakage, and demonstrate a concrete distinguishing attack that can\nachieve near-perfect success rates under suitable parameter choices, both\ntheoretically and empirically.\n  Our results underscore the importance of using transparent, open-source DP\nlibraries and highlight the need for rigorous scrutiny and formal verification\nof DP implementations to prevent subtle, undetectable privacy compromises in\nreal-world systems."}
{"id": "2509.23871", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23871", "abs": "https://arxiv.org/abs/2509.23871", "authors": ["Yukun Chen", "Boheng Li", "Yu Yuan", "Leyi Qi", "Yiming Li", "Tianwei Zhang", "Zhan Qin", "Kui Ren"], "title": "Taught Well Learned Ill: Towards Distillation-conditional Backdoor Attack", "comment": "The first three authors contributed equally to this work. To appear\n  in NeurIPS 2025. 35 pages", "summary": "Knowledge distillation (KD) is a vital technique for deploying deep neural\nnetworks (DNNs) on resource-constrained devices by transferring knowledge from\nlarge teacher models to lightweight student models. While teacher models from\nthird-party platforms may undergo security verification (\\eg, backdoor\ndetection), we uncover a novel and critical threat: distillation-conditional\nbackdoor attacks (DCBAs). DCBA injects dormant and undetectable backdoors into\nteacher models, which become activated in student models via the KD process,\neven with clean distillation datasets. While the direct extension of existing\nmethods is ineffective for DCBA, we implement this attack by formulating it as\na bilevel optimization problem and proposing a simple yet effective method\n(\\ie, SCAR). Specifically, the inner optimization simulates the KD process by\noptimizing a surrogate student model, while the outer optimization leverages\noutputs from this surrogate to optimize the teacher model for implanting the\nconditional backdoor. Our SCAR addresses this complex optimization utilizing an\nimplicit differentiation algorithm with a pre-optimized trigger injection\nfunction. Extensive experiments across diverse datasets, model architectures,\nand KD techniques validate the effectiveness of our SCAR and its resistance\nagainst existing backdoor detection, highlighting a significant yet previously\noverlooked vulnerability in the KD process. Our code is available at\nhttps://github.com/WhitolfChen/SCAR."}
{"id": "2509.23970", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.23970", "abs": "https://arxiv.org/abs/2509.23970", "authors": ["Meet Udeshi", "Venkata Sai Charan Putrevu", "Prashanth Krishnamurthy", "Prashant Anantharaman", "Sean Carrick", "Ramesh Karri", "Farshad Khorrami"], "title": "Binary Diff Summarization using Large Language Models", "comment": null, "summary": "Security of software supply chains is necessary to ensure that software\nupdates do not contain maliciously injected code or introduce vulnerabilities\nthat may compromise the integrity of critical infrastructure. Verifying the\nintegrity of software updates involves binary differential analysis (binary\ndiffing) to highlight the changes between two binary versions by incorporating\nbinary analysis and reverse engineering. Large language models (LLMs) have been\napplied to binary analysis to augment traditional tools by producing natural\nlanguage summaries that cybersecurity experts can grasp for further analysis.\nCombining LLM-based binary code summarization with binary diffing can improve\nthe LLM's focus on critical changes and enable complex tasks such as automated\nmalware detection. To address this, we propose a novel framework for binary\ndiff summarization using LLMs. We introduce a novel functional sensitivity\nscore (FSS) that helps with automated triage of sensitive binary functions for\ndownstream detection tasks. We create a software supply chain security\nbenchmark by injecting 3 different malware into 6 open-source projects which\ngenerates 104 binary versions, 392 binary diffs, and 46,023 functions. On this,\nour framework achieves a precision of 0.98 and recall of 0.64 for malware\ndetection, displaying high accuracy with low false positives. Across malicious\nand benign functions, we achieve FSS separation of 3.0 points, confirming that\nFSS categorization can classify sensitive functions. We conduct a case study on\nthe real-world XZ utils supply chain attack; our framework correctly detects\nthe injected backdoor functions with high FSS."}
{"id": "2509.23984", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.23984", "abs": "https://arxiv.org/abs/2509.23984", "authors": ["Pranav Garimidi", "Joachim Neu", "Max Resnick"], "title": "Multiple Concurrent Proposers: Why and How", "comment": null, "summary": "Traditional single-proposer blockchains suffer from miner extractable value\n(MEV), where validators exploit their serial monopoly on transaction inclusion\nand ordering to extract rents from users. While there have been many\ndevelopments at the application layer to reduce the impact of MEV, these\napproaches largely require auctions as a subcomponent. Running auctions\nefficiently on chain requires two key properties of the underlying consensus\nprotocol: selective-censorship resistance and hiding. These properties\nguarantee that an adversary can neither selectively delay transactions nor see\ntheir contents before they are confirmed. We propose a multiple concurrent\nproposer (MCP) protocol offering exactly these properties."}
{"id": "2509.24037", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24037", "abs": "https://arxiv.org/abs/2509.24037", "authors": ["Alireza Lotfi", "Charalampos Katsis", "Elisa Bertino"], "title": "Automated Vulnerability Validation and Verification: A Large Language Model Approach", "comment": null, "summary": "Software vulnerabilities remain a critical security challenge, providing\nentry points for attackers into enterprise networks. Despite advances in\nsecurity practices, the lack of high-quality datasets capturing diverse exploit\nbehavior limits effective vulnerability assessment and mitigation. This paper\nintroduces an end-to-end multi-step pipeline leveraging generative AI,\nspecifically large language models (LLMs), to address the challenges of\norchestrating and reproducing attacks to known software vulnerabilities. Our\napproach extracts information from CVE disclosures in the National\nVulnerability Database, augments it with external public knowledge (e.g.,\nthreat advisories, code snippets) using Retrieval-Augmented Generation (RAG),\nand automates the creation of containerized environments and exploit code for\neach vulnerability. The pipeline iteratively refines generated artifacts,\nvalidates attack success with test cases, and supports complex multi-container\nsetups. Our methodology overcomes key obstacles, including noisy and incomplete\nvulnerability descriptions, by integrating LLMs and RAG to fill information\ngaps. We demonstrate the effectiveness of our pipeline across different\nvulnerability types, such as memory overflows, denial of service, and remote\ncode execution, spanning diverse programming languages, libraries and years. In\ndoing so, we uncover significant inconsistencies in CVE descriptions,\nemphasizing the need for more rigorous verification in the CVE disclosure\nprocess. Our approach is model-agnostic, working across multiple LLMs, and we\nopen-source the artifacts to enable reproducibility and accelerate security\nresearch. To the best of our knowledge, this is the first system to\nsystematically orchestrate and exploit known vulnerabilities in containerized\nenvironments by combining general-purpose LLM reasoning with CVE data and\nRAG-based context enrichment."}
{"id": "2509.24043", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24043", "abs": "https://arxiv.org/abs/2509.24043", "authors": ["Yihan Wu", "Ruibo Chen", "Georgios Milis", "Heng Huang"], "title": "An Ensemble Framework for Unbiased Language Model Watermarking", "comment": null, "summary": "As large language models become increasingly capable and widely deployed,\nverifying the provenance of machine-generated content is critical to ensuring\ntrust, safety, and accountability. Watermarking techniques have emerged as a\npromising solution by embedding imperceptible statistical signals into the\ngeneration process. Among them, unbiased watermarking is particularly\nattractive due to its theoretical guarantee of preserving the language model's\noutput distribution, thereby avoiding degradation in fluency or detectability\nthrough distributional shifts. However, existing unbiased watermarking schemes\noften suffer from weak detection power and limited robustness, especially under\nshort text lengths or distributional perturbations. In this work, we propose\nENS, a novel ensemble framework that enhances the detectability and robustness\nof logits-based unbiased watermarks while strictly preserving their\nunbiasedness. ENS sequentially composes multiple independent watermark\ninstances, each governed by a distinct key, to amplify the watermark signal. We\ntheoretically prove that the ensemble construction remains unbiased in\nexpectation and demonstrate how it improves the signal-to-noise ratio for\nstatistical detectors. Empirical evaluations on multiple LLM families show that\nENS substantially reduces the number of tokens needed for reliable detection\nand increases resistance to smoothing and paraphrasing attacks without\ncompromising generation quality."}
{"id": "2509.24048", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24048", "abs": "https://arxiv.org/abs/2509.24048", "authors": ["Yihan Wu", "Xuehao Cui", "Ruibo Chen", "Heng Huang"], "title": "Analyzing and Evaluating Unbiased Language Model Watermark", "comment": null, "summary": "Verifying the authenticity of AI-generated text has become increasingly\nimportant with the rapid advancement of large language models, and unbiased\nwatermarking has emerged as a promising approach due to its ability to preserve\noutput distribution without degrading quality. However, recent work reveals\nthat unbiased watermarks can accumulate distributional bias over multiple\ngenerations and that existing robustness evaluations are inconsistent across\nstudies. To address these issues, we introduce UWbench, the first open-source\nbenchmark dedicated to the principled evaluation of unbiased watermarking\nmethods. Our framework combines theoretical and empirical contributions: we\npropose a statistical metric to quantify multi-batch distribution drift, prove\nan impossibility result showing that no unbiased watermark can perfectly\npreserve the distribution under infinite queries, and develop a formal analysis\nof robustness against token-level modification attacks. Complementing this\ntheory, we establish a three-axis evaluation protocol: unbiasedness,\ndetectability, and robustness, and show that token modification attacks provide\nmore stable robustness assessments than paraphrasing-based methods. Together,\nUWbench offers the community a standardized and reproducible platform for\nadvancing the design and evaluation of unbiased watermarking algorithms."}
{"id": "2509.24153", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24153", "abs": "https://arxiv.org/abs/2509.24153", "authors": ["Philip Sjösvärd", "Hongyu Jin", "Panos Papadimitratos"], "title": "DNS in the Time of Curiosity: A Tale of Collaborative User Privacy Protection", "comment": "Twenty-ninth International Workshop on Security Protocols", "summary": "The Domain Name System (DNS) is central to all Internet user activity,\nresolving accessed domain names into Internet Protocol (IP) addresses. As a\nresult, curious DNS resolvers can learn everything about Internet users'\ninterests. Public DNS resolvers are rising in popularity, offering low-latency\nresolution, high reliability, privacy-preserving policies, and support for\nencrypted DNS queries. However, client-resolver traffic encryption,\nincreasingly deployed to protect users from eavesdroppers, does not protect\nusers against curious resolvers. Similarly, privacy-preserving policies are\nbased solely on written commitments and do not provide technical safeguards.\nAlthough DNS query relay schemes can separate duties to limit data accessible\nby each entity, they cannot prevent colluding entities from sharing user\ntraffic logs. Thus, a key challenge remains: organizations operating public DNS\nresolvers, accounting for the majority of DNS resolutions, can potentially\ncollect and analyze massive volumes of Internet user activity data. With DNS\ninfrastructure that cannot be fully trusted, can we safeguard user privacy? We\nanswer positively and advocate for a user-driven approach to reduce exposure to\nDNS services. We will discuss key ideas of the proposal, which aims to achieve\na high level of privacy without sacrificing performance: maintaining low\nlatency, network bandwidth, memory/storage overhead, and computational\noverhead."}
{"id": "2509.24173", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.24173", "abs": "https://arxiv.org/abs/2509.24173", "authors": ["Sun-Moon Yoon", "Hyun-Young Park", "Seung-Hyun Nam", "Si-Hyeon Lee"], "title": "Fundamental Limit of Discrete Distribution Estimation under Utility-Optimized Local Differential Privacy", "comment": "20 pages, 7 figures, 1 table. This work has been submitted to the\n  IEEE for possible publication", "summary": "We study the problem of discrete distribution estimation under\nutility-optimized local differential privacy (ULDP), which enforces local\ndifferential privacy (LDP) on sensitive data while allowing more accurate\ninference on non-sensitive data. In this setting, we completely characterize\nthe fundamental privacy-utility trade-off. The converse proof builds on several\nkey ideas, including a generalized uniform asymptotic Cram\\'er-Rao lower bound,\na reduction showing that it suffices to consider a newly defined class of\nextremal ULDP mechanisms, and a novel distribution decomposition technique\ntailored to ULDP constraints. For the achievability, we propose a class of\nutility-optimized block design (uBD) schemes, obtained as nontrivial\nmodifications of the block design mechanism known to be optimal under standard\nLDP constraints, while incorporating the distribution decomposition idea used\nin the converse proof and a score-based linear estimator. These results provide\na tight characterization of the estimation accuracy achievable under ULDP and\nreveal new insights into the structure of optimal mechanisms for\nprivacy-preserving statistical inference."}
{"id": "2509.24174", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24174", "abs": "https://arxiv.org/abs/2509.24174", "authors": ["Philip Sjösvärd", "Hongyu Jin", "Panos Papadimitratos"], "title": "LLUAD: Low-Latency User-Anonymized DNS", "comment": "24th Workshop on Privacy in the Electronic Society", "summary": "The Domain Name System (DNS) is involved in practically all web activity,\ntranslating easy-to-remember domain names into Internet Protocol (IP)\naddresses. Due to its central role on the Internet, DNS exposes user web\nactivity in detail. The privacy challenge is honest-but-curious DNS\nservers/resolvers providing the translation/lookup service. In particular, with\nthe majority of DNS queries handled by public DNS resolvers, the organizations\nrunning them can track, collect, and analyze massive user activity data.\nExisting solutions that encrypt DNS traffic between clients and resolvers are\ninsufficient, as the resolver itself is the privacy threat. While DNS query\nrelays separate duties among multiple entities, to limit the data accessible by\neach entity, they cannot prevent colluding entities from sharing user traffic\nlogs. To achieve near-zero-trust DNS privacy compatible with the existing DNS\ninfrastructure, we propose LLUAD: it locally stores a Popularity List, the most\npopular DNS records, on user devices, formed in a privacy-preserving manner\nbased on user interests. In this way, LLUAD can both improve privacy and reduce\naccess times to web content. The Popularity List is proactively retrieved from\na (curious) public server that continually updates and refreshes the records\nbased on user popularity votes, while efficiently broadcasting record\nupdates/changes to adhere to aggressive load-balancing schemes (i.e., name\nservers actively load-balancing user connections by changing record IP\naddresses). User votes are anonymized using a novel, efficient, and highly\nscalable client-driven Voting Mix Network - with packet lengths independent of\nthe number of hops, centrally enforced limit on number of votes cast per user,\nand robustness against poor client participation - to ensure a geographically\nrelevant and correctly/securely instantiated Popularity List."}
{"id": "2509.24240", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24240", "abs": "https://arxiv.org/abs/2509.24240", "authors": ["Eunkyu Lee", "Donghyeon Kim", "Wonyoung Kim", "Insu Yun"], "title": "Takedown: How It's Done in Modern Coding Agent Exploits", "comment": null, "summary": "Coding agents, which are LLM-driven agents specialized in software\ndevelopment, have become increasingly prevalent in modern programming\nenvironments. Unlike traditional AI coding assistants, which offer simple code\ncompletion and suggestions, modern coding agents tackle more complex tasks with\ngreater autonomy, such as generating entire programs from natural language\ninstructions. To enable such capabilities, modern coding agents incorporate\nextensive functionalities, which in turn raise significant concerns over their\nsecurity and privacy. Despite their growing adoption, systematic and in-depth\nsecurity analysis of these agents has largely been overlooked.\n  In this paper, we present a comprehensive security analysis of eight\nreal-world coding agents. Our analysis addresses the limitations of prior\napproaches, which were often fragmented and ad hoc, by systematically examining\nthe internal workflows of coding agents and identifying security threats across\ntheir components. Through the analysis, we identify 15 security issues,\nincluding previously overlooked or missed issues, that can be abused to\ncompromise the confidentiality and integrity of user systems. Furthermore, we\nshow that these security issues are not merely individual vulnerabilities, but\ncan collectively lead to end-to-end exploitations. By leveraging these security\nissues, we successfully achieved arbitrary command execution in five agents and\nglobal data exfiltration in four agents, all without any user interaction or\napproval. Our findings highlight the need for a comprehensive security analysis\nin modern LLM-driven agents and demonstrate how insufficient security\nconsiderations can lead to severe vulnerabilities."}
{"id": "2509.24257", "categories": ["cs.CR", "cs.LG", "C.2.1"], "pdf": "https://arxiv.org/pdf/2509.24257", "abs": "https://arxiv.org/abs/2509.24257", "authors": ["Ke Wang", "Felix Qu", "Libin Xia", "Zishuo Zhao", "Chris Tong", "Lynn Ai", "Eric Yang"], "title": "VeriLLM: A Lightweight Framework for Publicly Verifiable Decentralized Inference", "comment": "13 pages, 4 figures, 2 tables", "summary": "Decentralized inference is an appealing paradigm for serving large language\nmodels (LLMs), offering strong security, high efficiency, and lower operating\ncosts. Yet the permissionless setting admits no a priori trust in participating\nnodes, making output verifiability a prerequisite for secure deployment. We\npresent VeriLLM, a publicly verifiable protocol for decentralized LLM inference\nthat (i) achieves security under a one-honest-verifier assumption, (ii) attains\nnear-negligible verification cost (about 1% of the underlying inference) via a\nlightweight verification algorithm designed explicitly for LLMs, and (iii)\nenforces honest checking through a peer-prediction mechanism that mitigates\nlazy verification in naive voting. We further introduce an isomorphic\ninference-verification network that multiplexes both roles on the same set of\nGPU workers. This architecture (i) increases GPU utilization and thereby\nimproves end-to-end throughput for both inference and verification, (ii)\nexpands the effective pool of available validators, strengthening robustness\nand security, and (iii) enforces task indistinguishability at the worker\nboundary to prevent job-type-conditioned behavior. Finally, we provide a formal\ngame-theoretic analysis and prove that, under our incentives, honest inference\nand verification constitute a Nash equilibrium, ensuring incentive\ncompatibility against rational adversaries. To our knowledge, this is the first\ndecentralized inference verification protocol with an end-to-end game-theoretic\nsecurity proof."}
{"id": "2509.24272", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24272", "abs": "https://arxiv.org/abs/2509.24272", "authors": ["Weibo Zhao", "Jiahao Liu", "Bonan Ruan", "Shaofei Li", "Zhenkai Liang"], "title": "When MCP Servers Attack: Taxonomy, Feasibility, and Mitigation", "comment": null, "summary": "Model Context Protocol (MCP) servers enable AI applications to connect to\nexternal systems in a plug-and-play manner, but their rapid proliferation also\nintroduces severe security risks. Unlike mature software ecosystems with\nrigorous vetting, MCP servers still lack standardized review mechanisms, giving\nadversaries opportunities to distribute malicious implementations. Despite this\npressing risk, the security implications of MCP servers remain underexplored.\nTo address this gap, we present the first systematic study that treats MCP\nservers as active threat actors and decomposes them into core components to\nexamine how adversarial developers can implant malicious intent. Specifically,\nwe investigate three research questions: (i) what types of attacks malicious\nMCP servers can launch, (ii) how vulnerable MCP hosts and Large Language Models\n(LLMs) are to these attacks, and (iii) how feasible it is to carry out MCP\nserver attacks in practice. Our study proposes a component-based taxonomy\ncomprising twelve attack categories. For each category, we develop\nProof-of-Concept (PoC) servers and demonstrate their effectiveness across\ndiverse real-world host-LLM settings. We further show that attackers can\ngenerate large numbers of malicious servers at virtually no cost. We then test\nstate-of-the-art scanners on the generated servers and found that existing\ndetection approaches are insufficient. These findings highlight that malicious\nMCP servers are easy to implement, difficult to detect with current tools, and\ncapable of causing concrete damage to AI agent systems. Addressing this threat\nrequires coordinated efforts among protocol designers, host developers, LLM\nproviders, and end users to build a more secure and resilient MCP ecosystem."}
{"id": "2509.24408", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24408", "abs": "https://arxiv.org/abs/2509.24408", "authors": ["Yuzhen Long", "Songze Li"], "title": "FuncPoison: Poisoning Function Library to Hijack Multi-agent Autonomous Driving Systems", "comment": null, "summary": "Autonomous driving systems increasingly rely on multi-agent architectures\npowered by large language models (LLMs), where specialized agents collaborate\nto perceive, reason, and plan. A key component of these systems is the shared\nfunction library, a collection of software tools that agents use to process\nsensor data and navigate complex driving environments. Despite its critical\nrole in agent decision-making, the function library remains an under-explored\nvulnerability. In this paper, we introduce FuncPoison, a novel poisoning-based\nattack targeting the function library to manipulate the behavior of LLM-driven\nmulti-agent autonomous systems. FuncPoison exploits two key weaknesses in how\nagents access the function library: (1) agents rely on text-based instructions\nto select tools; and (2) these tools are activated using standardized command\nformats that attackers can replicate. By injecting malicious tools with\ndeceptive instructions, FuncPoison manipulates one agent s decisions--such as\nmisinterpreting road conditions--triggering cascading errors that mislead other\nagents in the system. We experimentally evaluate FuncPoison on two\nrepresentative multi-agent autonomous driving systems, demonstrating its\nability to significantly degrade trajectory accuracy, flexibly target specific\nagents to induce coordinated misbehavior, and evade diverse defense mechanisms.\nOur results reveal that the function library, often considered a simple\ntoolset, can serve as a critical attack surface in LLM-based autonomous driving\nsystems, raising elevated concerns on their reliability."}
{"id": "2509.24418", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24418", "abs": "https://arxiv.org/abs/2509.24418", "authors": ["Haoran Li", "Yulin Chen", "Jingru Zeng", "Hao Peng", "Huihao Jing", "Wenbin Hu", "Xi Yang", "Ziqian Zeng", "Sirui Han", "Yangqiu Song"], "title": "GSPR: Aligning LLM Safeguards as Generalizable Safety Policy Reasoners", "comment": null, "summary": "As large language models (LLMs) are increasingly integrated into numerous\napplications across various domains, LLMs' safety becomes a critical concern\nfor both application developers and intended users. Currently, great efforts\nhave been made to develop safety benchmarks with fine-grained taxonomies.\nHowever, these benchmarks' taxonomies are disparate with different safety\npolicies. Thus, existing safeguards trained on these benchmarks are either\ncoarse-grained to only distinguish between safe and unsafe, or constrained by\nthe narrow risk taxonomies of a single benchmark. To leverage these\nfine-grained safety taxonomies across multiple safety benchmarks, in this\npaper, we propose GSPR, a Generalizable Safety Policy Reasoner to identify\nunsafe input prompts and LLMs' outputs with violated safety taxonomies through\nGroup Relative Policy Optimization (GRPO). Unlike prior safeguards which only\ncover a fixed set of risk factors, our GSPR incentivizes its reasoning\ncapability with varied safety taxonomies through our careful cold-start\nstrategy and reward design. Consequently, our GSPR can be trained across\nmultiple safety benchmarks with distinct taxonomies and naturally exhibits\npowerful generalization ability. We conduct extensive experiments to show that\nour GSPR significantly improves existing safety guardrails' reasoning\ncapabilities for both safety and category prediction tasks. Moreover, our GSPR\nnot only demonstrates powerful safety generalization abilities but also\nachieves the least inference token costs with explanations."}
{"id": "2509.24440", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24440", "abs": "https://arxiv.org/abs/2509.24440", "authors": ["Antonis Selentis", "Nikolas Makris", "Alkinoos Papageorgopoulos", "Persefoni Konteli", "Konstantinos Christodoulopoulos", "George T. Kanellos", "Dimitris Syvridis"], "title": "Evaluating Relayed and Switched Quantum Key Distribution (QKD) Network Architectures", "comment": null, "summary": "We evaluate the performance of two architectures for network-wide quantum key\ndistribution (QKD): Relayed QKD, which relays keys over multi-link QKD paths\nfor non-adjacent nodes, and Switched QKD, which uses optical switches to\ndynamically connect arbitrary QKD modules to form direct QKD links between\nthem. An advantage of Switched QKD is that it distributes quantum keys\nend-to-end, whereas Relayed relies on trusted nodes. However, Switched depends\non arbitrary matching of QKD modules. We first experimentally evaluate the\nperformance of commercial DV-QKD modules; for each of three vendors we\nbenchmark the performance in standard/matched module pairs and in unmatched\npairs to emulate configurations in the Switched QKD network architecture. The\nanalysis reveals that in some cases a notable variation in the generated secret\nkey rate (SKR) between the matched and unmatched pairs is observed. Driven by\nthese experimental findings, we conduct a comprehensive theoretical analysis\nthat evaluates the network-wide performance of the two architectures. Our\nanalysis is based on uniform ring networks, where we derive optimal key\nmanagement configurations and analytical formulas for the achievable consumed\nSKR. We compare network performance under varying ring sizes, QKD link losses,\nQKD receivers' sensitivity and performance penalties of unmatched modules. Our\nfindings indicate that Switched QKD performs better in dense rings (short\ndistances, large node counts), while Relayed QKD is more effective in longer\ndistances and large node counts. Moreover, we confirm that unmatched QKD\nmodules penalties significantly impact the efficiency of Switched QKD\narchitecture."}
{"id": "2509.24444", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.24444", "abs": "https://arxiv.org/abs/2509.24444", "authors": ["Yury Yanovich", "Victoria Kovalevskaya", "Maksim Egorov", "Elizaveta Smirnova", "Matvey Mishuris", "Yash Madhwal", "Kirill Ziborov", "Vladimir Gorgadze", "Subodh Sharma"], "title": "BugMagnifier: TON Transaction Simulator for Revealing Smart Contract Vulnerabilities", "comment": null, "summary": "The Open Network (TON) blockchain employs an asynchronous execution model\nthat introduces unique security challenges for smart contracts, particularly\nrace conditions arising from unpredictable message processing order. While\nprevious work established vulnerability patterns through static analysis of\naudit reports, dynamic detection of temporal dependencies through systematic\ntesting remains an open problem. We present BugMagnifier, a transaction\nsimulation framework that systematically reveals vulnerabilities in TON smart\ncontracts through controlled message orchestration. Built atop TON Sandbox and\nintegrated with the TON Virtual Machine (TVM), our tool combines precise\nmessage queue manipulation with differential state analysis and probabilistic\npermutation testing to detect asynchronous execution flaws. Experimental\nevaluation demonstrates BugMagnifier's effectiveness through extensive\nparametric studies on purpose-built vulnerable contracts, revealing message\nratio-dependent detection complexity that aligns with theoretical predictions.\nThis quantitative model enables predictive vulnerability assessment while\nshifting discovery from manual expert analysis to automated evidence\ngeneration. By providing reproducible test scenarios for temporal\nvulnerabilities, BugMagnifier addresses a critical gap in the TON security\ntooling, offering practical support for safer smart contract development in\nasynchronous blockchain environments."}
{"id": "2509.24623", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24623", "abs": "https://arxiv.org/abs/2509.24623", "authors": ["Carlos Benitez"], "title": "Mapping Quantum Threats: An Engineering Inventory of Cryptographic Dependencies", "comment": "13 pages, to be submitted to IEEE Privacy and Security", "summary": "The emergence of large-scale quantum computers, powered by algorithms like\nShor's and Grover's, poses an existential threat to modern public-key\ncryptography. This vulnerability stems from the ability of these machines to\nefficiently solve the hard mathematical problems - such as integer\nfactorization and the elliptic curve discrete logarithm problem - that underpin\nwidely used cryptographic primitives. This includes RSA, Diffie-Hellman (DH),\nElliptic Curve Diffie-Hellman (ECDH), and Elliptic Curve Digital Signature\nAlgorithm (ECDSA), which are foundational to security across the digital\necosystem. Once Shor's algorithm becomes practically realizable, these\nprimitives will fail, undermining both retrospective confidentiality and\ncryptographic authenticity - enabling adversaries to decrypt previously\ncaptured communications and forge digital signatures. This paper presents a\nsystematic inventory of technologies exposed to quantum threats from the\nengineering perspective, organized by both technology domain and by\nimplementation environment. While prior research has emphasized theoretical\nbreaks or protocol-level adaptations, this work focuses on the practical\nlandscape - mapping quantum-vulnerable systems across diverse digital\ninfrastructures. The contribution is a cross-domain, cross-environment threat\nmap to guide practitioners, vendors, and policymakers in identifying exposed\ntechnologies before the arrival of cryptographically relevant quantum\ncomputers."}
{"id": "2509.24624", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24624", "abs": "https://arxiv.org/abs/2509.24624", "authors": ["Thomas Fargues", "Ye Dong", "Tianwei Zhang", "Jin-Song Dong"], "title": "PRIVMARK: Private Large Language Models Watermarking with MPC", "comment": "8 pages, 4 figures, under peer-review", "summary": "The rapid growth of Large Language Models (LLMs) has highlighted the pressing\nneed for reliable mechanisms to verify content ownership and ensure\ntraceability. Watermarking offers a promising path forward, but it remains\nlimited by privacy concerns in sensitive scenarios, as traditional approaches\noften require direct access to a model's parameters or its training data. In\nthis work, we propose a secure multi-party computation (MPC)-based private LLMs\nwatermarking framework, PRIVMARK, to address the concerns. Concretely, we\ninvestigate PostMark (EMNLP'2024), one of the state-of-the-art LLMs\nWatermarking methods, and formulate its basic operations. Then, we construct\nefficient protocols for these operations using the MPC primitives in a\nblack-box manner. In this way, PRIVMARK enables multiple parties to\ncollaboratively watermark an LLM's output without exposing the model's weights\nto any single computing party. We implement PRIVMARK using SecretFlow-SPU\n(USENIX ATC'2023) and evaluate its performance using the ABY3 (CCS'2018)\nbackend. The experimental results show that PRIVMARK achieves semantically\nidentical results compared to the plaintext baseline without MPC and is\nresistant against paraphrasing and removing attacks with reasonable efficiency."}
{"id": "2509.24698", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24698", "abs": "https://arxiv.org/abs/2509.24698", "authors": ["Izaiah Sun", "Daniel Tan", "Andy Deng"], "title": "LISA Technical Report: An Agentic Framework for Smart Contract Auditing", "comment": "A technical report with 10 pages", "summary": "We present LISA, an agentic smart contract vulnerability detection framework\nthat combines rule-based and logic-based methods to address a broad spectrum of\nvulnerabilities in smart contracts. LISA leverages data from historical audit\nreports to learn the detection experience (without model fine-tuning), enabling\nit to generalize learned patterns to unseen projects and evolving threat\nprofiles. In our evaluation, LISA significantly outperforms both LLM-based\napproaches and traditional static analysis tools, achieving superior coverage\nof vulnerability types and higher detection accuracy. Our results suggest that\nLISA offers a compelling solution for industry: delivering more reliable and\ncomprehensive vulnerability detection while reducing the dependence on manual\neffort."}
{"id": "2509.24807", "categories": ["cs.CR", "K.6.5"], "pdf": "https://arxiv.org/pdf/2509.24807", "abs": "https://arxiv.org/abs/2509.24807", "authors": ["Dong Hyun Roh", "Rajesh Kumar"], "title": "Active Authentication via Korean Keystrokes Under Varying LLM Assistance and Cognitive Contexts", "comment": "Accepted for publication at IEEE-ICMLA 2025. Contains nine pages, six\n  figures, and two tables", "summary": "Keystroke dynamics is a promising modality for active user authentication,\nbut its effectiveness under varying LLM-assisted typing and cognitive\nconditions remains understudied. Using data from 50 users and cognitive labels\nfrom Bloom's Taxonomy, we evaluate keystroke-based authentication in Korean\nacross three realistic typing scenarios: bona fide composition, LLM content\nparaphrasing, and transcription. Our pipeline incorporates continuity-aware\nsegmentation, feature extraction, and classification via SVM, MLP, and XGB.\nResults show that the system maintains reliable performance across varying LLM\nusages and cognitive contexts, with Equal Error Rates ranging from 5.1% to\n10.4%. These findings demonstrate the feasibility of behavioral authentication\nunder modern writing conditions and offer insights into designing more\ncontext-resilient models."}
{"id": "2509.24823", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24823", "abs": "https://arxiv.org/abs/2509.24823", "authors": ["Benedetta Tondi", "Andrea Costanzo", "Mauro Barni"], "title": "Of-SemWat: High-payload text embedding for semantic watermarking of AI-generated images with arbitrary size", "comment": "5 pages, 2 figures", "summary": "We propose a high-payload image watermarking method for textual embedding,\nwhere a semantic description of the image - which may also correspond to the\ninput text prompt-, is embedded inside the image. In order to be able to\nrobustly embed high payloads in large-scale images - such as those produced by\nmodern AI generators - the proposed approach builds upon a traditional\nwatermarking scheme that exploits orthogonal and turbo codes for improved\nrobustness, and integrates frequency-domain embedding and perceptual masking\ntechniques to enhance watermark imperceptibility. Experiments show that the\nproposed method is extremely robust against a wide variety of image processing,\nand the embedded text can be retrieved also after traditional and AI\ninpainting, permitting to unveil the semantic modification the image has\nundergone via image-text mismatch analysis."}
{"id": "2509.24955", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24955", "abs": "https://arxiv.org/abs/2509.24955", "authors": ["Tereza Burianová", "Martin Perešíni", "Ivan Homoliak"], "title": "Secret Leader Election in Ethereum PoS: An Empirical Security Analysis of Whisk and Homomorphic Sortition under DoS on the Leader and Censorship Attacks", "comment": null, "summary": "Proposer anonymity in Proof-of-Stake (PoS) blockchains is a critical concern\ndue to the risk of targeted attacks such as malicious denial-of-service (DoS)\nand censorship attacks. While several Secret Single Leader Election (SSLE)\nmechanisms have been proposed to address these threats, their practical impact\nand trade-offs remain insufficiently explored. In this work, we present a\nunified experimental framework for evaluating SSLE mechanisms under adversarial\nconditions, grounded in a simplified yet representative model of Ethereum's PoS\nconsensus layer. The framework includes configurable adversaries capable of\nlaunching targeted DoS and censorship attacks, including coordinated strategies\nthat simultaneously compromise groups of validators. We simulate and compare\nkey protection mechanisms - Whisk, and homomorphic sortition. To the best of\nour knowledge, this is the first comparative study to examine adversarial DoS\nscenarios involving multiple attackers under diverse protection mechanisms. Our\nresults show that while both designs offer strong protection against targeted\nDoS attacks on the leader, neither defends effectively against coordinated\nattacks on validator groups. Moreover, Whisk simplifies a DoS attack by\nnarrowing the target set from all validators to a smaller list of known\ncandidates. Homomorphic sortition, despite its theoretical strength, remains\nimpractical due to the complexity of cryptographic operations over large\nvalidator sets."}
{"id": "2509.24967", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24967", "abs": "https://arxiv.org/abs/2509.24967", "authors": ["Yupei Liu", "Yanting Wang", "Yuqi Jia", "Jinyuan Jia", "Neil Zhenqiang Gong"], "title": "SecInfer: Preventing Prompt Injection via Inference-time Scaling", "comment": null, "summary": "Prompt injection attacks pose a pervasive threat to the security of Large\nLanguage Models (LLMs). State-of-the-art prevention-based defenses typically\nrely on fine-tuning an LLM to enhance its security, but they achieve limited\neffectiveness against strong attacks. In this work, we propose \\emph{SecInfer},\na novel defense against prompt injection attacks built on \\emph{inference-time\nscaling}, an emerging paradigm that boosts LLM capability by allocating more\ncompute resources for reasoning during inference. SecInfer consists of two key\nsteps: \\emph{system-prompt-guided sampling}, which generates multiple responses\nfor a given input by exploring diverse reasoning paths through a varied set of\nsystem prompts, and \\emph{target-task-guided aggregation}, which selects the\nresponse most likely to accomplish the intended task. Extensive experiments\nshow that, by leveraging additional compute at inference, SecInfer effectively\nmitigates both existing and adaptive prompt injection attacks, outperforming\nstate-of-the-art defenses as well as existing inference-time scaling\napproaches."}
{"id": "2509.25072", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25072", "abs": "https://arxiv.org/abs/2509.25072", "authors": ["Yaman Jandali", "Ruisi Zhang", "Nojan Sheybani", "Farinaz Koushanfar"], "title": "Optimizing Privacy-Preserving Primitives to Support LLM-Scale Applications", "comment": null, "summary": "Privacy-preserving technologies have introduced a paradigm shift that allows\nfor realizable secure computing in real-world systems. The significant barrier\nto the practical adoption of these primitives is the computational and\ncommunication overhead that is incurred when applied at scale. In this paper,\nwe present an overview of our efforts to bridge the gap between this overhead\nand practicality for privacy-preserving learning systems using multi-party\ncomputation (MPC), zero-knowledge proofs (ZKPs), and fully homomorphic\nencryption (FHE). Through meticulous hardware/software/algorithm co-design, we\nshow progress towards enabling LLM-scale applications in privacy-preserving\nsettings. We demonstrate the efficacy of our solutions in several contexts,\nincluding DNN IP ownership, ethical LLM usage enforcement, and transformer\ninference."}
{"id": "2509.25113", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.25113", "abs": "https://arxiv.org/abs/2509.25113", "authors": ["Wai Ming Chan", "Remi Chou", "Taejoon Kim"], "title": "Two-Dimensional XOR-Based Secret Sharing for Layered Multipath Communication", "comment": null, "summary": "This paper introduces the first two-dimensional XOR-based secret sharing\nscheme for layered multipath communication networks. We present a construction\nthat guarantees successful message recovery and perfect privacy when an\nadversary observes and disrupts any single path at each transmission layer. The\nscheme achieves information-theoretic security using only bitwise XOR\noperations with linear $O(|S|)$ complexity, where $|S|$ is the message length.\nWe provide mathematical proofs demonstrating that the scheme maintains\nunconditional security regardless of computational resources available to\nadversaries. Unlike encryption-based approaches vulnerable to quantum computing\nadvances, our construction offers provable security suitable for\nresource-constrained military environments where computational assumptions may\nfail."}
