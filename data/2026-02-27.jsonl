{"id": "2602.22218", "categories": ["cs.CR", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.22218", "abs": "https://arxiv.org/abs/2602.22218", "authors": ["Ashim Mahara"], "title": "Cybersecurity Data Extraction from Common Crawl", "comment": null, "summary": "Alpha-Root is a cybersecurity-focused dataset collected in a single shot from the Common Crawl web graph using community detection. Unlike iterative content-scoring approaches like DeepSeekMath, we mine quality domains directly from the web graph, starting from just 20 trusted seed domains."}
{"id": "2602.22230", "categories": ["cs.CR", "cs.GT", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.22230", "abs": "https://arxiv.org/abs/2602.22230", "authors": ["Nimrod Talmon", "Haim Zysberg"], "title": "An Adaptive Multichain Blockchain: A Multiobjective Optimization Approach", "comment": null, "summary": "Blockchains are widely used for secure transaction processing, but their scalability remains limited, and existing multichain designs are typically static even as demand and capacity shift. We cast blockchain configuration as a multiagent resource-allocation problem: applications and operators declare demand, capacity, and price bounds; an optimizer groups them into ephemeral chains each epoch and sets a chain-level clearing price. The objective maximizes a governance-weighted combination of normalized utilities for applications, operators, and the system. The model is modular -- accommodating capability compatibility, application-type diversity, and epoch-to-epoch stability -- and can be solved off-chain with outcomes verifiable on-chain. We analyze fairness and incentive issues and present simulations that highlight trade-offs among throughput, decentralization, operator yield, and service stability."}
{"id": "2602.22237", "categories": ["cs.CR", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2602.22237", "abs": "https://arxiv.org/abs/2602.22237", "authors": ["Prasanna Kumar", "Nishank Soni", "Gaurang Munje"], "title": "Optimized Disaster Recovery for Distributed Storage Systems: Lightweight Metadata Architectures to Overcome Cryptographic Hashing Bottleneck", "comment": "8 pages, 7 Tables", "summary": "Distributed storage architectures are foundational to modern cloud-native infrastructure, yet a critical operational bottleneck persists within disaster recovery (DR) workflows: the dependence on content-based cryptographic hashing for data identification and synchronization. While hash-based deduplication is effective for storage efficiency in steady-state operation, it becomes a systemic liability during failover and failback events when hash indexes are stale, incomplete, or must be rebuilt following a crash. This paper precisely characterizes the operational conditions under which full or partial re-hashing becomes unavoidable. The paper also analyzes the downstream impact of cryptographic re-hashing on Recovery Time Objective (RTO) compliance, and proposes a generalized architectural shift toward deterministic, metadata-driven identification. The proposed framework assigns globally unique composite identifiers to data blocks at ingestion time-independent of content analysis enabling instantaneous delta computation during DR without any cryptographic overhead."}
{"id": "2602.22238", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22238", "abs": "https://arxiv.org/abs/2602.22238", "authors": ["Kyeongpil Min", "Sangmin Jeon", "Jae-Jin Lee", "Woojoo Lee"], "title": "TT-SEAL: TTD-Aware Selective Encryption for Adversarially-Robust and Low-Latency Edge AI", "comment": "8 pages, 7 figures, 3 tables. This paper has been accepted at Design Automation Conference (DAC) 2026", "summary": "Cloud-edge AI must jointly satisfy model compression and security under tight device budgets. While Tensor-Train Decomposition (TTD) shrinks on-device models, prior selective-encryption studies largely assume dense weights, leaving its practicality under TTD compression unclear. We present TT-SEAL, a selective-encryption framework for TT-decomposed networks. TT-SEAL ranks TT cores with a sensitivity-based importance metric, calibrates a one-time robustness threshold, and uses a value-DP optimizer to encrypt the minimum set of critical cores with AES. Under TTD-aware, transfer-based threat models (and on an FPGA-prototyped edge processor) TT-SEAL matches the robustness of full (black-box) encryption while encrypting as little as 4.89-15.92% of parameters across ResNet-18, MobileNetV2, and VGG-16, and drives the share of AES decryption in end-to-end latency to low single digits (e.g., 58% -> 2.76% on ResNet-18), enabling secure, low-latency edge AI."}
{"id": "2602.22242", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22242", "abs": "https://arxiv.org/abs/2602.22242", "authors": ["Piyush Jaiswal", "Aaditya Pratap", "Shreyansh Saraswati", "Harsh Kasyap", "Somanath Tripathy"], "title": "Analysis of LLMs Against Prompt Injection and Jailbreak Attacks", "comment": "12 pages, 5 figures, 6 tables", "summary": "Large Language Models (LLMs) are widely deployed in real-world systems. Given their broader applicability, prompt engineering has become an efficient tool for resource-scarce organizations to adopt LLMs for their own purposes. At the same time, LLMs are vulnerable to prompt-based attacks. Thus, analyzing this risk has become a critical security requirement. This work evaluates prompt-injection and jailbreak vulnerability using a large, manually curated dataset across multiple open-source LLMs, including Phi, Mistral, DeepSeek-R1, Llama 3.2, Qwen, and Gemma variants. We observe significant behavioural variation across models, including refusal responses and complete silent non-responsiveness triggered by internal safety mechanisms. Furthermore, we evaluated several lightweight, inference-time defence mechanisms that operate as filters without any retraining or GPU-intensive fine-tuning. Although these defences mitigate straightforward attacks, they are consistently bypassed by long, reasoning-heavy prompts."}
{"id": "2602.22244", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.22244", "abs": "https://arxiv.org/abs/2602.22244", "authors": ["Aurora Arrus", "Maria di Gisi", "Sara Lilli", "Marco Quadrini"], "title": "Accelerating Incident Response: A Hybrid Approach for Data Breach Reporting", "comment": null, "summary": "The General Data Protection Regulation (GDPR) requires organisations to notify supervisory authorities of personal data breaches within 72 hours of discovery. Meeting this strict deadline is challenging because incident responders must manually translate low-level forensic artefacts such as malware traces, system-call logs, and network captures into the structured, legally framed information required by data-protection authorities. This gap between technical evidence and regulatory reporting often results in delays, incomplete notifications, and a high cognitive burden on analysts. We propose a hybrid malware analysis pipeline that automates the extraction and organisation of breach-relevant information, with a particular focus on exfiltration-oriented Linux/ARM malware, which is rapidly increasing in prevalence due to the widespread adoption of IoT and embedded devices. The system combines static analysis to identify potential exfiltrators with dynamic analysis to reconstruct their behaviour. It employs a Large Language Model (LLM) constrained by a formal JSON schema aligned with the official Italian Garante Privacy notification form. The LLM transforms heterogeneous forensic artefacts into a structured, compliance-ready report that a human operator can rapidly validate."}
{"id": "2602.22246", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22246", "abs": "https://arxiv.org/abs/2602.22246", "authors": ["Guangnian Wan", "Qi Li", "Gongfan Fang", "Xinyin Ma", "Xinchao Wang"], "title": "Self-Purification Mitigates Backdoors in Multimodal Diffusion Language Models", "comment": null, "summary": "Multimodal Diffusion Language Models (MDLMs) have recently emerged as a competitive alternative to their autoregressive counterparts. Yet their vulnerability to backdoor attacks remains largely unexplored. In this work, we show that well-established data-poisoning pipelines can successfully implant backdoors into MDLMs, enabling attackers to manipulate model behavior via specific triggers while maintaining normal performance on clean inputs. However, defense strategies effective to these models are yet to emerge. To bridge this gap, we introduce a backdoor defense framework for MDLMs named DiSP (Diffusion Self-Purification). DiSP is driven by a key observation: selectively masking certain vision tokens at inference time can neutralize a backdoored model's trigger-induced behaviors and restore normal functionality. Building on this, we purify the poisoned dataset using the compromised model itself, then fine-tune the model on the purified data to recover it to a clean one. Given such a specific design, DiSP can remove backdoors without requiring any auxiliary models or clean reference data. Extensive experiments demonstrate that our approach effectively mitigates backdoor effects, reducing the attack success rate (ASR) from over 90% to typically under 5%, while maintaining model performance on benign tasks."}
{"id": "2602.22250", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.22250", "abs": "https://arxiv.org/abs/2602.22250", "authors": ["Morteza Eskandarian", "Mahdi Rabbani", "Arun Kaniyamattam", "Fatemeh Nejati", "Mansur Mirani", "Gunjan Piya", "Igor Opushnyev", "Ali A. Ghorbani", "Sajjad Dadkhah"], "title": "A Lightweight Defense Mechanism against Next Generation of Phishing Emails using Distilled Attention-Augmented BiLSTM", "comment": null, "summary": "The current generation of large language models produces sophisticated social-engineering content that bypasses standard text screening systems in business communication platforms. Our proposed solution for mail gateway and endpoint deception detection operates in a privacy-protective manner while handling the performance requirements of network and mobile security systems. The MobileBERT teacher receives fine-tuning before its transformation into a BiLSTM model with multi-head attention which maintains semantic discrimination only with 4.5 million parameters. The hybrid dataset contains human-written messages together with LLM-generated paraphrases that use masking techniques and personalization methods to enhance modern attack resistance. The evaluation system uses five testing protocols which include human-only and LLM-only tests and two cross-distribution transfer tests and a production-like mixed traffic test to assess performance in native environments and across different distribution types and combined traffic scenarios. The distilled model maintains a weighted-F1 score difference of 1-2.5 points compared to the mixture split results of strong transformer baselines including ModernBERT, DeBERTaV3-base, T5-base, DeepSeek-R1 Distill Qwen-1.5B and Phi-4 mini while achieving 80-95\\% faster inference times and 95-99\\% smaller model sizes. The system demonstrates excellent performance in terms of accuracy and latency while maintaining a compact size which enables real-time filtering without acceleration hardware and supports policy-based management. The paper examines system performance under high traffic conditions and security measures for privacy protection and implementation methods for operational deployment."}
{"id": "2602.22258", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22258", "abs": "https://arxiv.org/abs/2602.22258", "authors": ["Harrison Dahme"], "title": "Poisoned Acoustics", "comment": "5 Pages", "summary": "Training-data poisoning attacks can induce targeted, undetectable failure in deep neural networks by corrupting a vanishingly small fraction of training labels. We demonstrate this on acoustic vehicle classification using the MELAUDIS urban intersection dataset (approx. 9,600 audio clips, 6 classes): a compact 2-D convolutional neural network (CNN) trained on log-mel spectrograms achieves 95.7% Attack Success Rate (ASR) -- the fraction of target-class test samples misclassified under the attack -- on a Truck-to-Car label-flipping attack at just p=0.5% corruption (48 records), with zero detectable change in aggregate accuracy (87.6% baseline; 95% CI: 88-100%, n=3 seeds). We prove this stealth is structural: the maximum accuracy drop from a complete targeted attack is bounded above by the minority class fraction (beta). For real-world class imbalances (Truck approx. 3%), this bound falls below training-run noise, making aggregate accuracy monitoring provably insufficient regardless of architecture or attack method. A companion backdoor trigger attack reveals a novel trigger-dominance collapse: when the target class is a dataset minority, the spectrogram patch trigger becomes functionally redundant--clean ASR equals triggered ASR, and the attack degenerates to pure label flipping. We formalize the ML training pipeline as an attack surface and propose a trust-minimized defense combining content-addressed artifact hashing, Merkle-tree dataset commitment, and post-quantum digital signatures (ML-DSA-65/CRYSTALS-Dilithium3, NIST FIPS 204) for cryptographically verifiable data provenance."}
{"id": "2602.22282", "categories": ["cs.CR", "cs.LG", "stat.AP", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.22282", "abs": "https://arxiv.org/abs/2602.22282", "authors": ["Zilong Cao", "Xuan Bi", "Hai Zhang"], "title": "Differentially Private Truncation of Unbounded Data via Public Second Moments", "comment": null, "summary": "Data privacy is important in the AI era, and differential privacy (DP) is one of the golden solutions. However, DP is typically applicable only if data have a bounded underlying distribution. We address this limitation by leveraging second-moment information from a small amount of public data. We propose Public-moment-guided Truncation (PMT), which transforms private data using the public second-moment matrix and applies a principled truncation whose radius depends only on non-private quantities: data dimension and sample size. This transformation yields a well-conditioned second-moment matrix, enabling its inversion with a significantly strengthened ability to resist the DP noise. Furthermore, we demonstrate the applicability of PMT by using penalized and generalized linear regressions. Specifically, we design new loss functions and algorithms, ensuring that solutions in the transformed space can be mapped back to the original domain. We have established improvements in the models' DP estimation through theoretical error bounds, robustness guarantees, and convergence results, attributing the gains to the conditioning effect of PMT. Experiments on synthetic and real datasets confirm that PMT substantially improves the accuracy and stability of DP models."}
{"id": "2602.22427", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22427", "abs": "https://arxiv.org/abs/2602.22427", "authors": ["Idan Habler", "Vineeth Sai Narajala", "Stav Koren", "Amy Chang", "Tiffany Saade"], "title": "HubScan: Detecting Hubness Poisoning in Retrieval-Augmented Generation Systems", "comment": "11 pages, 5 figures, 2 tables, Github: https://github.com/cisco-ai-defense/adversarial-hubness-detector", "summary": "Retrieval-Augmented Generation (RAG) systems are essential to contemporary AI applications, allowing large language models to obtain external knowledge via vector similarity search. Nevertheless, these systems encounter a significant security flaw: hubness - items that frequently appear in the top-k retrieval results for a disproportionately high number of varied queries. These hubs can be exploited to introduce harmful content, alter search rankings, bypass content filtering, and decrease system performance.\n  We introduce hubscan, an open-source security scanner that evaluates vector indices and embeddings to identify hubs in RAG systems. Hubscan presents a multi-detector architecture that integrates: (1) robust statistical hubness detection utilizing median/MAD-based z-scores, (2) cluster spread analysis to assess cross-cluster retrieval patterns, (3) stability testing under query perturbations, and (4) domain-aware and modality-aware detection for category-specific and cross-modal attacks. Our solution accommodates several vector databases (FAISS, Pinecone, Qdrant, Weaviate) and offers versatile retrieval techniques, including vector similarity, hybrid search, and lexical matching with reranking capabilities.\n  We evaluate hubscan on Food-101, MS-COCO, and FiQA adversarial hubness benchmarks constructed using state-of-the-art gradient-optimized and centroid-based hub generation methods. hubscan achieves 90% recall at a 0.2% alert budget and 100% recall at 0.4%, with adversarial hubs ranking above the 99.8th percentile. Domain-scoped scanning recovers 100% of targeted attacks that evade global detection. Production validation on 1M real web documents from MS MARCO demonstrates significant score separation between clean documents and adversarial content. Our work provides a practical, extensible framework for detecting hubness threats in production RAG systems."}
{"id": "2602.22433", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.22433", "abs": "https://arxiv.org/abs/2602.22433", "authors": ["Refat Othman"], "title": "Predicting Known Vulnerabilities from Attack Descriptions Using Sentence Transformers", "comment": "PhD thesis, Free University of Bozen-Bolzano, 2026", "summary": "Modern infrastructures rely on software systems that remain vulnerable to cyberattacks. These attacks frequently exploit vulnerabilities documented in repositories such as MITRE's Common Vulnerabilities and Exposures (CVE). However, Cyber Threat Intelligence resources, including MITRE ATT&CK and CVE, provide only partial coverage of attack-vulnerability relationships. Attack information often appears before vulnerabilities are formally linked, creating the need for automated methods that infer likely vulnerabilities directly from attack descriptions.\n  This thesis addresses the problem of predicting known vulnerabilities from natural-language descriptions of cyberattacks. We develop transformer-based sentence embedding methods that encode attack and vulnerability descriptions into semantic vector representations, enabling similarity-based ranking and recommendation.\n  Fourteen state-of-the-art transformer models were evaluated across four attack description types (Tactic, Technique, Procedure, and Attack Pattern). Results show that Technique descriptions in MITRE ATT&CK provide the strongest predictive signal. The multi-qa-mpnet-base-dot-v1 (MMPNet) model achieved the best performance due to its hybrid pre-training and optimization for semantic similarity.\n  The approach was implemented in the VULDAT tool, which automatically links attacks to vulnerabilities. Manual validation revealed previously undocumented relationships in MITRE repositories. Evaluation on unseen cyberattack reports demonstrates that the models generalize beyond curated datasets and support proactive vulnerability awareness."}
{"id": "2602.22443", "categories": ["cs.CR", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.22443", "abs": "https://arxiv.org/abs/2602.22443", "authors": ["Alexander Benvenuti", "Brandon Fallin", "Calvin Hawkins", "Brendan Bialy", "Miriam Dennis", "Warren Dixon", "Matthew Hale"], "title": "Differentially Private Data-Driven Markov Chain Modeling", "comment": "4 figures, 22 pages", "summary": "Markov chains model a wide range of user behaviors. However, generating accurate Markov chain models requires substantial user data, and sharing these models without privacy protections may reveal sensitive information about the underlying user data. We introduce a method for protecting user data used to formulate a Markov chain model. First, we develop a method for privatizing database queries whose outputs are elements of the unit simplex, and we prove that this method is differentially private. We quantify its accuracy by bounding the expected KL divergence between private and non-private queries. We extend this method to privatize stochastic matrices whose rows are each a simplex-valued query of a database, which includes data-driven Markov chain models. To assess their accuracy, we analytically bound the change in the stationary distribution and the change in the convergence rate between a non-private Markov chain model and its private form. Simulations show that under a typical privacy implementation, our method yields less than 2% error in the stationary distribution, indicating that our approach to private modeling faithfully captures the behavior of the systems we study."}
{"id": "2602.22450", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22450", "abs": "https://arxiv.org/abs/2602.22450", "authors": ["Qianlong Lan", "Anuj Kaul", "Shaun Jones", "Stephanie Westrum"], "title": "Silent Egress: When Implicit Prompt Injection Makes LLM Agents Leak Without a Trace", "comment": null, "summary": "Agentic large language model systems increasingly automate tasks by retrieving URLs and calling external tools. We show that this workflow gives rise to implicit prompt injection: adversarial instructions embedded in automatically generated URL previews, including titles, metadata, and snippets, can introduce a system-level risk that we refer to as silent egress. Using a fully local and reproducible testbed, we demonstrate that a malicious web page can induce an agent to issue outbound requests that exfiltrate sensitive runtime context, even when the final response shown to the user appears harmless. In 480 experimental runs with a qwen2.5:7b-based agent, the attack succeeds with high probability (P (egress) =0.89), and 95% of successful attacks are not detected by output-based safety checks. We also introduce sharded exfiltration, where sensitive information is split across multiple requests to avoid detection. This strategy reduces single-request leakage metrics by 73% (Leak@1) and bypasses simple data loss prevention mechanisms. Our ablation results indicate that defenses applied at the prompt layer offer limited protection, while controls at the system and network layers, such as domain allowlisting and redirect-chain analysis, are considerably more effective. These findings suggest that network egress should be treated as a first-class security outcome in agentic LLM systems. We outline architectural directions, including provenance tracking and capability isolation, that go beyond prompt-level hardening."}
{"id": "2602.22488", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22488", "abs": "https://arxiv.org/abs/2602.22488", "authors": ["Nelly Elsayed"], "title": "Explainability-Aware Evaluation of Transfer Learning Models for IoT DDoS Detection Under Resource Constraints", "comment": "24 pages, under review", "summary": "Distributed denial-of-service (DDoS) attacks threaten the availability of Internet of Things (IoT) infrastructures, particularly under resource-constrained deployment conditions. Although transfer learning models have shown promising detection accuracy, their reliability, computational feasibility, and interpretability in operational environments remain insufficiently explored. This study presents an explainability-aware empirical evaluation of seven pre-trained convolutional neural network architectures for multi-class IoT DDoS detection using the CICDDoS2019 dataset and an image-based traffic representation. The analysis integrates performance metrics, reliability-oriented statistics (MCC, Youden Index, confidence intervals), latency and training cost assessment, and interpretability evaluation using Grad-CAM and SHAP. Results indicate that DenseNet and MobileNet-based architectures achieve strong detection performance while demonstrating superior reliability and compact, class-consistent attribution patterns. DenseNet169 offers the strongest reliability and interpretability alignment, whereas MobileNetV3 provides an effective latency-accuracy trade-off for fog-level deployment. The findings emphasize the importance of combining performance, reliability, and explainability criteria when selecting deep learning models for IoT DDoS detection."}
{"id": "2602.22525", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.22525", "abs": "https://arxiv.org/abs/2602.22525", "authors": ["Zhonghao Zhan", "Krinos Li", "Yefan Zhang", "Hamed Haddadi"], "title": "Systems-Level Attack Surface of Edge Agent Deployments on IoT", "comment": null, "summary": "Edge deployment of LLM agents on IoT hardware introduces attack surfaces absent from cloud-hosted orchestration. We present an empirical security analysis of three architectures (cloud-hosted, edge-local swarm, and hybrid) using a multi-device home-automation testbed with local MQTT messaging and an Android smartphone as an edge inference node. We identify five systems-level attack surfaces, including two emergent failures observed during live testbed operation: coordination-state divergence and induced trust erosion. We frame core security properties as measurable systems metrics: data egress volume, failover window exposure, sovereignty boundary integrity, and provenance chain completeness. Our measurements show that edge-local deployments eliminate routine cloud data exposure but silently degrade sovereignty when fallback mechanisms trigger, with boundary crossings invisible at the application layer. Provenance chains remain complete under cooperative operation yet are trivially bypassed without cryptographic enforcement. Failover windows create transient blind spots exploitable for unauthorised actuation. These results demonstrate that deployment architecture, not just model or prompt design, is a primary determinant of security risk in agent-controlled IoT systems."}
{"id": "2602.22562", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.22562", "abs": "https://arxiv.org/abs/2602.22562", "authors": ["Taoran Li", "Varun Chandrasekaran", "Zhiyuan Yu"], "title": "Layer-Targeted Multilingual Knowledge Erasure in Large Language Models", "comment": null, "summary": "Recent work has demonstrated that machine unlearning in Large Language Models (LLMs) fails to generalize across languages: knowledge erased in one language frequently remains accessible through others. However, the underlying cause of this failure and a principled solution remain open. In this work, we identify intervention depth as the key factor determining multilingual generalization. Through systematic layer-wise experiments, we characterize two distinct failure modes: shallow-layer interventions achieve erasure but collapse multilingual capabilities in held-out languages, while deep-layer interventions preserve utility but fail to erase target knowledge even in source languages. These findings reveal that the choice of intervention layer is not a free parameter; it fundamentally determines whether multilingual unlearning succeeds. We propose MUTE (Multilingual Unlearning via Targeted Erasure), a framework that uses Centered Kernel Alignment (CKA) and Linguistic Regions Development Score (LRDS) to identify intermediate, language-agnostic layers where cross-lingual representations converge. By restricting unlearning updates to these layers, MUTE achieves robust multilingual knowledge erasure while optimizing on only a small set of source languages. Extensive experiments across three LLM architectures and three unlearning algorithms validate our approach, with mechanistic analysis via Logit Lens probing confirming genuine knowledge removal rather than output-level suppression."}
{"id": "2602.22699", "categories": ["cs.CR", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22699", "abs": "https://arxiv.org/abs/2602.22699", "authors": ["Tomoya Matsumoto", "Shokichi Takakura", "Shun Takagi", "Satoshi Hasegawa"], "title": "DPSQL+: A Differentially Private SQL Library with a Minimum Frequency Rule", "comment": null, "summary": "SQL is the de facto interface for exploratory data analysis; however, releasing exact query results can expose sensitive information through membership or attribute inference attacks. Differential privacy (DP) provides rigorous privacy guarantees, but in practice, DP alone may not satisfy governance requirements such as the \\emph{minimum frequency rule}, which requires each released group (cell) to include contributions from at least $k$ distinct individuals. In this paper, we present \\textbf{DPSQL+}, a privacy-preserving SQL library that simultaneously enforces user-level $(\\varepsilon,Î´)$-DP and the minimum frequency rule. DPSQL+ adopts a modular architecture consisting of: (i) a \\emph{Validator} that statically restricts queries to a DP-safe subset of SQL; (ii) an \\emph{Accountant} that consistently tracks cumulative privacy loss across multiple queries; and (iii) a \\emph{Backend} that interfaces with various database engines, ensuring portability and extensibility. Experiments on the TPC-H benchmark demonstrate that DPSQL+ achieves practical accuracy across a wide range of analytical workloads -- from basic aggregates to quadratic statistics and join operations -- and allows substantially more queries under a fixed global privacy budget than prior libraries in our evaluation."}
{"id": "2602.22700", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22700", "abs": "https://arxiv.org/abs/2602.22700", "authors": ["Yanpei Guo", "Wenjie Qu", "Linyu Wu", "Shengfang Zhai", "Lionel Z. Wang", "Ming Xu", "Yue Liu", "Binhang Yuan", "Dawn Song", "Jiaheng Zhang"], "title": "IMMACULATE: A Practical LLM Auditing Framework via Verifiable Computation", "comment": null, "summary": "Commercial large language models are typically deployed as black-box API services, requiring users to trust providers to execute inference correctly and report token usage honestly. We present IMMACULATE, a practical auditing framework that detects economically motivated deviations-such as model substitution, quantization abuse, and token overbilling-without trusted hardware or access to model internals. IMMACULATE selectively audits a small fraction of requests using verifiable computation, achieving strong detection guarantees while amortizing cryptographic overhead. Experiments on dense and MoE models show that IMMACULATE reliably distinguishes benign and malicious executions with under 1% throughput overhead. Our code is published at https://github.com/guo-yanpei/Immaculate."}
{"id": "2602.22724", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22724", "abs": "https://arxiv.org/abs/2602.22724", "authors": ["Tian Zhang", "Yiwei Xu", "Juan Wang", "Keyan Guo", "Xiaoyang Xu", "Bowen Xiao", "Quanlong Guan", "Jinlin Fan", "Jiawei Liu", "Zhiquan Liu", "Hongxin Hu"], "title": "AgentSentry: Mitigating Indirect Prompt Injection in LLM Agents via Temporal Causal Diagnostics and Context Purification", "comment": "23 pages, 8 figures. Under review", "summary": "Large language model (LLM) agents increasingly rely on external tools and retrieval systems to autonomously complete complex tasks. However, this design exposes agents to indirect prompt injection (IPI), where attacker-controlled context embedded in tool outputs or retrieved content silently steers agent actions away from user intent. Unlike prompt-based attacks, IPI unfolds over multi-turn trajectories, making malicious control difficult to disentangle from legitimate task execution. Existing inference-time defenses primarily rely on heuristic detection and conservative blocking of high-risk actions, which can prematurely terminate workflows or broadly suppress tool usage under ambiguous multi-turn scenarios. We propose AgentSentry, a novel inference-time detection and mitigation framework for tool-augmented LLM agents. To the best of our knowledge, AgentSentry is the first inference-time defense to model multi-turn IPI as a temporal causal takeover. It localizes takeover points via controlled counterfactual re-executions at tool-return boundaries and enables safe continuation through causally guided context purification that removes attack-induced deviations while preserving task-relevant evidence. We evaluate AgentSentry on the \\textsc{AgentDojo} benchmark across four task suites, three IPI attack families, and multiple black-box LLMs. AgentSentry eliminates successful attacks and maintains strong utility under attack, achieving an average Utility Under Attack (UA) of 74.55 %, improving UA by 20.8 to 33.6 percentage points over the strongest baselines without degrading benign performance."}
{"id": "2602.23121", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23121", "abs": "https://arxiv.org/abs/2602.23121", "authors": ["C. Seas", "G. Fitzpatrick", "J. A. Hamilton", "M. C. Carlisle"], "title": "Automated Vulnerability Detection in Source Code Using Deep Representation Learning", "comment": null, "summary": "Each year, software vulnerabilities are discovered, which pose significant risks of exploitation and system compromise. We present a convolutional neural network model that can successfully identify bugs in C code. We trained our model using two complementary datasets: a machine-labeled dataset created by Draper Labs using three static analyzers and the NIST SATE Juliet human-labeled dataset designed for testing static analyzers. In contrast with the work of Russell et al. on these datasets, we focus on C programs, enabling us to specialize and optimize our detection techniques for this language. After removing duplicates from the dataset, we tokenize the input into 91 token categories. The category values are converted to a binary vector to save memory. Our first convolution layer is chosen so that the entire encoding of the token is presented to the filter. We use two convolution and pooling layers followed by two fully connected layers to classify programs into either a common weakness enumeration category or as ``clean.'' We obtain higher recall than prior work by Russell et al. on this dataset when requiring high precision. We also demonstrate on a custom Linux kernel dataset that we are able to find real vulnerabilities in complex code with a low false-positive rate."}
{"id": "2602.23167", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23167", "abs": "https://arxiv.org/abs/2602.23167", "authors": ["Shuang Liang", "Yang Hua", "Linshan Jiang", "Peishen Yan", "Tao Song", "Bin Yao", "Haibing Guan"], "title": "SettleFL: Trustless and Scalable Reward Settlement Protocol for Federated Learning on Permissionless Blockchains (Extended version)", "comment": null, "summary": "In open Federated Learning (FL) environments where no central authority exists, ensuring collaboration fairness relies on decentralized reward settlement, yet the prohibitive cost of permissionless blockchains directly clashes with the high-frequency, iterative nature of model training. Existing solutions either compromise decentralization or suffer from scalability bottlenecks due to linear on-chain costs. To address this, we present SettleFL, a trustless and scalable reward settlement protocol designed to minimize total economic friction by offering a family of two interoperable protocols. Leveraging a shared domain-specific circuit architecture, SettleFL offers two interoperable strategies: (1) a Commit-and-Challenge variant that minimizes on-chain costs via optimistic execution and dispute-driven arbitration, and (2) a Commit-with-Proof variant that guarantees instant finality through per-round validity proofs. This design allows the protocol to flexibly adapt to varying latency and cost constraints while enforcing rational robustness without trusted coordination. We conduct extensive experiments combining real FL workloads and controlled simulations. Results show that SettleFL remains practical when scaling to 800 participants, achieving substantially lower gas cost."}
{"id": "2602.23261", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.23261", "abs": "https://arxiv.org/abs/2602.23261", "authors": ["David Polzoni", "Tommaso Bianchi", "Mauro Conti"], "title": "Strengthening security and noise resistance in one-way quantum key distribution protocols through hypercube-based quantum walks", "comment": null, "summary": "Quantum Key Distribution (QKD) is a foundational cryptographic protocol that ensures information-theoretic security. However, classical protocols such as BB84, though favored for their simplicity, offer limited resistance to eavesdropping, and perform poorly under realistic noise conditions. Recent research has explored the use of discrete-time Quantum Walks (QWs) to enhance QKD schemes. In this work, we specifically focus on a one-way QKD protocol, where security depends exclusively on the underlying Quantum Walk (QW) topology, rather than the details of the protocol itself. Our paper introduces a novel protocol based on QWs over a hypercube topology and demonstrates that, under identical parameters, it provides significantly enhanced security and noise resistance compared to the circular topology (i.e., state-of-the-art), thereby strengthening protection against eavesdropping. Furthermore, we introduce an efficient and extensible simulation framework for one-way QKD protocols based on QWs, supporting both circular and hypercube topologies. Implemented with IBM's software development kit for quantum computing (i.e., Qiskit), our toolkit enables noise-aware analysis under realistic noise models. To support reproducibility and future developments, we release our entire simulation framework as open-source. This contribution establishes a foundation for the design of topology-aware QKD protocols that combine enhanced noise tolerance with topologically driven security."}
