<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 23]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [What Hard Tokens Reveal: Exploiting Low-confidence Tokens for Membership Inference Attacks against Large Language Models](https://arxiv.org/abs/2601.20885)
*Md Tasnim Jawad,Mingyan Xiao,Yanzhao Wu*

Main category: cs.CR

TL;DR: HT-MIA是一种针对大语言模型的成员推理攻击方法，通过分析低置信度（hard）token级别的概率改进来区分泛化和记忆，显著提升了攻击效果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用和隐私法规日益严格，保护LLMs中的数据隐私变得至关重要。现有的成员推理攻击方法主要依赖序列级别的聚合预测统计，无法区分泛化改进和记忆改进，导致攻击效果不佳。

Method: 提出HT-MIA方法，专注于捕获低置信度（hard）token级别的概率。通过比较微调目标模型和预训练参考模型在hard token上的token级别概率改进，分离出被先前MIA方法掩盖的强大且鲁棒的成员信号。

Result: 在领域特定的医疗数据集和通用基准测试上的广泛实验表明，HT-MIA在七个最先进的MIA基线方法中表现一致更优。同时研究了差分隐私训练作为对抗LLMs中MIA的有效防御机制。

Conclusion: HT-MIA框架确立了基于hard token的分析作为推进LLMs成员推理攻击和防御的最先进基础，为隐私保护提供了重要工具。

Abstract: With the widespread adoption of Large Language Models (LLMs) and increasingly stringent privacy regulations, protecting data privacy in LLMs has become essential, especially for privacy-sensitive applications. Membership Inference Attacks (MIAs) attempt to determine whether a specific data sample was included in the model training/fine-tuning dataset, posing serious privacy risks. However, most existing MIA techniques against LLMs rely on sequence-level aggregated prediction statistics, which fail to distinguish prediction improvements caused by generalization from those caused by memorization, leading to low attack effectiveness. To address this limitation, we propose a novel membership inference approach that captures the token-level probabilities for low-confidence (hard) tokens, where membership signals are more pronounced. By comparing token-level probability improvements at hard tokens between a fine-tuned target model and a pre-trained reference model, HT-MIA isolates strong and robust membership signals that are obscured by prior MIA approaches. Extensive experiments on both domain-specific medical datasets and general-purpose benchmarks demonstrate that HT-MIA consistently outperforms seven state-of-the-art MIA baselines. We further investigate differentially private training as an effective defense mechanism against MIAs in LLMs. Overall, our HT-MIA framework establishes hard-token based analysis as a state-of-the-art foundation for advancing membership inference attacks and defenses for LLMs.

</details>


### [2] [Towards Sensitivity-Aware Language Models](https://arxiv.org/abs/2601.20901)
*Dren Fazlija,Iyiola E. Olatunji,Daniel Kudenko,Sandipan Sikdar*

Main category: cs.CR

TL;DR: 本文形式化了LLM的敏感性感知概念，建立了其与差分隐私的理论联系，并提出了一种监督微调方法，使量化LLM在保持敏感信息的同时提升性能达21.7%。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在企业数据管理中的部署，确保模型不泄露敏感信息变得至关重要。虽然已有敏感性感知概念使LLM能遵循访问权限规则，但其与差分隐私等隐私概念的关系尚不明确，阻碍了实际应用。

Method: 1. 形式化敏感性感知概念；2. 理论建立其与差分隐私的联系；3. 开发监督微调方法，使4位量化LLM更具敏感性感知能力。

Result: 微调后的LLM性能提升高达21.7%，不仅显著超越基线模型，还在敏感性感知方面优于类似规模的全精度开源和商业模型。同时，模型在通用指令遵循、数学推理和常识推理等任务上的性能也得到保留。

Conclusion: 本文为LLM在企业数据管理中的安全部署提供了理论基础和实用方法，通过形式化敏感性感知概念并建立其与差分隐私的联系，同时开发出有效的微调方法，使量化LLM在保护敏感信息的同时保持良好性能。

Abstract: With LLMs increasingly deployed in corporate data management, it is crucial to ensure that these models do not leak sensitive information. In the context of corporate data management, the concept of sensitivity awareness has been introduced, enabling LLMs to adhere to predefined access rights rules. However, it remains unclear how sensitivity awareness relates to established notions of privacy, such as differential privacy (DP), thereby making it difficult to deploy meaningfully in real-world applications. In this work, we formalize the notion of sensitivity awareness and theoretically establish its connection to DP. Additionally, we develop a supervised fine-tuning recipe to make existing, four-bit quantized LLMs more sensitivity-aware. With a performance boost of up to 21.7%, the finetuned LLMs not only substantially improve over their baseline but also outperform other full-precision open-source and commercial models of similar size in achieving sensitivity awareness, demonstrating the effectiveness of our proposed approach. At the same time, our method also largely preserves the models' performance on other tasks, such as general instruction-following, mathematical, and common-sense reasoning.

</details>


### [3] [ICON: Intent-Context Coupling for Efficient Multi-Turn Jailbreak Attack](https://arxiv.org/abs/2601.20903)
*Xingwei Lin,Wenhao Lin,Sicong Cao,Jiahao Yu,Renke Huang,Lei Xue,Chunming Wu*

Main category: cs.CR

TL;DR: ICON是一个自动化多轮越狱框架，通过意图-上下文耦合现象，利用先验引导的语义路由构建权威风格上下文，结合分层优化策略，在8个SOTA LLMs上达到97.1%的平均攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有多轮越狱攻击方法存在效率低下问题，需要逐步构建上下文，且容易陷入次优区域。研究发现意图-上下文耦合现象：当恶意意图与语义一致的上下文模式耦合时，LLM的安全约束会显著放松。

Method: ICON框架：1）通过先验引导的语义路由将恶意意图路由到一致的上下文模式（如科学研究）；2）实例化为攻击提示序列，逐步构建权威风格上下文；3）采用分层优化策略，结合局部提示细化和全局上下文切换，避免陷入无效上下文。

Result: 在8个最先进的LLMs上进行实验，ICON实现了97.1%的平均攻击成功率（ASR），达到最先进水平。

Conclusion: ICON通过利用意图-上下文耦合现象和分层优化策略，有效解决了现有多轮越狱攻击的效率问题，显著提高了攻击成功率，揭示了LLM安全机制中的关键漏洞。

Abstract: Multi-turn jailbreak attacks have emerged as a critical threat to Large Language Models (LLMs), bypassing safety mechanisms by progressively constructing adversarial contexts from scratch and incrementally refining prompts. However, existing methods suffer from the inefficiency of incremental context construction that requires step-by-step LLM interaction, and often stagnate in suboptimal regions due to surface-level optimization. In this paper, we characterize the Intent-Context Coupling phenomenon, revealing that LLM safety constraints are significantly relaxed when a malicious intent is coupled with a semantically congruent context pattern. Driven by this insight, we propose ICON, an automated multi-turn jailbreak framework that efficiently constructs an authoritative-style context via prior-guided semantic routing. Specifically, ICON first routes the malicious intent to a congruent context pattern (e.g., Scientific Research) and instantiates it into an attack prompt sequence. This sequence progressively builds the authoritative-style context and ultimately elicits prohibited content. In addition, ICON incorporates a Hierarchical Optimization Strategy that combines local prompt refinement with global context switching, preventing the attack from stagnating in ineffective contexts. Experimental results across eight SOTA LLMs demonstrate the effectiveness of ICON, achieving a state-of-the-art average Attack Success Rate (ASR) of 97.1\%. Code is available at https://github.com/xwlin-roy/ICON.

</details>


### [4] [Robust Federated Learning for Malicious Clients using Loss Trend Deviation Detection](https://arxiv.org/abs/2601.20915)
*Deepthy K Bhaskar,Minimol B,Binu V P*

Main category: cs.CR

TL;DR: 提出FL-LTD框架，通过监控损失趋势而非梯度来检测恶意客户端，在非IID数据下实现轻量级、隐私保护的联邦学习防御


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临恶意客户端发送误导性更新的风险，现有防御机制依赖梯度检查、复杂相似性计算或密码操作，在非IID数据下引入额外开销且不稳定

Method: 提出FL-LTD框架，通过监控时间损失动态检测异常客户端，识别损失停滞或突变，结合短期记忆机制持续缓解先前标记的异常客户端，同时允许稳定参与者恢复信任

Result: 在非IID联邦MNIST设置下，FL-LTD显著增强鲁棒性，最终测试准确率达到0.84，而标准FedAvg在攻击下仅为0.41，计算和通信开销可忽略，保持稳定收敛

Conclusion: FL-LTD展示了基于损失监控在安全联邦学习中的有效性，无需客户端排除或访问敏感数据，提供轻量级、隐私保护的防御方案

Abstract: Federated Learning (FL) facilitates collaborative model training among distributed clients while ensuring that raw data remains on local devices.Despite this advantage, FL systems are still exposed to risks from malicious or unreliable participants. Such clients can interfere with the training process by sending misleading updates, which can negatively affect the performance and reliability of the global model. Many existing defense mechanisms rely on gradient inspection, complex similarity computations, or cryptographic operations, which introduce additional overhead and may become unstable under non-IID data distributions. In this paper, we propose the Federated Learning with Loss Trend Detection (FL-LTD), a lightweight and privacy-preserving defense framework that detects and mitigates malicious behavior by monitoring temporal loss dynamics rather than model gradients. The proposed approach identifies anomalous clients by detecting abnormal loss stagnation or abrupt loss fluctuations across communication rounds. To counter adaptive attackers, a short-term memory mechanism is incorporated to sustain mitigation for clients previously flagged as anomalous, while enabling trust recovery for stable participants. We evaluate FL-LTD on a non-IID federated MNIST setup under loss manipulation attacks. Experimental results demonstrate that the proposed method significantly enhances robustness, achieving a final test accuracy of 0.84, compared to 0.41 for standard FedAvg under attack. FL-LTD incurs negligible computational and communication overhead, maintains stable convergence, and avoids client exclusion or access to sensitive data, highlighting the effectiveness of loss-based monitoring for secure federated learning.

</details>


### [5] [FIPS 204-Compatible Threshold ML-DSA via Masked Lagrange Reconstruction](https://arxiv.org/abs/2601.20917)
*Leo Kao*

Main category: cs.CR

TL;DR: 提出掩码拉格朗日重构技术，实现任意阈值T的ML-DSA阈值签名，产生标准3.3KB签名，兼容未修改的FIPS 204验证实现。


<details>
  <summary>Details</summary>
Motivation: 现有阈值ML-DSA方案存在限制：Bienstock等人方案需要诚实多数和37-136轮通信；Celi等人方案支持恶意多数但仅限于T≤6。需要解决拉格朗日系数随q增长导致拒绝采样失败的问题。

Method: 使用掩码拉格朗日重构技术，解决ML-DSA阈值签名的三个关键挑战：1) 掩码后仍需通过拒绝采样；2) 保护r0-check防止密钥恢复；3) 保持Irwin-Hall非ce分布的安全性。设计了三种部署方案：P1（TEE辅助）、P2（完全分布式MPC）、P3（2PC辅助）。

Result: 实现了任意阈值T的ML-DSA阈值签名，产生标准3.3KB签名。P1方案3轮签名，P2方案8轮完全分布式，P3方案3-5轮且性能最佳（249ms）。成功率23-32%，匹配单签名者ML-DSA。

Conclusion: 掩码拉格朗日重构技术成功解决了ML-DSA阈值签名的关键挑战，提供了三种安全部署方案，在保持FIPS 204兼容性的同时实现了灵活的阈值设置。

Abstract: We present masked Lagrange reconstruction, a technique that enables threshold ML-DSA (FIPS 204) with arbitrary thresholds $T$ while producing standard 3.3 KB signatures verifiable by unmodified FIPS 204 implementations. Concurrent approaches have limitations: Bienstock et al. (ePrint 2025/1163) achieve arbitrary $T$ but require honest-majority and 37--136 rounds; Celi et al. (ePrint 2026/013) achieve dishonest-majority but are limited to $T \leq 6$. Our technique addresses the barrier that Lagrange coefficients grow as $Θ(q)$ for moderate $T$, making individual contributions too large for ML-DSA's rejection sampling.
  Unlike ECDSA threshold schemes where pairwise masks suffice for correctness, ML-DSA requires solving three additional challenges absent in prior work: (1) rejection sampling on $\|z\|_\infty$ must still pass after masking, (2) the $r_0$-check exposes $c s_2$ enabling key recovery if unprotected, and (3) the resulting Irwin-Hall nonce distribution must preserve EUF-CMA security. We solve all three.
  We instantiate this technique in three deployment profiles with full security proofs. Profile P1 (TEE-assisted) achieves 3-round signing with a trusted coordinator, with EUF-CMA security under Module-SIS. Profile P2 (fully distributed) eliminates hardware trust via MPC in 8 rounds, achieving UC security against malicious adversaries corrupting up to $n-1$ parties. Profile P3 (2PC-assisted) uses lightweight 2PC for the $r_0$-check in 3--5 rounds, achieving UC security under a 1-of-2 CP honest assumption with the best empirical performance (249ms).
  Our scheme requires $|S| \geq T+1$ signers and achieves success rates of 23--32\%, matching single-signer ML-DSA.

</details>


### [6] [What Are Brands Telling You About Smishing? A Cross-Industry Evaluation of Customer Guidance](https://arxiv.org/abs/2601.20999)
*Dev Vikesh Doshi,Mehjabeen Tasnim,Fernando Landeros,Chinthagumpala Muni Venkatesh,Daniel Timko,Muhammad Lutfor Rahman*

Main category: cs.CR

TL;DR: 对149个知名品牌的钓鱼短信（smishing）防范教育现状分析显示，品牌在定义说明、视频教程和报告机制方面存在显著不足，行业间术语和预防建议差异大，需标准化改进。


<details>
  <summary>Details</summary>
Motivation: 钓鱼短信攻击日益普遍，品牌通过网络安全教育向客户传达防范意识，但现有指导内容差异很大。本研究旨在调查知名品牌如何教育客户防范钓鱼短信，以及它们提供的预防和报告建议的实际状况。

Method: 对25个行业的149个知名品牌进行全面的内容分析，研究它们如何教育客户防范钓鱼短信，包括提供的定义、预防建议、报告机制和视频教程等内容。

Result: 研究发现显著的信息缺口：仅46%的品牌提及钓鱼短信定义，不到1%提供视频教程，仅50%提供报告指导。行业间术语、预防建议和报告机制差异明显，部分品牌推荐"忽略可疑消息"等可能无效的策略。

Conclusion: 研究为行业钓鱼短信防范教育现状建立了基准，指出了需要标准化的具体领域。建议品牌提供更系统化的教育内容，以增强客户对日益增长的钓鱼短信攻击的防范意识和保护能力。

Abstract: Phishing attacks through text, also known as smishing, are a prevalent type of social engineering tactic in which attackers impersonate brands to deceive victims into providing personal information and/or money. While smishing awareness and cyber education are a key method by which organizations communicate this awareness, the guidance itself varies widely. In this paper, we investigate the state of practice of how 149 well-known brands across 25 categories educate their customers about smishing and what smishing prevention and reporting advice they provide. After conducting a comprehensive content analysis of the brands, we identified significant gaps in the smishing-related information provided: only 46\% of the 149 brands mentioned the definition of smishing, less than 1\% had a video tutorial on smishing, and only 50\% of brands provided instructions on how to report. Our study highlights variation in terminology, prevention advice, and reporting mechanisms across industries, with some brands recommending potentially ineffective strategies such as "ignoring suspicious messages." These findings establish a baseline for understanding the current state of industry smishing awareness advice and provide specific areas where standardization improvements are needed. From our evaluation, we provide recommendations for brands on how to offer streamlined education to their respective customers on smishing for better awareness and protection against increasing smishing attacks.

</details>


### [7] [Adaptive and Robust Cost-Aware Proof of Quality for Decentralized LLM Inference Networks](https://arxiv.org/abs/2601.21189)
*Arther Tian,Alex Ding,Frank Chen,Simon Wu,Aaron Chan*

Main category: cs.CR

TL;DR: 本文扩展了成本感知的Proof of Quality机制，通过添加抗对抗的共识形成方法来抵御评估者异质性和恶意评分操纵，提高了去中心化LLM推理网络的激励对齐。


<details>
  <summary>Details</summary>
Motivation: 去中心化大语言模型推理网络需要轻量级机制来奖励高质量输出，但评估者异质性和恶意评分操纵会扭曲共识并削弱激励对齐，特别是在开放参与环境中。

Method: 扩展成本感知Proof of Quality机制，添加抗对抗共识形成方法，包括中位数、修剪均值等鲁棒聚合规则，以及基于偏差信号更新评估者权重的自适应信任加权共识。

Result: 鲁棒聚合提高了共识与地面真值代理的对齐度，降低了对噪声和策略攻击的敏感性；评估者采样存在操作权衡，更大的评估者集合会减少评估者奖励并增加支付方差。

Conclusion: 鲁棒共识应作为成本感知Proof of Quality的默认组件，研究结果为在对抗风险和资源约束下选择评估者采样参数提供了实用指导。

Abstract: Decentralized large language model inference networks require lightweight mechanisms to reward high quality outputs under heterogeneous latency and cost. Proof of Quality provides scalable verification by sampling evaluator nodes that score candidate outputs, then aggregating their scores into a consensus signal that determines rewards. However, evaluator heterogeneity and malicious score manipulation can distort consensus and inflate payouts, which weakens incentive alignment in open participation settings.
  This paper extends a cost-aware Proof of Quality mechanism by adding adversary-resilient consensus formation. We study robust aggregation rules, including median and trimmed mean, and an adaptive trust-weighted consensus that updates evaluator weights from deviation signals. Using question answering and summarization workloads with a ground truth proxy for offline analysis, we quantify evaluator reliability and show strong variance across evaluators, including task-dependent misalignment that can invert correlations. We then evaluate robustness under four adversarial strategies, including noise injection, boosting, sabotage, and intermittent manipulation, across a sweep of malicious ratios and evaluator sample sizes. Our results show that robust aggregation improves consensus alignment with the ground truth proxy and reduces sensitivity to noisy and strategic attacks compared with simple averaging. We further characterize the operational trade-off introduced by evaluator sampling, where larger evaluator sets reduce evaluator rewards and increase payoff variance while inference rewards remain relatively stable in our configuration. These findings motivate robust consensus as a default component for cost-aware Proof of Quality and provide practical guidance for selecting evaluator sampling parameters under adversarial risk and resource constraints.

</details>


### [8] [SPOILER-GUARD: Gating Latency Effects of Memory Accesses through Randomized Dependency Prediction](https://arxiv.org/abs/2601.21211)
*Gayathri Subramanian,Girinath P,Nitya Ranganathan,Kamakoti Veezhinathan,Gopalakrishnan Srinivasan*

Main category: cs.CR

TL;DR: SPOILER-GUARD：一种硬件防御机制，通过动态随机化物理地址位和标记存储条目来防止SPOILER攻击，减少误预测至0.0004%，性能提升2-3%，硬件开销极小。


<details>
  <summary>Details</summary>
Motivation: 现代微处理器依赖推测执行，存在瞬态执行攻击漏洞。现有防御主要针对推测数据泄漏，但忽略了部分地址别名导致的虚假依赖问题，这种依赖通过重复的squash和reissue事件增加加载-存储延迟，被SPOILER攻击利用。

Method: 提出SPOILER-GUARD硬件防御机制：1）动态随机化用于加载-存储比较的物理地址位，混淆推测依赖解析；2）标记存储条目以防止延迟放大的误预测。在gem5中实现，使用SPEC 2017进行评估。

Result: 1）误预测率降至0.0004%；2）整数和浮点性能分别提升2.12%和2.87%；3）在14nm节点合成显示极小开销：关键路径延迟69ps，面积0.064mm²，功耗5.863mW。

Conclusion: SPOILER-GUARD有效防御了SPOILER攻击，通过硬件机制解决了部分地址别名导致的虚假依赖问题，在极小的硬件开销下显著降低了误预测并提升了性能。

Abstract: Modern microprocessors depend on speculative execution, creating vulnerabilities that enable transient execution attacks. Prior defenses target speculative data leakage but overlook false dependencies from partial address aliasing, where repeated squash and reissue events increase the load-store latency, which is exploited by the SPOILER attack. We present SPOILER-GUARD, a hardware defense that obfuscates speculative dependency resolution by dynamically randomizing the physical address bits used for load-store comparisons and tagging store entries to prevent latency-amplifying misspeculations. Implemented in gem5 and evaluated with SPEC 2017, SPOILER-GUARD reduces misspeculation to 0.0004 percent and improves integer and floating-point performance by 2.12 and 2.87 percent. HDL synthesis with Synopsys Design Compiler at 14 nm node demonstrates minimal overheads - 69 ps latency in critical path, 0.064 square millimeter in area, and 5.863 mW in power.

</details>


### [9] [Lossless Copyright Protection via Intrinsic Model Fingerprinting](https://arxiv.org/abs/2601.21252)
*Lingxiao Chen,Liqin Wang,Wei Lu,Xiangyang Luo*

Main category: cs.CR

TL;DR: TrajPrint是一种无损、免训练的扩散模型版权保护框架，通过提取确定性生成过程中形成的独特流形指纹来验证模型版权，支持黑盒API场景。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的卓越性能使其成为高价值知识产权，但也面临未经授权复制的风险。现有保护方法要么通过修改模型嵌入水印（损害性能），要么通过操纵去噪过程提取模型指纹（与黑盒API不兼容）。

Method: 1) 使用水印图像作为锚点，精确回溯到其轨迹原点，锁定该路径映射的模型指纹；2) 采用双端锚定的联合优化策略合成特定指纹噪声，严格遵循目标流形以实现鲁棒水印恢复；3) 通过原子推理和统计假设检验进行验证。

Result: TrajPrint在黑盒API场景中实现了无损验证，对模型修改具有优越的鲁棒性。目标模型能够恢复水印图像，而非目标模型则无法恢复。

Conclusion: TrajPrint提出了一种完全无损且免训练的框架，通过提取扩散模型确定性生成过程中形成的独特流形指纹来验证版权，解决了现有方法损害性能或与黑盒API不兼容的问题。

Abstract: The exceptional performance of diffusion models establishes them as high-value intellectual property but exposes them to unauthorized replication. Existing protection methods either modify the model to embed watermarks, which impairs performance, or extract model fingerprints by manipulating the denoising process, rendering them incompatible with black-box APIs. In this paper, we propose TrajPrint, a completely lossless and training-free framework that verifies model copyright by extracting unique manifold fingerprints formed during deterministic generation. Specifically, we first utilize a watermarked image as an anchor and exactly trace the path back to its trajectory origin, effectively locking the model fingerprint mapped by this path. Subsequently, we implement a joint optimization strategy that employs dual-end anchoring to synthesize a specific fingerprint noise, which strictly adheres to the target manifold for robust watermark recovery. As input, it enables the protected target model to recover the watermarked image, while failing on non-target models. Finally, we achieved verification via atomic inference and statistical hypothesis testing. Extensive experiments demonstrate that TrajPrint achieves lossless verification in black-box API scenarios with superior robustness against model modifications.

</details>


### [10] [Virtualization-based Penetration Testing Study for Detecting Accessibility Abuse Vulnerabilities in Banking Apps in East and Southeast Asia](https://arxiv.org/abs/2601.21258)
*Wei Minn,Phong Phan,Vikas K. Malviya,Benjamin Adolphi,Yan Naing Tun,Henning Benzon Treichl,Albert Ching,Lwin Khin Shar,David Lo*

Main category: cs.CR

TL;DR: 该研究分析了FjordPhantom恶意软件如何利用虚拟化和钩子技术绕过安卓银行应用的安全检测，通过实证研究评估了东亚和东南亚地区银行应用的易受攻击性，并提出了检测和防御方法。


<details>
  <summary>Details</summary>
Motivation: 安卓银行应用虽然方便了金融管理，但也吸引了网络犯罪分子利用安全漏洞窃取敏感财务数据。FjordPhantom恶意软件通过虚拟化和钩子技术绕过恶意无障碍服务的检测，对东亚和东南亚地区的银行应用构成严重威胁，需要研究其攻击机制和防御方法。

Method: 研究进行了实证研究，分析该地区银行应用对FjordPhantom的易受攻击性；评估了这些应用中现有保护措施的有效性；通过识别和缓解该恶意软件利用的漏洞，讨论了检测和预防此类攻击的方法。

Result: 研究发现FjordPhantom能够通过欺骗用户安装恶意组件并激活恶意无障碍服务，成功绕过安全检测，进行键盘记录、屏幕抓取和未经授权的数据访问。该恶意软件主要影响东亚和东南亚地区的银行和金融应用。

Conclusion: 研究强调了安卓银行应用面临的新型威胁，提出了通过识别漏洞和加强安全措施来检测和预防FjordPhantom类攻击的方法，为银行应用开发者提供了安全改进的方向。

Abstract: Android banking applications have revolutionized financial management by allowing users to perform various financial activities through mobile devices. However, this convenience has attracted cybercriminals who exploit security vulnerabilities to access sensitive financial data. FjordPhantom, a malware identified by our industry collaborator, uses virtualization and hooking to bypass the detection of malicious accessibility services, allowing it to conduct keylogging, screen scraping, and unauthorized data access. This malware primarily affects banking and finance apps across East and Southeast Asia region where our industry partner's clients are primarily based in. It requires users to be deceived into installing a secondary malicious component and activating a malicious accessibility service. In our study, we conducted an empirical study on the susceptibility of banking apps in the region to FjordPhantom, analyzed the effectiveness of protective measures currently implemented in those apps, and discussed ways to detect and prevent such attacks by identifying and mitigating the vulnerabilities exploited by this malware.

</details>


### [11] [User-Centric Phishing Detection: A RAG and LLM-Based Approach](https://arxiv.org/abs/2601.21261)
*Abrar Hamed Al Barwani,Abdelaziz Amara Korba,Raja Waseem Anwar*

Main category: cs.CR

TL;DR: 提出一个结合LLM与检索增强生成(RAG)的个性化钓鱼邮件检测框架，通过检索用户历史邮件和实时威胁情报来提升检测精度，显著降低误报率。


<details>
  <summary>Details</summary>
Motivation: 传统规则和机器学习方法难以应对日益复杂的钓鱼邮件，而单独使用LLM作为分类器会产生高误报率，给运营带来负担，需要更精准的个性化检测方案。

Method: 构建个性化钓鱼检测框架：为每条消息检索用户历史合法邮件形成用户特定上下文，结合实时域名和URL信誉威胁情报，使用RAG增强LLM决策。评估了Llama4-Scout等四种开源LLM。

Result: 在公开和机构邮件数据集上评估显示高性能，Llama4-Scout达到F1分数0.9703，RAG使误报率降低66.7%，验证了RAG用户画像方法的有效性。

Conclusion: 基于RAG的用户画像方法可行且有效，能够构建高精度、低摩擦的邮件安全系统，适应个人通信模式，显著降低误报率。

Abstract: The escalating sophistication of phishing emails necessitates a shift beyond traditional rule-based and conventional machine-learning-based detectors. Although large language models (LLMs) offer strong natural language understanding, using them as standalone classifiers often yields elevated falsepositive (FP) rates, which mislabel legitimate emails as phishing and create significant operational burden. This paper presents a personalized phishing detection framework that integrates LLMs with retrieval-augmented generation (RAG). For each message, the system constructs user-specific context by retrieving a compact set of the user's historical legitimate emails and enriching it with real-time domain and URL reputation from a cyber-threat intelligence platform, then conditions the LLM's decision on this evidence. We evaluate four open-source LLMs (Llama4-Scout, DeepSeek-R1, Mistral-Saba, and Gemma2) on an email dataset collected from public and institutional sources. Results show high performance; for example, Llama4-Scout attains an F1-score of 0.9703 and achieves a 66.7% reduction in FPs with RAG. These findings validate that a RAG-based, user-profiling approach is both feasible and effective for building high-precision, low-friction email security systems that adapt to individual communication patterns.

</details>


### [12] [Towards Zero Rotation and Beyond: Architecting Neural Networks for Fast Secure Inference with Homomorphic Encryption](https://arxiv.org/abs/2601.21287)
*Yifei Cai,Yizhou Feng,Qiao Zhang,Chunsheng Xin,Hongyi Wu*

Main category: cs.CR

TL;DR: StriaNet：针对同态加密优化的神经网络架构，通过消除外部旋转和减少内部旋转，在保持精度的同时实现6-10倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有隐私保护深度学习使用同态加密（HE）进行线性计算，但计算开销巨大。大多数方法基于明文推理设计的模型，这些模型在适应HE时存在架构低效问题。需要专门为HE设计的网络而非改造明文架构。

Method: 提出两个核心组件：1) StriaBlock：针对最昂贵的HE操作（旋转），集成ExRot-Free卷积和新型Cross Kernel，消除外部旋转，内部旋转仅需明文模型的19%；2) 架构原则：聚焦约束原则（限制成本敏感因素）和通道打包感知缩放原则（根据深度调整瓶颈比以适应密文通道容量）。

Result: 在ImageNet、Tiny ImageNet和CIFAR-10数据集上，StriaNet在保持相当精度的情况下，分别实现了9.78倍、6.01倍和9.24倍的加速。

Conclusion: 通过专门为同态加密设计的网络架构，而非改造明文模型，可以显著提升隐私保护深度学习的效率。StriaNet展示了通过优化HE特定操作和架构原则，在保持精度的同时实现数量级加速的可行性。

Abstract: Privacy-preserving deep learning addresses privacy concerns in Machine Learning as a Service (MLaaS) by using Homomorphic Encryption (HE) for linear computations. However, the computational overhead remains a major challenge. While prior work has improved efficiency, most approaches build on models originally designed for plaintext inference. Such models incur architectural inefficiencies when adapted to HE. We argue that substantial gains require networks tailored to HE rather than retrofitting plaintext architectures. Our design has two components: the building block and the overall architecture. First, StriaBlock targets the most expensive HE operation, rotation. It integrates ExRot-Free Convolution and a novel Cross Kernel, eliminating external rotations and requiring only 19% of the internal rotations used by plaintext models. Second, our architectural principles include (i) the Focused Constraint Principle, which limits cost-sensitive factors while preserving flexibility elsewhere, and (ii) the Channel Packing-Aware Scaling Principle, which adapts bottleneck ratios to ciphertext channel capacity that varies with depth. Together, these strategies control both local and end-to-end HE cost, enabling a balanced HE-tailored network. We evaluate the resulting StriaNet across datasets of varying scales, including ImageNet, Tiny ImageNet, and CIFAR-10. At comparable accuracy, StriaNet achieves speedups of 9.78x, 6.01x, and 9.24x on ImageNet, Tiny ImageNet, and CIFAR-10, respectively.

</details>


### [13] [SecIC3: Customizing IC3 for Hardware Security Verification](https://arxiv.org/abs/2601.21353)
*Qinhan Tan,Akash Gaonkar,Yu-Wei Fan,Aarti Gupta,Sharad Malik*

Main category: cs.CR

TL;DR: SecIC3：一种利用自组合结构进行硬件安全验证的IC3模型检查算法，通过对称状态探索和等价谓词技术显著提升非干扰性检查效率


<details>
  <summary>Details</summary>
Motivation: 现有硬件安全验证方法将非干扰超属性转换为自组合设计的安全属性检查，但缺乏针对这种特殊结构的优化模型检查器，导致验证效率不高

Method: 基于IC3算法开发SecIC3，利用自组合结构特点，采用对称状态探索和添加等价谓词两种互补技术来优化验证过程

Result: 在10个设计的非干扰检查基准测试中，SecIC3显著减少了安全证明时间，相比基线实现最高达到49.3倍的证明加速

Conclusion: SecIC3通过专门针对自组合结构优化的IC3算法，有效提升了硬件安全验证的效率，为硬件信息流安全验证提供了更高效的解决方案

Abstract: Recent years have seen significant advances in using formal verification to check hardware security properties. Of particular practical interest are checking confidentiality and integrity of secrets, by checking that there is no information flow between the secrets and observable outputs. A standard method for checking information flow is to translate the corresponding non-interference hyperproperty into a safety property on a self-composition of the design, which has two copies of the design composed together. Although prior efforts have aimed to reduce the size of the self-composed design, there are no state-of-the-art model checkers that exploit their special structure for hardware security verification. In this paper, we propose SecIC3, a hardware model checking algorithm based on IC3 that is customized to exploit this self-composition structure. SecIC3 utilizes this structure in two complementary techniques: symmetric state exploration and adding equivalence predicates. We implement SecIC3 on top of two open-source IC3 implementations and evaluate it on a non-interference checking benchmark consisting of 10 designs. The experiment results show that SecIC3 significantly reduces the time for finding security proofs, with up to 49.3x proof speedup compared to baseline implementations.

</details>


### [14] [RerouteGuard: Understanding and Mitigating Adversarial Risks for LLM Routing](https://arxiv.org/abs/2601.21380)
*Wenhui Zhang,Huiyu Xu,Zhibo Wang,Zhichao Li,Zeqing He,Xuelin Wei,Kui Ren*

Main category: cs.CR

TL;DR: 本文系统研究了LLM路由系统中的对抗性重路由攻击，提出了威胁分类、测量研究，并开发了RerouteGuard防御框架，能有效检测和防御此类攻击。


<details>
  <summary>Details</summary>
Motivation: 多模型AI系统中的LLM路由器虽然能降低计算成本，但作为分类器容易受到对抗性攻击。攻击者可以通过在查询前添加特制触发器来操纵路由决策，导致计算成本增加、响应质量下降甚至绕过安全防护，但这些安全影响尚未得到充分研究。

Method: 1) 基于攻击者目标（成本提升、质量劫持、安全绕过）和知识对LLM重路由威胁进行系统分类；2) 对现有LLM路由系统进行测量研究；3) 使用可解释性技术分析现有攻击方法；4) 提出RerouteGuard防御框架，采用动态嵌入检测和自适应阈值来过滤对抗性重路由提示。

Result: 测量研究发现现有路由系统容易受到重路由攻击，特别是在成本提升场景中。RerouteGuard在三种攻击设置和四个基准测试中实现了超过99%的检测准确率，同时对合法查询影响极小。

Conclusion: LLM重路由攻击对多模型AI系统构成严重威胁，但通过RerouteGuard这样的防御框架可以有效缓解这些风险。该研究为保护多模型AI系统免受对抗性重路由攻击提供了原则性和实用的解决方案。

Abstract: Recent advancements in multi-model AI systems have leveraged LLM routers to reduce computational cost while maintaining response quality by assigning queries to the most appropriate model. However, as classifiers, LLM routers are vulnerable to novel adversarial attacks in the form of LLM rerouting, where adversaries prepend specially crafted triggers to user queries to manipulate routing decisions. Such attacks can lead to increased computational cost, degraded response quality, and even bypass safety guardrails, yet their security implications remain largely underexplored. In this work, we bridge this gap by systematizing LLM rerouting threats based on the adversary's objectives (i.e., cost escalation, quality hijacking, and safety bypass) and knowledge. Based on the threat taxonomy, we conduct a measurement study of real-world LLM routing systems against existing LLM rerouting attacks. The results reveal that existing routing systems are vulnerable to rerouting attacks, especially in the cost escalation scenario. We then characterize existing rerouting attacks using interpretability techniques, revealing that they exploit router decision boundaries through confounder gadgets that prepend queries to force misrouting. To mitigate these risks, we introduce RerouteGuard, a flexible and scalable guardrail framework for LLM rerouting. RerouteGuard filters adversarial rerouting prompts via dynamic embedding-based detection and adaptive thresholding. Extensive evaluations in three attack settings and four benchmarks demonstrate that RerouteGuard achieves over 99% detection accuracy against state-of-the-art rerouting attacks, while maintaining negligible impact on legitimate queries. The experimental results indicate that RerouteGuard offers a principled and practical solution for safeguarding multi-model AI systems against adversarial rerouting.

</details>


### [15] [On the Adversarial Robustness of Large Vision-Language Models under Visual Token Compression](https://arxiv.org/abs/2601.21531)
*Xinwei Zhang,Hangcheng Liu,Li Bai,Hao Wang,Qingqing Ye,Tianwei Zhang,Haibo Hu*

Main category: cs.CR

TL;DR: 本文提出CAGE攻击方法，针对视觉语言模型中的视觉令牌压缩机制，通过对齐扰动优化与压缩推理，揭示了现有攻击方法高估压缩模型鲁棒性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于编码器的攻击方法在评估压缩视觉语言模型的对抗鲁棒性时存在严重高估，原因是存在优化-推理不匹配：扰动在完整令牌表示上优化，而推理通过令牌压缩瓶颈进行。

Method: 提出CAGE攻击方法，包含两个核心组件：(1) 预期特征破坏：将失真集中在可能在不同预算下存活的令牌上；(2) 秩失真对齐：主动将令牌失真与秩分数对齐，以促进高度失真证据的保留。

Result: 在多种代表性的即插即用压缩机制和数据集上，CAGE始终比基线方法获得更低的鲁棒准确率，证明了现有鲁棒性评估忽略压缩机制会过于乐观。

Conclusion: 本文揭示了忽略压缩机制的鲁棒性评估存在严重缺陷，呼吁对高效视觉语言模型进行压缩感知的安全评估和防御设计。

Abstract: Visual token compression is widely used to accelerate large vision-language models (LVLMs) by pruning or merging visual tokens, yet its adversarial robustness remains unexplored. We show that existing encoder-based attacks can substantially overestimate the robustness of compressed LVLMs, due to an optimization-inference mismatch: perturbations are optimized on the full-token representation, while inference is performed through a token-compression bottleneck. To address this gap, we propose the Compression-AliGnEd attack (CAGE), which aligns perturbation optimization with compression inference without assuming access to the deployed compression mechanism or its token budget. CAGE combines (i) expected feature disruption, which concentrates distortion on tokens likely to survive across plausible budgets, and (ii) rank distortion alignment, which actively aligns token distortions with rank scores to promote the retention of highly distorted evidence. Across diverse representative plug-and-play compression mechanisms and datasets, our results show that CAGE consistently achieves lower robust accuracy than the baseline. This work highlights that robustness assessments ignoring compression can be overly optimistic, calling for compression-aware security evaluation and defenses for efficient LVLMs.

</details>


### [16] [ICL-EVADER: Zero-Query Black-Box Evasion Attacks on In-Context Learning and Their Defenses](https://arxiv.org/abs/2601.21586)
*Ningyuan He,Ronghong Huang,Qianqian Tang,Hongyu Wang,Xianghang Mi,Shanqing Guo*

Main category: cs.CR

TL;DR: 本文提出了ICL-Evader，一种针对大语言模型上下文学习的黑盒规避攻击框架，在零查询威胁模型下显著降低分类器性能，并提出了有效的防御方案。


<details>
  <summary>Details</summary>
Motivation: 上下文学习已成为文本分类的强大数据高效范式，但其对现实对抗威胁的鲁棒性尚未充分探索。作者旨在揭示ICL的潜在安全漏洞，并在高度实用的威胁模型下评估其脆弱性。

Method: 提出了ICL-Evader攻击框架，包含三种新型攻击：Fake Claim、Template和Needle-in-a-Haystack，利用LLM处理上下文提示的固有局限性。在情感分析、毒性和非法推广任务上进行评估，并系统研究防御策略。

Result: 攻击显著降低了分类器性能（攻击成功率高达95.3%），远超传统NLP攻击。同时发现了联合防御方案，能有效缓解所有攻击且效用损失最小（准确率下降<5%）。

Conclusion: 这项工作全面评估了ICL的安全性，揭示了关键漏洞，并为构建更鲁棒的系统提供了实用解决方案。开发了自动化工具来主动加固标准ICL提示，对抗对抗性规避。

Abstract: In-context learning (ICL) has become a powerful, data-efficient paradigm for text classification using large language models. However, its robustness against realistic adversarial threats remains largely unexplored. We introduce ICL-Evader, a novel black-box evasion attack framework that operates under a highly practical zero-query threat model, requiring no access to model parameters, gradients, or query-based feedback during attack generation. We design three novel attacks, Fake Claim, Template, and Needle-in-a-Haystack, that exploit inherent limitations of LLMs in processing in-context prompts. Evaluated across sentiment analysis, toxicity, and illicit promotion tasks, our attacks significantly degrade classifier performance (e.g., achieving up to 95.3% attack success rate), drastically outperforming traditional NLP attacks which prove ineffective under the same constraints. To counter these vulnerabilities, we systematically investigate defense strategies and identify a joint defense recipe that effectively mitigates all attacks with minimal utility loss (<5% accuracy degradation). Finally, we translate our defensive insights into an automated tool that proactively fortifies standard ICL prompts against adversarial evasion. This work provides a comprehensive security assessment of ICL, revealing critical vulnerabilities and offering practical solutions for building more robust systems. Our source code and evaluation datasets are publicly available at: https://github.com/ChaseSecurity/ICL-Evader .

</details>


### [17] [Noise as a Probe: Membership Inference Attacks on Diffusion Models Leveraging Initial Noise](https://arxiv.org/abs/2601.21628)
*Puwei Lian,Yujun Cai,Songze Li,Bingkun Bao*

Main category: cs.CR

TL;DR: 该论文提出了一种针对微调扩散模型的成员推理攻击方法，利用噪声调度中残留的语义信息来推断样本是否属于训练数据。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成方面取得显著进展，但其部署引发了隐私担忧。微调模型尤其脆弱，因为它们通常在小型私有数据集上微调。现有成员推理攻击要么需要获取中间结果，要么需要辅助数据集训练影子模型，存在局限性。

Method: 利用噪声调度中残留语义信息的漏洞：即使最大噪声步长下，图像中仍存在残留语义信号。将语义信息注入初始噪声，通过分析模型生成结果来推断成员关系。

Result: 实验证明，语义初始噪声能强烈揭示成员信息，突显扩散模型对成员推理攻击的脆弱性。

Conclusion: 扩散模型存在严重隐私风险，当前噪声调度未能完全消除语义信息，导致残留语义信号可被用于成员推理攻击。这为扩散模型的隐私保护提出了重要警示。

Abstract: Diffusion models have achieved remarkable progress in image generation, but their increasing deployment raises serious concerns about privacy. In particular, fine-tuned models are highly vulnerable, as they are often fine-tuned on small and private datasets. Membership inference attacks (MIAs) are used to assess privacy risks by determining whether a specific sample was part of a model's training data. Existing MIAs against diffusion models either assume obtaining the intermediate results or require auxiliary datasets for training the shadow model. In this work, we utilized a critical yet overlooked vulnerability: the widely used noise schedules fail to fully eliminate semantic information in the images, resulting in residual semantic signals even at the maximum noise step. We empirically demonstrate that the fine-tuned diffusion model captures hidden correlations between the residual semantics in initial noise and the original images. Building on this insight, we propose a simple yet effective membership inference attack, which injects semantic information into the initial noise and infers membership by analyzing the model's generation result. Extensive experiments demonstrate that the semantic initial noise can strongly reveal membership information, highlighting the vulnerability of diffusion models to MIAs.

</details>


### [18] [Authenticated encryption for space telemetry](https://arxiv.org/abs/2601.21657)
*Andrew Savchenko*

Main category: cs.CR

TL;DR: 该论文提出了一种轻量级认证加密方案，用于满足NASA-STD-1006A标准中应急空间遥测的命令栈保护要求，在资源受限环境中提供强安全性而不牺牲性能。


<details>
  <summary>Details</summary>
Motivation: 满足NASA-STD-1006A标准对应急空间遥测命令栈保护的要求，在资源受限的航天环境中平衡安全需求与操作约束，保护遥测数据的机密性、完整性和真实性。

Method: 提出轻量级认证加密实现方案，该方案产生固定长度消息以保持与底层数据传输协议的兼容性，专注于可预测属性和强健认证机制。

Result: 该方案能够在资源受限环境中提供强安全性而不牺牲性能，保护应急通信中遥测数据的机密性、完整性和真实性，同时保持与现有协议的兼容性。

Conclusion: 轻量级认证加密方案能够有效满足NASA-STD-1006A标准对应急空间遥测的保护要求，在安全性和操作约束之间取得良好平衡，适用于资源受限的航天通信环境。

Abstract: We explore how command stack protection requirements outlined in NASA-STD-1006A can be satisfied within the context of emergency space telemetry. Proposed implementation of lightweight authenticated encryption offers strong security without sacrificing performance in resource-constrained environments. It produces fixed-length messages, maintaining compatibility with the underlying data transport protocols. By focusing on predictable properties and robust authentication, we create a scheme that protects the confidentiality, integrity and authenticity of telemetry data in emergency communications while balancing security requirements with the operational constraints.

</details>


### [19] [Incremental Fingerprinting in an Open World](https://arxiv.org/abs/2601.21680)
*Loes Kruger,Paul Kobialka,Andrea Pferscher,Einar Broch Johnsen,Sebastian Junges,Jurriaan Rot*

Main category: cs.CR

TL;DR: 提出开放世界网络协议指纹识别方法，结合主动自动机学习和闭世界指纹识别，解决传统方法在未知实现模型时的误分类问题。


<details>
  <summary>Details</summary>
Motivation: 传统协议指纹识别基于闭世界假设，要求所有实现模型都已知，这在实践中不现实。当假设不成立时，会导致大量误分类且无法指示缺失模型。

Method: 提出增量指纹识别方法：1) 使用指纹识别和一致性检查快速判断实现是否匹配现有模型；2) 若无匹配，利用现有模型结构学习新模型；3) 结合主动自动机学习与闭世界指纹识别。

Result: 证明了方法的正确性，相比朴素基线在渐近复杂度上有改进。实验结果显示在各种协议上显著减少了误分类和与黑盒系统的交互次数。

Conclusion: 提出的开放世界指纹识别方法有效解决了传统闭世界假设的局限性，减少了误分类，提高了实际应用中的可行性。

Abstract: Network protocol fingerprinting is used to identify a protocol implementation by analyzing its input-output behavior. Traditionally, fingerprinting operates under a closed-world assumption, where models of all implementations are assumed to be available. However, this assumption is unrealistic in practice. When this assumption does not hold, fingerprinting results in numerous misclassifications without indicating that a model for an implementation is missing. Therefore, we introduce an open-world variant of the fingerprinting problem, where not all models are known in advance. We propose an incremental fingerprinting approach to solve the problem by combining active automata learning with closed-world fingerprinting. Our approach quickly determines whether the implementation under consideration matches an available model using fingerprinting and conformance checking. If no match is found, it learns a new model by exploiting the structure of available models. We prove the correctness of our approach and improvements in asymptotic complexity compared to naive baselines. Moreover, experimental results on a variety of protocols demonstrate a significant reduction in misclassifications and interactions with these black-boxes.

</details>


### [20] [WADBERT: Dual-channel Web Attack Detection Based on BERT Models](https://arxiv.org/abs/2601.21893)
*Kangqiang Luo,Yi Xie,Shiqian Zhao,Jing Pan*

Main category: cs.CR

TL;DR: WADBERT是一个基于深度学习的Web攻击检测模型，通过混合粒度嵌入和BERT架构实现高精度检测，并能精确定位恶意参数。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的Web攻击检测方法在处理不规则HTTP请求时效果不佳，难以建模无序参数和实现攻击溯源。需要一种既能高精度检测攻击，又能精确定位恶意参数的方法。

Method: 1. 使用混合粒度嵌入（HGE）为URL和payload参数生成细粒度嵌入；2. 分别使用URLBERT和SecBERT提取语义特征；3. 通过多头注意力机制融合参数级特征；4. 将拼接的URL和payload特征输入线性分类器得到最终检测结果。

Result: 在CSIC2010和SR-BH2020数据集上分别达到99.63%和99.50%的F1分数，显著优于现有最先进方法。

Conclusion: WADBERT模型不仅实现了高精度的Web攻击检测，还能精确定位恶意参数，解决了现有方法在处理不规则HTTP请求和攻击溯源方面的不足。

Abstract: Web attack detection is the first line of defense for securing web applications, designed to preemptively identify malicious activities. Deep learning-based approaches are increasingly popular for their advantages: automatically learning complex patterns and extracting semantic features from HTTP requests to achieve superior detection performance. However, existing methods are less effective in embedding irregular HTTP requests, even failing to model unordered parameters and achieve attack traceability. In this paper, we propose an effective web attack detection model, named WADBERT. It achieves high detection accuracy while enabling the precise identification of malicious parameters. To this end, we first employ Hybrid Granularity Embedding (HGE) to generate fine-grained embeddings for URL and payload parameters. Then, URLBERT and SecBERT are respectively utilized to extract their semantic features. Further, parameter-level features (extracted by SecBERT) are fused through a multi-head attention mechanism, resulting in a comprehensive payload feature. Finally, by feeding the concatenated URL and payload features into a linear classifier, a final detection result is obtained. The experimental results on CSIC2010 and SR-BH2020 datasets validate the efficacy of WADBERT, which respectively achieves F1-scores of 99.63% and 99.50%, and significantly outperforms state-of-the-art methods.

</details>


### [21] [Beyond the Finite Variant Property: Extending Symbolic Diffie-Hellman Group Models (Extended Version)](https://arxiv.org/abs/2601.21910)
*Sofia Giampietro,Ralf Sasse,David Basin*

Main category: cs.CR

TL;DR: 该论文扩展了Tamarin验证器以支持完整的Diffie-Hellman群理论，包括群元素乘法和指数加法，使协议验证工具能够处理使用所有DH操作的协议。


<details>
  <summary>Details</summary>
Motivation: 当前符号协议验证器虽然支持Diffie-Hellman群，但缺乏对指数加法等数学操作的支持，因为这些工具使用合一推理，而完整DH理论的合一是不可判定的。

Method: 提出了一种近似理论并设计了半决策过程，通过扩展Tamarin验证器来支持完整的Diffie-Hellman理论，包括群元素乘法和指数加法。

Result: 成功实现了首个能够建模和推理使用完整DH操作协议的最先进工具，通过ElGamal加密和MQV案例研究验证了方法的有效性。

Conclusion: 该研究使协议验证工具能够处理更广泛的密码协议，填补了现有工具在支持完整DH操作方面的空白，为协议安全性分析提供了新能力。

Abstract: Diffie-Hellman groups are commonly used in cryptographic protocols. While most state-of-the-art, symbolic protocol verifiers support them to some degree, they do not support all mathematical operations possible in these groups. In particular, they lack support for exponent addition, as these tools reason about terms using unification, which is undecidable in the theory describing all Diffie-Hellman operators. In this paper we approximate such a theory and propose a semi-decision procedure to determine whether a protocol, which may use all operations in such groups, satisfies user-defined properties. We implement this approach by extending the Tamarin prover to support the full Diffie-Hellman theory, including group element multiplication and hence addition of exponents. This is the first time a state-of-the-art tool can model and reason about such protocols. We illustrate our approach's effectiveness with different case studies: ElGamal encryption and MQV. Using Tamarin, we prove security properties of ElGamal, and we rediscover known attacks on MQV.

</details>


### [22] [Secure Group Key Agreement on Cyber-Physical System Buses](https://arxiv.org/abs/2601.21966)
*Sebastian N. Peters,Lukas Lautenschlager,David Emeis,Jason Lochert*

Main category: cs.CR

TL;DR: 本文针对工业与信息物理系统中的总线拓扑结构，设计了一种适用于受限环境的认证式完全分布式群组密钥协商协议，基于TreeKEM实现，解决了现有协议在广播链路、半双工操作等约束下的适应性问题。


<details>
  <summary>Details</summary>
Motivation: 信息物理系统依赖分布式嵌入式设备通过总线进行安全通信，需要群组共享密钥用于消息认证码。为避免不安全的固定预共享密钥和首次使用信任机制，需要动态协商群组密钥的协议，但现有群组密钥协商协议无法适应受限的信息物理系统总线约束。

Method: 首先系统化分析现有协议，然后推导总线系统上认证式完全分布式群组密钥协商的需求，最后基于TreeKEM设计、实现并评估定制化的群组密钥协商协议。

Result: 设计出适用于总线拓扑的认证式完全分布式群组密钥协商协议，能够应对工业与信息物理系统的多种约束条件，包括广播链路、半双工操作、资源限制、动态成员变更、长设备寿命以及能够分区总线的强Dolev-Yao敌手。

Conclusion: 通过基于TreeKEM的定制化协议设计，实现了适合受限总线环境的认证式完全分布式群组密钥协商，解决了现有协议在信息物理系统总线拓扑中的适应性问题。

Abstract: Cyber-Physical Systems (CPSs) rely on distributed embedded devices that often must communicate securely over buses. Ensuring message integrity and authenticity on these buses typically requires group-shared keys for Message Authentication Codes (MACs). To avoid insecure fixed pre-shared keys and trust-on-first-use concepts, a Group Key Agreement (GKA) protocol is needed to dynamically agree on a key amongst the devices. Yet existing GKA protocols lack adaptability to constrained CPS buses. This paper targets authenticated, fully distributed GKA suitable for bus topologies under constraints of industrial and cyber-physical systems, including broadcast-only links, half-duplex operation, resource limits, dynamic membership (including unannounced leaves), a long device lifetime, and a strong Dolev-Yao adversary capable of partitioning the bus. We first systematise existing protocols, then derive the requirements necessary for an authenticated and fully distributed GKA on bus systems. Finally, we design, implement, and evaluate a custom GKA protocol based on TreeKEM.

</details>


### [23] [RedSage: A Cybersecurity Generalist LLM](https://arxiv.org/abs/2601.22159)
*Naufal Suryanto,Muzammal Naseer,Pengfei Li,Syed Talal Wasim,Jinhui Yi,Juergen Gall,Paolo Ceravolo,Ernesto Damiani*

Main category: cs.CR

TL;DR: RedSage是一个开源的、可本地部署的网络安全助手LLM，通过领域感知的持续预训练和基于专家工作流程的智能增强微调，在网络安全任务上显著超越基线模型，同时提升通用推理能力。


<details>
  <summary>Details</summary>
Motivation: 网络安全操作需要既能支持多样化工作流程又不暴露敏感数据的助手LLM。现有解决方案要么依赖有隐私风险的专有API，要么使用缺乏领域适应的开源模型，需要填补这一空白。

Method: 1) 通过大规模网页过滤和手动收集高质量资源，整理11.8B tokens的网络安全持续预训练数据；2) 设计智能增强管道，模拟专家工作流程生成266K多轮网络安全样本用于监督微调；3) 结合通用开源LLM数据训练RedSage模型；4) 引入RedSage-Bench基准进行评估。

Result: 在8B规模上，RedSage在网络安全基准上比基线模型提升高达+5.59分，在Open LLM Leaderboard任务上提升+5.05分。模型在CTI-Bench、CyberMetric、SECURE等基准上也表现优异。

Conclusion: 领域感知的智能增强和预/后训练不仅能增强网络安全专业知识，还能改善通用推理和指令跟随能力。所有模型、数据集和代码均已公开。

Abstract: Cybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, spanning 28.6K documents across frameworks, offensive techniques, and security tools. Building on this, we design an agentic augmentation pipeline that simulates expert workflows to generate 266K multi-turn cybersecurity samples for supervised fine-tuning. Combined with general open-source LLM data, these resources enable the training of RedSage, an open-source, locally deployable cybersecurity assistant with domain-aware pretraining and post-training. To rigorously evaluate the models, we introduce RedSage-Bench, a benchmark with 30K multiple-choice and 240 open-ended Q&A items covering cybersecurity knowledge, skills, and tool expertise. RedSage is further evaluated on established cybersecurity benchmarks (e.g., CTI-Bench, CyberMetric, SECURE) and general LLM benchmarks to assess broader generalization. At the 8B scale, RedSage achieves consistently better results, surpassing the baseline models by up to +5.59 points on cybersecurity benchmarks and +5.05 points on Open LLM Leaderboard tasks. These findings demonstrate that domain-aware agentic augmentation and pre/post-training can not only enhance cybersecurity-specific expertise but also help to improve general reasoning and instruction-following. All models, datasets, and code are publicly available.

</details>
