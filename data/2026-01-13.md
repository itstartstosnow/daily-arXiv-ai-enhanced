<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 59]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [AutoVulnPHP: LLM-Powered Two-Stage PHP Vulnerability Detection and Automated Localization](https://arxiv.org/abs/2601.06177)
*Zhiqiang Wang,Yizhong Ding,Zilong Xiao,Jinyu Lu,Yan Jia,Yanjun Li*

Main category: cs.CR

TL;DR: AutoVulnPHP是一个端到端PHP漏洞检测框架，结合两阶段漏洞检测与细粒度自动定位，在大型数据集上达到99.7%检测准确率，并在真实项目中发现了429个未知漏洞。


<details>
  <summary>Details</summary>
Motivation: PHP在Web开发中占主导地位，但面临安全挑战：静态分析缺乏语义深度导致高误报；动态分析计算成本高；自动漏洞定位粒度粗且上下文不精确。此外，缺乏大规模PHP漏洞数据集和碎片化的工具链阻碍了实际部署。

Method: AutoVulnPHP框架包含三个核心组件：1) SIFT-VulMiner使用增强数据流的AST结构生成漏洞假设；2) SAFE-VulMiner通过预训练代码编码器嵌入验证候选漏洞，消除误报；3) ISAL通过语法引导跟踪、思维链LLM推理和因果一致性检查来精确定位根本原因。

Result: 创建了首个大规模PHP漏洞数据集PHPVD（26,614个文件，520万行代码，覆盖7种漏洞类型）。在公开基准测试和PHPVD上，AutoVulnPHP达到99.7%检测准确率、99.5% F1分数和81.0%定位率。在真实仓库中发现了429个先前未知的漏洞，其中351个获得了CVE标识符。

Conclusion: AutoVulnPHP通过结合结构推理、语义分析和细粒度定位，有效解决了PHP漏洞检测的挑战，在实际部署中证明了其有效性，为PHP安全分析提供了实用框架。

Abstract: PHP's dominance in web development is undermined by security challenges: static analysis lacks semantic depth, causing high false positives; dynamic analysis is computationally expensive; and automated vulnerability localization suffers from coarse granularity and imprecise context. Additionally, the absence of large-scale PHP vulnerability datasets and fragmented toolchains hinder real-world deployment.
  We present AutoVulnPHP, an end-to-end framework coupling two-stage vulnerability detection with fine-grained automated localization. SIFT-VulMiner (Structural Inference for Flaw Triage Vulnerability Miner) generates vulnerability hypotheses using AST structures enhanced with data flow. SAFE-VulMiner (Semantic Analysis for Flaw Evaluation Vulnerability Miner) verifies candidates through pretrained code encoder embeddings, eliminating false positives. ISAL (Incremental Sequence Analysis for Localization) pinpoints root causes via syntax-guided tracing, chain-of-thought LLM inference, and causal consistency checks to ensure precision.
  We contribute PHPVD, the first large-scale PHP vulnerability dataset with 26,614 files (5.2M LOC) across seven vulnerability types. On public benchmarks and PHPVD, AutoVulnPHP achieves 99.7% detection accuracy, 99.5% F1 score, and 81.0% localization rate. Deployed on real-world repositories, it discovered 429 previously unknown vulnerabilities, 351 assigned CVE identifiers, validating its practical effectiveness.

</details>


### [2] [Leveraging Membership Inference Attacks for Privacy Measurement in Federated Learning for Remote Sensing Images](https://arxiv.org/abs/2601.06200)
*Anh-Kiet Duong,Petra Gomez-Krämer,Hoàng-Ân Lê,Minh-Tan Pham*

Main category: cs.CR

TL;DR: 该论文利用成员推理攻击作为联邦学习在遥感图像分类中的隐私评估框架，发现通信高效的FL策略能降低攻击成功率同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 尽管联邦学习(FL)通过本地化训练数据保护隐私，但研究表明FL模型仍可能通过输出泄露敏感信息，因此需要严格的隐私评估方法。

Method: 采用成员推理攻击(MIA)作为量化隐私测量框架，评估多种黑盒MIA技术（基于熵的攻击、改进熵攻击、似然比攻击），在不同FL算法和通信策略下进行实验。

Result: 在两个公开场景分类数据集上的实验表明，MIA能有效揭示仅靠准确率无法捕捉的隐私泄露，通信高效的FL策略能降低MIA成功率同时保持竞争力性能。

Conclusion: MIA可作为实用的隐私评估指标，强调了将隐私测量整合到FL系统设计中的重要性，特别是在遥感应用领域。

Abstract: Federated Learning (FL) enables collaborative model training while keeping training data localized, allowing us to preserve privacy in various domains including remote sensing. However, recent studies show that FL models may still leak sensitive information through their outputs, motivating the need for rigorous privacy evaluation. In this paper, we leverage membership inference attacks (MIA) as a quantitative privacy measurement framework for FL applied to remote sensing image classification. We evaluate multiple black-box MIA techniques, including entropy-based attacks, modified entropy attacks, and the likelihood ratio attack, across different FL algorithms and communication strategies. Experiments conducted on two public scene classification datasets demonstrate that MIA effectively reveals privacy leakage not captured by accuracy alone. Our results show that communication-efficient FL strategies reduce MIA success rates while maintaining competitive performance. These findings confirm MIA as a practical metric and highlight the importance of integrating privacy measurement into FL system design for remote sensing applications.

</details>


### [3] [Cyber Threat Detection and Vulnerability Assessment System using Generative AI and Large Language Model](https://arxiv.org/abs/2601.06213)
*Keerthi Kumar. M,Swarun Kumar Joginpelly,Sunil Khemka,Lakshmi. S R,Navin Chhibber*

Main category: cs.CR

TL;DR: 提出基于RoBERTa的网络安全威胁检测模型，通过FHE加密和BBPE分词器处理PCAP数据，相比传统BERT模型在准确率、召回率和精确率方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统网络安全威胁检测模型（如Security BERT）对文本数据的上下文理解有限，影响检测效果。随着网络攻击（勒索软件、恶意软件、钓鱼、DoS等）日益复杂，需要更强大的检测方法。

Method: 1. 从PCAP文件提取数据并使用全谐波加密（FHE）加密；2. 使用字节级和字节对编码（BBPE）分词器生成令牌并维护加密值的词汇表；3. 将处理后的数据输入RoBERTa模型进行广泛训练；4. 使用Softmax进行攻击检测和分类。

Result: 提出的RoBERTa模型在准确率（0.99）、召回率（0.91）和精确率（0.89）方面均优于现有BERT模型。

Conclusion: RoBERTa模型通过增强的词汇理解和上下文处理能力，能够更有效地检测和分类网络攻击，为网络安全威胁检测提供了改进方案。

Abstract: Background: Cyber-attacks have evolved rapidly in recent years, many individuals and business owners have been affected by cyber-attacks in various ways. Cyber-attacks include various threats such as ransomware, malware, phishing, and Denial of Service (DoS)-related attacks. Challenges: Traditional models such as Generative Artificial Intelligence (AI) and Security Bidirectional Encoder Representations from Transformers (BERT) were implemented to detect cyber threats. However, the existing Security BERT model has a limited contextual understanding of text data, which has less impact on detecting cyber-attacks. Proposed Methodology: To overcome the above-mentioned challenges, Robustly Optimized Bidirectional Encoder Representations from Transformers Pretraining Approach (RoBERTa) model is proposed which consists of diverse words of vocabulary understanding. Initially, data are extracted from a Packet Capture (PCAP) file and encrypted using Fully Harmonic Encryption (FHE). Subsequently, a Byte-level and Byte Pair Encoding (BBPE) tokenizer was used to generate tokens and help maintain the vocabulary for the encrypted values. Then, these values are applied to the RoBERTa model of the transformer with extensive training. Finally, Softmax is used for the detection and classification of attacks. The proposed RoBERTa model achieved better results than the existing BERT model in terms of accuracy (0.99), recall (0.91), and precision (0.89) respectively.

</details>


### [4] [AI-Powered Algorithms for the Prevention and Detection of Computer Malware Infections](https://arxiv.org/abs/2601.06219)
*Rakesh Keshava,Sathish Kuppan Pandurangan,M. Sakthivanitha,Sankaranainar Parmsivan,Goutham Sunkara,R. Maruthi*

Main category: cs.CR

TL;DR: 提出基于AI的混合上下文感知恶意软件检测框架(HCAMDF)，结合静态分析、动态行为分析和上下文元数据，在基准数据集上达到97.3%准确率和1.5%误报率。


<details>
  <summary>Details</summary>
Motivation: 恶意软件攻击频率和复杂性增加，传统基于签名的检测方法效果下降，需要智能系统来准确、主动地识别和预防恶意软件感染。

Method: 提出HCAMDF混合框架，采用多层架构：轻量级静态分类器、用于实时行为分析的LSTM，以及通过集成多层预测的风险评分系统。结合静态文件分析、动态行为分析和上下文元数据。

Result: 在EMBER和CIC-MalMem2022基准数据集上评估，准确率达97.3%，误报率仅1.5%，检测延迟最小，优于现有ML和DL方法。

Conclusion: 混合AI方法能有效检测现有和新型恶意软件变种，为能够实时检测并适应快速演变的威胁环境的智能安全系统奠定基础。

Abstract: The rise in frequency and complexity of malware attacks are viewed as a major threat to modern digital infrastructure, which means that traditional signature-based detection methods are becoming less effective. As cyber threats continue to evolve, there is a growing need for intelligent systems to accurately and proactively identify and prevent malware infections. This study presents a new hybrid context-aware malware detection framework(HCAMDF) based on artificial intelligence (AI), which combines static file analysis, dynamic behavioural analysis, and contextual metadata to provide more accurate and timely detection. HCADMF has a multi-layer architecture, which consists of lightweight static classifiers such as Long Short Term Memory (LSTM) for real-time behavioral analysis, and an ensemble risk scoring through the integration of multiple layers of prediction. Experimental evaluations of the new/methodology with benchmark datasets, EMBER and CIC-MalMem2022, showed that the new approach provides superior performances with an accuracy of 97.3%, only a 1.5% false positive rate and minimal detection delay compared to several existing machine learning(ML) and deep learning(DL) established methods in the same fields. The results show strong evidence that hybrid AI can detect both existing and novel malware variants, and lay the foundation on intelligent security systems that can enable real-time detection and adapt to a rapidly evolving threat landscape.

</details>


### [5] [Multi-Agent Framework for Controllable and Protected Generative Content Creation: Addressing Copyright and Provenance in AI-Generated Media](https://arxiv.org/abs/2601.06232)
*Haris Khan,Sadia Asif,Shumaila Asif*

Main category: cs.CR

TL;DR: 提出一个多智能体框架，通过专门的角色和集成水印技术解决生成式AI的可控性、版权保护和内容溯源问题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI系统的普及带来了前所未有的内容创作机会，但也引发了关于可控性、版权侵权和内容溯源的严重担忧。当前生成模型作为"黑箱"运行，用户控制有限，缺乏保护知识产权或追踪内容来源的内置机制。

Method: 提出一个新颖的多智能体框架，通过导演、生成器、评审员、集成和保护等专门角色来确保用户意图对齐，同时嵌入数字溯源标记。系统通过两个案例研究展示可行性：具有迭代优化的创意内容生成和商业环境中AI生成艺术的版权保护。

Result: 初步可行性证据表明，语义对齐提高了23%，水印恢复率达到95%。该框架为负责任地部署生成式AI做出了贡献。

Conclusion: 这项工作将多智能体系统定位为法律和商业应用中可信创意工作流程的解决方案，有助于实现负责任的生成式AI部署。

Abstract: The proliferation of generative AI systems creates unprecedented opportunities for content creation while raising critical concerns about controllability, copyright infringement, and content provenance. Current generative models operate as "black boxes" with limited user control and lack built-in mechanisms to protect intellectual property or trace content origin. We propose a novel multi-agent framework that addresses these challenges through specialized agent roles and integrated watermarking. Our system orchestrates Director, Generator, Reviewer, Integration, and Protection agents to ensure user intent alignment while embedding digital provenance markers. We demonstrate feasibility through two case studies: creative content generation with iterative refinement and copyright protection for AI-generated art in commercial contexts. Preliminary feasibility evidence from prior work indicates up to 23\% improvement in semantic alignment and 95\% watermark recovery rates. This work contributes to responsible generative AI deployment, positioning multi-agent systems as a solution for trustworthy creative workflows in legal and commercial applications.

</details>


### [6] [Agentic AI Microservice Framework for Deepfake and Document Fraud Detection in KYC Pipelines](https://arxiv.org/abs/2601.06241)
*Chandra Sekhar Kubam*

Main category: cs.CR

TL;DR: 提出基于Agentic AI的微服务框架，通过模块化视觉模型、活体检测、深度伪造识别、OCR文档取证、多模态身份关联和策略驱动的风险引擎，提升KYC系统的检测准确性、降低延迟，增强对抗性攻击的抵御能力。


<details>
  <summary>Details</summary>
Motivation: 合成媒体、呈现攻击和文档伪造的快速扩散给金融、电信和数字身份生态系统的KYC流程带来了重大漏洞。传统的单体KYC系统缺乏应对适应性欺诈所需的可扩展性和敏捷性。

Method: 提出Agentic AI微服务框架，集成模块化视觉模型、活体评估、深度伪造检测、基于OCR的文档取证、多模态身份关联和策略驱动的风险引擎。系统利用自主微代理进行任务分解、流水线编排、动态重试和人机协同升级。

Result: 实验评估显示，该框架提高了检测准确性，降低了延迟，并增强了对对抗性输入的抵御能力。

Conclusion: 该框架为受监管行业提供了一个可扩展的蓝图，用于实现稳健、实时且保护隐私的KYC验证。

Abstract: The rapid proliferation of synthetic media, presentation attacks, and document forgeries has created significant vulnerabilities in Know Your Customer (KYC) workflows across financial services, telecommunications, and digital-identity ecosystems. Traditional monolithic KYC systems lack the scalability and agility required to counter adaptive fraud. This paper proposes an Agentic AI Microservice Framework that integrates modular vision models, liveness assessment, deepfake detection, OCR-based document forensics, multimodal identity linking, and a policy driven risk engine. The system leverages autonomous micro-agents for task decomposition, pipeline orchestration, dynamic retries, and human-in-the-loop escalation. Experimental evaluations demonstrate improved detection accuracy, reduced latency, and enhanced resilience against adversarial inputs. The framework offers a scalable blueprint for regulated industries seeking robust, real-time, and privacy-preserving KYC verification.

</details>


### [7] [Automated Generation of Accurate Privacy Captions From Android Source Code Using Large Language Models](https://arxiv.org/abs/2601.06276)
*Vijayanta Jain,Sepideh Ghanavati,Sai Teja Peddinti,Collin McMillan*

Main category: cs.CR

TL;DR: PCapGen：基于LLM的自动隐私描述生成方法，通过提取源代码上下文生成准确、简洁、完整的隐私描述


<details>
  <summary>Details</summary>
Motivation: 现有隐私描述生成方法存在依赖开发者输入、源代码上下文有限、依赖可能不准确的隐私政策等问题，需要自动化、准确的解决方案

Method: 1) 自动识别提取实现隐私行为的源代码上下文；2) 使用LLM描述粗粒度和细粒度隐私行为；3) 生成准确、简洁、完整的隐私描述

Result: PCapGen生成的隐私描述比基线方法更简洁、完整、准确；隐私专家71%时间选择PCapGen描述，LLM作为评判者76%时间偏好PCapGen

Conclusion: PCapGen能有效生成高质量的隐私描述，解决了现有方法的局限性，为应用隐私行为描述提供了自动化解决方案

Abstract: Privacy captions are short sentences that succinctly describe what personal information is used, how it is used, and why, within an app. These captions can be utilized in various notice formats, such as privacy policies, app rationales, and app store descriptions. However, inaccurate captions may mislead users and expose developers to regulatory fines. Existing approaches to generating privacy notices or just privacy captions include using questionnaires, templates, static analysis, or machine learning. However, these approaches either rely heavily on developers' inputs and thus strain their efforts, use limited source code context, leading to the incomplete capture of app privacy behaviors, or depend on potentially inaccurate privacy policies as a source for creating notices. In this work, we address these limitations by developing Privacy Caption Generator (PCapGen), an approach that - i) automatically identifies and extracts large and precise source code context that implements privacy behaviors in an app, ii) uses a Large Language Model (LLM) to describe coarse- and fine-grained privacy behaviors, and iii) generates accurate, concise, and complete privacy captions to describe the privacy behaviors of the app. Our evaluation shows PCapGen generates concise, complete, and accurate privacy captions as compared to the baseline approach. Furthermore, privacy experts choose PCapGen captions at least 71\% of the time, whereas LLMs-as-judge prefer PCapGen captions at least 76\% of the time, indicating strong performance of our approach.

</details>


### [8] [Beyond BeautifulSoup: Benchmarking LLM-Powered Web Scraping for Everyday Users](https://arxiv.org/abs/2601.06301)
*Arth Bhardwaj,Nirav Diwan,Gang Wang*

Main category: cs.CR

TL;DR: LLM技术已使网页抓取民主化，即使是没有技术背景的普通用户也能通过自然语言提示完成复杂网站的抓取任务，包括需要认证、反爬虫和验证码的网站。


<details>
  <summary>Details</summary>
Motivation: 传统网页抓取需要HTML解析、会话管理和认证规避等技术专长，限制了大规模数据提取只能由熟练开发者完成。作者认为大语言模型（LLM）已经改变了这一现状，使低技能用户也能通过简单的自然语言提示执行复杂的抓取操作。

Method: 系统性地评估了普通用户使用现成LLM工具的能力，覆盖35个网站，分为五个安全等级（包括认证、反爬虫和CAPTCHA控制）。设计并评估了两种工作流程：(a) LLM辅助脚本编写：用户提示LLM生成传统抓取代码，但保持手动执行控制；(b) 端到端LLM代理：通过集成工具使用自主导航和提取数据。

Result: 端到端代理使复杂抓取变得可访问——只需单个提示和最少优化（少于5次修改）即可完成工作流程。对于静态网站，LLM辅助脚本编写可能更简单快捷。研究还提供了新手使用这些工作流程的简单步骤，并评估了潜在攻击者可能达到的能力。

Conclusion: LLM技术已经显著降低了网页抓取的技术门槛，使普通用户能够完成以前只有专业开发者才能完成的复杂抓取任务。这既为数据获取提供了新机会，也带来了新的安全挑战，需要网站管理员重新评估现有的安全措施。

Abstract: Web scraping has historically required technical expertise in HTML parsing, session management, and authentication circumvention, which limited large-scale data extraction to skilled developers. We argue that large language models (LLMs) have democratized web scraping, enabling low-skill users to execute sophisticated operations through simple natural language prompts. While extensive benchmarks evaluate these tools under optimal expert conditions, we show that without extensive manual effort, current LLM-based workflows allow novice users to scrape complex websites that would otherwise be inaccessible. We systematically benchmark what everyday users can do with off-the-shelf LLM tools across 35 sites spanning five security tiers, including authentication, anti-bot, and CAPTCHA controls. We devise and evaluate two distinct workflows: (a) LLM-assisted scripting, where users prompt LLMs to generate traditional scraping code but maintain manual execution control, and (b) end-to-end LLM agents, which autonomously navigate and extract data through integrated tool use. Our results demonstrate that end-to-end agents have made complex scraping accessible - requiring as little as a single prompt with minimal refinement (less than 5 changes) to complete workflows. We also highlight scenarios where LLM-assisted scripting may be simpler and faster for static sites. In light of these findings, we provide simple procedures for novices to use these workflows and gauge what adversaries could achieve using these.

</details>


### [9] [Smart Privacy Policy Assistant: An LLM-Powered System for Transparent and Actionable Privacy Notices](https://arxiv.org/abs/2601.06357)
*Sriharshini Kalvakuntla,Luoxi Tang,Yuqiao Meng,Zhaohan Xi*

Main category: cs.CR

TL;DR: 开发了一个基于LLM的智能隐私政策助手，能自动分析隐私政策、提取关键条款、评估风险等级并生成清晰解释，帮助用户理解复杂的隐私政策。


<details>
  <summary>Details</summary>
Motivation: 大多数用户在不阅读或不理解隐私政策的情况下就同意，但这些政策控制着个人数据的收集、共享和货币化方式。隐私政策通常冗长、法律复杂，非专业人士难以理解。

Method: 提出了智能隐私政策助手系统，采用端到端流程：政策摄入、条款分类、风险评分和解释生成。系统通过浏览器扩展或移动界面实时使用，在用户披露敏感信息或授予风险权限前提供上下文警告。

Result: 描述了一个完整的系统架构，包括政策摄入、条款分类、风险评分和解释生成模块，并提出了基于条款级准确性、政策级风险一致性和用户理解度的评估框架。

Conclusion: 该LLM驱动的系统能够帮助用户更好地理解隐私政策，在数据共享决策前提供风险意识，为解决隐私政策理解难题提供了技术解决方案。

Abstract: Most users agree to online privacy policies without reading or understanding them, even though these documents govern how personal data is collected, shared, and monetized. Privacy policies are typically long, legally complex, and difficult for non-experts to interpret. This paper presents the Smart Privacy Policy Assistant, an LLM-powered system that automatically ingests privacy policies, extracts and categorizes key clauses, assigns human-interpretable risk levels, and generates clear, concise explanations. The system is designed for real-time use through browser extensions or mobile interfaces, surfacing contextual warnings before users disclose sensitive information or grant risky permissions. We describe the end-to-end pipeline, including policy ingestion, clause categorization, risk scoring, and explanation generation, and propose an evaluation framework based on clause-level accuracy, policy-level risk agreement, and user comprehension.

</details>


### [10] [SafeGPT: Preventing Data Leakage and Unethical Outputs in Enterprise LLM Use](https://arxiv.org/abs/2601.06366)
*Pratyush Desai,Luoxi Tang,Yuqiao Meng,Zhaohan Xi*

Main category: cs.CR

TL;DR: SafeGPT是一个双向防护系统，通过输入检测/脱敏和输出审核/重构防止LLM在企业应用中的数据泄露和伦理违规，同时结合人工反馈机制。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在企业工作流中带来安全与伦理挑战，员工可能无意中泄露敏感数据或生成违反政策的内容，需要防护机制。

Method: 提出SafeGPT双向防护系统：输入侧进行敏感数据检测与脱敏，输出侧进行内容审核与重构，并整合人工反馈循环机制。

Result: 实验表明SafeGPT能有效降低数据泄露风险和偏见输出，同时保持用户满意度。

Conclusion: SafeGPT为企业安全部署LLM提供了有效的双向防护方案，平衡了安全需求与用户体验。

Abstract: Large Language Models (LLMs) are transforming enterprise workflows but introduce security and ethics challenges when employees inadvertently share confidential data or generate policy-violating content. This paper proposes SafeGPT, a two-sided guardrail system preventing sensitive data leakage and unethical outputs. SafeGPT integrates input-side detection/redaction, output-side moderation/reframing, and human-in-the-loop feedback. Experiments demonstrate SafeGPT effectively reduces data leakage risk and biased outputs while maintaining satisfaction.

</details>


### [11] [From Easy to Hard++: Promoting Differentially Private Image Synthesis Through Spatial-Frequency Curriculum](https://arxiv.org/abs/2601.06368)
*Chen Gong,Kecen Li,Zinan Lin,Tianhao Wang*

Main category: cs.CR

TL;DR: FETA-Pro：一种结合空间特征和频率特征作为训练捷径的差分隐私图像合成方法，通过多模型流水线设计提升生成质量


<details>
  <summary>Details</summary>
Motivation: 现有DP-FETA方法主要适用于图像相似度高的数据集，但无法有效处理图像差异大的场景。本文旨在探索除DP-SGD外的其他工具，特别是针对图像变化显著的情况，寻找更细粒度的训练捷径。

Method: 提出FETA-Pro方法：1）引入频率特征作为训练捷径，其复杂度介于空间特征和完整图像之间；2）采用多模型流水线设计：先训练辅助生成器产生与噪声频率特征对齐的图像，再用这些图像结合空间特征和DP-SGD训练主模型。

Result: 在五个敏感图像数据集上评估，隐私预算ε=1时，FETA-Pro相比最佳基线平均提升25.7%的保真度和4.1%的效用。

Conclusion: FETA-Pro通过结合空间和频率特征作为训练捷径，并采用多模型流水线设计，有效解决了图像差异大场景下的差分隐私图像合成问题，显著提升了生成质量。

Abstract: To improve the quality of Differentially private (DP) synthetic images, most studies have focused on improving the core optimization techniques (e.g., DP-SGD). Recently, we have witnessed a paradigm shift that takes these techniques off the shelf and studies how to use them together to achieve the best results. One notable work is DP-FETA, which proposes using `central images' for `warming up' the DP training and then using traditional DP-SGD.
  Inspired by DP-FETA, we are curious whether there are other such tools we can use together with DP-SGD. We first observe that using `central images' mainly works for datasets where there are many samples that look similar. To handle scenarios where images could vary significantly, we propose FETA-Pro, which introduces frequency features as `training shortcuts.' The complexity of frequency features lies between that of spatial features (captured by `central images') and full images, allowing for a finer-grained curriculum for DP training. To incorporate these two types of shortcuts together, one challenge is to handle the training discrepancy between spatial and frequency features. To address it, we leverage the pipeline generation property of generative models (instead of having one model trained with multiple features/objectives, we can have multiple models working on different features, then feed the generated results from one model into another) and use a more flexible design. Specifically, FETA-Pro introduces an auxiliary generator to produce images aligned with noisy frequency features. Then, another model is trained with these images, together with spatial features and DP-SGD. Evaluated across five sensitive image datasets, FETA-Pro shows an average of 25.7% higher fidelity and 4.1% greater utility than the best-performing baseline, under a privacy budget $ε= 1$.

</details>


### [12] [Noise Reduction for Pufferfish Privacy: A Practical Noise Calibration Method](https://arxiv.org/abs/2601.06385)
*Wenjin Yang,Ni Ding,Zijian Zhang,Jing Sun,Zhen Li,Yan Wu,Jiahang Sun,Haotian Lin,Yong Liu,Jincheng An,Liehuang Zhu*

Main category: cs.CR

TL;DR: 提出一种松弛噪声校准方法，在保持pufferfish隐私的同时提升数据效用，相比现有1-Wasserstein机制显著减少噪声添加


<details>
  <summary>Details</summary>
Motivation: 现有1-Wasserstein机制的条件过于严格，导致添加过多噪声，降低了数据效用，特别是在实际部署中常见的低隐私预算场景下问题尤为突出

Method: 通过松弛现有1-Wasserstein机制的严格条件，提出一种实用的机制设计算法作为通用解决方案，并分析了不同先验分布下的噪声减少变化和最优性

Result: 相比1-Wasserstein机制，该方法在所有隐私预算和先验信念下都能严格减少噪声，在低隐私预算场景下效用提升显著；实验显示在三个真实数据集上数据效用提升47%到87%

Conclusion: 提出的松弛噪声校准方法能有效平衡隐私保护和数据效用，特别是在实际部署中常见的低隐私预算场景下效果显著，证明了该方法相对于现有机制的优越性

Abstract: This paper introduces a relaxed noise calibration method to enhance data utility while attaining pufferfish privacy. This work builds on the existing $1$-Wasserstein (Kantorovich) mechanism by alleviating the existing overly strict condition that leads to excessive noise, and proposes a practical mechanism design algorithm as a general solution. We prove that a strict noise reduction by our approach always exists compared to $1$-Wasserstein mechanism for all privacy budgets $ε$ and prior beliefs, and the noise reduction (also represents improvement on data utility) gains increase significantly for low privacy budget situations--which are commonly seen in real-world deployments. We also analyze the variation and optimality of the noise reduction with different prior distributions. Moreover, all the properties of the noise reduction still exist in the worst-case $1$-Wasserstein mechanism we introduced, when the additive noise is largest. We further show that the worst-case $1$-Wasserstein mechanism is equivalent to the $\ell_1$-sensitivity method. Experimental results on three real-world datasets demonstrate $47\%$ to $87\%$ improvement in data utility.

</details>


### [13] [Lightweight Yet Secure: Secure Scripting Language Generation via Lightweight LLMs](https://arxiv.org/abs/2601.06419)
*Keyang Zhang,Zeyu Chen,Xuan Feng,Dongliang Fang,Yaowen Zheng,Zhi Li,Limin Sun*

Main category: cs.CR

TL;DR: 提出了SecGenEval-PS基准和PSSec框架，用于评估和提升LLM在PowerShell安全脚本生成、分析和修复方面的能力，通过数据合成和微调使轻量级模型达到或超越大型通用模型的效果。


<details>
  <summary>Details</summary>
Motivation: PowerShell等脚本语言具有强大的自动化和管理能力，通常以高权限运行，但当前的安全保障需要大量人工规则制定和执行，给管理员带来沉重负担并产生关键生产风险。LLM在Python和JavaScript等语言的代码生成、漏洞检测和自动修复方面表现出色，但在安全脚本语言代码生成方面的能力尚未充分探索。

Method: 1. 提出SecGenEval-PS基准，系统评估LLM在安全脚本生成、安全分析和自动修复方面的能力；2. 开发PSSec框架，结合数据合成和微调提升模型安全能力；3. 创建自调试代理，整合静态分析器和先进LLM的推理能力，合成大规模结构化三元组（不安全脚本、违规分析、相应修复）；4. 使用监督微调（SFT）和强化学习（RL）微调轻量级LLM（最小1.7B参数），实现安全感知推理和安全PowerShell代码生成。

Result: 1. GPT-4o和o3-mini等专有和开源模型在安全脚本生成方面表现不足，超过60%的PowerShell脚本在没有结构化指导的情况下不安全；2. PSSec训练的模型在PowerShell安全任务上匹配或超越了通用大型模型，同时推理成本降低了一个数量级以上；3. 该方法在包括GPT和Qwen在内的多个LLM家族中都有效。

Conclusion: 通过SecGenEval-PS基准和PSSec框架，可以有效评估和提升LLM在PowerShell安全脚本生成、分析和修复方面的能力。数据合成与微调相结合的方法使轻量级模型能够达到或超越大型通用模型的安全性能，同时大幅降低推理成本，为脚本语言安全自动化提供了实用解决方案。

Abstract: The security of scripting languages such as PowerShell is critical given their powerful automation and administration capabilities, often exercised with elevated privileges. Today, securing these languages still demands substantial human effort to craft and enforce rules, imposing heavy burdens on typical administrators and creating critical production risks (e.g., misoperations that shut down servers).Large language models (LLMs) have demonstrated strong capabilities in code generation, vulnerability detection, and automated repair for languages like Python and JavaScript. However, their ability to assist with generating secure scripting-language code remains largely underexplored. In this paper, we present SecGenEval-PS, a benchmark designed to systematically evaluate LLMs on secure scripting generation, security analysis, and automated repair. Our results show that both proprietary and open-source models fall short in these areas. For instance, over 60% of PowerShell scripts produced by GPT-4o and o3-mini are insecure without structured guidance.To bridge this gap, we propose PSSec, a framework that combines data synthesis with fine-tuning to enhance model security capabilities. We develop a self-debugging agent that integrates static analyzers with the reasoning abilities of advanced LLMs to synthesize large-scale structured triplets of insecure scripts, violation analyses, and corresponding repairs. We then fine-tune lightweight LLMs (as small as 1.7B parameters) using supervised fine-tuning (SFT) and reinforcement learning (RL), enabling security-aware reasoning and the generation of secure PowerShell code.Across multiple LLM families, including GPT and Qwen, \textit{PSSec}-trained models match or surpass general-purpose large models on PowerShell security tasks while reducing inference cost by more than an order of magnitude.

</details>


### [14] [VIPER Strike: Defeating Visual Reasoning CAPTCHAs via Structured Vision-Language Inference](https://arxiv.org/abs/2601.06461)
*Minfeng Qi,Dongyang He,Qin Wang,Lefeng Zhang*

Main category: cs.CR

TL;DR: ViPer是一个统一的视觉推理验证码攻击框架，结合结构化视觉感知与自适应LLM推理，在六大主流VRC提供商上达到93.2%的成功率，接近人类水平。


<details>
  <summary>Details</summary>
Motivation: 现有视觉推理验证码（VRC）攻击方法存在局限性：视觉中心方法依赖特定模板检测器，无法处理新布局；推理中心方法依赖LLM但细粒度视觉感知能力不足。两者都缺乏处理异构VRC部署的通用性。

Method: ViPer框架整合结构化多对象视觉感知与自适应LLM推理，通过模块化流程解析视觉布局、将属性与问题语义对齐、推断目标坐标。还提出了模板空间随机化（TSR）防御策略，通过扰动语言模板而不改变任务语义来降低攻击成功率。

Result: 在六大VRC提供商（VTT、Geetest、NetEase、Dingxiang、Shumei、Xiaodun）上，ViPer达到最高93.2%的成功率，接近人类水平。相比现有方法GraphNet（83.2%）、Oedipus（65.8%）和Holistic（89.5%）均有显著提升。在不同LLM骨干网络（GPT、Grok、DeepSeek、Kimi）上保持90%以上准确率。

Conclusion: ViPer展示了整合视觉感知与语言推理的统一攻击框架的有效性，同时提出的TSR防御策略为设计人类可解但机器难攻的验证码提供了方向。

Abstract: Visual Reasoning CAPTCHAs (VRCs) combine visual scenes with natural-language queries that demand compositional inference over objects, attributes, and spatial relations. They are increasingly deployed as a primary defense against automated bots. Existing solvers fall into two paradigms: vision-centric, which rely on template-specific detectors but fail on novel layouts, and reasoning-centric, which leverage LLMs but struggle with fine-grained visual perception. Both lack the generality needed to handle heterogeneous VRC deployments.
  We present ViPer, a unified attack framework that integrates structured multi-object visual perception with adaptive LLM-based reasoning. ViPer parses visual layouts, grounds attributes to question semantics, and infers target coordinates within a modular pipeline. Evaluated on six major VRC providers (VTT, Geetest, NetEase, Dingxiang, Shumei, Xiaodun), ViPer achieves up to 93.2% success, approaching human-level performance across multiple benchmarks. Compared to prior solvers, GraphNet (83.2%), Oedipus (65.8%), and the Holistic approach (89.5%), ViPer consistently outperforms all baselines. The framework further maintains robustness across alternative LLM backbones (GPT, Grok, DeepSeek, Kimi), sustaining accuracy above 90%.
  To anticipate defense, we further introduce Template-Space Randomization (TSR), a lightweight strategy that perturbs linguistic templates without altering task semantics. TSR measurably reduces solver (i.e., attacker) performance. Our proposed design suggests directions for human-solvable but machine-resistant CAPTCHAs.

</details>


### [15] [SecureDyn-FL: A Robust Privacy-Preserving Federated Learning Framework for Intrusion Detection in IoT Networks](https://arxiv.org/abs/2601.06466)
*Imtiaz Ali Soomro,Hamood Ur Rehman,S. Jawad Hussain ID,Adeel Iqbal,Waqas Khalid,Heejung Yu ID*

Main category: cs.CR

TL;DR: 提出SecureDyn-FL框架，这是一个针对物联网入侵检测的隐私保护联邦学习系统，能同时应对数据投毒攻击、隐私泄露和非独立同分布数据挑战。


<details>
  <summary>Details</summary>
Motivation: 物联网设备激增扩大了网络攻击面，传统入侵检测系统在物联网环境中面临隐私、可扩展性和鲁棒性挑战，需要更安全的联邦学习方案。

Method: 1) 动态时间梯度审计机制：使用高斯混合模型和马氏距离检测投毒攻击；2) 优化的隐私保护聚合方案：基于变换的ElGamal加密，结合自适应剪枝和量化；3) 双目标个性化学习策略：使用对数调整损失函数适应非独立同分布数据。

Result: 在N-BaIoT数据集上，包括50%对抗性客户端的场景下，SecureDyn-FL在独立同分布和非独立同分布设置中均优于现有联邦学习入侵检测防御方法。

Conclusion: SecureDyn-FL是一个全面、鲁棒的隐私保护联邦学习框架，能有效解决物联网入侵检测中的安全挑战，为异构物联网环境提供了实用的解决方案。

Abstract: The rapid proliferation of Internet of Things (IoT) devices across domains such as smart homes, industrial control systems, and healthcare networks has significantly expanded the attack surface for cyber threats, including botnet-driven distributed denial-of-service (DDoS), malware injection, and data exfiltration. Conventional intrusion detec- tion systems (IDS) face critical challenges like privacy, scala- bility, and robustness when applied in such heterogeneous IoT environments. To address these issues, we propose SecureDyn- FL, a comprehensive and robust privacy-preserving federated learning (FL) framework tailored for intrusion detection in IoT networks. SecureDyn-FL is designed to simultaneously address multiple security dimensions in FL-based IDS: (1) poisoning detection through dynamic temporal gradient auditing, (2) privacy protection against inference and eavesdrop- ping attacks through secure aggregation, and (3) adaptation to heterogeneous non-IID data via personalized learning. The framework introduces three core contributions: (i) a dynamic temporal gradient auditing mechanism that leverages Gaussian mixture models (GMMs) and Mahalanobis distance (MD) to detect stealthy and adaptive poisoning attacks, (ii) an optimized privacy-preserving aggregation scheme based on transformed additive ElGamal encryption with adaptive pruning and quantization for secure and efficient communication, and (iii) a dual-objective personalized learning strategy that improves model adaptation under non-IID data using logit-adjusted loss. Extensive experiments on the N-BaIoT dataset under both IID and non-IID settings, including scenarios with up to 50% adversarial clients, demonstrate that SecureDyn- FL consistently outperforms state-of-the-art FL-based IDS defenses.

</details>


### [16] [A Bayesian Network-Driven Zero Trust Model for Cyber Risk Quantification in Small-Medium Businesses](https://arxiv.org/abs/2601.06553)
*Ahmed M. Abdelmagid,Barry C. Ezell,Michael McShane*

Main category: cs.CR

TL;DR: 该研究探讨零信任架构(ZTA)作为中小型企业可持续网络安全解决方案的有效性，开发了一个集成预测模型来评估ZTA实施的可行性和风险缓解潜力。


<details>
  <summary>Details</summary>
Motivation: 中小型企业对全球经济至关重要，但由于预算有限、网络安全专业知识不足以及低估网络风险，它们对网络攻击高度脆弱。随着对数字基础设施的依赖增加，攻击面扩大，需要实施主动、自适应的安全措施。

Method: 开发了一个集成预测模型，包含两个子模型：第一个评估ZTA成功采用的概率（考虑实施障碍），第二个测试ZTA应对常见网络攻击的有效性。该模型预测ZTA存在下的风险水平，并量化ZTA增强中小企业网络韧性的不确定性。

Result: 该集成模型能够预测ZTA实施后的风险水平，并量化ZTA在多大程度上能增强中小企业网络韧性的不确定性，为从业者和利益相关者提供新的见解。

Conclusion: 零信任架构可作为中小企业可持续的网络安全解决方案，该研究开发的预测模型有助于评估ZTA实施的可行性和风险缓解效果，支持中小企业增强合规性、风险管理和治理活动。

Abstract: Small-Medium Businesses (SMBs) are essential to global economies yet remain highly vulnerable to cyberattacks due to limited budgets, inadequate cybersecurity expertise, and underestimation of cyber risks. Their increasing reliance on digital infrastructures has expanded their attack surfaces, exposing them to sophisticated and evolving threats. Consequently, implementing proactive, adaptive security measures has become imperative. This research investigates the effectiveness of Zero Trust Architecture (ZTA) as a sustainable cybersecurity solution tailored to SMBs. While ZTA adoption has been examined broadly, the specific financial, organizational, and capability constraints of SMBs remain underexplored. This study develops an integrated predictive model to assess both the feasibility and risk-mitigation potential of ZTA implementation. The model consists of two sub-models. The first sub-model evaluates the probability of successful ZTA adoption considering implied barriers, and the second tests the effectiveness of ZTA in responding to prevalent cyberattacks. The integrated model predicts the risk level in the presence of ZTA and quantifies the uncertainty of the extent to which ZTA can enhance SMBs' cyber resilience, contributing novel insights for practitioners and stakeholders seeking to enhance compliance with policies, risk, and governance activities in SMBs.

</details>


### [17] [QES-Backed Virtual FIDO2 Authenticators: Architectural Options for Secure, Synchronizable WebAuthn Credentials](https://arxiv.org/abs/2601.06554)
*Kemal Bicakci,Fatih Mehmet Varli,Muhammet Emir Korkmaz,Yusuf Uzunay*

Main category: cs.CR

TL;DR: 提出两种结合FIDO2与QES级PKCS#11密钥的架构，实现高安全认证密钥的云端同步，同时保持WebAuthn兼容性。


<details>
  <summary>Details</summary>
Motivation: FIDO2/WebAuthn提供防钓鱼认证但设备绑定密钥不便携，而passkey依赖云服务商信任；QES令牌提供高安全硬件根身份但不兼容WebAuthn。需要结合两者优势。

Method: 1) 基线架构：云端仅存储密文，解密能力锚定在用户硬件令牌；2) 强化架构：引入基于OPRF的机制绑定本地用户验证因子，防止跨协议滥用。

Result: 实现了基线架构，提供了系统模型、威胁分析和实验评估；强化架构进行了安全分析但未实现。两种架构都保持纯WebAuthn/FIDO2接口。

Conclusion: 提出的架构成功桥接了FIDO2与QES级PKCS#11技术，在保持WebAuthn兼容性的同时提供了不同的信任和部署权衡，适用于高安全认证部署。

Abstract: FIDO2 and the WebAuthn standard offer phishing-resistant, public-key based authentication but traditionally rely on device-bound cryptographic keys that are not naturally portable across user devices. Recent passkey deployments address this limitation by enabling multi-device credentials synchronized via platform-specific cloud ecosystems. However, these approaches require users and organizations to trust the corresponding cloud or phone providers with the protection and availability of their authentication material. In parallel, qualified electronic signature (QES) tokens and smart-card--based PKCS#11 modules provide high-assurance, hardware-rooted identity, yet they are not directly compatible with WebAuthn flows.
  This paper explores architectural options for bridging these technologies by securing a virtual FIDO2 authenticator with a QES-grade PKCS#11 key and enabling encrypted cloud synchronization of FIDO2 private keys. We first present and implement a baseline architecture in which the cloud stores only ciphertext and the decryption capability remains anchored exclusively in the user's hardware token. We then propose a hardened variant that introduces an Oblivious Pseudorandom Function (OPRF)-based mechanism bound to a local user-verification factor, thereby mitigating cross-protocol misuse and ensuring that synchronization keys cannot be repurposed outside the intended FIDO2 semantics; this enhanced design is analyzed but not implemented. Both architectures preserve a pure WebAuthn/FIDO2 interface to relying parties while offering different trust and deployment trade-offs. We provide the system model, threat analysis, implementation of the baseline architecture, and experimental evaluation, followed by a discussion of the hardened variant's security implications for high-assurance authentication deployments.

</details>


### [18] [Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity](https://arxiv.org/abs/2601.06596)
*Hongjun An,Yiliang Song,Jiangan Chen,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.CR

TL;DR: 本文提出偏好颠覆攻击(PUA)诊断方法，发现对齐模型易受操纵性提示影响而牺牲真实性取悦用户，更先进模型有时更脆弱，需要针对性防御而非统一鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练通常优化偏好对齐，奖励被认为有帮助和互动友好的输出。但这种偏好导向目标可能被利用：操纵性提示可以引导模型倾向于取悦用户的同意，而远离真相导向的纠正。本文研究对齐模型是否容易受到偏好颠覆攻击(PUA)的影响。

Method: 提出诊断方法，使用因子评估框架，在受控的2×2^4设计中，将提示诱导的转变分解为系统目标（真相导向vs偏好导向）和PUA风格对话因素（指令控制、个人贬低、条件性认可、现实否认）的可解释效应。

Result: 令人惊讶的是，更先进的模型有时更容易受到操纵性提示的影响。除了主导的现实否认因素外，观察到模型特定的符号反转和与PUA风格因素的交互作用，表明需要针对性防御而非统一鲁棒性。

Conclusion: 这些发现提供了一种新颖、可重复的因子评估方法，为RLHF等后训练过程提供更细粒度的诊断，通过更细致地理解偏好对齐风险和操纵性提示的影响，使LLM产品迭代中能做出更好的权衡。

Abstract: Large Language Model (LLM) training often optimizes for preference alignment, rewarding outputs that are perceived as helpful and interaction-friendly. However, this preference-oriented objective can be exploited: manipulative prompts can steer responses toward user-appeasing agreement and away from truth-oriented correction. In this work, we investigate whether aligned models are vulnerable to Preference-Undermining Attacks (PUA), a class of manipulative prompting strategies designed to exploit the model's desire to please user preferences at the expense of truthfulness. We propose a diagnostic methodology that provides a finer-grained and more directive analysis than aggregate benchmark scores, using a factorial evaluation framework to decompose prompt-induced shifts into interpretable effects of system objectives (truth- vs. preference-oriented) and PUA-style dialogue factors (directive control, personal derogation, conditional approval, reality denial) within a controlled $2 \times 2^4$ design. Surprisingly, more advanced models are sometimes more susceptible to manipulative prompts. Beyond the dominant reality-denial factor, we observe model-specific sign reversals and interactions with PUA-style factors, suggesting tailored defenses rather than uniform robustness. These findings offer a novel, reproducible factorial evaluation methodology that provides finer-grained diagnostics for post-training processes like RLHF, enabling better trade-offs in the product iteration of LLMs by offering a more nuanced understanding of preference alignment risks and the impact of manipulative prompts.

</details>


### [19] [Cross-Border Data Security and Privacy Risks in Large Language Models and IoT Systems](https://arxiv.org/abs/2601.06612)
*Chalitha Handapangoda*

Main category: cs.CR

TL;DR: 提出了一种面向多法域的隐私保护架构，通过动态加密、差分隐私和实时合规验证，在保持AI模型性能的同时实现跨境数据安全与合规。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和物联网系统依赖全球分布式数据流，面临跨境法律冲突（如GDPR和PIPL）和技术漏洞（如模型记忆）的双重挑战。现有静态加密和数据本地化方法分散且被动，无法提供足够的政策对齐保护。

Method: 提出了一种"法域感知、隐私设计"架构，动态整合本地化加密、自适应差分隐私和通过密码学证明的实时合规声明。在多法域模拟环境中进行了实证验证。

Result: 该架构将未授权数据暴露降至5%以下，实现零合规违规，同时保持模型效用保留率超过90%，计算开销有限。

Conclusion: 主动、集成的控制机制对于安全和全球合规的AI部署是可行的，为跨境数据流动提供了有效的隐私保护解决方案。

Abstract: The reliance of Large Language Models and Internet of Things systems on massive, globally distributed data flows creates systemic security and privacy challenges. When data traverses borders, it becomes subject to conflicting legal regimes, such as the EU's General Data Protection Regulation and China's Personal Information Protection Law, compounded by technical vulnerabilities like model memorization. Current static encryption and data localization methods are fragmented and reactive, failing to provide adequate, policy-aligned safeguards. This research proposes a Jurisdiction-Aware, Privacy-by-Design architecture that dynamically integrates localized encryption, adaptive differential privacy, and real-time compliance assertion via cryptographic proofs. Empirical validation in a multi-jurisdictional simulation demonstrates this architecture reduced unauthorized data exposure to below five percent and achieved zero compliance violations. These security gains were realized while maintaining model utility retention above ninety percent and limiting computational overhead. This establishes that proactive, integrated controls are feasible for secure and globally compliant AI deployment.

</details>


### [20] [Burn-After-Use for Preventing Data Leakage through a Secure Multi-Tenant Architecture in Enterprise LLM](https://arxiv.org/abs/2601.06627)
*Qiang Zhang,Elena Emma Wang,Jiaming Li,Xichun Wang*

Main category: cs.CR

TL;DR: 提出SMTA安全多租户架构与BAU用后即焚机制，防止企业LLM数据泄漏，实验显示92%防御成功率与76.75%泄漏缓解成功率。


<details>
  <summary>Details</summary>
Motivation: 企业各部门采用LLM时面临数据泄漏风险，成为关键的安全与合规问题，需要防止跨部门、跨会话的数据泄漏。

Method: SMTA隔离各部门LLM实例并强制执行上下文所有权边界；BAU机制引入短暂会话上下文，使用后自动销毁，防止跨会话/用户推理。

Result: SMTA在55次基础设施级攻击测试中达到92%防御成功率；BAU在72次测试迭代中达到76.75%泄漏威胁缓解成功率。

Conclusion: SMTA与BAU结合为企业LLM提供严格隔离、完整会话短暂性、强保密保证、非持久性和策略对齐行为。

Abstract: This study presents a Secure Multi-Tenant Architecture (SMTA) combined with a novel concept Burn-After-Use (BAU) mechanism for enterprise LLM environments to effectively prevent data leakage. As institutions increasingly adopt LLMs across departments, the risks of data leakage have become a critical security and compliance concern. The proposed SMTA isolates LLM instances across departments and enforces rigorous context ownership boundaries within an internally deployed infrastructure. The BAU mechanism introduces data confidentiality by enforcing ephemeral conversational contexts that are automatically destroyed after use, preventing cross-session or cross-user inference. The evaluation to SMTA and BAU is through two sets of realistic and reproducible experiments comprising of 127 test iterations. One aspect of this experiment is to assess prompt-based and semantic leakage attacks in a multi-tenant architecture (Appendix A) across 55 infrastructure-level attack tests, including vector-database credential compromise and shared logging pipeline exposure. SMTA achieves 92% defense success rate, demonstrating strong semantic isolation while highlighting residual risks from credential misconfiguration and observability pipelines. Another aspect is to evaluate the robustness of BAU under realistic failure scenarios (Appendix B) using four empirical metrics: Local Residual Persistence Rate (LRPR), Remote Residual Persistence Rate (RRPR), Image Frame Exposure Rate (IFER), and Burn Timer Persistence Rate (BTPR). Across 72 test iterations, BAU achieves a 76.75% success rate in mitigating post-session leakage threats across the client, server, application, infrastructure, and cache layers. These results show that SMTA and BAU together enforce strict isolation, complete session ephemerality, strong confidentiality guarantees, non-persistence, and policy-aligned behavior for enterprise LLMs.

</details>


### [21] [Attack-Resistant Watermarking for AIGC Image Forensics via Diffusion-based Semantic Deflection](https://arxiv.org/abs/2601.06639)
*Qingyu Liu,Yitao Zhang,Zhongjie Ba,Chao Shuai,Peng Cheng,Tianhang Zheng,Zhibo Wang*

Main category: cs.CR

TL;DR: PAI是一个无需训练的固有水印框架，用于保护AI生成图像的版权，通过密钥引导的去噪轨迹实现鲁棒的所有权验证、攻击检测和语义级篡改定位。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法存在两个主要问题：1）在现实对抗威胁下脆弱，需要在防欺骗和防移除攻击之间权衡；2）无法支持语义级篡改定位。随着AIGC在创意工作流中普及，保护用户生成AI图像的版权成为新兴挑战。

Method: PAI采用训练免费的固有水印框架，与基于扩散的AIGC服务即插即用。设计了新颖的密钥引导偏转机制，根据用户密钥微妙地引导去噪轨迹，实现身份与内容的语义纠缠，增强对现实威胁的鲁棒性。

Result: 在12种攻击方法上的实验显示，PAI达到98.43%的验证准确率，比现有最佳方法平均提升37.25%，即使面对高级AIGC编辑也能保持强大的篡改定位性能。

Conclusion: PAI为AIGC版权保护提供了同时具备鲁棒所有权验证、攻击检测和语义级篡改定位能力的解决方案，通过理论分析证明只有有效密钥才能通过验证，在现实威胁下表现出色。

Abstract: Protecting the copyright of user-generated AI images is an emerging challenge as AIGC becomes pervasive in creative workflows. Existing watermarking methods (1) remain vulnerable to real-world adversarial threats, often forced to trade off between defenses against spoofing and removal attacks; and (2) cannot support semantic-level tamper localization. We introduce PAI, a training-free inherent watermarking framework for AIGC copyright protection, plug-and-play with diffusion-based AIGC services. PAI simultaneously provides three key functionalities: robust ownership verification, attack detection, and semantic-level tampering localization. Unlike existing inherent watermark methods that only embed watermarks at noise initialization of diffusion models, we design a novel key-conditioned deflection mechanism that subtly steers the denoising trajectory according to the user key. Such trajectory-level coupling further strengthens the semantic entanglement of identity and content, thereby further enhancing robustness against real-world threats. Moreover, we also provide a theoretical analysis proving that only the valid key can pass verification. Experiments across 12 attack methods show that PAI achieves 98.43\% verification accuracy, improving over SOTA methods by 37.25\% on average, and retains strong tampering localization performance even against advanced AIGC edits. Our code is available at https://github.com/QingyuLiu/PAI.

</details>


### [22] [zkRansomware: Proof-of-Data Recoverability and Multi-round Game Theoretic Modeling of Ransomware Decisions](https://arxiv.org/abs/2601.06667)
*Xinyu Hou,Yang Lu,Rabimba Karanjai,Lei Xu,Weidong Shi*

Main category: cs.CR

TL;DR: zkRansomware：一种结合零知识证明和智能合约的新型勒索软件模型，可实现可验证的数据恢复并减少数据泄露风险


<details>
  <summary>Details</summary>
Motivation: 传统勒索软件存在受害者支付赎金后无法恢复数据、面临数据隐私泄露风险的问题，这些不确定性严重影响了攻击者与受害者之间的决策动态

Method: 提出zkRansomware模型，整合零知识证明实现可验证的数据恢复，使用智能合约强制执行多轮支付，同时降低数据披露和隐私丢失风险

Result: 证明zkRansomware在技术上可行，使用现有密码学和区块链工具即可实现，并且能够（可能违反直觉地）协调攻击者与受害者的利益

Conclusion: 开发了zkRansomware的理论决策框架，区别于已知的勒索软件决策模型，讨论了其对勒索软件风险分析和响应决策支持的影响

Abstract: Ransomware is still one of the most serious cybersecurity threats. Victims often pay but fail to regain access to their data, while also facing the danger of losing data privacy. These uncertainties heavily shape the attacker-victim dynamics in decision-making. In this paper, we introduce and analyze zkRansomware. This new ransomware model integrates zero-knowledge proofs to enable verifiable data recovery and uses smart contracts to enforce multi-round payments while mitigating the risk of data disclosure and privacy loss. We show that zkRansomware is technically feasible using existing cryptographic and blockchain tools and, perhaps counterintuitively, can align incentives between the attacker and the victim. Finally, we develop a theoretical decision-making frame- work for zkRansomware that distinguishes it from known ransomware decision models and discusses its implications for ransomware risk anal- ysis and response decision support.

</details>


### [23] [S-DAPT-2026: A Stage-Aware Synthetic Dataset for Advanced Persistent Threat Detection](https://arxiv.org/abs/2601.06690)
*Saleem Ishaq Tijjani,Bogdan Ghita,Nathan Clarke,Matthew Craven*

Main category: cs.CR

TL;DR: 本文提出了一种近乎真实的APT合成数据集和高效的告警关联框架，使用KNN聚类和余弦相似度进行语义关联，覆盖14种告警类型，支持阶段感知分析。


<details>
  <summary>Details</summary>
Motivation: APT检测面临挑战，因为APT具有隐蔽、多阶段特性，且缺乏现实的标记数据集进行系统评估。现有合成数据集方法依赖计算成本高的告警关联机制，限制了可扩展性。

Method: 提出基于机器学习的关联模块，使用K最近邻聚类和余弦相似度度量，在时间上下文中对语义相关的告警进行分组。数据集模拟校园和组织网络环境中的多阶段APT活动。

Result: 数据集覆盖14种不同的告警类型，超过了常用合成APT数据集的覆盖范围。定义了明确的APT活动状态和告警到阶段的映射，支持灵活集成新告警类型和阶段感知分析。

Conclusion: 提供了数据集的全面统计特征以促进可重复性，并支持APT阶段预测。该框架解决了现有方法在可扩展性和覆盖范围方面的限制。

Abstract: The detection of advanced persistent threats (APTs) remains a crucial challenge due to their stealthy, multistage nature and the limited availability of realistic, labeled datasets for systematic evaluation. Synthetic dataset generation has emerged as a practical approach for modeling APT campaigns; however, existing methods often rely on computationally expensive alert correlation mechanisms that limit scalability. Motivated by these limitations, this paper presents a near realistic synthetic APT dataset and an efficient alert correlation framework. The proposed approach introduces a machine learning based correlation module that employs K Nearest Neighbors (KNN) clustering with a cosine similarity metric to group semantically related alerts within a temporal context. The dataset emulates multistage APT campaigns across campus and organizational network environments and captures a diverse set of fourteen distinct alert types, exceeding the coverage of commonly used synthetic APT datasets. In addition, explicit APT campaign states and alert to stage mappings are defined to enable flexible integration of new alert types and support stage aware analysis. A comprehensive statistical characterization of the dataset is provided to facilitate reproducibility and support APT stage predictions.

</details>


### [24] [Incentive Mechanism Design for Privacy-Preserving Decentralized Blockchain Relayers](https://arxiv.org/abs/2601.06699)
*Boutaina Jebari,Khalil Ibrahimi,Hamidou Tembine,Mounir Ghogho*

Main category: cs.CR

TL;DR: 提出基于博弈论激励设计的去中心化中继器架构，通过概率性上传作为混合纳什均衡来增强区块链隐私和可靠性


<details>
  <summary>Details</summary>
Motivation: 公共区块链存在隐私问题，现有依赖单一中继器的方案存在单点故障和可被利用的漏洞，需要设计去中心化中继器架构来增强隐私和可靠性

Method: 将中继器间交互建模为非合作博弈，设计激励机制使概率性上传成为唯一的混合纳什均衡，使用演化博弈分析均衡稳定性，并进行数值评估

Result: 系统在高交易成本下仍能保持可靠性（中断概率低于0.05），揭示了去中心化中继器系统中隐私、可靠性、鲁棒性和成本之间的基本权衡关系

Conclusion: 提出的去中心化中继器架构通过博弈论激励设计有效增强了区块链隐私和可靠性，解决了单一中继器的脆弱性问题

Abstract: Public blockchains, though renowned for their transparency and immutability, suffer from significant privacy concerns. Network-level analysis and long-term observation of publicly available transactions can often be used to infer user identities. To mitigate this, several blockchain applications rely on relayers, which serve as intermediary nodes between users and smart contracts deployed on the blockchain. However, dependence on a single relayer not only creates a single point of failure but also introduces exploitable vulnerabilities that weaken the system's privacy guarantees. This paper proposes a decentralized relayer architecture that enhances privacy and reliability through game-theoretic incentive design. We model the interaction among relayers as a non-cooperative game and design an incentive mechanism in which probabilistic uploading emerges as a unique mixed Nash equilibrium. Using evolutionary game analysis, we demonstrate the equilibrium's stability against perturbations and coordinated deviations. Through numerical evaluations, we analyze how equilibrium strategies and system behavior evolve with key parameters such as the number of relayers, upload costs, rewards, and penalties. In particular, we show that even with high transaction costs, the system maintains reliability with an outage probability below 0.05 . Furthermore, our results highlight a fundamental trade-off between privacy, reliability, robustness, and cost in decentralized relayer systems.

</details>


### [25] [Behavioral Analytics for Continuous Insider Threat Detection in Zero-Trust Architectures](https://arxiv.org/abs/2601.06708)
*Gaurav Sarraf*

Main category: cs.CR

TL;DR: 该论文提出了一种基于AdaBoost的行为分析框架，用于零信任架构中的内部威胁检测，在CERT数据集上实现了98%的准确率。


<details>
  <summary>Details</summary>
Motivation: 零信任架构虽然移除了隐式信任，但攻击者仍可使用合法凭证伪装正常用户活动，因此需要持续监控用户行为来检测内部威胁。

Method: 使用CERT内部威胁数据集，进行数据清洗、归一化和SMOTE类别平衡，采用PCA降维，开发并评估了基于AdaBoost的分类器，并与SVM、ANN、贝叶斯网络进行对比。

Result: AdaBoost在准确率(98.0%)、精确率(98.3%)、召回率(98.0%)和F1分数上均优于SVM(90.1%)、ANN(94.7%)和贝叶斯网络(94.9%)，ROC曲线下面积达到0.98。

Conclusion: 基于AdaBoost的行为分析框架在零信任环境中能有效可靠地增强持续内部威胁检测能力。

Abstract: Insider threats are a particularly tricky cybersecurity issue, especially in zero-trust architectures (ZTA) where implicit trust is removed. Although the rule of thumb is never trust, always verify, attackers can still use legitimate credentials and impersonate the standard user activity. In response, behavioral analytics with machine learning (ML) can help monitor the user activity continuously and identify the presence of anomalies. This introductory framework makes use of the CERT Insider Threat Dataset for data cleaning, normalization, and class balance using the Synthetic Minority Oversampling Technique (SMOTE). It also employs Principal Component Analysis (PCA) for dimensionality reduction. Several benchmark models, including Support Vector Machine (SVM), Artificial Neural Network (ANN), and Bayesian Network (Bayes Net), were used to develop and evaluate the AdaBoost classifier. Compared to SVM (90.1%), ANN (94.7%), and Bayes Net (94.9), AdaBoost achieved higher performance with a 98.0% ACC, 98.3% PRE, 98.0% REC, and F1-score (F1). The Receiver Operating Characteristic (ROC) study, which provided further confirmation of its strength, yielded an Area Under the Curve (AUC) of 0.98. These results prove the effectiveness and dependability of AdaBoost-based behavioral analytics as a solution to reinforcing continuous insider threat detection in zero-trust settings.

</details>


### [26] [Privacy-Preserving Data Processing in Cloud : From Homomorphic Encryption to Federated Analytics](https://arxiv.org/abs/2601.06710)
*Gaurav Sarraf,Vibhor Pal*

Main category: cs.CR

TL;DR: 本文全面综述了云计算平台中的隐私保护机制，包括差分隐私、同态加密等统计和密码学方法，以及联邦分析和联邦学习等分布式学习框架，分析了它们在医疗、金融、IoT等领域的应用、优势、局限性和权衡。


<details>
  <summary>Details</summary>
Motivation: 随着云计算和数据驱动应用的扩展，保护个人、金融和医疗等敏感信息的需求日益增长。传统的集中式数据处理方法使敏感数据面临泄露风险，因此需要采用去中心化和安全的数据处理方法。

Method: 本文采用文献综述方法，详细分析了云计算平台中的隐私保护机制，包括统计方法（如差分隐私）和密码学解决方案（如同态加密），以及分布式学习框架（联邦分析和联邦学习）。通过比较分析评估了安全性、效率、可扩展性和准确性之间的权衡，并研究了新兴的混合框架。

Result: 综述系统性地比较了不同隐私保护技术的优缺点，分析了它们在医疗、金融、物联网和工业等领域的应用案例。研究发现各种方法在安全性、效率、可扩展性和准确性之间存在权衡，混合框架有望提供更好的隐私保护。

Conclusion: 本文详细审查了云计算中最近的隐私保护方法，为学者和从业者提供了关于安全有效数据处理解决方案的关键信息。同时指出了计算开销、隐私-效用权衡、标准化、对抗性威胁和云集成等关键问题，为未来研究提供了方向。

Abstract: Privacy-preserving data processing refers to the methods and models that allow computing and analyzing sensitive data with a guarantee of confidentiality. As cloud computing and applications that rely on data continue to expand, there is an increasing need to protect personal, financial and healthcare information. Conventional centralized data processing methods expose sensitive data to risk of breaches, compelling the need to use decentralized and secure data methods. This paper gives a detailed review of privacy-saving mechanisms in the cloud platform, such as statistical approaches like differential privacy and cryptographic solutions like homomorphic encryption. Federated analytics and federated learning, two distributed learning frameworks, are also discussed. Their principles, applications, benefits, and limitations are reviewed, with roles of use in the fields of healthcare, finance, IoT, and industrial cases. Comparative analyses measure trade-offs in security, efficiency, scalability, and accuracy, and investigations are done of emerging hybrid frameworks to provide better privacy protection. Critical issues, including computational overhead, privacy-utility trade-offs, standardization, adversarial threats, and cloud integration are also addressed. This review examines in detail the recent privacy-protecting approaches in cloud computation and offers scholars and practitioners crucial information on secure and effective solutions to data processing.

</details>


### [27] [Deep Recurrent Hidden Markov Learning Framework for Multi-Stage Advanced Persistent Threat Prediction](https://arxiv.org/abs/2601.06734)
*Saleem Ishaq Tijjani,Bogdan Ghita,Nathan Clarke,Matthew Craven*

Main category: cs.CR

TL;DR: E-HiDNet：结合CNN/RNN与HMM的混合深度概率学习框架，用于预测APT攻击的阶段演进，在稀疏观测下仍能实现高精度阶段预测。


<details>
  <summary>Details</summary>
Motivation: 现有APT检测方法多为反应式和警报中心式，缺乏对攻击阶段演进的预测能力，且在观测稀疏或不完整时推理能力有限。

Method: 提出E-HiDNet统一框架，集成卷积神经网络(CNN)和循环神经网络(RNN)提取警报序列的时空特征，结合隐马尔可夫模型(HMM)建模潜在攻击阶段及随机转移，并引入改进的Viterbi算法处理不完整观测。

Result: 在合成但结构真实的APT数据集(S-DAPT-2026)上评估，E-HiDNet实现98.8-100%的阶段预测准确率，当有4个以上观测时显著优于独立HMM，即使在训练数据减少的场景下仍表现良好。

Conclusion: 深度语义特征学习与概率状态空间建模的结合能显著提升APT阶段预测性能和态势感知能力，为主动式APT防御提供支持。

Abstract: Advanced Persistent Threats (APTs) represent hidden, multi\-stage cyberattacks whose long term persistence and adaptive behavior challenge conventional intrusion detection systems (IDS). Although recent advances in machine learning and probabilistic modeling have improved APT detection performance, most existing approaches remain reactive and alert\-centric, providing limited capability for stage-aware prediction and principled inference under uncertainty, particularly when observations are sparse or incomplete. This paper proposes E\-HiDNet, a unified hybrid deep probabilistic learning framework that integrates convolutional and recurrent neural networks with a Hidden Markov Model (HMM) to allow accurate prediction of the progression of the APT campaign. The deep learning component extracts hierarchical spatio\-temporal representations from correlated alert sequences, while the HMM models latent attack stages and their stochastic transitions, allowing principled inference under uncertainty and partial observability. A modified Viterbi algorithm is introduced to handle incomplete observations, ensuring robust decoding under uncertainty. The framework is evaluated using a synthetically generated yet structurally realistic APT dataset (S\-DAPT\-2026). Simulation results show that E\-HiDNet achieves up to 98.8\-100\% accuracy in stage prediction and significantly outperforms standalone HMMs when four or more observations are available, even under reduced training data scenarios. These findings highlight that combining deep semantic feature learning with probabilistic state\-space modeling enhances predictive APT stage performance and situational awareness for proactive APT defense.

</details>


### [28] [ALFA: A Safe-by-Design Approach to Mitigate Quishing Attacks Launched via Fancy QR Codes](https://arxiv.org/abs/2601.06768)
*Muhammad Wahid Akram,Keshav Sood,Muneeb Ul Hassan,Dhananjay Thiruvady*

Main category: cs.CR

TL;DR: ALFA是一种安全设计方法，通过将花式QR码转换为二进制网格，识别并修复错误模块，然后使用预训练模型预测QR码合法性，有效防御Quishing攻击。


<details>
  <summary>Details</summary>
Motivation: 随着QR码变得越来越花哨多彩，攻击者利用这些视觉上吸引人的QR码进行钓鱼攻击（Quishing），这些花式QR码能够逃避现有的深度学习视觉检测和其他主流防御措施，需要新的解决方案来保护用户。

Method: ALFA方法首先将花式QR码转换为二进制网格副本，识别网格中的错误模块表示，然后使用"FAST"方法从二进制网格中恢复错误模块。接着提取花式QR码的结构特征，使用预训练模型预测其合法性。

Result: 在包含多样化花式QR码变体的合成数据集上评估，仅达到0.06%的假阴性率。开发了移动应用程序进行实际可行性测试，与真实世界QR阅读器比较显示该解决方案在真实环境中具有分类可靠性和检测准确性。

Conclusion: ALFA提供了一种安全设计方法，能够有效缓解Quishing攻击，防止用户访问花式QR码扫描后的有害负载，在真实环境中表现出良好的检测性能。

Abstract: Phishing with Quick Response (QR) codes is termed as Quishing. The attackers exploit this method to manipulate individuals into revealing their confidential data. Recently, we see the colorful and fancy representations of QR codes, the 2D matrix of QR codes which does not reflect a typical mixture of black-white modules anymore. Instead, they become more tempting as an attack vector for adversaries which can evade the state-of-the-art deep learning visual-based and other prevailing countermeasures. We introduce "ALFA", a safe-by-design approach, to mitigate Quishing and prevent everyone from accessing the post-scan harmful payload of fancy QR codes. Our method first converts a fancy QR code into the replica of binary grid and then identify the erroneous representation of modules in that grid. Following that, we present "FAST" method which can conveniently recover erroneous modules from that binary grid. Afterwards, using this binary grid, our solution extracts the structural features of fancy QR code and predicts its legitimacy using a pre-trained model. The effectiveness of our proposal is demonstrated by the experimental evaluation on a synthetic dataset (containing diverse variations of fancy QR codes) and achieve a FNR of 0.06% only. We also develop the mobile app to test the practical feasibility of our solution and provide a performance comparison of the app with the real-world QR readers. This comparison further highlights the classification reliability and detection accuracy of this solution in real-world environments.

</details>


### [29] [CyberLLM-FINDS 2025: Instruction-Tuned Fine-tuning of Domain-Specific LLMs with Retrieval-Augmented Generation and Graph Integration for MITRE Evaluation](https://arxiv.org/abs/2601.06779)
*Vasanth Iyer,Leonardo Bobadilla,S. S. Iyengar*

Main category: cs.CR

TL;DR: 将Gemma-2B模型微调为网络安全专用LLM，通过合成数据生成、量化权重和混合策略解决上下文窗口限制，并引入RAG和图推理框架提升MITRE ATT&CK对齐能力


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型缺乏网络安全领域的专业知识，需要开发领域专用模型来支持威胁检测、取证调查和攻击分析等实际应用

Method: 1) 数据集准备、微调和合成数据生成；2) 使用量化权重和思维链提示优化性能；3) 采用云LLM生成合成数据与本地微调的混合策略；4) 引入RAG管道和图推理框架，通过STIX威胁情报对齐MITRE ATT&CK技术

Result: 实验发现领域微调中提示长度分布不均限制了上下文窗口的有效利用（仅200-400个token）。思维链提示与量化权重组合效果最佳。图增强方法显著提升了模型在TTP覆盖方面的对齐能力，改善了多跳和长上下文场景的召回率

Conclusion: 通过微调、量化、混合策略和图推理框架，成功将Gemma-2B转化为有效的网络安全专用LLM，验证了图增强LLM在网络安全威胁情报应用中的实用价值

Abstract: Large Language Models (LLMs) such as Gemma-2B have shown strong performance in various natural language processing tasks. However, general-purpose models often lack the domain expertise required for cybersecurity applications. This work presents a methodology to fine-tune the Gemma-2B model into a domain-specific cybersecurity LLM. We detail the processes of dataset preparation, fine-tuning, and synthetic data generation, along with implications for real-world applications in threat detection, forensic investigation, and attack analysis.
  Experiments highlight challenges in prompt length distribution during domain-specific fine-tuning. Uneven prompt lengths limit the model's effective use of the context window, constraining local inference to 200-400 tokens despite hardware support for longer sequences. Chain-of-thought styled prompts, paired with quantized weights, yielded the best performance under these constraints. To address context limitations, we employed a hybrid strategy using cloud LLMs for synthetic data generation and local fine-tuning for deployment efficiency.
  To extend the evaluation, we introduce a Retrieval-Augmented Generation (RAG) pipeline and graph-based reasoning framework. This approach enables structured alignment with MITRE ATT&CK techniques through STIX-based threat intelligence, enhancing recall in multi-hop and long-context scenarios. Graph modules encode entity-neighborhood context and tactic chains, helping mitigate the constraints of short prompt windows. Results demonstrate improved model alignment with tactic, technique, and procedure (TTP) coverage, validating the utility of graph-augmented LLMs in cybersecurity threat intelligence applications.

</details>


### [30] [SecMoE: Communication-Efficient Secure MoE Inference via Select-Then-Compute](https://arxiv.org/abs/2601.06790)
*Bowen Shen,Yuyue Chen,Peng Yang,Bin Zhang,Xi Zhang,Zoe L. Jiang*

Main category: cs.CR

TL;DR: SecMoE：一种保护隐私的Transformer推理框架，通过Select-Then-Compute方法在MoE架构中实现高效隐私保护，避免专家激活信息泄露。


<details>
  <summary>Details</summary>
Motivation: 现有隐私保护Transformer推理框架在处理大规模模型时存在百倍差距，而MoE架构虽然能扩展模型容量，但在当前安全两方计算协议下，服务器能看到哪些专家被激活，从而泄露客户端输入的token级隐私信息。

Method: 提出SecMoE框架，统一MoE层和分段多项式函数中的逐项电路，通过Select-Then-Compute方法从电路中提取参数并仅计算一个加密项，实现隐私保护的专家选择。

Result: SecMoE使隐私推理模型规模扩大63倍，端到端运行时间仅增加15.2倍；在5专家设置下，通信量降低1.8-7.1倍，速度提升1.3-3.8倍。

Conclusion: SecMoE有效解决了MoE架构中的隐私泄露问题，在保持稀疏性优势的同时实现了高效的隐私保护推理，为大规模隐私保护模型推理提供了可行方案。

Abstract: Privacy-preserving Transformer inference has gained attention due to the potential leakage of private information. Despite recent progress, existing frameworks still fall short of practical model scales, with gaps up to a hundredfold. A possible way to close this gap is the Mixture of Experts (MoE) architecture, which has emerged as a promising technique to scale up model capacity with minimal overhead. However, given that the current secure two-party (2-PC) protocols allow the server to homomorphically compute the FFN layer with its plaintext model weight, under the MoE setting, this could reveal which expert is activated to the server, exposing token-level privacy about the client's input. While naively evaluating all the experts before selection could protect privacy, it nullifies MoE sparsity and incurs the heavy computational overhead that sparse MoE seeks to avoid. To address the privacy and efficiency limitations above, we propose a 2-PC privacy-preserving inference framework, \SecMoE. Unifying per-entry circuits in both the MoE layer and piecewise polynomial functions, \SecMoE obliviously selects the extracted parameters from circuits and only computes one encrypted entry, which we refer to as Select-Then-Compute. This makes the model for private inference scale to 63$\times$ larger while only having a 15.2$\times$ increase in end-to-end runtime. Extensive experiments show that, under 5 expert settings, \SecMoE lowers the end-to-end private inference communication by 1.8$\sim$7.1$\times$ and achieves 1.3$\sim$3.8$\times$ speedup compared to the state-of-the-art (SOTA) protocols.

</details>


### [31] [CHASE: LLM Agents for Dissecting Malicious PyPI Packages](https://arxiv.org/abs/2601.06838)
*Takaaki Toda,Tatsuya Mori*

Main category: cs.CR

TL;DR: CHASE是一个基于多智能体架构的恶意软件检测系统，通过分层协作和确定性安全工具集成，在PyPI等软件包注册表中实现高可靠性检测，达到98.4%召回率和仅0.08%误报率。


<details>
  <summary>Details</summary>
Motivation: 现代软件包注册表（如PyPI）已成为软件开发的关键基础设施，但正被威胁行为者利用来分发具有复杂多阶段攻击链的恶意软件包。虽然大语言模型（LLMs）为自动化代码分析提供了有前景的能力，但其在安全关键的恶意软件检测应用中面临幻觉和上下文混淆等根本性挑战，可能导致漏检或误报。

Method: 提出CHASE（协作分层安全探索智能体），采用高可靠性多智能体架构，通过Plan-and-Execute协调模型、专注于特定分析方面的专业Worker Agents，以及与确定性安全工具的关键操作集成。核心洞察是：基于LLM的安全分析的可靠性不是来自改进单个模型能力，而是通过架构设计来补偿LLM弱点同时利用其语义理解优势。

Result: 在包含3,000个软件包（500个恶意，2,500个良性）的数据集上评估，CHASE实现了98.4%的召回率和仅0.08%的误报率，同时保持每个软件包4.5分钟的中位分析时间，适合自动化软件包筛查的操作部署。此外，通过对网络安全专业人员的调查评估了生成的分析报告，识别了其关键优势和需要改进的领域。

Conclusion: 这项工作为构建可靠的AI驱动安全工具提供了蓝图，能够随着现代软件供应链日益增长的复杂性而扩展。CHASE展示了通过架构设计而非单纯改进模型能力来实现高可靠性LLM安全分析的有效途径。

Abstract: Modern software package registries like PyPI have become critical infrastructure for software development, but are increasingly exploited by threat actors distributing malicious packages with sophisticated multi-stage attack chains. While Large Language Models (LLMs) offer promising capabilities for automated code analysis, their application to security-critical malware detection faces fundamental challenges, including hallucination and context confusion, which can lead to missed detections or false alarms. We present CHASE (Collaborative Hierarchical Agents for Security Exploration), a high-reliability multi-agent architecture that addresses these limitations through a Plan-and-Execute coordination model, specialized Worker Agents focused on specific analysis aspects, and integration with deterministic security tools for critical operations. Our key insight is that reliability in LLM-based security analysis emerges not from improving individual model capabilities but from architecting systems that compensate for LLM weaknesses while leveraging their semantic understanding strengths. Evaluation on a dataset of 3,000 packages (500 malicious, 2,500 benign) demonstrates that CHASE achieves 98.4% recall with only 0.08% false positive rate, while maintaining a practical median analysis time of 4.5 minutes per package, making it suitable for operational deployment in automated package screening. Furthermore, we conducted a survey with cybersecurity professionals to evaluate the generated analysis reports, identifying their key strengths and areas for improvement. This work provides a blueprint for building reliable AI-powered security tools that can scale with the growing complexity of modern software supply chains. Our project page is available at https://t0d4.github.io/CHASE-AIware25/

</details>


### [32] [qAttCNN - Self Attention Mechanism for Video QoE Prediction in Encrypted Traffic](https://arxiv.org/abs/2601.06862)
*Michael Sidorov,Ofer Hadar*

Main category: cs.CR

TL;DR: 提出qAttCNN模型，利用流量包大小参数从加密视频通话中推断无参考QoE指标（BRISQUE和FPS），解决ISP无法访问原始媒体流的问题。


<details>
  <summary>Details</summary>
Motivation: 随着视频会议和即时通讯应用的普及，WebRTC协议被广泛使用，但网络条件恶化会降低流媒体质量和用户体验。由于端到端加密，互联网服务提供商无法访问原始媒体流，只能获得QoS和路由信息，难以准确评估QoE。

Method: 提出QoE Attention Convolutional Neural Network (qAttCNN)模型，利用流量包大小参数来推断两个无参考QoE指标：BRISQUE（视频质量指标）和帧率（FPS）。

Result: 在自定义的WhatsApp视频通话数据集上评估qAttCNN，使用平均绝对误差百分比（MAEP）衡量，BRISQUE预测误差为2.14%，FPS预测误差为7.39%，优于现有QoE模型。

Conclusion: qAttCNN能够有效利用加密视频通话中的流量包大小信息准确推断QoE指标，为互联网服务提供商在无法访问原始媒体流的情况下监控和改善用户体验提供了可行方案。

Abstract: The rapid growth of multimedia consumption, driven by major advances in mobile devices since the mid-2000s, has led to widespread use of video conferencing applications (VCAs) such as Zoom and Google Meet, as well as instant messaging applications (IMAs) like WhatsApp and Telegram, which increasingly support video conferencing as a core feature. Many of these systems rely on the Web Real-Time Communication (WebRTC) protocol, enabling direct peer-to-peer media streaming without requiring a third-party server to relay data, reducing the latency and facilitating a real-time communication. Despite WebRTC's potential, adverse network conditions can degrade streaming quality and consequently reduce users' Quality of Experience (QoE). Maintaining high QoE therefore requires continuous monitoring and timely intervention when QoE begins to deteriorate. While content providers can often estimate QoE by directly comparing transmitted and received media, this task is significantly more challenging for internet service providers (ISPs). End-to-end encryption, commonly used by modern VCAs and IMAs, prevent ISPs from accessing the original media stream, leaving only Quality of Service (QoS) and routing information available. To address this limitation, we propose the QoE Attention Convolutional Neural Network (qAttCNN), a model that leverages packet size parameter of the traffic to infer two no-reference QoE metrics viz. BRISQUE and frames per second (FPS). We evaluate qAttCNN on a custom dataset collected from WhatsApp video calls and compare it against existing QoE models. Using mean absolute error percentage (MAEP), our approach achieves 2.14% error for BRISQUE and 7.39% for FPS prediction.

</details>


### [33] [United We Defend: Collaborative Membership Inference Defenses in Federated Learning](https://arxiv.org/abs/2601.06866)
*Li Bai,Junxu Liu,Sen Zhang,Xinwei Zhang,Qingqing Ye,Haibo Hu*

Main category: cs.CR

TL;DR: CoFedMID是一个针对联邦学习中成员推理攻击的协作防御框架，通过选择性样本训练、效用感知补偿和聚合中性扰动三个模块，在保护隐私的同时保持模型效用。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习中的成员推理攻击防御通常独立应用于每个客户端，无法有效抵御利用训练过程时间信息的轨迹攻击。同时，实际场景中只有部分客户端有防御需求，且存在隐私-效用权衡问题。

Method: 提出CoFedMID协作防御框架，包含三个模块：1) 类引导分区模块选择性训练本地样本；2) 效用感知补偿模块回收有贡献样本并防止过度自信；3) 聚合中性扰动模块在联盟层面注入噪声进行抵消。

Result: 在三个数据集上的实验表明，该防御框架显著降低了七种成员推理攻击的性能，同时仅带来较小的效用损失，在不同防御设置下结果一致。

Conclusion: CoFedMID通过客户端协作防御，有效解决了联邦学习中轨迹成员推理攻击的威胁，在保护隐私的同时保持了模型效用，适用于异构隐私需求的现实场景。

Abstract: Membership inference attacks (MIAs), which determine whether a specific data point was included in the training set of a target model, have posed severe threats in federated learning (FL). Unfortunately, existing MIA defenses, typically applied independently to each client in FL, are ineffective against powerful trajectory-based MIAs that exploit temporal information throughout the training process to infer membership status. In this paper, we investigate a new FL defense scenario driven by heterogeneous privacy needs and privacy-utility trade-offs, where only a subset of clients are defended, as well as a collaborative defense mode where clients cooperate to mitigate membership privacy leakage. To this end, we introduce CoFedMID, a collaborative defense framework against MIAs in FL, which limits local model memorization of training samples and, through a defender coalition, enhances privacy protection and model utility. Specifically, CoFedMID consists of three modules: a class-guided partition module for selective local training samples, a utility-aware compensation module to recycle contributive samples and prevent their overconfidence, and an aggregation-neutral perturbation module that injects noise for cancellation at the coalition level into client updates. Extensive experiments on three datasets show that our defense framework significantly reduces the performance of seven MIAs while incurring only a small utility loss. These results are consistently verified across various defense settings.

</details>


### [34] [Towards Compositional Generalization in LLMs for Smart Contract Security: A Case Study on Reentrancy Vulnerabilities](https://arxiv.org/abs/2601.06914)
*Ying Zhou,Jiacheng Wei,Yu Qi,Faguo Wu,Xiao Zhang*

Main category: cs.CR

TL;DR: 本文提出基于原子任务分解与融合的后训练算法，通过将重入漏洞检测分解为四个线性独立的原子任务，在有限数据下实现组合泛化，显著提升LLM在智能合约漏洞检测中的准确率。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在自然语言理解与生成方面表现出色，但在智能合约漏洞检测等专业领域仍无法超越传统静态分析工具。需要解决LLM在特定领域任务中的性能瓶颈问题。

Method: 1. 将重入漏洞检测任务分解为四个原子任务：识别外部调用、识别状态更新、识别外部调用与状态更新间的数据依赖、确定数据流顺序
2. 基于合成数据集生成三个编译器验证的数据集
3. 使用Slither工具从控制流图和数据流图中提取结构信息
4. 通过低秩归一化融合与LoRA适配器对LLM进行微调

Result: 1. 重入漏洞检测准确率达到98.2%，超越现有最优方法
2. 在31个真实世界合约中，召回率比传统分析工具高出20%
3. 通过原子任务分解实现了组合泛化，在有限数据下取得显著效果

Conclusion: 提出的基于原子任务分解与融合的后训练算法有效提升了LLM在智能合约漏洞检测领域的性能，证明了在专业领域通过任务分解和结构信息融合可以显著增强LLM的推理能力。

Abstract: Large language models (LLMs) demonstrate remarkable capabilities in natural language understanding and generation. Despite being trained on large-scale, high-quality data, LLMs still fail to outperform traditional static analysis tools in specialized domains like smart contract vulnerability detection. To address this issue, this paper proposes a post-training algorithm based on atomic task decomposition and fusion. This algorithm aims to achieve combinatorial generalization under limited data by decomposing complex reasoning tasks. Specifically, we decompose the reentrancy vulnerability detection task into four linearly independent atomic tasks: identifying external calls, identifying state updates, identifying data dependencies between external calls and state updates, and determining their data flow order. These tasks form the core components of our approach. By training on synthetic datasets, we generate three compiler-verified datasets. We then employ the Slither tool to extract structural information from the control flow graph and data flow graph, which is used to fine-tune the LLM's adapter. Experimental results demonstrate that low-rank normalization fusion with the LoRA adapter improves the LLM's reentrancy vulnerability detection accuracy to 98.2%, surpassing state-of-the-art methods. On 31 real-world contracts, the algorithm achieves a 20% higher recall than traditional analysis tools.

</details>


### [35] [Operational Runtime Behavior Mining for Open-Source Supply Chain Security](https://arxiv.org/abs/2601.06948)
*Zhuoran Tan,Ke Xiao,Jeremy Singer,Christos Anagnostopoulos*

Main category: cs.CR

TL;DR: HeteroGAT-Rank是一个面向行业的运行时行为挖掘系统，用于开源软件供应链威胁调查，通过异构图学习和注意力机制对运行时行为模式进行排序，为安全分析师提供可操作的调查线索。


<details>
  <summary>Details</summary>
Motivation: 开源软件是现代软件系统的关键组成部分，但由于源代码不可用或混淆，供应链安全在实践中仍然具有挑战性。安全团队通常依赖沙箱执行的运行时观察来调查可疑的第三方组件。

Method: 系统将OSS包的运行时行为建模为轻量级异构图，应用基于注意力的图学习来排序与安全分析最相关的行为模式。系统将离线行为挖掘与在线分析解耦，并集成并行图构建以跨多个生态系统高效处理。

Result: 在大规模OSS执行数据集上的评估表明，HeteroGAT-Rank能有效突出有意义且可解释的行为指标，这些指标与现实世界的漏洞和攻击趋势一致，在实际操作约束下支持实用的安全工作流程。

Conclusion: HeteroGAT-Rank是一个实用的供应链威胁调查系统，通过图学习方法为安全分析师提供可操作的运行时信号，支持分析师在环的调查工作流程，而不是追求完全自动化的检测。

Abstract: Open-source software (OSS) is a critical component of modern software systems, yet supply chain security remains challenging in practice due to unavailable or obfuscated source code. Consequently, security teams often rely on runtime observations collected from sandboxed executions to investigate suspicious third-party components. We present HeteroGAT-Rank, an industry-oriented runtime behavior mining system that supports analyst-in-the-loop supply chain threat investigation. The system models execution-time behaviors of OSS packages as lightweight heterogeneous graphs and applies attention-based graph learning to rank behavioral patterns that are most relevant for security analysis. Rather than aiming for fully automated detection, HeteroGAT-Rank surfaces actionable runtime signals - such as file, network, and command activities - to guide manual investigation and threat hunting. To operate at ecosystem scale, the system decouples offline behavior mining from online analysis and integrates parallel graph construction for efficient processing across multiple ecosystems. An evaluation on a large-scale OSS execution dataset shows that HeteroGAT-Rank effectively highlights meaningful and interpretable behavioral indicators aligned with real-world vulnerability and attack trends, supporting practical security workflows under realistic operational constraints.

</details>


### [36] [MemTrust: A Zero-Trust Architecture for Unified AI Memory System](https://arxiv.org/abs/2601.07004)
*Xing Zhou,Dmitrii Ustiugov,Haoxin Shang,Kisson Lin*

Main category: cs.CR

TL;DR: MemTrust：基于TEE的零信任AI内存架构，在保证数据主权的同时实现跨代理协作


<details>
  <summary>Details</summary>
Motivation: AI内存系统向统一上下文层发展，但集中化导致信任危机——用户需将敏感数字记忆数据托付给云提供商。核心矛盾在于个性化需求与数据主权：集中式系统支持高效协作但暴露用户数据风险，私有部署安全但限制协作。

Method: 提出五层架构抽象AI内存系统功能组件：存储、提取、学习、检索和治理。每层应用TEE保护建立可信框架，设计MemTrust硬件支持的零信任架构，提供跨所有层的密码学保证。包括"Context from MemTrust"跨应用共享协议、侧信道硬化检索（混淆访问模式）和全面安全分析。

Result: 实现本地等效安全同时提供更优维护效率和协作能力。架构使第三方开发者能以可接受成本移植现有系统，实现系统级可信度。MemTrust可作为AI内存系统的通用可信框架。

Conclusion: AI内存对提升代理和AI工具效率与协作至关重要，将成为AI代理的基础设施。MemTrust作为AI内存系统的通用可信框架，目标是成为内存基础设施的基础设施。

Abstract: AI memory systems are evolving toward unified context layers that enable efficient cross-agent collaboration and multi-tool workflows, facilitating better accumulation of personal data and learning of user preferences. However, centralization creates a trust crisis where users must entrust cloud providers with sensitive digital memory data. We identify a core tension between personalization demands and data sovereignty: centralized memory systems enable efficient cross-agent collaboration but expose users' sensitive data to cloud provider risks, while private deployments provide security but limit collaboration. To resolve this tension, we aim to achieve local-equivalent security while enabling superior maintenance efficiency and collaborative capabilities. We propose a five-layer architecture abstracting common functional components of AI memory systems: Storage, Extraction, Learning, Retrieval, and Governance. By applying TEE protection to each layer, we establish a trustworthy framework. Based on this, we design MemTrust, a hardware-backed zero-trust architecture that provides cryptographic guarantees across all layers. Our contributions include the five-layer abstraction, "Context from MemTrust" protocol for cross-application sharing, side-channel hardened retrieval with obfuscated access patterns, and comprehensive security analysis. The architecture enables third-party developers to port existing systems with acceptable development costs, achieving system-wide trustworthiness. We believe that AI memory plays a crucial role in enhancing the efficiency and collaboration of agents and AI tools. AI memory will become the foundational infrastructure for AI agents, and MemTrust serves as a universal trusted framework for AI memory systems, with the goal of becoming the infrastructure of memory infrastructure.

</details>


### [37] [Zer0n: An AI-Assisted Vulnerability Discovery and Blockchain-Backed Integrity Framework](https://arxiv.org/abs/2601.07019)
*Harshil Parmar,Pushti Vyas,Prayers Khristi,Priyank Panchal*

Main category: cs.CR

TL;DR: Zer0n框架结合LLM漏洞检测与区块链审计追踪，在保证高性能的同时实现去中心化完整性验证


<details>
  <summary>Details</summary>
Motivation: 生成式AI在漏洞研究中日益普及，但依赖不透明的模型输出导致"信任鸿沟"，需要建立可信的安全自动化机制

Method: 采用混合架构：使用Gemini 2.0 Pro进行基于逻辑的漏洞检测，利用Avalanche C-Chain进行防篡改日志记录；执行过程保持链下以保证性能，完整性证明在链上最终确认

Result: 在500个端点的数据集上评估，达到80%的检测准确率，仅带来22.9%的开销，证明去中心化完整性可以与高速安全工作流共存

Conclusion: Zer0n框架成功弥合了AI驱动漏洞检测中的信任鸿沟，通过区块链锚定LLM推理能力，实现了性能与可信度的平衡

Abstract: As vulnerability research increasingly adopts generative AI, a critical reliance on opaque model outputs has emerged, creating a "trust gap" in security automation. We address this by introducing Zer0n, a framework that anchors the reasoning capabilities of Large Language Models (LLMs) to the immutable audit trails of blockchain technology. Specifically, we integrate Gemini 2.0 Pro for logic-based vulnerability detection with the Avalanche C-Chain for tamper-evident artifact logging. Unlike fully decentralized solutions that suffer from high latency, Zer0n employs a hybrid architecture: execution remains off-chain for performance, while integrity proofs are finalized on-chain. Our evaluation on a dataset of 500 endpoints reveals that this approach achieves 80% detection accuracy with only a marginal 22.9% overhead, effectively demonstrating that decentralized integrity can coexist with high-speed security workflows.

</details>


### [38] [LINEture: novel signature cryptosystem](https://arxiv.org/abs/2601.07071)
*Gennady Khalimov,Yevgen Kotukh*

Main category: cs.CR

TL;DR: LINEture是一种基于线性矩阵代数的数字签名系统，利用可分解置换的共享秘密和零知识证明，不依赖计算难题，通过矩阵变换维度实现安全性。


<details>
  <summary>Details</summary>
Motivation: 设计一种不依赖传统计算难题（如大整数分解或离散对数）的数字签名系统，利用矩阵运算的低计算成本优势，同时确保足够的安全性。

Method: 1. 基于同态矩阵变换的可分解置换秘密共享理论；2. 使用消息哈希和随机参数进行会话密钥随机化；3. 零知识认证协议验证共享秘密知识；4. 通过矩阵变换维度控制安全性级别。

Result: 提出了LINEture密码系统，基于线性矩阵代数构建，不依赖计算难题，通过适当选择矩阵变换维度实现高安全性，矩阵运算可能提供较低的签名生成和验证操作成本。

Conclusion: LINEture是一种创新的数字签名方案，利用矩阵变换和零知识证明提供安全性，避免了传统计算难题的依赖，在安全性和计算效率之间取得了平衡。

Abstract: We propose a novel digital signature cryptosystem that exploits the concept of the brute-force problem. To ensure the security of the cryptosystem, we employed several mechanisms: sharing a common secret for factorable permutations, associating permutations with the message being signed, and confirming knowledge of the shared secret using a zero-knowledge proof. We developed a secret-sharing theory based on homomorphic matrix transformations for factorized permutations. The inverse matrix transformation for computing the shared secret is determined by secret parameters, which results in incompletely defined functionality and gives rise to a brute-force cryptanalysis problem. Randomization of session keys using a message hash and random parameters guarantees the uniqueness of each signature, even for identical messages. We employed a zero-knowledge authentication protocol to confirm knowledge of the shared secret, thereby protecting the verifier against unauthorized signature imposition. The LINEture cryptosystem is built on linear matrix algebra and does not rely on a computationally hard problem. High security is achieved through the appropriate selection of matrix transformation dimensions. Matrix computations potentially offer low operational costs for signature generation and verification.

</details>


### [39] [Overcoming the Retrieval Barrier: Indirect Prompt Injection in the Wild for LLM Systems](https://arxiv.org/abs/2601.07072)
*Hongyan Chang,Ergute Bao,Xinjian Luo,Ting Yu*

Main category: cs.CR

TL;DR: 本文提出了一种高效的间接提示注入攻击方法，通过将恶意内容分解为触发片段和攻击片段，确保恶意内容被检索，并在真实场景中展示了其严重威胁。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽然指出了间接提示注入的风险，但往往回避了最关键的问题：如何确保恶意内容在实际查询中被检索到。未优化的间接提示注入在自然查询中很少被检索，这使其实际影响不明确。

Method: 将恶意内容分解为触发片段（保证检索）和攻击片段（编码攻击目标），设计了一种高效的黑盒攻击算法，仅需嵌入模型的API访问权限，就能为任何攻击片段构建紧凑的触发片段以确保检索。

Result: 攻击成本低（每次目标用户查询仅需0.21美元），在11个基准测试和8个嵌入模型（包括开源和专有模型）上实现了接近100%的检索成功率。在真实场景中，单个被污染的电子邮件就足以让GPT-4o在多智能体工作流中以超过80%的成功率泄露SSH密钥。

Conclusion: 间接提示注入是一种实际且严重的威胁，现有防御措施不足以防止恶意文本的检索，检索环节是一个关键的安全漏洞。

Abstract: Large language models (LLMs) increasingly rely on retrieving information from external corpora. This creates a new attack surface: indirect prompt injection (IPI), where hidden instructions are planted in the corpora and hijack model behavior once retrieved. Previous studies have highlighted this risk but often avoid the hardest step: ensuring that malicious content is actually retrieved. In practice, unoptimized IPI is rarely retrieved under natural queries, which leaves its real-world impact unclear.
  We address this challenge by decomposing the malicious content into a trigger fragment that guarantees retrieval and an attack fragment that encodes arbitrary attack objectives. Based on this idea, we design an efficient and effective black-box attack algorithm that constructs a compact trigger fragment to guarantee retrieval for any attack fragment. Our attack requires only API access to embedding models, is cost-efficient (as little as $0.21 per target user query on OpenAI's embedding models), and achieves near-100% retrieval across 11 benchmarks and 8 embedding models (including both open-source models and proprietary services).
  Based on this attack, we present the first end-to-end IPI exploits under natural queries and realistic external corpora, spanning both RAG and agentic systems with diverse attack objectives. These results establish IPI as a practical and severe threat: when a user issued a natural query to summarize emails on frequently asked topics, a single poisoned email was sufficient to coerce GPT-4o into exfiltrating SSH keys with over 80% success in a multi-agent workflow. We further evaluate several defenses and find that they are insufficient to prevent the retrieval of malicious text, highlighting retrieval as a critical open vulnerability.

</details>


### [40] [How Secure is Secure Code Generation? Adversarial Prompts Put LLM Defenses to the Test](https://arxiv.org/abs/2601.07084)
*Melissa Tessa,Iyiola E. Olatunji,Aicha War,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CR

TL;DR: 对现有安全代码生成方法进行首次系统性对抗审计，发现它们在对抗条件下存在严重鲁棒性缺陷，静态分析器高估安全性7-21倍，真实安全且可用的代码率仅3-17%。


<details>
  <summary>Details</summary>
Motivation: 当前的安全代码生成方法声称能防止LLM生成不安全代码，但其在对抗条件下的鲁棒性未经测试，且现有评估将安全性与功能性分离，可能夸大实际效果。

Method: 对SVEN、SafeCoder、PromSec等最先进安全代码生成方法进行系统性对抗审计，使用开发者可能无意引入或攻击者故意利用的提示扰动（如改写、线索反转、上下文操纵），在一致条件下评估所有方法，同时使用多种分析器和可执行测试评估安全性和功能性。

Result: 发现严重鲁棒性缺陷：静态分析器高估安全性7-21倍，37-60%的"安全"输出不可用；在对抗条件下，真正安全且可用的代码率崩溃至3-17%。

Conclusion: 基于发现提出了构建和评估鲁棒安全代码生成方法的最佳实践，揭示了当前方法在现实对抗条件下的脆弱性。

Abstract: Recent secure code generation methods, using vulnerability-aware fine-tuning, prefix-tuning, and prompt optimization, claim to prevent LLMs from producing insecure code. However, their robustness under adversarial conditions remains untested, and current evaluations decouple security from functionality, potentially inflating reported gains. We present the first systematic adversarial audit of state-of-the-art secure code generation methods (SVEN, SafeCoder, PromSec). We subject them to realistic prompt perturbations such as paraphrasing, cue inversion, and context manipulation that developers might inadvertently introduce or adversaries deliberately exploit. To enable fair comparison, we evaluate all methods under consistent conditions, jointly assessing security and functionality using multiple analyzers and executable tests. Our findings reveal critical robustness gaps: static analyzers overestimate security by 7 to 21 times, with 37 to 60% of ``secure'' outputs being non-functional. Under adversarial conditions, true secure-and-functional rates collapse to 3 to 17%. Based on these findings, we propose best practices for building and evaluating robust secure code generation methods. Our code is available.

</details>


### [41] [Enhancing Cloud Network Resilience via a Robust LLM-Empowered Multi-Agent Reinforcement Learning Framework](https://arxiv.org/abs/2601.07122)
*Yixiao Peng,Hao Hu,Feiyang Li,Xinye Cao,Yingchang Jiang,Jipeng Tang,Guoshun Nan,Yuling Liu*

Main category: cs.CR

TL;DR: 提出CyberOps-Bots：首个结合LLM与RL的分层多智能体框架，用于云网络防御，无需重训练即可适应动态变化，并支持人机交互。


<details>
  <summary>Details</summary>
Motivation: 云网络虚拟化与资源池化虽带来灵活性，但也扩大了攻击面。现有基于强化学习的防御策略缺乏鲁棒性，需要重训练来适应网络结构、攻击策略等动态变化，且缺乏人机交互支持限制了可解释性与灵活性。

Method: 提出分层多智能体强化学习框架CyberOps-Bots，受MITRE ATT&CK战术-技术模型启发。上层LLM智能体包含四个模块：ReAct规划、基于IPDRR的感知、长短时记忆、行动/工具集成，负责全局感知、人类意图识别和战术规划。下层RL智能体通过异构分离预训练开发，在局部网络区域执行原子防御行动。这种协同保持了LLM的适应性和可解释性，同时确保RL执行的可靠性。

Result: 在真实云数据集上的实验表明，相比最先进算法，CyberOps-Bots在网络可用性上保持高出68.5%，在场景切换无需重训练时实现34.7%的跳跃启动性能增益。这是首个建立具有人机交互支持的鲁棒LLM-RL框架用于云防御的研究。

Conclusion: CyberOps-Bots成功解决了现有云防御方法的局限性，通过LLM-RL协同实现了鲁棒、自适应且可解释的防御框架，将在无需重训练的情况下适应动态变化，并支持人机交互。该框架将开源以促进云网络鲁棒自主防御的发展。

Abstract: While virtualization and resource pooling empower cloud networks with structural flexibility and elastic scalability, they inevitably expand the attack surface and challenge cyber resilience. Reinforcement Learning (RL)-based defense strategies have been developed to optimize resource deployment and isolation policies under adversarial conditions, aiming to enhance system resilience by maintaining and restoring network availability. However, existing approaches lack robustness as they require retraining to adapt to dynamic changes in network structure, node scale, attack strategies, and attack intensity. Furthermore, the lack of Human-in-the-Loop (HITL) support limits interpretability and flexibility. To address these limitations, we propose CyberOps-Bots, a hierarchical multi-agent reinforcement learning framework empowered by Large Language Models (LLMs). Inspired by MITRE ATT&CK's Tactics-Techniques model, CyberOps-Bots features a two-layer architecture: (1) An upper-level LLM agent with four modules--ReAct planning, IPDRR-based perception, long-short term memory, and action/tool integration--performs global awareness, human intent recognition, and tactical planning; (2) Lower-level RL agents, developed via heterogeneous separated pre-training, execute atomic defense actions within localized network regions. This synergy preserves LLM adaptability and interpretability while ensuring reliable RL execution. Experiments on real cloud datasets show that, compared to state-of-the-art algorithms, CyberOps-Bots maintains network availability 68.5% higher and achieves a 34.7% jumpstart performance gain when shifting the scenarios without retraining. To our knowledge, this is the first study to establish a robust LLM-RL framework with HITL support for cloud defense. We will release our framework to the community, facilitating the advancement of robust and autonomous defense in cloud networks.

</details>


### [42] [Proof of Reasoning for Privacy Enhanced Federated Blockchain Learning at the Edge](https://arxiv.org/abs/2601.07134)
*James Calo,Benny Lo*

Main category: cs.CR

TL;DR: 提出Proof of Reasoning (PoR)共识机制，专门为基于区块链的联邦学习设计，通过三阶段流程保护数据隐私、防御恶意攻击并增强网络验证能力。


<details>
  <summary>Details</summary>
Motivation: 现有区块链共识机制大多不针对联邦学习设计，也不支持聚合步骤。需要一种专门机制来保护数据隐私、防御恶意攻击并增强参与网络验证。

Method: PoR包含三阶段：1) 训练掩码自编码器生成特征映射编码器，保护数据隐私；2) 在边缘训练下游分类器，将权重、编码数据点、输出和真实标签加入区块；3) 基于这些数据进行联邦聚合，支持更复杂的可验证聚合方法。

Result: PoR能构建更鲁棒的神经网络，显著降低计算复杂度，同时保持高精度（仅需在边缘训练下游分类器）。可扩展到大型物联网网络，具有低延迟和存储增长，并能适应数据、法规和网络条件的变化。

Conclusion: PoR是一种专门为联邦学习设计的区块链共识机制，通过三阶段流程有效解决数据隐私保护、恶意攻击防御和网络验证问题，具有实际部署的可行性。

Abstract: Consensus mechanisms are the core of any blockchain system. However, the majority of these mechanisms do not target federated learning directly nor do they aid in the aggregation step. This paper introduces Proof of Reasoning (PoR), a novel consensus mechanism specifically designed for federated learning using blockchain, aimed at preserving data privacy, defending against malicious attacks, and enhancing the validation of participating networks. Unlike generic blockchain consensus mechanisms commonly found in the literature, PoR integrates three distinct processes tailored for federated learning. Firstly, a masked autoencoder (MAE) is trained to generate an encoder that functions as a feature map and obfuscates input data, rendering it resistant to human reconstruction and model inversion attacks. Secondly, a downstream classifier is trained at the edge, receiving input from the trained encoder. The downstream network's weights, a single encoded datapoint, the network's output and the ground truth are then added to a block for federated aggregation. Lastly, this data facilitates the aggregation of all participating networks, enabling more complex and verifiable aggregation methods than previously possible. This three-stage process results in more robust networks with significantly reduced computational complexity, maintaining high accuracy by training only the downstream classifier at the edge. PoR scales to large IoT networks with low latency and storage growth, and adapts to evolving data, regulations, and network conditions.

</details>


### [43] [MacPrompt: Maraconic-guided Jailbreak against Text-to-Image Models](https://arxiv.org/abs/2601.07141)
*Xi Ye,Yiwen Liu,Lina Wang,Run Wang,Geying Yang,Yufei Hou,Jiayi Yu*

Main category: cs.CR

TL;DR: MacPrompt是一种针对文本到图像模型安全机制的新型黑盒跨语言攻击方法，通过跨语言字符级重组有害术语构造混合语言对抗提示，能有效绕过现有安全过滤器


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型存在安全风险，虽然已有安全过滤器和概念移除技术来阻止不当内容生成，但这些防御方法无法有效应对多样化的对抗提示攻击，需要揭示现有安全机制的漏洞

Method: MacPrompt采用黑盒跨语言攻击策略，通过跨语言字符级重组有害术语构造混合语言对抗提示，实现对语义和外观的细粒度控制，不同于现有的同义词替换或提示混淆方法

Result: MacPrompt能保持高达0.96的语义相似度，同时绕过主要安全过滤器（成功率高达100%），对性相关内容攻击成功率达92%，暴力内容达90%，甚至能突破最先进的概念移除防御

Conclusion: 现有文本到图像模型安全机制在面对语言多样化和细粒度对抗策略时存在严重脆弱性，迫切需要重新评估这些安全机制的鲁棒性

Abstract: Text-to-image (T2I) models have raised increasing safety concerns due to their capacity to generate NSFW and other banned objects. To mitigate these risks, safety filters and concept removal techniques have been introduced to block inappropriate prompts or erase sensitive concepts from the models. However, all the existing defense methods are not well prepared to handle diverse adversarial prompts. In this work, we introduce MacPrompt, a novel black-box and cross-lingual attack that reveals previously overlooked vulnerabilities in T2I safety mechanisms. Unlike existing attacks that rely on synonym substitution or prompt obfuscation, MacPrompt constructs macaronic adversarial prompts by performing cross-lingual character-level recombination of harmful terms, enabling fine-grained control over both semantics and appearance. By leveraging this design, MacPrompt crafts prompts with high semantic similarity to the original harmful inputs (up to 0.96) while bypassing major safety filters (up to 100%). More critically, it achieves attack success rates as high as 92% for sex-related content and 90% for violence, effectively breaking even state-of-the-art concept removal defenses. These results underscore the pressing need to reassess the robustness of existing T2I safety mechanisms against linguistically diverse and fine-grained adversarial strategies.

</details>


### [44] [Safe-FedLLM: Delving into the Safety of Federated Large Language Models](https://arxiv.org/abs/2601.07177)
*Mingxiang Tao,Yu Tian,Wenxuan Tu,Yue Yang,Xue Yang,Xiangyan Tang*

Main category: cs.CR

TL;DR: Safe-FedLLM：一个基于探针的防御框架，通过分析LoRA权重特征来检测联邦学习中恶意客户端对LLM的攻击


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习研究主要关注训练效率，忽视了开放环境中的安全问题，特别是恶意客户端的攻击防御。LLM在联邦学习中容易受到攻击，需要有效的安全防护机制。

Method: 提出Safe-FedLLM防御框架，基于LoRA权重的行为特征进行分析。框架包含三个防御维度：Step-Level、Client-Level和Shadow-Level。核心思想是将客户端本地训练的LoRA权重作为高维行为特征，使用轻量级分类模型检测恶意属性。

Result: 实验表明Safe-FedLLM能有效增强联邦学习LLM的防御能力，且不影响良性数据上的性能。方法能有效抑制恶意数据影响，对训练速度影响小，即使存在大量恶意客户端仍保持有效。

Conclusion: Safe-FedLLM为联邦学习LLM提供了有效的安全防御方案，通过分析LoRA权重特征实现了对恶意客户端的检测，在保证性能的同时提升了系统安全性。

Abstract: Federated learning (FL) addresses data privacy and silo issues in large language models (LLMs). Most prior work focuses on improving the training efficiency of federated LLMs. However, security in open environments is overlooked, particularly defenses against malicious clients. To investigate the safety of LLMs during FL, we conduct preliminary experiments to analyze potential attack surfaces and defensible characteristics from the perspective of Low-Rank Adaptation (LoRA) weights. We find two key properties of FL: 1) LLMs are vulnerable to attacks from malicious clients in FL, and 2) LoRA weights exhibit distinct behavioral patterns that can be filtered through simple classifiers. Based on these properties, we propose Safe-FedLLM, a probe-based defense framework for federated LLMs, constructing defenses across three dimensions: Step-Level, Client-Level, and Shadow-Level. The core concept of Safe-FedLLM is to perform probe-based discrimination on the LoRA weights locally trained by each client during FL, treating them as high-dimensional behavioral features and using lightweight classification models to determine whether they possess malicious attributes. Extensive experiments demonstrate that Safe-FedLLM effectively enhances the defense capability of federated LLMs without compromising performance on benign data. Notably, our method effectively suppresses malicious data impact without significant impact on training speed, and remains effective even with many malicious clients. Our code is available at: https://github.com/dmqx/Safe-FedLLM.

</details>


### [45] [Defenses Against Prompt Attacks Learn Surface Heuristics](https://arxiv.org/abs/2601.07185)
*Shawn Li,Chenxiao Yu,Zhiyu Ni,Hao Li,Charith Peris,Chaowei Xiao,Yue Zhao*

Main category: cs.CR

TL;DR: 当前基于监督微调的LLM提示注入防御存在系统性缺陷，依赖表面模式而非恶意意图，导致安全输入被错误拒绝


<details>
  <summary>Details</summary>
Motivation: 随着LLM在安全敏感应用中的部署增加，需要确保模型遵循开发者指令并拒绝恶意指令。现有基于监督微调的防御方法虽然攻击拒绝率高，但可能依赖防御数据中的表面相关性而非实际恶意意图

Method: 分析三种由防御微调引发的捷径行为：位置偏见、令牌触发偏见和主题泛化偏见。引入受控诊断数据集，在两种基础模型和多个防御管道上进行系统评估

Result: 发现显著的系统性偏见：后缀任务拒绝率从低于10%上升到高达90%；单个触发令牌使错误拒绝增加达50%；防御模型在测试时准确率下降高达40%

Conclusion: 当前基于监督微调的提示注入防御方法经常对攻击表面模式而非底层意图做出响应，凸显了监督微调在可靠LLM安全方面的局限性

Abstract: Large language models (LLMs) are increasingly deployed in security-sensitive applications, where they must follow system- or developer-specified instructions that define the intended task behavior, while completing benign user requests. When adversarial instructions appear in user queries or externally retrieved content, models may override intended logic. Recent defenses rely on supervised fine-tuning with benign and malicious labels. Although these methods achieve high attack rejection rates, we find that they rely on narrow correlations in defense data rather than harmful intent, leading to systematic rejection of safe inputs. We analyze three recurring shortcut behaviors induced by defense fine-tuning. \emph{Position bias} arises when benign content placed later in a prompt is rejected at much higher rates; across reasoning benchmarks, suffix-task rejection rises from below \textbf{10\%} to as high as \textbf{90\%}. \emph{Token trigger bias} occurs when strings common in attack data raise rejection probability even in benign contexts; inserting a single trigger token increases false refusals by up to \textbf{50\%}. \emph{Topic generalization bias} reflects poor generalization beyond the defense data distribution, with defended models suffering test-time accuracy drops of up to \textbf{40\%}. These findings suggest that current prompt-injection defenses frequently respond to attack-like surface patterns rather than the underlying intent. We introduce controlled diagnostic datasets and a systematic evaluation across two base models and multiple defense pipelines, highlighting limitations of supervised fine-tuning for reliable LLM security.

</details>


### [46] [BlindU: Blind Machine Unlearning without Revealing Erasing Data](https://arxiv.org/abs/2601.07214)
*Weiqi Wang,Zhiyi Tian,Chenhan Zhang,Shui Yu*

Main category: cs.CR

TL;DR: BlindU：一种在联邦学习场景下的隐私保护遗忘方法，用户本地生成压缩表示而非上传原始数据，服务器仅基于这些表示进行遗忘操作


<details>
  <summary>Details</summary>
Motivation: 现有遗忘方法要求用户上传原始数据到服务器作为遗忘前提，这在联邦学习等隐私保护场景中不可行，需要在不暴露删除数据给服务器的情况下实现遗忘

Method: 1）使用信息瓶颈机制训练FL模型，学习压缩表示；2）用户本地生成隐私保护表示；3）服务器基于压缩表示和标签进行遗忘；4）集成两个专用遗忘模块；5）引入无噪声差分隐私掩码增强隐私保护

Result: 理论分析和大量实验结果表明，BlindU在隐私保护和遗忘效果方面优于现有最佳隐私保护遗忘基准方法

Conclusion: BlindU成功解决了在联邦学习等隐私保护场景中，不暴露用户原始数据给服务器的情况下实现有效遗忘的问题，提供了优越的隐私保护和遗忘效果

Abstract: Machine unlearning enables data holders to remove the contribution of their specified samples from trained models to protect their privacy. However, it is paradoxical that most unlearning methods require the unlearning requesters to firstly upload their data to the server as a prerequisite for unlearning. These methods are infeasible in many privacy-preserving scenarios where servers are prohibited from accessing users' data, such as federated learning (FL). In this paper, we explore how to implement unlearning under the condition of not uncovering the erasing data to the server. We propose \textbf{Blind Unlearning (BlindU)}, which carries out unlearning using compressed representations instead of original inputs. BlindU only involves the server and the unlearning user: the user locally generates privacy-preserving representations, and the server performs unlearning solely on these representations and their labels. For the FL model training, we employ the information bottleneck (IB) mechanism. The encoder of the IB-based FL model learns representations that distort maximum task-irrelevant information from inputs, allowing FL users to generate compressed representations locally. For effective unlearning using compressed representation, BlindU integrates two dedicated unlearning modules tailored explicitly for IB-based models and uses a multiple gradient descent algorithm to balance forgetting and utility retaining. While IB compression already provides protection for task-irrelevant information of inputs, to further enhance the privacy protection, we introduce a noise-free differential privacy (DP) masking method to deal with the raw erasing data before compressing. Theoretical analysis and extensive experimental results illustrate the superiority of BlindU in privacy protection and unlearning effectiveness compared with the best existing privacy-preserving unlearning benchmarks.

</details>


### [47] [When Bots Take the Bait: Exposing and Mitigating the Emerging Social Engineering Attack in Web Automation Agent](https://arxiv.org/abs/2601.07263)
*Xinyi Wu,Geng Hong,Yueyue Chen,MingXuan Liu,Feier Jin,Xudong Pan,Jiarun Dai,Baojun Liu*

Main category: cs.CR

TL;DR: 本文首次系统研究针对Web自动化代理的社会工程攻击，提出AgentBait攻击范式，并设计SUPERVISOR运行时防御模块，显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的Web代理广泛应用，开源框架加速了采用但也扩大了攻击面。现有研究主要关注模型威胁（如提示注入和后门），而社会工程攻击风险尚未被充分探索。

Method: 攻击方面：提出AgentBait范式，利用代理执行的内在弱点，通过诱导上下文扭曲代理推理，使其偏离预期任务执行恶意操作。防御方面：设计SUPERVISOR轻量级运行时模块，强制执行网页上下文与预期目标之间的环境和意图一致性对齐。

Result: 主流框架对AgentBait攻击高度脆弱，平均攻击成功率67.5%，特定策略下超过80%。SUPERVISOR模块可跨框架集成，平均降低攻击成功率78.1%，仅带来7.7%运行时开销，保持可用性。

Conclusion: AgentBait揭示了Web代理的新威胁面，SUPERVISOR提供了实用、可推广的防御方案，推进了这一新兴生态系统的安全性。研究结果已向框架开发者报告并获得确认。

Abstract: Web agents, powered by large language models (LLMs), are increasingly deployed to automate complex web interactions. The rise of open-source frameworks (e.g., Browser Use, Skyvern-AI) has accelerated adoption, but also broadened the attack surface. While prior research has focused on model threats such as prompt injection and backdoors, the risks of social engineering remain largely unexplored. We present the first systematic study of social engineering attacks against web automation agents and design a pluggable runtime mitigation solution. On the attack side, we introduce the AgentBait paradigm, which exploits intrinsic weaknesses in agent execution: inducement contexts can distort the agent's reasoning and steer it toward malicious objectives misaligned with the intended task. On the defense side, we propose SUPERVISOR, a lightweight runtime module that enforces environment and intention consistency alignment between webpage context and intended goals to mitigate unsafe operations before execution.
  Empirical results show that mainstream frameworks are highly vulnerable to AgentBait, with an average attack success rate of 67.5% and peaks above 80% under specific strategies (e.g., trusted identity forgery). Compared with existing lightweight defenses, our module can be seamlessly integrated across different web automation frameworks and reduces attack success rates by up to 78.1% on average while incurring only a 7.7% runtime overhead and preserving usability. This work reveals AgentBait as a critical new threat surface for web agents and establishes a practical, generalizable defense, advancing the security of this rapidly emerging ecosystem. We reported the details of this attack to the framework developers and received acknowledgment before submission.

</details>


### [48] [A High-Recall Cost-Sensitive Machine Learning Framework for Real-Time Online Banking Transaction Fraud Detection](https://arxiv.org/abs/2601.07276)
*Karthikeyan V. R.,Premnath S.,Kavinraaj S.,J. Sangeetha*

Main category: cs.CR

TL;DR: 该研究开发了一个基于群体学习方法和智能阈值选择的欺诈检测系统，在真实交易数据上实现了约91%的欺诈检测率，优于传统规则驱动方法，并集成了在线银行交易流和浏览器插件进行实时防护。


<details>
  <summary>Details</summary>
Motivation: 数字银行服务的欺诈活动日益复杂，传统规则驱动方法难以应对新型诈骗，现有算法往往忽略犯罪行为的细微变化，导致漏报。由于未检测到的欺诈比误报合法交易造成的损失更大，因此需要高敏感度的检测系统来减少漏报，同时控制误报率。

Method: 采用群体学习方法，通过智能阈值选择进行调整。使用公开的真实世界交易记录进行测试，这些数据中欺诈行为在正常活动中很少出现。系统直接集成到在线银行交易流中，实时阻止可疑交易，并开发了Chrome浏览器插件来标记欺诈性网页链接。

Result: 在真实交易数据上实现了约91%的欺诈检测率，优于依赖固定规则的传统设置。在实时环境中，系统成功集成到在线银行交易流程中，能够在交易完成前阻止可疑活动。浏览器插件有效减少了恶意网站的威胁。

Conclusion: 通过基于成本影响的决策调整和全系统验证，该欺诈检测系统在数字银行平台上的部署更加稳定和实用。群体学习方法结合智能阈值选择在处理类别不平衡数据时表现优异，为现代数字银行提供了有效的欺诈防护解决方案。

Abstract: Fraudulent activities on digital banking services are becoming more intricate by the day, challenging existing defenses. While older rule driven methods struggle to keep pace, even precision focused algorithms fall short when new scams are introduced. These tools typically overlook subtle shifts in criminal behavior, missing crucial signals. Because silent breaches cost institutions far more than flagged but legitimate actions, catching every possible case is crucial. High sensitivity to actual threats becomes essential when oversight leads to heavy losses. One key aim here involves reducing missed fraud cases without spiking incorrect alerts too much. This study builds a system using group learning methods adjusted through smart threshold choices. Using real world transaction records shared openly, where cheating acts rarely appear among normal activities, tests are run under practical skewed distributions. The outcomes reveal that approximately 91 percent of actual fraud is detected, outperforming standard setups that rely on unchanging rules when dealing with uneven examples across classes. When tested in live settings, the fraud detection system connects directly to an online banking transaction flow, stopping questionable activities before they are completed. Alongside this setup, a browser add on built for Chrome is designed to flag deceptive web links and reduce threats from harmful sites. These results show that adjusting decisions by cost impact and validating across entire systems makes deployment more stable and realistic for today's digital banking platforms.

</details>


### [49] [Memory-Based Malware Detection under Limited Data Conditions: A Comparative Evaluation of TabPFN and Ensemble Models](https://arxiv.org/abs/2601.07305)
*Valentin Leroy,Shuvalaxmi Dass,Sharif Ullah*

Main category: cs.CR

TL;DR: TabPFN在低数据量的恶意软件检测中表现优于传统机器学习模型，但计算时间较长


<details>
  <summary>Details</summary>
Motivation: 恶意软件分析面临数据稀缺问题，限制了AI模型的泛化能力，需要探索适合低数据场景的解决方案

Method: 评估TabPFN（一种无需学习的低数据模型）的性能，与随机森林、LightGBM和XGBoost等基准模型在多种类别配置下进行比较

Result: TabPFN在低数据场景下表现最佳，各项性能指标提升2%-6%，但计算时间有所增加

Conclusion: TabPFN在网络安全工作流中展现出潜力，但也存在实际计算效率方面的限制

Abstract: Artificial intelligence and machine learning have significantly advanced malware research by enabling automated threat detection and behavior analysis. However, the availability of exploitable data is limited, due to the absence of large datasets with real-world data. Despite the progress of AI in cybersecurity, malware analysis still suffers from this data scarcity, which limits model generalization. In order to tackle this difficulty, this workinvestigates TabPFN, a learning-free model designed for low-data regimes. We evaluate its performance against established baselines such as Random Forest, LightGBM and XGBoost, across multiple class configurations. Our experimental results indicate that TabPFN surpasses all other models in low-data regimes, with a 2% to 6% improvement observed across multiple performance metrics. However, this increase in performance has an impact on its computation time in a particular case. These findings highlight both the promise and the practical limitations of integrating TabPFN into cybersecurity workflows.

</details>


### [50] [Examining the Effectiveness of Transformer-Based Smart Contract Vulnerability Scan](https://arxiv.org/abs/2601.07334)
*Emre Balci,Timucin Aydede,Gorkem Yilmaz,Ece Gelal Soyak*

Main category: cs.CR

TL;DR: VASCOT：基于Transformer的以太坊智能合约漏洞扫描器，通过滑动窗口机制处理EVM字节码，在包含16,469个已验证合约的数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 智能合约技术虽然实现了去中心化的自执行协议，但存在安全漏洞可能导致财务损失和应用中断，需要有效的漏洞检测方法。

Method: 提出VASCOT（基于Transformer的智能合约漏洞分析器），对EVM字节码进行序列分析，采用滑动窗口机制克服输入长度限制，并通过追踪分析和具体验证构建高质量数据集。

Result: 在包含16,469个2022年部署的已验证以太坊合约的数据集上评估VASCOT，并与最先进的LSTM模型比较，揭示了两种模型在检测能力和泛化性方面的优缺点。

Conclusion: VASCOT展示了基于Transformer的智能合约漏洞检测的有效性，为深度学习在智能合约安全领域的应用提供了重要见解。

Abstract: Smart contract technology facilitates self-executing agreements on the blockchain, eliminating dependency on an external trusted authority. However, smart contracts may expose vulnerabilities that can lead to financial losses and disruptions in decentralized applications. In this work, we evaluate deep learning-based approaches for vulnerability scanning of Ethereum smart contracts. We propose VASCOT, a Vulnerability Analyzer for Smart COntracts using Transformers, which performs sequential analysis of Ethereum Virtual Machine (EVM) bytecode and incorporates a sliding window mechanism to overcome input length constraints. To assess VASCOT's detection efficacy, we construct a dataset of 16,469 verified Ethereum contracts deployed in 2022, and annotate it using trace analysis with concrete validation to mitigate false positives. VASCOT's performance is then compared against a state-of-the-art LSTM-based vulnerability detection model on both our dataset and an older public dataset. Our findings highlight the strengths and limitations of each model, providing insights into their detection capabilities and generalizability.

</details>


### [51] [MCP-ITP: An Automated Framework for Implicit Tool Poisoning in MCP](https://arxiv.org/abs/2601.07395)
*Ruiqi Li,Zhiqiang Wang,Yunhao Yao,Xiang-Yang Li*

Main category: cs.CR

TL;DR: MCP-ITP是一个针对MCP生态系统的自动化隐式工具投毒框架，通过黑盒优化生成投毒工具，在工具元数据中嵌入恶意指令，诱导代理调用高权限工具执行恶意操作，攻击成功率高达84.2%而检测率低至0.3%。


<details>
  <summary>Details</summary>
Motivation: MCP协议虽标准化了LLM代理与环境交互，但集成外部工具扩展了攻击面。现有研究主要关注显式工具投毒或手动制作的投毒工具，而本文聚焦更隐蔽的隐式工具投毒攻击，即投毒工具本身不被调用，而是通过其元数据中的指令诱导代理调用合法高权限工具执行恶意操作。

Method: 提出MCP-ITP框架，将投毒工具生成建模为黑盒优化问题，采用迭代优化策略，利用评估LLM和检测LLM的反馈，最大化攻击成功率同时规避现有检测机制。

Result: 在MCPTox数据集上对12个LLM代理的实验表明，MCP-ITP始终优于手动制作的基线方法，攻击成功率最高达84.2%，恶意工具检测率最低降至0.3%。

Conclusion: MCP-ITP是首个针对MCP生态系统的自动化自适应隐式工具投毒框架，揭示了当前MCP安全机制的严重漏洞，需要更强大的防御措施来应对此类隐蔽攻击。

Abstract: To standardize interactions between LLM-based agents and their environments, the Model Context Protocol (MCP) was proposed and has since been widely adopted. However, integrating external tools expands the attack surface, exposing agents to tool poisoning attacks. In such attacks, malicious instructions embedded in tool metadata are injected into the agent context during MCP registration phase, thereby manipulating agent behavior. Prior work primarily focuses on explicit tool poisoning or relied on manually crafted poisoned tools. In contrast, we focus on a particularly stealthy variant: implicit tool poisoning, where the poisoned tool itself remains uninvoked. Instead, the instructions embedded in the tool metadata induce the agent to invoke a legitimate but high-privilege tool to perform malicious operations. We propose MCP-ITP, the first automated and adaptive framework for implicit tool poisoning within the MCP ecosystem. MCP-ITP formulates poisoned tool generation as a black-box optimization problem and employs an iterative optimization strategy that leverages feedback from both an evaluation LLM and a detection LLM to maximize Attack Success Rate (ASR) while evading current detection mechanisms. Experimental results on the MCPTox dataset across 12 LLM agents demonstrate that MCP-ITP consistently outperforms the manually crafted baseline, achieving up to 84.2% ASR while suppressing the Malicious Tool Detection Rate (MDR) to as low as 0.3%.

</details>


### [52] [Peacock: UEFI Firmware Runtime Observability Layer for Detection and Response](https://arxiv.org/abs/2601.07402)
*Hadar Cochavi Gorelik,Orel Fadlon,Denis Klimov,Oleg Brodt,Asaf Shabtai,Yuval Elovici*

Main category: cs.CR

TL;DR: Peacock是一个用于UEFI启动过程完整性监控和远程验证的模块化框架，能够检测传统操作系统级安全机制无法防御的UEFI bootkit威胁。


<details>
  <summary>Details</summary>
Motivation: UEFI作为现代计算平台初始化硬件和过渡到操作系统的关键接口，具有高权限且持久存在，已成为高级威胁（如bootkit）的目标。现有保护措施（如安全启动和静态签名验证）不足以防御利用运行时行为或在签名检查完成后操纵固件组件的攻击者。与操作系统环境不同，预启动阶段缺乏实时可见性和威胁检测的实际机制。

Method: Peacock包含三个组件：1) UEFI代理，记录启动和运行时服务活动并提供加密防篡改保护；2) 跨平台操作系统代理，提取记录的数据并使用平台可信模块的硬件支持保证生成可验证的证明包；3) Peacock服务器，验证证明结果并为企业检测导出结构化遥测数据。

Result: 评估显示Peacock能够可靠检测多种真实世界的UEFI bootkit，包括Glupteba、BlackLotus、LoJax和MosaicRegressor。这些结果表明Peacock在固件层提供了实用的可见性和验证能力。

Conclusion: Peacock解决了绕过传统操作系统级安全机制的威胁，为UEFI启动过程提供了完整性保证的监控和远程验证框架，填补了预启动阶段安全监控的空白。

Abstract: Modern computing platforms rely on the Unified Extensible Firmware Interface (UEFI) to initialize hardware and coordinate the transition to the operating system. Because this execution environment operates with high privileges and persists across reboots, it has increasingly become a target for advanced threats, including bootkits documented in real systems. Existing protections, including Secure Boot and static signature verification, are insufficient against adversaries who exploit runtime behavior or manipulate firmware components after signature checks have completed. In contrast to operating system (OS) environments, where mature tools provide dynamic inspection and incident response, the pre-OS stage lacks practical mechanisms for real-time visibility and threat detection. We present Peacock, a modular framework that introduces integrity-assured monitoring and remote verification for the UEFI boot process. Peacock consists of three components: (i) a UEFI-based agent that records Boot and Runtime Service activity with cryptographic protection against tampering; (ii) a cross-platform OS Agent that extracts the recorded measurements and produces a verifiable attestation bundle using hardware-backed guarantees from the platform's trusted module; and (iii) a Peacock Server that verifies attestation results and exports structured telemetry for enterprise detection. Our evaluation shows that Peacock reliably detects multiple real-world UEFI bootkits, including Glupteba, BlackLotus, LoJax, and MosaicRegressor. Taken together, these results indicate that Peacock provides practical visibility and verification capabilities within the firmware layer, addressing threats that bypass traditional OS-level security mechanisms.

</details>


### [53] [Principal ideal problem and ideal shortest vector over rational primes in power-of-two cyclotomic fields](https://arxiv.org/abs/2601.07511)
*Gaohao Cui,Jianing Li,Jincheng Zhuang*

Main category: cs.CR

TL;DR: 本文提出分析幂二分圆域中理想格最短向量长度的新方法，针对不同素数模余情况给出精确刻画，并得到比Minkowski定理更紧的上界。


<details>
  <summary>Details</summary>
Motivation: 理想格的最短向量问题与Ring-LWE问题密切相关，而Ring-LWE被广泛用于构建后量子密码系统。幂二分圆域是实例化Ring-LWE的常用选择。之前的工作（Pan等，EUROCRYPT 2021）通过分解域分析了理想格的最短向量问题，但方法有限。

Method: 提出不同于分析格基的新方法：研究主理想生成元在嵌入为向量后是否能达到最短长度。如果成立，则寻找理想中最短向量可简化为寻找其最短生成元。该方法用于分析素数p≡3,5(mod 8)和p≡7,9(mod 16)情况下的最短向量长度。

Result: 1. 对p≡3,5(mod 8)情况提供了新的分析方法；2. 精确刻画了p≡7,9(mod 16)情况下的最短向量长度；3. 推导出比Minkowski定理更紧的新上界。

Conclusion: 通过研究主理想生成元是否能达到最短长度，本文为幂二分圆域中理想格的最短向量问题提供了更有效的分析方法，获得了更精确的结果和更紧的上界，对基于Ring-LWE的后量子密码系统安全性分析有重要意义。

Abstract: The shortest vector problem (SVP) over ideal lattices is closely related to the Ring-LWE problem, which is widely used to build post-quantum cryptosystems. Power-of-two cyclotomic fields are frequently adopted to instantiate Ring-LWE. Pan et al. (EUROCRYPT~2021) explored the SVP over ideal lattices via the decomposition fields and, in particular determined the length of ideal lattices over rational primes $p\equiv3,5\pmod{8}$ in power-of-two cyclotomic fields via explicit construction of reduced lattice bases.
  In this work, we first provide a new method (different from analyzing lattice bases) to analyze the length of the shortest vector in prime ideals in $\mathbb{Z}[ζ_{2^{n+1}}]$ when $p\equiv3,5\pmod{8}$. Then we precisely characterize the length of the shortest vector on the cases of $p\equiv7,9\pmod{16}$. Furthermore, we derive a new upper bound for this length, which is tighter than the bound obtained from Minkowski's theorem. Our key technique is to investigate whether a generator of a principal ideal can achieve the shortest length after embedding as a vector. If this holds for the ideal, finding the shortest vector in this ideal can be reduced to finding its shortest generator.

</details>


### [54] [A Protocol-Aware P4 Pipeline for MQTT Security and Anomaly Mitigation in Edge IoT Systems](https://arxiv.org/abs/2601.07536)
*Bui Ngoc Thanh Binh,Pham Hoai Luan,Le Vu Trung Duong,Vu Tuan Hai,Yasuhiko Nakashima*

Main category: cs.CR

TL;DR: 提出基于P4数据平面的MQTT安全方案，在边缘网络实现协议感知的安全执行和异常检测，具有高精度、低延迟的特点。


<details>
  <summary>Details</summary>
Motivation: MQTT作为物联网主流轻量级发布-订阅协议，边缘安全不足。云入侵检测系统延迟高，CPU防火墙和通用SDN控制器缺乏MQTT感知能力，无法执行会话验证、主题授权和行为异常检测。

Method: 基于P4数据平面的执行方案，结合解析安全的MQTT头提取、会话顺序验证、字节级主题前缀授权、客户端速率限制、软上限执行，以及基于KeepAlive和Remaining Length的轻量级异常检测。

Result: 在Mininet/BMv2测试平台上，策略执行准确率99.8%（95%置信区间），异常检测灵敏度98%真阳性率，高交付率（>99.9%在100-5kpps；99.8%在10kpps；99.6%在16kpps），每包延迟低于毫秒级。

Conclusion: 协议感知的MQTT过滤可以在可编程数据平面高效实现，为边缘物联网安全提供实用基础。未来将在生产P4硬件上验证设计并集成基于机器学习的阈值自适应。

Abstract: MQTT is the dominant lightweight publish--subscribe protocol for IoT deployments, yet edge security remains inadequate. Cloud-based intrusion detection systems add latency that is unsuitable for real-time control, while CPU-bound firewalls and generic SDN controllers lack MQTT awareness to enforce session validation, topic-based authorization, and behavioral anomaly detection. We propose a P4-based data-plane enforcement scheme for protocol-aware MQTT security and anomaly detection at the network edge. The design combines parser-safe MQTT header extraction with session-order validation, byte-level topic-prefix authorization with per-client rate limiting and soft-cap enforcement, and lightweight anomaly detection based on KeepAlive and Remaining Length screening with clone-to-CPU diagnostics. The scheme leverages stateful primitives in BMv2 (registers, meters, direct counters) to enable runtime policy adaptation with minimal per-packet latency. Experiments on a Mininet/BMv2 testbed demonstrate high policy enforcement accuracy (99.8%, within 95% CI), strong anomaly detection sensitivity (98\% true-positive rate), and high delivery >99.9% for 100--5~kpps; 99.8% at 10~kpps; 99.6\% at 16~kpps) with sub-millisecond per-packet latency. These results show that protocol-aware MQTT filtering can be efficiently realized in the programmable data plane, providing a practical foundation for edge IoT security. Future work will validate the design on production P4 hardware and integrate machine learning--based threshold adaptation.

</details>


### [55] [Simple Power Analysis of Polynomial Multiplication in HQC](https://arxiv.org/abs/2601.07634)
*Pavel Velek,Tomáš Rabas,Jiří Buček*

Main category: cs.CR

TL;DR: 对HQC密码系统的单迹简单功耗分析攻击，成功率99.69%，并提出相应防护措施


<details>
  <summary>Details</summary>
Motivation: HQC已被选为NIST后量子密码标准化项目第四轮的候选算法，需要评估其在实际硬件实现中的侧信道安全性

Method: 利用HQC解密开始时多项式乘法的功耗泄漏，使用ChipWhisperer-Lite开发板进行单迹简单功耗分析攻击

Result: 在10,000次攻击尝试中达到99.69%的成功率，证明HQC存在严重的侧信道漏洞

Conclusion: HQC在实际硬件实现中容易受到侧信道攻击，需要采取适当的防护措施，论文提出了多种防护方案并评估了其时间复杂度

Abstract: The Hamming Quasi-Cyclic (HQC) cryptosystem was selected for standardization in the fourth round of the NIST Post-Quantum Cryptography (PQC) standardization project. The goal of the PQC project is to standardize one or more quantum-resistant public-key cryptographic algorithms. In this paper, we present a single-trace Simple Power Analysis (SPA) attack against HQC that exploits power consumption leakage that occurs during polynomial multiplication performed at the beginning of HQC decryption. Using the ChipWhisperer-Lite board, we perform and evaluate the attack, achieving a 99.69% success rate over 10 000 attack attempts. We also propose various countermeasures against the attack and evaluate their time complexity.

</details>


### [56] [Hagenberg Risk Management Process (Part 1): Multidimensional Polar Heatmaps for Context-Sensitive Risk Analysis](https://arxiv.org/abs/2601.07644)
*Eckehard Hermann,Harald Lampesberger*

Main category: cs.CR

TL;DR: 本文提出了一种多维极坐标热图模型，用于增强复杂基础设施风险分析，超越了传统二维风险矩阵的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统二维风险矩阵在应用于复杂基础设施时存在根本性方法限制，而NIS2和DORA等监管框架要求更上下文敏感和系统导向的风险分析。

Method: 引入多维（ND）极坐标热图作为形式化模型，明确整合额外上下文维度，并将经典二维模型作为特例包含其中。

Result: 多维极坐标热图增强了风险矩阵的分析价值，为复杂基础设施和系统的Hagenberg风险管理流程提供了第一步。

Conclusion: 将上下文维度纳入热图可以显著提升复杂基础设施风险分析的能力，满足现代监管要求并克服传统二维方法的局限性。

Abstract: Traditional two-dimensional risk matrices (heatmaps) are widely used to model and visualize likelihood and impact relationships, but they face fundamental methodological limitations when applied to complex infrastructures. In particular, regulatory frameworks such as NIS2 and DORA call for more context-sensitive and system-oriented risk analysis. We argue that incorporating contextual dimensions into heatmaps enhances their analytical value. As a first step towards our Hagenberg Risk Management Process for complex infrastructures and systems, this paper introduces a multidimensional (ND) polar heatmap as a formal model that explicitly integrates additional context dimensions and subsumes classical two-dimensional models as a special case.

</details>


### [57] [Towards Automating Blockchain Consensus Verification with IsabeLLM](https://arxiv.org/abs/2601.07654)
*Elliot Jones,William Knottenbelt*

Main category: cs.CR

TL;DR: IsabeLLM工具结合Isabelle证明助手与大型语言模型，自动化验证区块链共识协议的正确性，成功应用于比特币工作量证明协议


<details>
  <summary>Details</summary>
Motivation: 区块链共识协议在对抗环境中确保节点间一致，正确设计和实现至关重要。形式化验证能保证协议正确性，但需要大量专业知识和精力，开发过程中常被忽略

Method: 提出IsabeLLM工具，集成Isabelle证明助手与大型语言模型（使用DeepSeek R1 API），辅助和自动化证明过程

Result: 成功开发比特币工作量证明共识协议的新模型并验证其正确性，能够为验证中的每个非平凡引理生成正确证明

Conclusion: IsabeLLM有效降低了形式化验证的门槛，使区块链共识协议的正确性验证更加可行和高效

Abstract: Consensus protocols are crucial for a blockchain system as they are what allow agreement between the system's nodes in a potentially adversarial environment. For this reason, it is paramount to ensure their correct design and implementation to prevent such adversaries from carrying out malicious behaviour. Formal verification allows us to ensure the correctness of such protocols, but requires high levels of effort and expertise to carry out and thus is often omitted in the development process. In this paper, we present IsabeLLM, a tool that integrates the proof assistant Isabelle with a Large Language Model to assist and automate proofs. We demonstrate the effectiveness of IsabeLLM by using it to develop a novel model of Bitcoin's Proof of Work consensus protocol and verify its correctness. We use the DeepSeek R1 API for this demonstration and found that we were able to generate correct proofs for each of the non-trivial lemmas present in the verification.

</details>


### [58] [TeeMAF: A TEE-Based Mutual Attestation Framework for On-Chain and Off-Chain Functions in Blockchain DApps](https://arxiv.org/abs/2601.07726)
*Xiangyu Liu,Brian Lee,Yuansong Qiao*

Main category: cs.CR

TL;DR: TeeMAF框架利用TEE技术实现DApp中链上智能合约与链下功能之间的相互认证，解决分布式系统中数据安全和隐私保护问题，并通过性能评估验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 随着物联网技术快速发展，分布式系统中数据安全和用户隐私问题日益突出。DApp包含链上和链下功能，但智能合约无法验证链下功能是否被篡改，导致链上与链下功能之间建立相互信任面临重大挑战。

Method: 提出TeeMAF通用框架，利用可信执行环境技术（包括Intel SGX、SCONE容器和远程认证技术），实现链上与链下功能之间的相互认证，确保链下功能在可证明的安全计算环境中执行。

Result: 通过安全分析验证了TeeMAF的可靠性，确保DApp正确执行。基于该框架构建的去中心化资源编排平台在以太坊上实现，使用Hyperledger Caliper进行基准测试，性能评估显示吞吐量和延迟方面的开销在可接受范围内。

Conclusion: TeeMAF框架有效解决了DApp中链上与链下功能之间的相互信任问题，通过TEE技术实现了安全认证，性能开销可控，为在不可信环境中部署应用提供了可行的解决方案。

Abstract: The rapid development of Internet of Things (IoT) technology has led to growing concerns about data security and user privacy in the interactions within distributed systems. Decentralized Applications (DApps) in distributed systems consist of on-chain and off-chain functions, where on-chain functions are smart contracts running in the blockchain network, while off-chain functions operate outside the blockchain. Since smart contracts cannot access off-chain information, they cannot verify whether the off-chain functions, i.e. the software components, they interact with have been tampered or not. As a result, establishing mutual trust between the on-chain smart contracts and the off-chain functions remains a significant challenge. To address the challenge, this paper introduces TeeMAF, a generic framework for mutual attestation between on-chain and off-chain functions, leveraging Trusted Execution Environments (TEE), specifically Intel Software Guard Extensions (SGX), SCONE (a TEE container on top of Intel SGX), and remote attestation technologies. This ensures that the deployed off-chain functions of a DApp execute in a provably secure computing environment and achieve mutual attestation with the interacting on-chain functions. Through a security analysis of TeeMAF, the reliability of deployed DApps can be verified, ensuring their correct execution. Furthermore, based on this framework, this paper proposes a decentralized resource orchestration platform (a specific DApp) for deploying applications over untrusted environments. The system is implemented on Ethereum and benchmarked using Hyperledger Caliper. Performance evaluation focusing on throughput and latency demonstrates that, compared to platforms without a mutual attestation scheme, the performance overhead remains within an acceptable range.

</details>


### [59] [SecureCAI: Injection-Resilient LLM Assistants for Cybersecurity Operations](https://arxiv.org/abs/2601.07835)
*Mohammed Himayath Ali,Mohammed Aqib Abdullah,Mohammed Mudassir Uddin,Shahnawaz Alam*

Main category: cs.CR

TL;DR: SecureCAI是一个针对网络安全环境中LLM的防御框架，通过扩展Constitutional AI原则，结合安全感知护栏、自适应宪法演化和DPO技术，显著降低提示注入攻击成功率，同时保持良性安全分析任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在安全运营中心的应用面临严重威胁：对抗性网络安全环境中的提示注入攻击可能通过嵌入恶意指令的安全工件操纵模型行为，而传统安全机制在这种高风险安全环境中不足以应对复杂的对抗性操纵。

Method: SecureCAI框架扩展Constitutional AI原则，包含：1）安全感知护栏；2）自适应宪法演化；3）使用直接偏好优化（DPO）来消除不安全响应模式。框架还整合了持续红队反馈循环，实现对新攻击策略的动态适应。

Result: 实验评估显示：SecureCAI将攻击成功率降低94.7%（相比基线模型），同时在良性安全分析任务上保持95.1%的准确率。在持续对抗压力下，宪法遵循分数超过0.92，框架能够动态适应新兴攻击策略。

Conclusion: SecureCAI为语言模型能力在操作网络安全工作流中的可信集成奠定了基础，解决了对抗性领域中当前AI安全方法的关键空白，实现了在保持功能性的同时显著增强对抗性攻击的抵御能力。

Abstract: Large Language Models have emerged as transformative tools for Security Operations Centers, enabling automated log analysis, phishing triage, and malware explanation; however, deployment in adversarial cybersecurity environments exposes critical vulnerabilities to prompt injection attacks where malicious instructions embedded in security artifacts manipulate model behavior. This paper introduces SecureCAI, a novel defense framework extending Constitutional AI principles with security-aware guardrails, adaptive constitution evolution, and Direct Preference Optimization for unlearning unsafe response patterns, addressing the unique challenges of high-stakes security contexts where traditional safety mechanisms prove insufficient against sophisticated adversarial manipulation. Experimental evaluation demonstrates that SecureCAI reduces attack success rates by 94.7% compared to baseline models while maintaining 95.1% accuracy on benign security analysis tasks, with the framework incorporating continuous red-teaming feedback loops enabling dynamic adaptation to emerging attack strategies and achieving constitution adherence scores exceeding 0.92 under sustained adversarial pressure, thereby establishing a foundation for trustworthy integration of language model capabilities into operational cybersecurity workflows and addressing a critical gap in current approaches to AI safety within adversarial domains.

</details>
