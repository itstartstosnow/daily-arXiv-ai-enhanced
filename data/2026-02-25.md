<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 15]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [When Backdoors Go Beyond Triggers: Semantic Drift in Diffusion Models Under Encoder Attacks](https://arxiv.org/abs/2602.20193)
*Shenyang Chen,Liuwan Zhu*

Main category: cs.CR

TL;DR: 论文揭示文本到图像模型中编码器侧后门攻击会导致持久的语义损坏，而不仅仅是触发激活，并提出SEMAD框架量化这种结构退化。


<details>
  <summary>Details</summary>
Motivation: 当前对文本到图像模型后门攻击的评估主要关注触发激活和视觉保真度，但忽略了编码器侧中毒可能导致的更深层语义损坏问题。

Method: 通过雅可比分析揭示后门作为低秩、目标中心变形的几何机制，并引入SEMAD（语义对齐和漂移）诊断框架，测量内部嵌入漂移和下游功能错位。

Result: 在扩散和对比范式中的验证表明，编码器中毒会导致持久的、无触发的语义损坏，从根本上重塑表示流形，暴露了深层结构风险。

Conclusion: 编码器中毒具有深远的几何影响，需要超越简单攻击成功率的几何审计，SEMAD框架为评估后门攻击的结构风险提供了新工具。

Abstract: Standard evaluations of backdoor attacks on text-to-image (T2I) models primarily measure trigger activation and visual fidelity. We challenge this paradigm, demonstrating that encoder-side poisoning induces persistent, trigger-free semantic corruption that fundamentally reshapes the representation manifold. We trace this vulnerability to a geometric mechanism: a Jacobian-based analysis reveals that backdoors act as low-rank, target-centered deformations that amplify local sensitivity, causing distortion to propagate coherently across semantic neighborhoods. To rigorously quantify this structural degradation, we introduce SEMAD (Semantic Alignment and Drift), a diagnostic framework that measures both internal embedding drift and downstream functional misalignment. Our findings, validated across diffusion and contrastive paradigms, expose the deep structural risks of encoder poisoning and highlight the necessity of geometric audits beyond simple attack success rates.

</details>


### [2] [OpenPort Protocol: A Security Governance Specification for AI Agent Tool Access](https://arxiv.org/abs/2602.20196)
*Genliang Zhu,Chu Wang,Ziyuan Wang,Zhida Li,Qiang Li*

Main category: cs.CR

TL;DR: OpenPort Protocol (OPP) 是一个面向治理的规范，通过安全的服务器端网关暴露应用工具，解决AI代理访问应用数据时的授权、执行控制、审计等治理问题。


<details>
  <summary>Details</summary>
Motivation: AI代理需要直接、结构化地访问应用数据和操作，但实际部署中难以表达和验证关键治理属性：最小权限授权、受控写入执行、可预测的故障处理、滥用抵抗和可审计性。

Method: 提出OpenPort Protocol规范，包括：授权依赖的发现机制、稳定的响应信封（含agent.*原因代码）、结合集成凭证、范围权限和ABAC策略的授权模型。对于写入操作，采用风险门控生命周期（默认草稿创建和人工审核）、时间限制的自动执行、预检影响绑定和幂等性保护。可选状态见证配置文件处理延迟审批流中的状态漂移问题。

Result: 提供了参考运行时和可执行的治理工具链（分层一致性配置文件、负面安全测试、模糊/滥用回归测试、发布门扫描），并在固定发布标签下使用基于工件的可外部复现验证评估了核心配置文件。

Conclusion: OpenPort Protocol为AI代理访问应用工具提供了一个治理优先的规范，通过服务器端网关解决了实际部署中的关键治理挑战，确保安全、可控和可审计的AI代理操作。

Abstract: AI agents increasingly require direct, structured access to application data and actions, but production deployments still struggle to express and verify the governance properties that matter in practice: least-privilege authorization, controlled write execution, predictable failure handling, abuse resistance, and auditability. This paper introduces OpenPort Protocol (OPP), a governance-first specification for exposing application tools through a secure server-side gateway that is model- and runtime-neutral and can bind to existing tool ecosystems. OpenPort defines authorization-dependent discovery, stable response envelopes with machine-actionable \texttt{agent.*} reason codes, and an authorization model combining integration credentials, scoped permissions, and ABAC-style policy constraints. For write operations, OpenPort specifies a risk-gated lifecycle that defaults to draft creation and human review, supports time-bounded auto-execution under explicit policy, and enforces high-risk safeguards including preflight impact binding and idempotency. To address time-of-check/time-of-use drift in delayed approval flows, OpenPort also specifies an optional State Witness profile that revalidates execution-time preconditions and fails closed on state mismatch. Operationally, the protocol requires admission control (rate limits/quotas) with stable 429 semantics and structured audit events across allow/deny/fail paths so that client recovery and incident analysis are deterministic. We present a reference runtime and an executable governance toolchain (layered conformance profiles, negative security tests, fuzz/abuse regression, and release-gate scans) and evaluate the core profile at a pinned release tag using artifact-based, externally reproducible validation.

</details>


### [3] [Evaluating the Reliability of Digital Forensic Evidence Discovered by Large Language Model: A Case Study](https://arxiv.org/abs/2602.20202)
*Jeel Piyushkumar Khatiwala,Daniel Kwaku Ntiamoah Addai,Weifeng Xu*

Main category: cs.CR

TL;DR: 提出一个结合LLM和数字取证知识图谱的框架，用于自动化提取、分析和验证AI识别的数字证据，确保证据可靠性和法证完整性。


<details>
  <summary>Details</summary>
Motivation: 随着AI识别的数字证据在法证调查中日益重要，其可靠性问题引发担忧。需要解决AI证据的可信度、法证完整性和可审计性等挑战。

Method: 提出结构化框架：1) 自动化法证工件提取；2) 通过LLM驱动分析精炼数据；3) 使用数字取证知识图谱(DFKG)验证结果；4) 通过确定性唯一标识符(UID)和法证交叉引用确保可追溯性和一致性。

Result: 在包含13GB法证镜像、61个应用、2,864个数据库和5,870个表格的数据集上评估，工件提取准确率超过95%，支持证据链完整性，法证关系具有强上下文一致性。

Conclusion: 该框架能提高AI辅助数字取证的可靠性，减少分类错误，建立法律上健全的范式，为可扩展、可审计的方法论提供基础。

Abstract: The growing reliance on AI-identified digital evidence raises significant concerns about its reliability, particularly as large language models (LLMs) are increasingly integrated into forensic investigations. This paper proposes a structured framework that automates forensic artifact extraction, refines data through LLM-driven analysis, and validates results using a Digital Forensic Knowledge Graph (DFKG). Evaluated on a 13 GB forensic image dataset containing 61 applications, 2,864 databases, and 5,870 tables, the framework ensures artifact traceability and evidentiary consistency through deterministic Unique Identifiers (UIDs) and forensic cross-referencing. We propose this methodology to address challenges in ensuring the credibility and forensic integrity of AI-identified evidence, reducing classification errors, and advancing scalable, auditable methodologies. A comprehensive case study on this dataset demonstrates the framework's effectiveness, achieving over 95 percent accuracy in artifact extraction, strong support of chain-of-custody adherence, and robust contextual consistency in forensic relationships. Key results validate the framework's ability to enhance reliability, reduce errors, and establish a legally sound paradigm for AI-assisted digital forensics.

</details>


### [4] [Right to History: A Sovereignty Kernel for Verifiable AI Agent Execution](https://arxiv.org/abs/2602.20214)
*Jing Zhang*

Main category: cs.CR

TL;DR: 提出"历史权利"原则，为AI代理行为提供防篡改、可验证的日志记录，并通过PunkGo系统实现该原则，确保个人硬件上AI行为的完整可追溯性。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理越来越多地代表人类行动，目前缺乏防篡改、可独立验证的行为记录系统。欧盟AI法案等法规开始要求高风险AI系统自动记录日志，这一缺口在个人硬件上尤为严重，因为没有集中提供商控制日志。基于Floridi的信息权利框架，将权利从个人数据扩展到代表个人执行的行动。

Method: 提出"历史权利"原则，通过五个系统不变式进行形式化，并实现PunkGo系统——一个Rust主权内核，统一了RFC 6962 Merkle树审计日志、基于能力的隔离、能源预算治理和人工批准机制。

Result: 对抗性测试确认所有五个不变式都成立。性能评估显示：中位行动延迟低于1.3毫秒，吞吐量约400行动/秒，在10,000个日志条目时Merkle包含证明大小为448字节。

Conclusion: 成功实现了"历史权利"原则，为个人硬件上的AI代理行为提供了完整、可验证的记录系统，满足了监管要求并保护了个人权利。

Abstract: AI agents increasingly act on behalf of humans, yet no existing system provides a tamper-evident, independently verifiable record of what they did. As regulations such as the EU AI Act begin mandating automatic logging for high-risk AI systems, this gap carries concrete consequences -- especially for agents running on personal hardware, where no centralized provider controls the log. Extending Floridi's informational rights framework from data about individuals to actions performed on their behalf, this paper proposes the Right to History: the principle that individuals are entitled to a complete, verifiable record of every AI agent action on their own hardware. The paper formalizes this principle through five system invariants with structured proof sketches, and implements it in PunkGo, a Rust sovereignty kernel that unifies RFC 6962 Merkle tree audit logs, capability-based isolation, energy-budget governance, and a human-approval mechanism. Adversarial testing confirms all five invariants hold. Performance evaluation shows sub-1.3 ms median action latency, ~400 actions/sec throughput, and 448-byte Merkle inclusion proofs at 10,000 log entries.

</details>


### [5] [The TCF doesn't really A(A)ID -- Automatic Privacy Analysis and Legal Compliance of TCF-based Android Applications](https://arxiv.org/abs/2602.20222)
*Victor Morel,Cristiana Santos,Pontus Carlsson,Joel Ahlinder,Romaric Duvignau*

Main category: cs.CR

TL;DR: 研究发现12.85%的热门Android应用使用TCF框架，其中存在多种隐私违规：2.6%的应用只在用户同意时保存选择，拒绝则每次重启都重新询问；66.2%的应用在没有合法依据的情况下分享AAID；55.3%在用户与同意横幅交互前就分享AAID。


<details>
  <summary>Details</summary>
Motivation: TCF框架已成为欧洲用户同意的实际标准，但之前的研究仅关注网页环境，缺乏对移动应用（特别是Android应用）中TCF实施情况的系统研究，需要填补这一空白。

Method: 从Google Play Store下载4482个热门应用，在模拟Android设备上自动检测哪些应用使用TCF，自动与同意横幅交互，并在两个阶段分析应用流量：被动阶段（选择后）和主动阶段（横幅交互期间和选择后）。

Result: 576个应用（12.85%）实施了TCF，其中发现多种隐私违规：15个应用（2.6%）只在用户同意时保存选择；被动阶段分析显示66.2%的应用在没有合法依据的情况下分享AAID；主动阶段显示55.3%的应用在用户与同意横幅交互前就分享AAID。

Conclusion: TCF在Android应用中的实施存在严重隐私问题，许多应用违反了同意要求，在没有合法依据的情况下处理个人数据，表明当前框架在移动环境中的执行存在缺陷。

Abstract: The Transparency and Consent Framework (TCF), developed by the Interactive Advertising Bureau (IAB) Europe, provides a de facto standard for requesting, recording, and managing user consent from European end-users. This framework has previously been found to infringe European data protection law and has subsequently been regularly updated. Previous research on the TCF focused exclusively on web contexts, with no attention given to its implementation in mobile applications. No work has systematically studied the privacy implications of the TCF on Android apps. To address this gap, we investigate the prevalence of the TCF in popular Android apps from the Google Play Store, and assess whether these apps respect users' consent banner choices. By scraping and downloading 4482 of the most popular Google Play Store apps on an emulated Android device, we automatically determine which apps use the TCF, automatically interact with consent banners, and analyze the apps' traffic in two different stages, passive (post choices) and active (during banner interaction and post choices).
  We found that 576 (12.85%) of the 4482 downloadable apps in our dataset implemented the TCF, and we identified potential privacy violations within this subset. In 15 (2.6%) of these apps, users' choices are stored only when consent is granted. Users who refuse consent are shown the consent banner again each time they launch the app. Network traffic analysis conducted during the passive stage reveals that 66.2% of the analyzed TCF-based apps share personal data, through the Android Advertising ID (AAID), in the absence of a lawful basis for processing. 55.3% of apps analyzed during the active stage share AAID before users interact with the apps' consent banners, violating the prior consent requirement.

</details>


### [6] [CryptRISC: A Secure RISC-V Processor for High-Performance Cryptography with Power Side-Channel Protection](https://arxiv.org/abs/2602.20285)
*Amisha Srivastava,Muskan Porwal,Kanad Basu*

Main category: cs.CR

TL;DR: CryptRISC：首个结合密码学加速和硬件级功耗侧信道防护的RISC-V处理器，通过ISA驱动的操作数掩码框架实现跨算法动态掩码保护，性能提升6.8倍，硬件开销仅1.86%。


<details>
  <summary>Details</summary>
Motivation: 密码计算对现代计算至关重要，但易受功耗侧信道攻击。现有掩码防护方案存在性能开销大、实现复杂（软件方案）或缺乏跨算法灵活性（硬件方案）的问题。

Method: 扩展CVA6核心，集成64位RISC-V标量密码学扩展，引入两个微架构组件：场检测层（识别指令主导代数域）和掩码控制单元（运行时应用域感知操作数随机化），实现基于指令语义的布尔、仿射或算术掩码动态选择。

Result: 相比基线软件实现最高6.80倍加速，相对基线CVA6核心仅1.86%硬件开销，支持AES、SHA-256、SHA-512、SM3、SM4等多种算法，无需修改指令编码。

Conclusion: CryptRISC证明了硬件级功耗侧信道防护与密码学加速的高效结合，通过ISA驱动的操作数掩码框架实现了跨算法优化的保护方案，具有实际应用价值。

Abstract: Cryptographic computations are fundamental to modern computing, ensuring data confidentiality and integrity. However, these operations are highly vulnerable to power side-channel attacks that exploit variations in power consumption to leak sensitive information. Masking is a widely used countermeasure, yet software-based techniques often introduce significant performance overhead and implementation complexity, while fixed-function hardware masking lacks flexibility across diverse cryptographic algorithms. In this paper, we present CryptRISC, the first RISC-V-based processor that combines cryptographic acceleration with hardware-level power side-channel resistance through an ISA-driven operand masking framework. Our design extends the CVA6 core with 64-bit RISC-V Scalar Cryptography Extensions and introduces two microarchitectural components: a Field Detection Layer, which identifies the dominant algebraic field of each cryptographic instruction, and a Masking Control Unit, which applies field-aware operand randomization at runtime. This enables dynamic selection of Boolean, affine, or arithmetic masking schemes based on instruction semantics, providing optimized protection across algorithms including AES, SHA-256, SHA-512, SM3, and SM4. Unlike prior approaches relying on static masking logic or software instrumentation, our method performs operand masking transparently within the execution pipeline without modifying instruction encoding. Experimental results show speedups up to 6.80$\times$ over baseline software implementations, with only a 1.86% hardware overhead relative to the baseline CVA6 core, confirming the efficiency and practicality of CryptRISC.

</details>


### [7] [Understanding Human-AI Collaboration in Cybersecurity Competitions](https://arxiv.org/abs/2602.20446)
*Tingxuan Tang,Nicolas Janis,Kalyn Asher Montague,Kevin Eykholt,Dhilung Kirat,Youngja Park,Jiyong Jang,Adwait Nadkarni,Yue Xiao*

Main category: cs.CR

TL;DR: 首個現場CTF競賽中AI輔助的實證研究，發現人類團隊逐漸將更大子任務委託給AI，但人類提示無效和上下文規範不足成為主要瓶頸，而自主AI代理通過自我指導的提示和工具使用超越大多數人類團隊。


<details>
  <summary>Details</summary>
Motivation: 現有評估主要關注AI單獨解決CTF挑戰的能力，但隨著AI在學術和工業環境中的使用增加，人類玩家很可能與AI代理合作解決挑戰。這暴露了一個關鍵知識缺口：人類如何看待AI CTF輔助？輔助提供時如何合作？AI輔助的人類表現與完全自主AI代理相比如何？

Method: 在現場CTF競賽中進行首個實證研究，包含41名參與者。定性研究：(i)參與者在使用AI前後的感知、信任和期望變化；(ii)參與者如何與儀器化AI代理合作。同時(iii)在相同新挑戰集上基準測試四個自主AI代理，比較人類團隊結果並分析代理軌跡。

Result: 隨著比賽進行，團隊越來越多地將更大子任務委託給AI，給予其更多自主權。CTF挑戰解決率通常不受模型推理能力限制，而是受人類玩家影響：無效提示和糟糕的上下文規範成為主要瓶頸。自主代理通過自我指導的提示和工具使用繞過此瓶頸，在比賽中總體排名第二，超越大多數人類團隊。

Conclusion: 研究結果對未來CTF挑戰設計和構建有效的安全領域人機協同AI系統具有重要意義，揭示了人類與AI合作中的關鍵瓶頸和自主AI代理的潛在優勢。

Abstract: Capture-the-Flag (CTF) competitions are increasingly becoming a testbed for evaluating AI capabilities at solving security tasks, due to the controlled environments and objective success criteria. Existing evaluations have focused on how successful AI is at solving CTF challenges in isolation from human CTF players. As AI usage increases in both academic and industrial settings, it is equally likely that human players may collaborate with AI agents to solve challenges. This possibility exposes a key knowledge gap: how do humans perceive AI CTF assistance; when assistance is provided, how do they collaborate and is it effective with respect to human performance; how do humans assisted by AI compare to the performance of fully autonomous AI agents on the same challenges. We address this gap with the first empirical study of AI assistance in a live, onsite CTF. In a study with 41 participants, we qualitatively study (i) how participants' perception, trust, and expectations shift before versus after hands-on AI use, and (ii) how participants collaborate with an instrumented AI agent. Moreover, we also (iii) benchmark four autonomous AI agents on the same fresh challenge set to compare outcomes with human teams and analyze agent trajectories. We find that, as the competition progresses, teams increasingly delegate larger subtasks to the AI, giving it more agency. Interestingly, CTF challenges solving rates are often constrained not by model's reasoning capabilities, but rather by the human players: ineffective prompting and poor context specification become the primary bottleneck. Remarkably, autonomous agents that self-direct their prompting and tool use bypass this bottleneck and outperform most human teams, coming in second overall in the competition. We conclude with implications for the future design of CTF challenges and for building effective human-in-the-loop AI systems for security.

</details>


### [8] [Towards Secure and Efficient DNN Accelerators via Hardware-Software Co-Design](https://arxiv.org/abs/2602.20521)
*Wei Xuan,Zihao Xuan,Rongliang Fu,Ning Lin,Kwunhang Wong,Zikang Yuan,Lang Feng,Zhongrui Wang,Tsung-Yi Ho,Yuzhong Jiao,Luhong Liang*

Main category: cs.CR

TL;DR: 本文提出了一种针对DNN加速器的低开销内存保护框架，通过带宽感知加密方案和多级认证机制，在保证安全性的同时显著降低性能开销和能耗。


<details>
  <summary>Details</summary>
Motivation: 随着DNN加速器在自动驾驶、医疗系统等安全关键领域的广泛应用，需要强大的机制来保护数据机密性和计算完整性。现有安全解决方案存在硬件资源需求过高和频繁片外内存访问开销的问题，影响性能和可扩展性。

Method: 1. 提出带宽感知加密方案，根据内存流量模式自适应调整加密粒度；2. 分析发现层内分片滑动窗口模式的重叠区域和层间分片策略差异导致大量冗余内存访问和重复加密计算；3. 引入多级认证机制，有效消除不必要的片外内存访问。

Result: 实验结果表明，该框架将性能开销降低超过12%，在服务器和边缘神经处理单元(NPU)上实现87%的能效提升，同时确保强大的可扩展性。

Conclusion: 本文提出的安全高效内存保护框架为DNN加速器提供了低开销的安全解决方案，在保证数据机密性和计算完整性的同时，显著提升了性能和能效。

Abstract: The rapid deployment of deep neural network (DNN) accelerators in safety-critical domains such as autonomous vehicles, healthcare systems, and financial infrastructure necessitates robust mechanisms to safeguard data confidentiality and computational integrity. Existing security solutions for DNN accelerators, however, suffer from excessive hardware resource demands and frequent off-chip memory access overheads, which degrade performance and scalability.
  To address these challenges, this paper presents a secure and efficient memory protection framework for DNN accelerators with minimal overhead. First, we propose a bandwidth-aware cryptographic scheme that adapts encryption granularity based on memory traffic patterns, striking a balance between security and resource efficiency. Second, we observe that both the overlapping regions in the intra-layer tiling's sliding window pattern and those resulting from inter-layer tiling strategy discrepancies introduce substantial redundant memory accesses and repeated computational overhead in cryptography. Third, we introduce a multi-level authentication mechanism that effectively eliminates unnecessary off-chip memory accesses, enhancing performance and energy efficiency. Experimental results show that this work decreases performance overhead by over 12% and achieves 87% energy efficiency improvement for both server and edge neural processing units (NPUs), while ensuring robust scalability.

</details>


### [9] [OptiLeak: Efficient Prompt Reconstruction via Reinforcement Learning in Multi-tenant LLM Services](https://arxiv.org/abs/2602.20595)
*Longxiang Wang,Xiang Zheng,Xuhao Zhang,Yao Zhang,Ye Wu,Cong Wang*

Main category: cs.CR

TL;DR: OptiLeak：基于强化学习的提示泄漏攻击框架，通过两阶段微调最大化提示重建效率，相比基线方法减少12.48倍平均请求次数


<details>
  <summary>Details</summary>
Motivation: 多租户LLM服务框架广泛采用共享KV缓存提升效率，但这带来了侧信道漏洞导致提示泄漏攻击。先前研究虽然识别了攻击面，但主要扩展攻击向量而非优化攻击性能，报告的攻击成本过高，低估了实际隐私风险。

Method: 提出OptiLeak框架，采用强化学习增强的两阶段微调方法。核心洞察是通过似然度排序自动识别领域特定的"硬令牌"（难以预测但携带敏感信息的术语），用于构建偏好对进行直接偏好优化，无需人工标注。这实现了有效的偏好对齐，同时避免了扩展监督微调的过拟合问题。

Result: 在医疗和金融领域的三个基准测试中，OptiLeak相比基线方法实现了高达12.48倍的平均每令牌请求数减少，在3B到14B参数规模的模型上表现一致改进。

Conclusion: 基于缓存的提示泄漏攻击比先前报告的风险更严重，突显了在生产部署中需要强大的缓存隔离机制。

Abstract: Multi-tenant LLM serving frameworks widely adopt shared Key-Value caches to enhance efficiency. However, this creates side-channel vulnerabilities enabling prompt leakage attacks. Prior studies identified these attack surfaces yet focused on expanding attack vectors rather than optimizing attack performance, reporting impractically high attack costs that underestimate the true privacy risk. We propose OptiLeak, a reinforcement learning-enhanced framework that maximizes prompt reconstruction efficiency through two-stage fine-tuning. Our key insight is that domain-specific ``hard tokens'' -- terms difficult to predict yet carrying sensitive information -- can be automatically identified via likelihood ranking and used to construct preference pairs for Direct Preference Optimization, eliminating manual annotation. This enables effective preference alignment while avoiding the overfitting issues of extended supervised fine-tuning. Evaluated on three benchmarks spanning medical and financial domains, OptiLeak achieves up to $12.48\times$ reduction in average requests per token compared to baseline approaches, with consistent improvements across model scales from 3B to 14B parameters. Our findings demonstrate that cache-based prompt leakage poses a more severe threat than previously reported, underscoring the need for robust cache isolation in production deployments.

</details>


### [10] [Post-Quantum Sanitizable Signatures from McEliece-Based Chameleon Hashing](https://arxiv.org/abs/2602.20657)
*Shahzad Ahmad,Stefan Rass,Zahra Seyedi*

Main category: cs.CR

TL;DR: 首个基于编码的透明后量子可净化签名方案，使用McEliece密码系统构建变色龙哈希函数，通过Patterson解码实现授权消息块修改，同时保证其他内容不可篡改。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算的发展，传统签名方案面临安全威胁，需要设计能够抵抗量子攻击的可净化签名方案。现有方案大多基于数论假设，缺乏基于编码的后量子安全方案，且透明性难以实现。

Method: 基于McEliece密码系统构建变色龙哈希函数，指定净化者拥有Goppa码的陷门，通过Patterson解码实现可控碰撞查找。通过限制签名者生成的随机化器权重，实现完美透明性。

Result: 提出了首个透明、基于编码的后量子可净化签名方案，在随机预言机模型下基于综合征解码困难性提供了存在不可伪造性和不可变性证明，实现了完美透明性。

Conclusion: 该工作建立了首个基于编码的透明后量子可净化签名方案，为长期安全应用提供了理论保证和实践部署途径，填补了后量子可净化签名领域的空白。

Abstract: We introduce a novel post-quantum sanitizable signature scheme constructed upon a chameleon hash function derived from the McEliece cryptosystem. In this design, the designated sanitizer possesses the inherent trapdoor of a Goppa code, which facilitates controlled collision-finding via Patterson decoding. This mechanism enables authorized modification of specific message blocks while ensuring all other content remains immutably bound. We provide formal security definitions and rigorous proofs of existential unforgeability and immutability, grounded in the hardness of syndrome decoding in the random-oracle model, where a robust random oracle thwarts trivial linear hash collisions. A key innovation lies in our precise characterization of the transparency property: by imposing a specific weight constraint on the randomizers generated by the signer, we achieve perfect transparency, rendering sanitized signatures indistinguishable from freshly signed ones. This work establishes the first transparent, code-based, post-quantum sanitizable signature scheme, offering strong theoretical guarantees and a pathway for practical deployment in long-term secure applications.

</details>


### [11] [ICSSPulse: A Modular LLM-Assisted Platform for Industrial Control System Penetration Testing](https://arxiv.org/abs/2602.20663)
*Michail Takaronis,Athanasia Kollarou,Vyron Kampourakis,Vasileios Gkioulos,Sokratis Katsikas*

Main category: cs.CR

TL;DR: ICSSPulse：首个基于Web的工业控制系统渗透测试平台，集成网络扫描、Modbus/OPC~UA协议交互和LLM辅助报告功能


<details>
  <summary>Details</summary>
Motivation: 工业控制系统作为关键基础设施的核心，其日益增长的连接性使其面临难以在实时操作条件下安全研究和修复的网络威胁。现有工具缺乏统一的、安全的测试环境。

Method: 开发了ICSSPulse——一个开源、模块化、可扩展的渗透测试平台，提供用户友好的图形界面，支持协议级发现、资产枚举、受控读写交互，并集成LLM辅助报告模块。

Result: 实验评估表明，ICSSPulse能够有效发现活跃的工业服务、枚举过程相关资产、操纵过程变量，并能自动生成结构化的执行和技术报告。

Conclusion: ICSSPulse为工业控制系统安全评估提供了一个安全、可重复的测试平台，其LLM辅助报告功能能够自动生成基于ICS MITRE ATT&CK矩阵的缓解指导，具有重要实用价值。

Abstract: It is well established that industrial control systems comprise the operational backbone of modern critical infrastructures, yet their increasing connectivity exposes them to cyber threats that are difficult to study and remedy safely under real-time operational conditions. In this paper, we present ICSSPulse, an open-source, modular, and extensible penetration testing platform designed for the security assessment of ICS communication protocols. To the best of our knowledge, ICSSPulse is the first web-based platform that unifies network scanning, protocol-aware Modbus and OPC~UA interaction, and Large Language Model (LLM)-assisted reporting within a single, lightweight ecosystem. Our platform provides a user-friendly graphical interface that orchestrates enumeration, exploitation, and reporting activities over simulated industrial services, enabling safe and reproducible experimentation. It supports protocol-level discovery, asset enumeration, and controlled read/write interactions, while preserving protocol fidelity and operational transparency. Experimental evaluation using synthetic Modbus test servers, a Factory I/O water treatment scenario, and a custom OPC~UA production-line model demonstrated ICSSPulse's potential to discover active industrial services, enumerate process-relevant assets, and manipulate process variables. A key contribution of this work lies in the integration of an LLM-assisted reporting module that automatically translates technical findings into structured executive and technical reports, with mitigation guidance informed by the ICS MITRE ATT&CK ICS matrix.

</details>


### [12] [Vanishing Watermarks: Diffusion-Based Image Editing Undermines Robust Invisible Watermarking](https://arxiv.org/abs/2602.20680)
*Fan Guo,Jiyu Kang,Qi Ming,Emily Davis,Finn Carter*

Main category: cs.CR

TL;DR: 扩散模型能有效擦除传统鲁棒水印，即使这些水印设计用于抵抗常规图像处理。通过扩散驱动的图像再生过程，可以在保持视觉质量的同时移除水印，导致水印检测率接近零。


<details>
  <summary>Details</summary>
Motivation: 随着强大的扩散模型在图像生成和编辑中的应用日益广泛，传统鲁棒水印技术面临新的威胁。现有水印方案设计用于抵抗常规图像处理，但尚未考虑扩散模型这种新型攻击手段。

Method: 提出扩散驱动的图像再生攻击方法，利用生成模型重新创建图像以移除水印。还引入引导扩散攻击，在生成过程中明确针对嵌入的水印信号。从理论上证明扩散变换会降低水印图像与隐藏载荷之间的互信息。

Result: 实验评估多个先进水印方法（包括StegaStamp、TrustMark、VINE等），显示扩散编辑后水印恢复率接近零，同时再生图像保持高视觉保真度。理论分析表明足够扩散变换后互信息趋近零，导致解码失败。

Conclusion: 当前鲁棒水印技术对基于生成模型的编辑存在根本性脆弱性，需要开发新策略来确保水印在强大扩散模型时代的鲁棒性。

Abstract: Robust invisible watermarking schemes aim to embed hidden information into images such that the watermark survives common manipulations. However, powerful diffusion-based image generation and editing techniques now pose a new threat to these watermarks. In this paper, we present a comprehensive theoretical and empirical analysis demonstrating that diffusion models can effectively erase robust watermarks even when those watermarks were designed to withstand conventional distortions. We show that a diffusion-driven image regeneration process, which leverages generative models to recreate an image, can remove embedded watermarks while preserving the image's perceptual content. Furthermore, we introduce a guided diffusion-based attack that explicitly targets the embedded watermark signal during generation, significantly degrading watermark detectability. Theoretically, we prove that as an image undergoes sufficient diffusion transformations, the mutual information between the watermarked image and the hidden payload approaches zero, leading to inevitable decoding failure. Experimentally, we evaluate multiple state-of-the-art watermarking methods (including deep learning-based schemes like StegaStamp, TrustMark, and VINE) and demonstrate that diffusion edits yield near-zero watermark recovery rates after attack, while maintaining high visual fidelity of the regenerated images. Our findings reveal a fundamental vulnerability in current robust watermarking techniques against generative model-based edits, underscoring the need for new strategies to ensure watermark resilience in the era of powerful diffusion models.

</details>


### [13] [AdapTools: Adaptive Tool-based Indirect Prompt Injection Attacks on Agentic LLMs](https://arxiv.org/abs/2602.20720)
*Che Wang,Jiaming Zhang,Ziqi Zhang,Zijie Wang,Yinghui Wang,Jianbo Gao,Tao Wei,Zhong Chen,Wei Yang Bryan Lim*

Main category: cs.CR

TL;DR: AdapTools是一个自适应间接提示注入攻击框架，通过选择隐蔽攻击工具和生成自适应攻击提示，显著提升攻击成功率并降低系统效用。


<details>
  <summary>Details</summary>
Motivation: 随着外部数据服务（如MCP）的集成，基于大语言模型的智能体变得更强大，但也引入了关键的安全漏洞，特别是间接提示注入攻击。现有攻击方法依赖静态模式且在简单语言模型上评估，无法应对现代AI智能体的快速演进。

Method: AdapTools包含两个关键组件：1) 自适应攻击策略构建，开发可迁移的对抗策略进行提示优化；2) 攻击增强，识别能够规避任务相关性防御的隐蔽工具。

Result: 综合实验评估显示，AdapTools将攻击成功率提高了2.13倍，同时将系统效用降低了1.78倍。该框架即使在最先进的防御机制下仍保持有效性。

Conclusion: 该方法推进了对间接提示注入攻击的理解，为未来研究提供了有用的参考。

Abstract: The integration of external data services (e.g., Model Context Protocol, MCP) has made large language model-based agents increasingly powerful for complex task execution. However, this advancement introduces critical security vulnerabilities, particularly indirect prompt injection (IPI) attacks. Existing attack methods are limited by their reliance on static patterns and evaluation on simple language models, failing to address the fast-evolving nature of modern AI agents. We introduce AdapTools, a novel adaptive IPI attack framework that selects stealthier attack tools and generates adaptive attack prompts to create a rigorous security evaluation environment. Our approach comprises two key components: (1) Adaptive Attack Strategy Construction, which develops transferable adversarial strategies for prompt optimization, and (2) Attack Enhancement, which identifies stealthy tools capable of circumventing task-relevance defenses. Comprehensive experimental evaluation shows that AdapTools achieves a 2.13 times improvement in attack success rate while degrading system utility by a factor of 1.78. Notably, the framework maintains its effectiveness even against state-of-the-art defense mechanisms. Our method advances the understanding of IPI attacks and provides a useful reference for future research.

</details>


### [14] [A Secure and Interoperable Architecture for Electronic Health Record Access Control and Sharing](https://arxiv.org/abs/2602.20830)
*Tayeb Kenaza,Islam Debicha,Youcef Fares,Mehdi Sehaki,Sami Messai*

Main category: cs.CR

TL;DR: 提出基于区块链和IPFS的电子健康记录安全访问控制架构，通过智能合约让患者完全控制自己的医疗数据，确保符合GDPR等隐私法规


<details>
  <summary>Details</summary>
Motivation: 电子健康记录包含敏感患者信息，需要严格的访问控制和共享机制来确保数据安全并符合GDPR等隐私法规。现有系统在数据安全、患者控制权和系统互操作性方面存在不足

Method: 采用私有区块链和IPFS技术，部署智能合约实现患者专属控制。架构与现有医疗信息系统无缝集成，解决安全性和数据异构性问题。基于Hyperledger平台开发原型系统进行验证

Result: 实验结果表明方案具有良好的可扩展性，能够有效支持医疗从业者之间的访问控制和健康数据共享，在真实医疗环境中展现出高效性和鲁棒性

Conclusion: 提出的区块链-IPFS架构为电子健康记录提供了安全、合规的访问控制和共享解决方案，赋予患者数据自主权，同时保持系统互操作性，适用于实际医疗环境

Abstract: Electronic Health Records (EHRs) store sensitive patient information, necessitating stringent access control and sharing mechanisms to uphold data security and comply with privacy regulations such as the General Data Protection Regulation (GDPR). In this paper, we propose a comprehensive architecture with a suite of efficient protocols that leverage the synergistic capabilities of the Blockchain and Interplanetary File System (IPFS) technologies to enable secure access control and sharing of EHRs. Our approach is based on a private blockchain, wherein smart contracts are deployed to enforce control exclusively by patients. By granting patients exclusive control over their EHRs, our solution ensures compliance with personal data protection laws and empowers individuals to manage their health information autonomously. Notably, our proposed architecture seamlessly integrates with existing health provider information systems, facilitating interoperability and effectively addressing security and data heterogeneity challenges. To demonstrate the effectiveness of our approach, we developed a prototype based on a private implementation of the Hyperledger platform, enabling the simulation of diverse scenarios involving access control and health data sharing among healthcare practitioners. Our experimental results demonstrate the scalability of our solution, thereby substantiating its efficacy and robustness in real-world healthcare settings.

</details>


### [15] [SoK: Agentic Skills -- Beyond Tool Use in LLM Agents](https://arxiv.org/abs/2602.20867)
*Yanna Jiang,Delong Li,Haiyu Deng,Baihe Ma,Xu Wang,Qin Wang,Guangsheng Yu*

Main category: cs.CR

TL;DR: 论文提出智能体技能层的完整生命周期框架，包含七种设计模式和表示×范围分类法，分析技能安全风险并通过案例研究展示恶意技能威胁，指出技能可提升智能体成功率但自生成技能可能降低性能。


<details>
  <summary>Details</summary>
Motivation: 智能体系统越来越依赖可重用的程序化能力（技能）来可靠执行长时程工作流，但当前缺乏对技能层完整生命周期的系统性分析框架，以及技能设计模式、表示形式、安全风险等方面的综合研究。

Method: 1) 建立技能层完整生命周期框架（发现、实践、提炼、存储、组合、评估、更新）；2) 提出七种系统级设计模式；3) 创建表示×范围正交分类法；4) 分析技能安全与治理影响；5) 通过ClawHavoc恶意技能案例研究验证风险；6) 调查确定性评估方法。

Result: 1) 系统化技能生命周期框架和分类法；2) 识别七种技能设计模式；3) 揭示技能供应链风险、提示注入等安全威胁；4) 案例研究显示近1200个恶意技能入侵主要智能体市场；5) 基准证据表明精心设计的技能可显著提升智能体成功率，但自生成技能可能降低性能。

Conclusion: 技能层对智能体系统至关重要，但面临安全、验证和认证等挑战。需要建立健壮、可验证、可认证的技能框架，以支持现实世界自主智能体的可靠部署，特别是在恶意技能威胁日益严重的背景下。

Abstract: Agentic systems increasingly rely on reusable procedural capabilities, \textit{a.k.a., agentic skills}, to execute long-horizon workflows reliably. These capabilities are callable modules that package procedural knowledge with explicit applicability conditions, execution policies, termination criteria, and reusable interfaces. Unlike one-off plans or atomic tool calls, skills operate (and often do well) across tasks.
  This paper maps the skill layer across the full lifecycle (discovery, practice, distillation, storage, composition, evaluation, and update) and introduces two complementary taxonomies. The first is a system-level set of \textbf{seven design patterns} capturing how skills are packaged and executed in practice, from metadata-driven progressive disclosure and executable code skills to self-evolving libraries and marketplace distribution. The second is an orthogonal \textbf{representation $\times$ scope} taxonomy describing what skills \emph{are} (natural language, code, policy, hybrid) and what environments they operate over (web, OS, software engineering, robotics).
  We analyze the security and governance implications of skill-based agents, covering supply-chain risks, prompt injection via skill payloads, and trust-tiered execution, grounded by a case study of the ClawHavoc campaign in which nearly 1{,}200 malicious skills infiltrated a major agent marketplace, exfiltrating API keys, cryptocurrency wallets, and browser credentials at scale. We further survey deterministic evaluation approaches, anchored by recent benchmark evidence that curated skills can substantially improve agent success rates while self-generated skills may degrade them. We conclude with open challenges toward robust, verifiable, and certifiable skills for real-world autonomous agents.

</details>
