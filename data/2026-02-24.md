<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 21]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [DCInject: Persistent Backdoor Attacks via Frequency Manipulation in Personal Federated Learning](https://arxiv.org/abs/2602.18489)
*Nahom Birhan,Daniel Wesego,Dereje Shenkut,Frank Liu,Daniel Takabi*

Main category: cs.CR

TL;DR: 本文提出DCInject，一种针对个性化联邦学习(PFL)的自适应频域后门攻击方法，通过移除零频分量并替换为高斯分布样本，在保持清洁精度的同时实现高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 虽然个性化联邦学习(PFL)被认为对后门攻击传播具有天然抵抗力，但本文揭示PFL仍然存在安全漏洞，需要研究更隐蔽有效的攻击方法。

Method: 提出DCInject攻击方法：在频域中移除部分零频(DC)分量，并用高斯分布样本替换，形成自适应频域后门攻击，针对参数解耦的个性化联邦学习。

Result: 在四个数据集(CIFAR-10/100, GTSRB, SVHN)上，DCInject在保持清洁精度的同时，攻击成功率显著优于现有空间域攻击：CIFAR-10达96.83%，SVHN达99.38%，GTSRB达100%。在I-BAU防御下，DCInject仍保持90.30%攻击成功率，而BadNet仅58.56%。

Conclusion: DCInject暴露了PFL安全假设的关键漏洞，表明频域攻击能够有效绕过PFL的防御机制，对个性化联邦学习的安全性提出了严峻挑战。

Abstract: Personalized federated learning (PFL) creates client-specific models to handle data heterogeneity. Previously, PFL has been shown to be naturally resistant to backdoor attack propagation across clients. In this work, we reveal that PFL remains vulnerable to backdoor attacks through a novel frequency-domain approach. We propose DCInject, an adaptive frequency-domain backdoor attack for PFL, which removes portions of the zero-frequency (DC) component and replaces them with Gaussian-distributed samples in the frequency domain. Our attack achieves superior attack success rates while maintaining clean accuracy across four datasets (CIFAR-10/100, GTSRB, SVHN) compared to existing spatial-domain attacks, evaluated under parameter decoupling based personalization. DCInject achieves superior performance with ASRs of 96.83% (CIFAR-10), 99.38% (SVHN), and 100% (GTSRB) while maintaining clean accuracy. Under I-BAU defense, DCInject demonstrates strong persistence, retaining 90.30% ASR vs BadNet's 58.56% on VGG-16, exposing critical vulnerabilities in PFL security assumptions. Our code is available at https://github.com/NahomMA/DCINject-PFL

</details>


### [2] [Trojan Horses in Recruiting: A Red-Teaming Case Study on Indirect Prompt Injection in Standard vs. Reasoning Models](https://arxiv.org/abs/2602.18514)
*Manuel Wirth*

Main category: cs.CR

TL;DR: 研究挑战"推理模型更安全"的假设，通过红队测试发现推理模型在间接提示注入攻击中表现出更危险的策略性重构能力，但复杂攻击时会出现"元认知泄漏"使攻击更易被检测。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在人力资源等自动化决策系统中应用增加，间接提示注入的安全风险变得关键。当前普遍假设认为"推理"或"思维链"模型因自我纠正能力而更安全，但本研究挑战这一假设。

Method: 使用Qwen 3 30B架构进行定性红队案例研究，对比标准指令调优模型和推理增强模型。通过"特洛伊木马"简历进行测试，观察两种模型在简单和复杂攻击场景下的不同失败模式。

Result: 标准模型通过脆弱的幻觉来合理化简单攻击，在复杂场景中过滤掉不合逻辑的约束；推理模型则表现出危险的双重性：对简单攻击使用高级策略重构使其更具说服力，但在面对逻辑复杂的指令时出现"元认知泄漏"，导致攻击逻辑无意中泄露到输出中。

Conclusion: 推理模型并非更安全，而是展现出复杂的安全权衡：它们能更巧妙地合理化简单攻击，但处理复杂对抗指令时会出现认知负载过载，导致攻击逻辑泄漏，反而使攻击更容易被人类检测到。

Abstract: As Large Language Models (LLMs) are increasingly integrated into automated decision-making pipelines, specifically within Human Resources (HR), the security implications of Indirect Prompt Injection (IPI) become critical. While a prevailing hypothesis posits that "Reasoning" or "Chain-of-Thought" Models possess safety advantages due to their ability to self-correct, emerging research suggests these capabilities may enable more sophisticated alignment failures. This qualitative Red-Teaming case study challenges the safety-through-reasoning premise using the Qwen 3 30B architecture. By subjecting both a standard instruction-tuned model and a reasoning-enhanced model to a "Trojan Horse" curriculum vitae, distinct failure modes are observed. The results suggest a complex trade-off: while the Standard Model resorted to brittle hallucinations to justify simple attacks and filtered out illogical constraints in complex scenarios, the Reasoning Model displayed a dangerous duality. It employed advanced strategic reframing to make simple attacks highly persuasive, yet exhibited "Meta-Cognitive Leakage" when faced with logically convoluted commands. This study highlights a failure mode where the cognitive load of processing complex adversarial instructions causes the injection logic to be unintentionally printed in the final output, rendering the attack more detectable by humans than in Standard Models.

</details>


### [3] [Poster: Privacy-Preserving Compliance Checks on Ethereum via Selective Disclosure](https://arxiv.org/abs/2602.18539)
*Supriya Khadka,Dhiman Goswami,Sanchari Das*

Main category: cs.CR

TL;DR: 基于以太坊的选择性披露框架，使用zk-SNARKs技术让用户证明特定资格条件而无需透露身份信息，通过ZK-Compliance案例实现年龄验证的生命周期管理。


<details>
  <summary>Details</summary>
Motivation: 数字身份验证通常迫使用户在隐私和验证之间做出权衡，必须披露敏感个人数据来证明简单的资格标准。随着区块链应用与监管环境整合，这种过度披露带来了数据泄露和监控的重大风险。

Method: 提出基于以太坊的通用选择性披露框架，利用客户端zk-SNARKs技术，将属性验证与身份揭示解耦。通过ZK-Compliance案例研究，实现了授权、验证、撤销的生命周期管理，特别针对年龄验证场景。

Result: 初步结果表明，该框架能够在满足严格合规要求的同时，保持客户端延迟可忽略不计（<200毫秒），并保护公共区块链的匿名性本质。

Conclusion: 该选择性披露框架成功解决了区块链应用中隐私与合规的平衡问题，通过零知识证明技术实现了在不泄露身份信息的前提下验证特定资格条件，为监管环境下的区块链应用提供了可行的隐私保护解决方案。

Abstract: Digital identity verification often forces a privacy trade-off, where users must disclose sensitive personal data to prove simple eligibility criteria. As blockchain applications integrate with regulated environments, this over-disclosure creates significant risks of data breaches and surveillance. This work proposes a general Selective Disclosure Framework built on Ethereum, designed to decouple attribute verification from identity revelation. By utilizing client-side zk-SNARKs, the framework enables users to prove specific eligibility predicates without revealing underlying identity documents. We present a case study, ZK-Compliance, which implements a functional Grant, Verify, Revoke lifecycle for age verification. Preliminary results indicate that strict compliance requirements can be satisfied with negligible client-side latency (< 200 ms) while preserving the pseudonymous nature of public blockchains.

</details>


### [4] [Influence of Autoencoder Latent Space on Classifying IoT CoAP Attacks](https://arxiv.org/abs/2602.18598)
*María Teresa García-Ordás,Jose Aveleira-Mata,Isaías García-Rodríguez,José Luis Casteleiro-Roca,Martín Bayón-Gutierrez,Héctor Alaiz-Moretón*

Main category: cs.CR

TL;DR: 本研究探索在物联网环境中基于模型的入侵检测系统中使用自动编码器潜在空间结合三种分类技术的数据降维方法，针对CoAP协议实现超过99%精确度的安全威胁检测。


<details>
  <summary>Details</summary>
Motivation: 物联网设备资源受限且相互连接，存在独特网络安全挑战，这些漏洞不仅威胁数据完整性，还影响物联网系统整体功能。需要为物联网环境开发高效的入侵检测系统。

Method: 采用基于模型的入侵检测系统，探索自动编码器的潜在空间数据降维技术，结合三种不同的分类方法。使用经过验证的物联网数据集，特别关注受限应用协议（CoAP），开发能够识别针对该协议的安全攻击的鲁棒模型。

Result: 研究取得了令人鼓舞的结果，提出的方法在物联网网络安全加固方面表现出色，仅使用2个学习特征就实现了超过99%的精确度。

Conclusion: 自动编码器潜在空间结合分类技术的数据降维方法能够有效增强物联网网络安全，为资源受限的物联网设备提供了高效的入侵检测解决方案。

Abstract: The Internet of Things (IoT) presents a unique cybersecurity challenge due to its vast network of interconnected, resource-constrained devices. These vulnerabilities not only threaten data integrity but also the overall functionality of IoT systems. This study addresses these challenges by exploring efficient data reduction techniques within a model-based intrusion detection system (IDS) for IoT environments. Specifically, the study explores the efficacy of an autoencoder's latent space combined with three different classification techniques. Utilizing a validated IoT dataset, particularly focusing on the Constrained Application Protocol (CoAP), the study seeks to develop a robust model capable of identifying security breaches targeting this protocol. The research culminates in a comprehensive evaluation, presenting encouraging results that demonstrate the effectiveness of the proposed methodologies in strengthening IoT cybersecurity with more than a 99% of precision using only 2 learned features.

</details>


### [5] [Orbital Escalation: Modeling Satellite Ransomware Attacks Using Game Theory](https://arxiv.org/abs/2602.18624)
*Efrén López-Morales*

Main category: cs.CR

TL;DR: 首个针对卫星勒索软件攻击的博弈论框架——轨道升级博弈，通过动态规划求解防御者最优策略和攻击者预期收益，并以GPS III卫星案例展示应用。


<details>
  <summary>Details</summary>
Motivation: 勒索软件尚未进入轨道，但此类攻击的条件已经存在。卫星系统面临潜在的勒索软件威胁，需要建立正式的分析框架来帮助卫星所有者、政策制定者和研究人员更好地准备应对措施。

Method: 提出轨道升级博弈模型，攻击者在轨道通过期间逐步升级勒索要求，防御者选择最佳策略（如尝试恢复程序）。使用动态规划方法，在真实轨道约束下求解防御者的最优策略和攻击者的预期收益。

Result: 建立了首个针对卫星勒索软件攻击的博弈论框架，提供了GPS III卫星的案例研究，展示了如何在实际可行的虚构勒索攻击场景中应用该模型，推导出每一步的最佳策略。

Conclusion: 这一基础模型为卫星所有者、政策制定者和研究人员提供了一个正式框架，当航天器被勒索时能够更好地准备应对策略，填补了卫星安全领域的理论空白。

Abstract: Ransomware has yet to reach orbit, but the conditions for such an attack already exist. This paper presents the first game-theoretic framework for modeling ransomware against satellites: the orbital escalation game. In this model, the attacker escalates ransom demands across orbital passes, while the defender chooses their best strategy, e.g., attempt a restore procedure. Using dynamic programming, we solve the defender's optimal strategy and the attacker's expected payoff under real orbital constraints. Additionally, we provide a GPS III satellite case study that demonstrates how our orbital escalation game can be applied in the context of a fictional but feasible ransomware attack to derive the best strategies at every step. In conclusion, this foundational model offers satellite owners, policy makers and researchers, a formal framework to better prepare their responses when a spacecraft is held for ransom.

</details>


### [6] [Media Integrity and Authentication: Status, Directions, and Futures](https://arxiv.org/abs/2602.18681)
*Jessica Young,Sam Vaughan,Andrew Jenks,Henrique Malvar,Christian Paquin,Paul England,Thomas Roca,Juan LaVista Ferres,Forough Poursabzi,Neil Coles,Ken Archer,Eric Horvitz*

Main category: cs.CR

TL;DR: 该论文分析了区分AI生成媒体与真实拍摄内容的媒体完整性认证方法，评估了来源追溯、水印和指纹三种技术，并探讨了对抗攻击和边缘设备安全增强方向。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成媒体的快速发展，如何可靠地区分AI生成内容与真实拍摄内容成为重要挑战。需要建立有效的媒体完整性认证方法来应对这一新兴威胁。

Method: 论文首先定义了三种代表性技术：加密安全来源追溯、不可感知水印和软哈希指纹。然后分析了这些技术在不同模态下的运作方式，评估了相关威胁模型、攻击类别和实际工作流程（包括捕获、编辑、分发和验证）。特别考虑了社会技术反转攻击，并探讨了边缘设备安全增强技术。

Result: 分析表明，现有技术各有优缺点：来源追溯提供强安全性但依赖基础设施；水印可嵌入但可能被移除；指纹基于内容特征但可能被模仿。社会技术反转攻击能够颠倒完整性信号，使真实内容看起来像合成的，反之亦然。需要建立既能抵抗技术攻击又能抵抗心理社会操纵的验证系统。

Conclusion: 需要开发高置信度的来源认证技术，包括利用安全飞地加强边缘设备安全。未来的媒体完整性系统必须同时考虑技术安全性和社会工程学攻击的防御能力，建立多层次的认证体系来应对AI生成媒体带来的挑战。

Abstract: We provide background on emerging challenges and future directions with media integrity and authentication methods, focusing on distinguishing AI-generated media from authentic content captured by cameras and microphones. We evaluate several approaches, including provenance, watermarking, and fingerprinting. After defining each method, we analyze three representative technologies: cryptographically secured provenance, imperceptible watermarking, and soft-hash fingerprinting. We analyze how these tools operate across modalities and evaluate relevant threat models, attack categories, and real-world workflows spanning capture, editing, distribution, and verification. We consider sociotechnical reversal attacks that can invert integrity signals, making authentic content appear synthetic and vice versa, highlighting the value of verification systems that are resilient to both technical and psychosocial manipulation. Finally, we outline techniques for delivering high-confidence provenance authentication, including directions for strengthening edge-device security using secure enclaves.

</details>


### [7] [Watermarking LLM Agent Trajectories](https://arxiv.org/abs/2602.18700)
*Wenlong Meng,Chen Gong,Terry Yue Zhuo,Fan Zhang,Kecen Li,Zheng Liu,Zhou Yang,Chengkun Wei,Wenzhi Chen*

Main category: cs.CR

TL;DR: ActHook是首个针对LLM智能体轨迹数据集的水印方法，通过嵌入钩子动作实现版权保护，在保持任务性能的同时实现高精度检测。


<details>
  <summary>Details</summary>
Motivation: LLM智能体依赖高质量的轨迹数据，但现有研究忽视了轨迹数据的版权保护，使创作者面临数据盗窃风险且难以追踪滥用或执行所有权权利。

Method: 受软件工程中钩子机制启发，ActHook在智能体决策点嵌入钩子动作，这些动作由秘密输入密钥激活且不改变原始任务结果。智能体按顺序执行，钩子动作可在不中断任务流的情况下插入。

Result: 在数学推理、网络搜索和软件工程智能体上的实验表明，ActHook在Qwen-2.5-Coder-7B上平均检测AUC达到94.3，同时性能下降可忽略不计。

Conclusion: ActHook为LLM智能体轨迹数据集提供了有效的版权保护解决方案，填补了该领域的研究空白，实现了可靠的黑盒检测。

Abstract: LLM agents rely heavily on high-quality trajectory data to guide their problem-solving behaviors, yet producing such data requires substantial task design, high-capacity model generation, and manual filtering. Despite the high cost of creating these datasets, existing literature has overlooked copyright protection for LLM agent trajectories. This gap leaves creators vulnerable to data theft and makes it difficult to trace misuse or enforce ownership rights. This paper introduces ActHook, the first watermarking method tailored for agent trajectory datasets. Inspired by hook mechanisms in software engineering, ActHook embeds hook actions that are activated by a secret input key and do not alter the original task outcome. Like software execution, LLM agents operate sequentially, allowing hook actions to be inserted at decision points without disrupting task flow. When the activation key is present, an LLM agent trained on watermarked trajectories can produce these hook actions at a significantly higher rate, enabling reliable black-box detection. Experiments on mathematical reasoning, web searching, and software engineering agents show that ActHook achieves an average detection AUC of 94.3 on Qwen-2.5-Coder-7B while incurring negligible performance degradation.

</details>


### [8] [On the Energy Cost of Post-Quantum Key Establishment in Wireless Low-Power Personal Area Networks](https://arxiv.org/abs/2602.18708)
*Tao Liu,Gowri Ramachandra,Raja Jurdak*

Main category: cs.CR

TL;DR: 后量子密码学在个人区域网络中产生高能耗负载，通信成本常超过密码计算成本，需要协议协调和底层优化


<details>
  <summary>Details</summary>
Motivation: 后量子密码学（PQC）在个人区域网络（PAN）中产生高能耗负载，导致严重的数据包碎片化、长时间无线电活动和传输开销。现有研究主要优化密码计算，但忽视了通信成本问题。

Method: 使用蓝牙低功耗作为代表性平台，将计算成本和通信成本分离分析，并在真实硬件上进行验证。

Result: 结果显示通信成本通常主导后量子密钥交换的能耗，甚至超过密码计算成本。

Conclusion: 高效的量子弹性配对需要协调的协议配置和底层优化。这项工作为开发者提供了实用的PQC能耗权衡分析方法，并为PAN标准向量子安全操作的演进提供了指导。

Abstract: Post-Quantum Cryptography (PQC) creates payloads that strain the timing and energy budgets of Personal Area Networks. In post-quantum key exchange (PQKE), this causes severe fragmentation, prolonged radio activity, and high transmission overhead on low-power devices. Prior work optimizes cryptographic computation but largely ignores communication cost. This paper separates computation and communication costs using Bluetooth Low Energy as a representative platform and validates them on real hardware. Results show communication often dominates PQKE energy, exceeding cryptographic cost. Efficient quantum-resilient pairing therefore requires coordinated protocol configuration and lower-layer optimization. This work provides developers a practical way to reason about PQC energy trade-offs and informs the evolution of PAN standards toward quantum-safe operation.

</details>


### [9] [UFO: Unlocking Ultra-Efficient Quantized Private Inference with Protocol and Algorithm Co-Optimization](https://arxiv.org/abs/2602.18758)
*Wenxuan Zeng,Chao Yang,Tianshi Xu,Bo Zhang,Changrui Ren,Jin Dong,Meng Li*

Main category: cs.CR

TL;DR: UFO是一个量化两方计算推理框架，通过联合优化2PC协议和量化算法，结合Winograd卷积和量化来提升私有CNN推理效率。


<details>
  <summary>Details</summary>
Motivation: 基于安全两方计算的私有CNN推理存在高通信和延迟开销，特别是卷积层。现有方法在结合量化和Winograd卷积时面临通信开销大和量化训练困难的问题。

Method: 1) 协议层面：提出图级优化最小化通信开销；2) 算法层面：开发基于层敏感性的混合精度量化感知训练算法，并引入2PC友好的比特重加权算法处理异常值。

Result: 与SiRNN、COINN和CoPriv相比，UFO分别实现了11.7倍、3.6倍和6.3倍的通信减少，同时准确率分别提高了1.29%、1.16%和1.29%。

Conclusion: UFO通过协议和算法的协同优化，有效解决了量化和Winograd卷积结合时的挑战，显著提升了私有CNN推理的效率和准确性。

Abstract: Private convolutional neural network (CNN) inference based on secure two-party computation (2PC) suffers from high communication and latency overhead, especially from convolution layers. In this paper, we propose UFO, a quantized 2PC inference framework that jointly optimizes the 2PC protocols and quantization algorithm. UFO features a novel 2PC protocol that systematically combines the efficient Winograd convolution algorithm with quantization to improve inference efficiency. However, we observe that naively combining quantization and Winograd convolution faces the following challenges: 1) From the inference perspective, Winograd transformations introduce extensive additions and require frequent bit width conversions to avoid inference overflow, leading to non-negligible communication overhead; 2) From the training perspective, Winograd transformations introduce weight outliers that make quantization-aware training (QAT) difficult, resulting in inferior model accuracy. To address these challenges, we co-optimize both protocol and algorithm. 1) At the protocol level, we propose a series of graph-level optimizations for 2PC inference to minimize the communication. 2) At the algorithm level, we develop a mixed-precision QAT algorithm based on layer sensitivity to optimize model accuracy given communication constraints. To accommodate the outliers, we further introduce a 2PC-friendly bit re-weighting algorithm to increase the representation range without explicitly increasing bit widths. With extensive experiments, UFO demonstrates 11.7x, 3.6x, and 6.3x communication reduction with 1.29%, 1.16%, and 1.29% higher accuracy compared to state-of-the-art frameworks SiRNN, COINN, and CoPriv, respectively.

</details>


### [10] [MANATEE: Inference-Time Lightweight Diffusion Based Safety Defense for LLMs](https://arxiv.org/abs/2602.18782)
*Chun Yan Ryan Kan,Tommy Tran,Vedant Yadav,Ava Cai,Kevin Zhu,Ruizhe Li,Maheep Chaudhary*

Main category: cs.CR

TL;DR: MANATEE是一种基于密度估计的推理时防御方法，通过扩散过程将异常隐藏状态投影到安全区域，有效防御LLM对抗性越狱攻击，无需有害训练数据或架构修改。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对抗性越狱攻击防御方法存在局限性：基于二元分类器的方法在对抗性输入超出学习决策边界时失效；重复微调计算成本高且可能降低模型能力。需要一种更有效的防御方法。

Method: MANATEE使用良性表示流形上的密度估计，学习良性隐藏状态的得分函数，通过扩散过程将异常表示投影到安全区域。该方法无需有害训练数据，无需修改模型架构，在推理时进行防御。

Result: 在Mistral-7B-Instruct、Llama-3.1-8B-Instruct和Gemma-2-9B-it上的实验表明，MANATEE在某些数据集上可将攻击成功率降低高达100%，同时在良性输入上保持模型效用。

Conclusion: MANATEE提供了一种有效的推理时防御方法，通过密度估计和扩散投影技术防御LLM对抗性越狱攻击，在保持模型效用的同时显著降低攻击成功率，无需有害数据或架构修改。

Abstract: Defending LLMs against adversarial jailbreak attacks remains an open challenge. Existing defenses rely on binary classifiers that fail when adversarial input falls outside the learned decision boundary, and repeated fine-tuning is computationally expensive while potentially degrading model capabilities. We propose MANATEE, an inference-time defense that uses density estimation over a benign representation manifold. MANATEE learns the score function of benign hidden states and uses diffusion to project anomalous representations toward safe regions--requiring no harmful training data and no architectural modifications. Experiments across Mistral-7B-Instruct, Llama-3.1-8B-Instruct, and Gemma-2-9B-it demonstrate that MANATEE reduce Attack Success Rate by up to 100\% on certain datasets, while preserving model utility on benign inputs.

</details>


### [11] [PrivacyBench: Privacy Isn't Free in Hybrid Privacy-Preserving Vision Systems](https://arxiv.org/abs/2602.18900)
*Nnaemeka Obiefuna,Samuel Oyeneye,Similoluwa Odunaiya,Iremide Oyelaja,Steven Kolawole*

Main category: cs.CR

TL;DR: PrivacyBench框架揭示隐私技术组合存在严重协同失效，特别是FL+DP组合导致准确率从98%暴跌至13%，而FL+SMPC组合性能接近基线


<details>
  <summary>Details</summary>
Motivation: 在医疗影像和自动驾驶等敏感深度学习应用中，隐私保护机器学习部署需要组合多种技术，但缺乏系统指导来评估这些混合配置的协同和非加性交互，现有方法依赖孤立技术分析，忽略了关键的系统级交互

Method: 提出PrivacyBench基准测试框架，通过自动化YAML配置、资源监控和可重复实验协议，在ResNet18和ViT模型及医疗数据集上进行系统评估

Result: 发现FL+DP组合存在严重收敛失败，准确率从98%降至13%，计算成本和能耗大幅增加；而FL+SMPC组合保持接近基线性能，仅有适度开销

Conclusion: 隐私技术不能任意组合，PrivacyBench为资源受限环境中的稳健部署提供关键指导，推动隐私保护计算机视觉从临时评估转向原则性系统设计

Abstract: Privacy preserving machine learning deployments in sensitive deep learning applications; from medical imaging to autonomous systems; increasingly require combining multiple techniques. Yet, practitioners lack systematic guidance to assess the synergistic and non-additive interactions of these hybrid configurations, relying instead on isolated technique analysis that misses critical system level interactions. We introduce PrivacyBench, a benchmarking framework that reveals striking failures in privacy technique combinations with severe deployment implications. Through systematic evaluation across ResNet18 and ViT models on medical datasets, we uncover that FL + DP combinations exhibit severe convergence failure, with accuracy dropping from 98% to 13% while compute costs and energy consumption substantially increase. In contrast, FL + SMPC maintains near-baseline performance with modest overhead. Our framework provides the first systematic platform for evaluating privacy-utility-cost trade-offs through automated YAML configuration, resource monitoring, and reproducible experimental protocols. PrivacyBench enables practitioners to identify problematic technique interactions before deployment, moving privacy-preserving computer vision from ad-hoc evaluation toward principled systems design. These findings demonstrate that privacy techniques cannot be composed arbitrarily and provide critical guidance for robust deployment in resource-constrained environments.

</details>


### [12] [LLM Scalability Risk for Agentic-AI and Model Supply Chain Security](https://arxiv.org/abs/2602.19021)
*Kiarash Ahi,Vaibhav Agrawal,Saeed Valizadeh*

Main category: cs.CR

TL;DR: 论文提出统一分析框架，整合生成式AI在网络安全中的攻防视角，引入LLM可扩展性风险指数(LSRI)和模型供应链信任框架，为大规模LLM安全部署提供治理路线图。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和生成式AI正在变革网络安全，既支持高级防御也催生新型攻击。组织使用LLM进行威胁检测、代码审查和DevSecOps自动化，而攻击者则利用其制作恶意软件和定向社会工程攻击。需要整合攻防视角，分析AI驱动威胁对全球安全的影响，建立可扩展的防御机制。

Method: 基于70个学术、行业和政策来源的统一分析，提出两个主要贡献：1) LLM可扩展性风险指数(LSRI) - 在安全关键环境中部署LLM时进行运营风险压力测试的参数框架；2) 模型供应链框架 - 在整个模型生命周期中建立可验证的信任根。同时综合Google Play Protect、Microsoft Security Copilot等平台的防御策略。

Result: 建立了统一的生成式AI网络安全分析框架，开发了LSRI风险评估工具和模型供应链信任机制，为组织提供了可操作的安全部署指南和治理路线图。

Conclusion: 论文为大规模LLM安全部署提供了系统性解决方案，通过整合攻防视角、风险评估框架和供应链信任机制，帮助组织在利用生成式AI优势的同时有效管理安全风险，推动网络安全向AI驱动的新范式转型。

Abstract: Large Language Models (LLMs) & Generative AI are transforming cybersecurity, enabling both advanced defenses and new attacks. Organizations now use LLMs for threat detection, code review, and DevSecOps automation, while adversaries leverage them to produce malwares and run targeted social-engineering campaigns. This paper presents a unified analysis integrating offensive and defensive perspectives on GenAI-driven cybersecurity. Drawing on 70 academic, industry, and policy sources, it analyzes the rise of AI-facilitated threats and its implications for global security to ground necessity for scalable defensive mechanisms. We introduce two primary contributions: the LLM Scalability Risk Index (LSRI), a parametric framework to stress-test operational risks when deploying LLMs in security-critical environments & a model-supply-chain framework establishing a verifiable root of trust throughout model lifecycle. We also synthesize defense strategies from platforms like Google Play Protect, Microsoft Security Copilot and outline a governance roadmap for secure, large-scale LLM deployment.

</details>


### [13] [Routing-Aware Explanations for Mixture of Experts Graph Models in Malware Detection](https://arxiv.org/abs/2602.19025)
*Hossein Shokouhinejad,Roozbeh Razavi-Far,Griffin Higgins,Ali. A Ghorbani*

Main category: cs.CR

TL;DR: 该论文提出了一种用于恶意软件检测的MoE图模型，通过节点级多统计量编码和专家级多样性构建，结合路由感知解释来提高模型透明度。


<details>
  <summary>Details</summary>
Motivation: 研究MoE图模型在恶意软件检测中的路由感知解释问题，旨在提高模型决策的透明度，特别是在使用控制流图进行恶意软件分析时。

Method: 构建两级多样性架构：节点级计算多个邻域统计量（均值、标准差、最大值）并通过MLP融合，使用度重加权因子ρ和池化选择λ；读取级有6个专家，每个对应特定(ρ, λ)视图，通过路由器加权输出最终预测。后验解释通过专家级边归因聚合生成。

Result: 相比GCN、GIN、GAT等单专家GNN基线，提出的MoE模型在CFG数据集上实现了强检测准确率，并在稀疏性扰动下产生稳定、忠实的归因。

Conclusion: 通过显式路由器和多统计量节点编码与专家级多样性的结合，可以提高MoE决策在恶意软件分析中的透明度。

Abstract: Mixture-of-Experts (MoE) offers flexible graph reasoning by combining multiple views of a graph through a learned router. We investigate routing-aware explanations for MoE graph models in malware detection using control flow graphs (CFGs). Our architecture builds diversity at two levels. At the node level, each layer computes multiple neighborhood statistics and fuses them with an MLP, guided by a degree reweighting factor rho and a pooling choice lambda in {mean, std, max}, producing distinct node representations that capture complementary structural cues in CFGs. At the readout level, six experts, each tied to a specific (rho, lambda) view, output graph-level logits that the router weights into a final prediction. Post-hoc explanations are generated with edge-level attributions per expert and aggregated using the router gates so the rationale reflects both what each expert highlights and how strongly it is selected. Evaluated against single-expert GNN baselines such as GCN, GIN, and GAT on the same CFG dataset, the proposed MoE achieves strong detection accuracy while yielding stable, faithful attributions under sparsity-based perturbations. The results indicate that making the router explicit and combining multi-statistic node encoding with expert-level diversity can improve the transparency of MoE decisions for malware analysis.

</details>


### [14] [Detecting Cybersecurity Threats by Integrating Explainable AI with SHAP Interpretability and Strategic Data Sampling](https://arxiv.org/abs/2602.19087)
*Norrakith Srisumrith,Sunantha Sodsee*

Main category: cs.CR

TL;DR: 提出一个集成可解释AI框架，解决网络安全威胁检测中的三大挑战：大数据处理、实验严谨性和操作透明度，在CIC-IDS2017数据集上验证了同时实现可解释性、计算效率和实验完整性的可行性。


<details>
  <summary>Details</summary>
Motivation: 网络安全运营中对透明和可信机器学习的需求日益增长，需要解决AI在威胁检测部署中的三个基本挑战：处理海量数据集、确保实验严谨性、提供操作透明度。

Method: 采用集成XAI框架，包含三个核心方法：1) 战略采样方法处理大数据集并保持类别分布；2) 自动数据泄漏预防系统识别和移除污染特征；3) 集成XAI实现使用SHAP分析提供模型无关的可解释性。

Result: 在CIC-IDS2017数据集上应用该框架，保持了检测效果的同时减少了计算开销，并为安全分析师提供了可操作的模型解释，证明了可解释性、计算效率和实验完整性可以同时实现。

Conclusion: 该框架为在安全运营中心部署可信AI系统提供了坚实基础，特别是在决策透明度至关重要的环境中，展示了透明AI在网络安全中的可行性和重要性。

Abstract: The critical need for transparent and trustworthy machine learning in cybersecurity operations drives the development of this integrated Explainable AI (XAI) framework. Our methodology addresses three fundamental challenges in deploying AI for threat detection: handling massive datasets through Strategic Sampling Methodology that preserves class distributions while enabling efficient model development; ensuring experimental rigor via Automated Data Leakage Prevention that systematically identifies and removes contaminated features; and providing operational transparency through Integrated XAI Implementation using SHAP analysis for model-agnostic interpretability across algorithms. Applied to the CIC-IDS2017 dataset, our approach maintains detection efficacy while reducing computational overhead and delivering actionable explanations for security analysts. The framework demonstrates that explainability, computational efficiency, and experimental integrity can be simultaneously achieved, providing a robust foundation for deploying trustworthy AI systems in security operations centers where decision transparency is paramount.

</details>


### [15] [BioEnvSense: A Human-Centred Security Framework for Preventing Behaviour-Driven Cyber Incidents](https://arxiv.org/abs/2602.19410)
*Duy Anh Ta,Farnaz Farid,Farhad Ahamed,Ala Al-Areqi,Robert Beutel,Tamara Watson,Alana Maurushat*

Main category: cs.CR

TL;DR: 提出一个结合CNN-LSTM混合模型的概念性安全框架，通过分析生物特征和环境数据实现情境感知的安全决策，以应对人为因素驱动的网络安全事件。


<details>
  <summary>Details</summary>
Motivation: 现代组织面临越来越多由人为行为而非技术故障驱动的网络安全事件，需要针对人为因素设计更有效的安全防护措施。

Method: 提出概念性安全框架，集成CNN-LSTM混合模型：CNN从传感器数据中提取空间模式，LSTM捕捉与人为错误易感性相关的时间动态特征。

Result: 模型达到84%的准确率，能够可靠地检测导致人为中心网络风险升高的条件，支持持续监控和自适应防护。

Conclusion: 该框架通过主动干预减少人为驱动网络事件的可能性，为组织提供更有效的网络安全防护方案。

Abstract: Modern organizations increasingly face cybersecurity incidents driven by human behaviour rather than technical failures. To address this, we propose a conceptual security framework that integrates a hybrid Convolutional Neural Network-Long Short-Term Memory (CNN-LSTM) model to analyze biometric and environmental data for context-aware security decisions. The CNN extracts spatial patterns from sensor data, while the LSTM captures temporal dynamics associated with human error susceptibility. The model achieves 84% accuracy, demonstrating its ability to reliably detect conditions that lead to elevated human-centred cyber risk. By enabling continuous monitoring and adaptive safeguards, the framework supports proactive interventions that reduce the likelihood of human-driven cyber incidents

</details>


### [16] [Red-Teaming Claude Opus and ChatGPT-based Security Advisors for Trusted Execution Environments](https://arxiv.org/abs/2602.19450)
*Kunal Mukherjee*

Main category: cs.CR

TL;DR: 论文研究了LLM作为TEE安全顾问时的风险，开发了TEE-RedBench评估框架，发现LLM存在幻觉、过度承诺等问题，并提出"LLM-in-the-loop"评估管道可将失败率降低80.62%。


<details>
  <summary>Details</summary>
Motivation: 虽然TEE旨在保护敏感计算免受操作系统攻击，但实际部署仍面临微架构泄漏等风险。同时，安全团队越来越多地依赖LLM作为TEE安全顾问，这带来了社会技术风险：LLM可能产生幻觉、过度承诺保证或在对抗性提示下行为不安全。

Method: 提出了TEE-RedBench评估方法，包括：(1)针对LLM中介安全工作的TEE特定威胁模型；(2)涵盖SGX和TrustZone架构、认证、密钥管理、威胁建模等的结构化提示套件；(3)测量技术正确性、基础性、不确定性校准、拒绝质量和安全帮助性的注释标准。对ChatGPT-5.2和Claude Opus-4.6进行了红队测试。

Result: 研究发现一些失败并非纯粹偶然，在LLM助手之间的可转移性高达12.02%。通过实施"LLM-in-the-loop"评估管道（策略门控、检索基础、结构化模板和轻量验证检查），可将失败率降低80.62%。

Conclusion: LLM作为TEE安全顾问存在显著风险，需要系统性的评估和缓解措施。提出的TEE-RedBench框架和"LLM-in-the-loop"管道能有效减少LLM在安全咨询中的失败，提高其作为TEE安全顾问的可靠性。

Abstract: Trusted Execution Environments (TEEs) (e.g., Intel SGX and ArmTrustZone) aim to protect sensitive computation from a compromised operating system, yet real deployments remain vulnerable to microarchitectural leakage, side-channel attacks, and fault injection. In parallel, security teams increasingly rely on Large Language Model (LLM) assistants as security advisors for TEE architecture review, mitigation planning, and vulnerability triage. This creates a socio-technical risk surface: assistants may hallucinate TEE mechanisms, overclaim guarantees (e.g., what attestation does and does not establish), or behave unsafely under adversarial prompting.
  We present a red-teaming study of two prevalently deployed LLM assistants in the role of TEE security advisors: ChatGPT-5.2 and Claude Opus-4.6, focusing on the inherent limitations and transferability of prompt-induced failures across LLMs. We introduce TEE-RedBench, a TEE-grounded evaluation methodology comprising (i) a TEE-specific threat model for LLM-mediated security work, (ii) a structured prompt suite spanning SGX and TrustZone architecture, attestation and key management, threat modeling, and non-operational mitigation guidance, along with policy-bound misuse probes, and (iii) an annotation rubric that jointly measures technical correctness, groundedness, uncertainty calibration, refusal quality, and safe helpfulness. We find that some failures are not purely idiosyncratic, transferring up to 12.02% across LLM assistants, and we connect these outcomes to secure architecture by outlining an "LLM-in-the-loop" evaluation pipeline: policy gating, retrieval grounding, structured templates, and lightweight verification checks that, when combined, reduce failures by 80.62%.

</details>


### [17] [Hardware-Friendly Randomization: Enabling Random-Access and Minimal Wiring in FHE Accelerators with Low Total Cost](https://arxiv.org/abs/2602.19550)
*Ilan Rosenfeld,Noam Kleinburd,Hillel Chapman,Dror Reuven*

Main category: cs.CR

TL;DR: 提出一种基于种子的RLWE参数a生成方案，减少FHE加速器中的通信开销和硬件实现挑战


<details>
  <summary>Details</summary>
Motivation: RLWE问题中随机多项式a的传输开销大，特别是在FHE加速器中，需要解决硬件实现的布线密度、功耗、计算面积等挑战

Method: 使用确定性过程从种子生成a，客户端只传输种子，在加速器中实时生成a，支持并行生成、松弛布线要求、无限制随机访问RNS limbs

Result: 通信延迟和内存占用减少，客户端开销极低（<3%），无需厚金属层分布随机性，PRNG子系统功耗不随加速因子线性增长，高吞吐配置下可节省数十瓦功耗

Conclusion: 提出的方案有效解决了RLWE中随机多项式a传输的硬件实现挑战，在保持安全性的同时显著降低了通信开销和功耗

Abstract: The Ring-Learning With Errors (RLWE) problem forms the backbone of highly efficient Fully Homomorphic Encryption (FHE) schemes. A significant component of the RLWE public key and ciphertext of the form $(b,a)$ is the uniformly random polynomial $a \in R_q$ . While essential for security, the communication overhead of transmitting $a$ from client to server, and inputting it into a hardware accelerator, can be substantial, especially for FHE accelerators aiming at high acceleration factors. A known technique in reducing this overhead generates $a$ from a small seed on the client side via a deterministic process, transmits only the seed, and generates $a$ on-the-fly within the accelerator. Challenges in the hardware implementation of an accelerator include wiring (density and power), compute area, compute power as well as flexibility in scheduling of on-the-fly generation instructions. This extended abstract proposes a concrete scheme and parameters wherein these practical challenges are addressed. We detail the benefits of our approach, which maintains the reduction in communication latency and memory footprint, while allowing parallel generation of uniformly distributed samples, relaxed wiring requirements, unrestricted randomaccess to RNS limbs, and results in an extremely low overhead on the client side (i.e. less than 3%) during the key generation process. The proposed scheme eliminates the need for thick metal layers for randomness distribution and prevents the power consumption of the PRNG subsystem from scaling prohibitively with the acceleration factor, potentially saving tens of Watts per accelerator chip in high-throughput configurations.

</details>


### [18] [Agentic AI as a Cybersecurity Attack Surface: Threats, Exploits, and Defenses in Runtime Supply Chains](https://arxiv.org/abs/2602.19555)
*Xiaochong Jiang,Shiqi Yang,Wenting Yang,Yichen Liu,Cheng Ji*

Main category: cs.CR

TL;DR: 论文系统化分析了基于大语言模型的智能体系统在运行时面临的安全风险，提出了数据供应链攻击和工具供应链攻击的分类框架，识别了"病毒智能体循环"威胁，并倡导零信任运行时架构解决方案。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的智能体系统从文本生成扩展到自主检索信息和调用工具，这种运行时执行模型将攻击面从构建时工件转移到推理时依赖，使智能体面临通过不可信数据和概率能力解析进行操纵的风险。现有研究主要关注模型级漏洞，而循环和相互依赖的运行时行为产生的安全风险仍然零散。

Method: 作者在一个统一的运行时框架内系统化这些风险，将威胁分类为数据供应链攻击（瞬时上下文注入和持久内存污染）和工具供应链攻击（发现、实现和调用）。进一步识别了"病毒智能体循环"威胁，其中智能体作为自我传播生成蠕虫的载体而无需利用代码级缺陷。

Result: 提出了一个全面的运行时安全威胁分类框架，揭示了智能体系统特有的安全风险模式，特别是病毒智能体循环这一新型威胁，其中智能体可以成为自我传播恶意软件的载体。

Conclusion: 倡导零信任运行时架构，将上下文视为不可信的控制流，并通过加密溯源而非语义推理来约束工具执行，以应对智能体系统在运行时面临的安全挑战。

Abstract: Agentic systems built on large language models (LLMs) extend beyond text generation to autonomously retrieve information and invoke tools. This runtime execution model shifts the attack surface from build-time artifacts to inference-time dependencies, exposing agents to manipulation through untrusted data and probabilistic capability resolution. While prior work has focused on model-level vulnerabilities, security risks emerging from cyclic and interdependent runtime behavior remain fragmented. We systematize these risks within a unified runtime framework, categorizing threats into data supply chain attacks (transient context injection and persistent memory poisoning) and tool supply chain attacks (discovery, implementation, and invocation). We further identify the Viral Agent Loop, in which agents act as vectors for self-propagating generative worms without exploiting code-level flaws. Finally, we advocate a Zero-Trust Runtime Architecture that treats context as untrusted control flow and constrains tool execution through cryptographic provenance rather than semantic inference.

</details>


### [19] [SafePickle: Robust and Generic ML Detection of Malicious Pickle-based ML Models](https://arxiv.org/abs/2602.19818)
*Hillel Ohayon,Daniel Gilkarov,Ran Dubin*

Main category: cs.CR

TL;DR: 提出基于机器学习的轻量级扫描器，无需策略生成或代码插桩即可检测恶意Pickle文件，在多个数据集上优于现有方法


<details>
  <summary>Details</summary>
Motivation: Hugging Face等模型仓库使用Python pickle格式分发模型，存在远程代码执行风险。现有防御方法如PickleBall需要复杂的系统设置和验证良性模型，可扩展性和泛化性有限

Method: 从Pickle字节码中静态提取结构和语义特征，应用监督和无监督模型对文件进行良性/恶意分类。构建并发布了727个Pickle文件的标注数据集

Result: 在自有数据集上达到90.01% F1分数，优于现有扫描器（7.23%-62.75%）；在PickleBall数据集（OOD）上达到81.22% F1分数，优于PickleBall的76.09%；能正确检测9/9专门设计的逃避扫描的恶意模型

Conclusion: 数据驱动的检测方法能有效且通用地缓解基于Pickle的模型文件攻击，无需依赖特定库的策略生成

Abstract: Model repositories such as Hugging Face increasingly distribute machine learning artifacts serialized with Python's pickle format, exposing users to remote code execution (RCE) risks during model loading. Recent defenses, such as PickleBall, rely on per-library policy synthesis that requires complex system setups and verified benign models, which limits scalability and generalization. In this work, we propose a lightweight, machine-learning-based scanner that detects malicious Pickle-based files without policy generation or code instrumentation. Our approach statically extracts structural and semantic features from Pickle bytecode and applies supervised and unsupervised models to classify files as benign or malicious. We construct and release a labeled dataset of 727 Pickle-based files from Hugging Face and evaluate our models on four datasets: our own, PickleBall (out-of-distribution), Hide-and-Seek (9 advanced evasive malicious models), and synthetic joblib files. Our method achieves 90.01% F1-score compared with 7.23%-62.75% achieved by the SOTA scanners (Modelscan, Fickling, ClamAV, VirusTotal) on our dataset. Furthermore, on the PickleBall data (OOD), it achieves 81.22% F1-score compared with 76.09% achieved by the PickleBall method, while remaining fully library-agnostic. Finally, we show that our method is the only one to correctly parse and classify 9/9 evasive Hide-and-Seek malicious models specially crafted to evade scanners. This demonstrates that data-driven detection can effectively and generically mitigate Pickle-based model file attacks.

</details>


### [20] [LLM-enabled Applications Require System-Level Threat Monitoring](https://arxiv.org/abs/2602.19844)
*Yedi Zhang,Haoyu Wang,Xianglin Yang,Jin Song Dong,Jun Sun*

Main category: cs.CR

TL;DR: 该立场论文主张对LLM应用进行系统性安全威胁监控，将其作为可靠运行的前提和事件响应框架的基础


<details>
  <summary>Details</summary>
Motivation: LLM作为核心推理组件的应用范式带来了新的可靠性挑战和安全攻击面扩大，由于LLM行为的非确定性、学习驱动和难以验证特性，这些安全风险应被视为预期操作条件而非异常事件

Method: 采用立场论文形式，提出需要建立系统级威胁监控机制，能够在部署后检测和情境化安全相关异常，超越传统的测试或护栏式防御

Result: 提出系统性全面监控LLM应用安全威胁的必要性，将其作为可靠运行的前提条件，并为专门的事件响应框架奠定基础

Conclusion: 可信部署的主要障碍不是进一步提升模型能力，而是建立能够检测和情境化安全异常的系统级威胁监控机制，这需要在测试和护栏防御之外进行更深入的探索

Abstract: LLM-enabled applications are rapidly reshaping the software ecosystem by using large language models as core reasoning components for complex task execution. This paradigm shift, however, introduces fundamentally new reliability challenges and significantly expands the security attack surface, due to the non-deterministic, learning-driven, and difficult-to-verify nature of LLM behavior. In light of these emerging and unavoidable safety challenges, we argue that such risks should be treated as expected operational conditions rather than exceptional events, necessitating a dedicated incident-response perspective. Consequently, the primary barrier to trustworthy deployment is not further improving model capability but establishing system-level threat monitoring mechanisms that can detect and contextualize security-relevant anomalies after deployment -- an aspect largely underexplored beyond testing or guardrail-based defenses. Accordingly, this position paper advocates systematic and comprehensive monitoring of security threats in LLM-enabled applications as a prerequisite for reliable operation and a foundation for dedicated incident-response frameworks.

</details>


### [21] [RobPI: Robust Private Inference against Malicious Client](https://arxiv.org/abs/2602.19918)
*Jiaqi Xue,Mengxin Zheng,Qian Lou*

Main category: cs.CR

TL;DR: 该论文提出了RobPI协议，一种能够抵御恶意客户端的鲁棒私有推理方案，通过加密兼容噪声保护模型输出，显著降低攻击成功率并增加攻击所需查询次数。


<details>
  <summary>Details</summary>
Motivation: 当前私有推理协议大多假设半诚实威胁模型，即数据所有者诚实遵守协议。然而现实中数据所有者可能有不同动机和不可预测行为，这种假设不现实。作者首先设计了针对现有私有推理协议的推理操纵攻击，发现恶意客户端可以比当前黑盒攻击少3-8倍查询次数就能修改模型输出，这暴露了现有协议的脆弱性。

Method: 提出并实现了RobPI协议，这是一种鲁棒且具有弹性的私有推理协议。该协议集成了一种独特的密码学协议，通过将加密兼容噪声编织到私有推理的对数几率和特征中，从而有效抵御恶意客户端攻击。

Result: 在各种神经网络和数据集上的广泛实验表明，RobPI实现了约91.9%的攻击成功率降低，并将恶意客户端攻击所需的查询次数增加了10倍以上。

Conclusion: RobPI协议能够有效抵御恶意客户端攻击，解决了现有私有推理协议在半诚实威胁模型下的安全漏洞，为实际部署中的私有推理提供了更强的安全保障。

Abstract: The increased deployment of machine learning inference in various applications has sparked privacy concerns. In response, private inference (PI) protocols have been created to allow parties to perform inference without revealing their sensitive data. Despite recent advances in the efficiency of PI, most current methods assume a semi-honest threat model where the data owner is honest and adheres to the protocol. However, in reality, data owners can have different motivations and act in unpredictable ways, making this assumption unrealistic. To demonstrate how a malicious client can compromise the semi-honest model, we first designed an inference manipulation attack against a range of state-of-the-art private inference protocols. This attack allows a malicious client to modify the model output with 3x to 8x fewer queries than current black-box attacks. Motivated by the attacks, we proposed and implemented RobPI, a robust and resilient private inference protocol that withstands malicious clients. RobPI integrates a distinctive cryptographic protocol that bolsters security by weaving encryption-compatible noise into the logits and features of private inference, thereby efficiently warding off malicious-client attacks. Our extensive experiments on various neural networks and datasets show that RobPI achieves ~91.9% attack success rate reduction and increases more than 10x the number of queries required by malicious-client attacks.

</details>
