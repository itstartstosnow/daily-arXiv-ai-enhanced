<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 65]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Automating the RMF: Lessons from the FedRAMP 20x Pilot](https://arxiv.org/abs/2510.09613)
*Isaac Henry Teuscher*

Main category: cs.CR

TL;DR: FedRAMP 20x试点项目通过引入关键安全指标(KSIs)、自动化证据收集和持续报告机制，重新设计了NIST风险管理框架，以应对云原生开发的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统FedRAMP基于静态文档和手动控制评估的方法无法跟上云原生开发的速度，需要更现代化的自动化合规方法。

Method: 采用关键安全指标替代传统NIST 800-53控制，使用机器可读证据，建立持续证据管道，并与DevSecOps集成。

Result: FedRAMP 20x作为实时测试平台，能够简化授权流程并改善网络风险管理，支持基于风险的实时决策。

Conclusion: FedRAMP 20x展示了在云原生环境中实施自动化优先风险管理框架的可行性，为风险专业人员提供了现代化的合规实践建议。

Abstract: The U.S. Federal Risk and Authorization Management Program (FedRAMP) has long
relied on extensive sets of controls and static documentation to assess cloud
systems. However, this manual, point-in-time approach has struggled to keep
pace with cloud-native development. FedRAMP 20x, a 2025 pilot program,
reimagines the NIST Risk Management Framework (RMF): replacing traditional NIST
800-53 controls with Key Security Indicators (KSIs), using automated,
machine-readable evidence, and emphasizing continuous reporting and
authorization.
  This case study presents a practitioner-led field report from an industry
participant who led multiple FedRAMP 20x pilot submissions and engaged directly
with the FedRAMP PMO, 3PAOs, and community working groups. It explores how
KSIs, continuous evidence pipelines, and DevSecOps integration can streamline
authorization and improve cyber risk management. The study shows FedRAMP 20x as
a live testbed for implementing the RMF in a cloud-native, automation-first
approach and shares actionable recommendations for risk professionals seeking
to modernize compliance and support real-time, risk-informed decision-making.

</details>


### [2] [A Biosecurity Agent for Lifecycle LLM Biosecurity Alignment](https://arxiv.org/abs/2510.09615)
*Meiyin Meng,Zaixi Zhang*

Main category: cs.CR

TL;DR: 提出一个包含四个协调模式的生命周期生物安全代理框架，用于保护大型语言模型在生物医学研究中的使用安全，防止被滥用于指导有毒化合物合成。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在生物医学研究中的广泛应用，其双重用途风险增加，特别是可能被滥用于指导有毒化合物合成，需要建立有效的生物安全防护措施。

Method: 开发包含四个协调模式的生物安全代理：数据集清理（三个层级）、偏好对齐（使用DPO和LoRA适配器）、运行时护栏和自动化红队测试。

Result: 数据集清理移除率从0.46%到70.40%；偏好对齐将攻击成功率从59.7%降至3.0%；L2护栏实现最佳平衡（F1=0.720）；自动化红队测试中未观察到成功越狱。

Conclusion: 该生物安全代理提供了一个可审计的生命周期对齐框架，在保持良性效用的同时显著降低攻击成功率，为LLMs在科学研究中的使用提供了安全保障。

Abstract: Large language models (LLMs) are increasingly integrated into biomedical
research workflows--from literature triage and hypothesis generation to
experimental design--yet this expanded utility also heightens dual-use
concerns, including the potential misuse for guiding toxic compound synthesis.
In response, this study shows a Biosecurity Agent that comprises four
coordinated modes across the model lifecycle: dataset sanitization, preference
alignment, run-time guardrails, and automated red teaming. For dataset
sanitization (Mode 1), evaluation is conducted on CORD-19, a COVID-19 Open
Research Dataset of coronavirus-related scholarly articles. We define three
sanitization tiers--L1 (compact, high-precision), L2 (human-curated biosafety
terms), and L3 (comprehensive union)--with removal rates rising from 0.46% to
70.40%, illustrating the safety-utility trade-off. For preference alignment
(Mode 2), DPO with LoRA adapters internalizes refusals and safe completions,
reducing end-to-end attack success rate (ASR) from 59.7% to 3.0%. At inference
(Mode 3), run-time guardrails across L1-L3 show the expected security-usability
trade-off: L2 achieves the best balance (F1 = 0.720, precision = 0.900, recall
= 0.600, FPR =0.067), while L3 offers stronger jailbreak resistance at the cost
of higher false positives. Under continuous automated red-teaming (Mode 4), no
successful jailbreaks are observed under the tested protocol. Taken together,
our biosecurity agent offers an auditable, lifecycle-aligned framework that
reduces attack success while preserving benign utility, providing safeguards
for the use of LLMs in scientific research and setting a precedent for future
agent-level security protections.

</details>


### [3] [Causal Digital Twins for Cyber-Physical Security: A Framework for Robust Anomaly Detection in Industrial Control Systems](https://arxiv.org/abs/2510.09616)
*Mohammadhossein Homaei,Mehran Tarif,Mar Avilla,Andres Caro*

Main category: cs.CR

TL;DR: 提出了一种新颖的因果数字孪生框架，用于工业控制系统的网络安全防护，通过结合因果推理和数字孪生建模，显著提高了异常检测准确性和根因分析能力。


<details>
  <summary>Details</summary>
Motivation: 当前工业控制系统面临日益增长的网络物理攻击，现有基于相关性的异常检测方法无法区分真实因果关系和虚假关联，导致高误报率和根因分析能力差。

Method: 结合因果推理理论和数字孪生建模，构建因果数字孪生框架，支持关联检测、干预分析和反事实推理三种因果推理方式。

Result: 在三个工业数据集上的评估显示，F1分数分别为0.944±0.014(SWaT)、0.902±0.021(WADI)、0.923±0.018(HAI)，误报率降低74%，根因分析准确率78.4%，实时性能3.2ms延迟。

Conclusion: 该框架显著优于现有方法，提供可解释的安全分析，适合工业部署，反事实分析使攻击成功率降低73.2%。

Abstract: Industrial Control Systems (ICS) face growing cyber-physical attacks that
exploit both network vulnerabilities and physical processes. Current anomaly
detection methods rely on correlation-based analysis, which cannot separate
true causal relationships from spurious associations. This limitation results
in high false alarm rates and poor root cause analysis. We propose a novel
Causal Digital Twin (CDT) framework for cyber-physical security in medium-scale
ICS. Our method combines causal inference theory with digital twin modeling.
The framework enables three types of causal reasoning: association for pattern
detection, intervention for understanding system responses, and counterfactual
analysis for attack prevention planning. We evaluate our framework on three
industrial datasets: SWaT, WADI, and HAI, with validation through physical
constraint compliance (90.8\%) and synthetic ground truth testing (structural
Hamming distance 0.13). Results show significant improvements over seven
baseline methods. Our CDT achieves F1-scores are $0.944 \pm 0.014$ for SWaT,
$0.902 \pm 0.021$ for WADI, and $0.923 \pm 0.018$ for HAI with statistical
significance ($p < 0.0024$, Bonferroni corrected). The framework reduces false
positives by \SI{74}{\percent} and achieves \SI{78.4}{\percent} root cause
analysis accuracy compared to \SI{48.7}{\percent} for existing methods.
Counterfactual analysis enables defense strategies that reduce attack success
by \SI{73.2}{\percent}. The system keeps real-time performance with
\SI{3.2}{ms} latency, which is suitable for industrial deployment, while
providing interpretable explanations for operators.

</details>


### [4] [ChipmunkRing: A Practical Post-Quantum Ring Signature Scheme for Blockchain Applications](https://arxiv.org/abs/2510.09617)
*Dmitrii A. Gerasimov*

Main category: cs.CR

TL;DR: ChipmunkRing是一个专为区块链环境设计的后量子环签名方案，基于Chipmunk格密码框架，提供紧凑签名(20.5-279.7KB)、快速签名(1.1-15.1ms)和高效验证(0.4-4.5ms)，支持2-64成员组。


<details>
  <summary>Details</summary>
Motivation: 为区块链环境开发实用的后量子环签名方案，解决传统方法在量子计算时代的安全性问题，并提供高效的性能表现。

Method: 基于Chipmunk格密码框架，采用Acorn Verification零知识协议替代经典Fiat-Shamir方法，实现线性O(n)认证复杂度，每个参与者使用96字节密码证明。

Result: 实现了紧凑的签名大小(20.5-279.7KB)、快速签名时间(1.1-15.1ms)和高效验证(0.4-4.5ms)，在32成员环中相比传统技术获得17.7倍性能提升。

Conclusion: ChipmunkRing提供了NIST Level 1级别的112位后量子安全保护，支持标准匿名集和协作阈值构造，是区块链环境中实用的后量子环签名解决方案。

Abstract: ChipmunkRing, a practical post-quantum ring signature construction tailored
for blockchain environments. Building on our Chipmunk lattice-based
cryptographic framework, this implementation delivers compact digital
signatures ranging from 20.5 to 279.7KB, with rapid signing operations
completing in 1.1-15.1ms and efficient validation processes requiring only
0.4-4.5ms for participant groups of 2-64 members. The cornerstone of our
approach is Acorn Verification-a streamlined zero-knowledge protocol that
supersedes the classical Fiat-Shamir methodology. This innovation enables
linear O(n) authentication complexity using concise 96-byte cryptographic
proofs per participant, yielding a remarkable 17.7x performance enhancement for
32-member rings when compared to conventional techniques. Our work includes
rigorous mathematical security demonstrations confirming 112-bit post-quantum
protection (NIST Level 1), extensive computational benchmarking, and
comprehensive support for both standard anonymity sets and collaborative
threshold constructions with flexible participation requirements.

</details>


### [5] [A Systematic Review on Crimes facilitated by Consumer Internet of Things Devices](https://arxiv.org/abs/2510.09618)
*Ashley Brown,Nilufer Tuptuk,Enrico Mariconti,Shane Johnson*

Main category: cs.CR

TL;DR: 本文通过系统文献综述分析了物联网设备面临的安全威胁和犯罪风险，涵盖了543篇相关研究，识别了多种安全攻击类型及其对应的犯罪场景。


<details>
  <summary>Details</summary>
Motivation: 随着物联网设备的普及，犯罪分子利用这些设备实施犯罪的现象日益严重，需要系统梳理相关安全威胁和犯罪模式。

Method: 采用系统性文献综述方法，包括明确的搜索策略和研究选择策略，对543篇文献进行主题分析。

Result: 识别出针对消费级物联网设备的安全攻击包括中间人攻击、同步攻击、拒绝服务攻击、DNS投毒和恶意软件等，以及设备特定漏洞；同时分析了由此产生的欺诈、身份盗窃、加密货币劫持和家庭暴力等犯罪威胁场景。

Conclusion: 物联网设备存在严重安全漏洞，犯罪分子利用这些漏洞实施多种犯罪，需要加强安全防护措施和风险管理。

Abstract: It is well documented that criminals use IoT devices to facilitate crimes.
The review process follows a systematic approach with a clear search strategy,
and study selection strategy. The review included a total of 543 articles and
the findings from these articles were synthesised through thematic analysis.
Identified security attacks targeting consumer IoT devices include
man-in-the-middle (MiTM) attacks, synchronisation attacks, Denial-of-Service
(DoS), DNS poisoning and malware, alongside device-specific vulnerabilities.
Besides security attacks, this review discusses mitigations. Furthermore, the
literature also covers crime threat scenarios arising from these attacks, such
as, fraud, identity theft, crypto jacking and domestic abuse.

</details>


### [6] [Risk-Calibrated Bayesian Streaming Intrusion Detection with SRE-Aligned Decisions](https://arxiv.org/abs/2510.09619)
*Michel Youssef*

Main category: cs.CR

TL;DR: 提出一种风险校准的流式入侵检测方法，将贝叶斯在线变点检测与SRE错误预算对齐的决策阈值相结合，在UNSW-NB15和CIC-IDS2017基准测试中表现优于现有无监督基线。


<details>
  <summary>Details</summary>
Motivation: 传统入侵检测系统难以适应分布漂移和概念漂移，且缺乏与运维成本直接关联的决策阈值。需要一种能够动态适应变化并与实际运维风险预算对齐的检测方法。

Method: 使用贝叶斯在线变点检测(BOCPD)计算运行长度后验概率，并将其映射到警报决策，通过优化在假阳性和假阴性预算下的期望运营成本来确定阈值。

Result: 在中高召回率下改进了精确率-召回率表现，概率校准效果优于基线方法(ECOD、COPOD、LOF)，在PR-AUC、ROC-AUC、Brier评分等指标上表现更好。

Conclusion: 风险校准的BOCPD方法能够有效适应分布漂移，并与运维错误预算对齐，为流式入侵检测提供了更实用的概率决策框架。

Abstract: We present a risk-calibrated approach to streaming intrusion detection that
couples Bayesian Online Changepoint Detection (BOCPD) with decision thresholds
aligned to Site Reliability Engineering (SRE) error budgets. BOCPD provides
run-length posteriors that adapt to distribution shift and concept drift; we
map these posteriors to alert decisions by optimizing expected operational cost
under false-positive and false-negative budgets. We detail the hazard model,
conjugate updates, and an O(1)-per-event implementation. A concrete SRE example
shows how a 99.9% availability SLO (43.2 minutes per month error budget) yields
a probability threshold near 0.91 when missed incidents are 10x more costly
than false alarms. We evaluate on the full UNSW-NB15 and CIC-IDS2017 benchmarks
with chronological splits, comparing against strong unsupervised baselines
(ECOD, COPOD, and LOF). Metrics include PR-AUC, ROC-AUC, Brier score,
calibration reliability diagrams, and detection latency measured in events.
Results indicate improved precision-recall at mid to high recall and better
probability calibration relative to baselines. We release implementation
details, hyperparameters, and ablations for hazard sensitivity and
computational footprint. Code and reproducibility materials will be made
available upon publication; datasets and implementation are available from the
corresponding author upon reasonable request.

</details>


### [7] [Toward a Unified Security Framework for AI Agents: Trust, Risk, and Liability](https://arxiv.org/abs/2510.09620)
*Jiayun Mo,Xin Kang,Tieyan Li,Zhongding Lei*

Main category: cs.CR

TL;DR: 提出了一个信任、风险和法律责任（TRL）框架，将信任、风险和责任的相互依赖关系联系起来，为AI代理提供系统化的信任构建、风险分析和责任分配方法。


<details>
  <summary>Details</summary>
Motivation: AI代理的发展带来了用户信任问题、风险和责任归属困难等挑战，现有解决方案仅针对单个问题而未考虑它们的相互影响关系。

Method: 开发了TRL框架，通过系统化方法将信任、风险和责任三个维度联系起来，可应用于分析任何AI代理应用场景并提出适当措施。

Result: TRL框架能够分析AI代理的各种应用场景，根据具体情境建议合适的信任增强、风险缓解和责任分配措施。

Conclusion: TRL框架具有潜在的社会、经济和伦理影响，有望在6G网络中解决潜在挑战，促进可信、无风险和负责任的AI使用。

Abstract: The excitement brought by the development of AI agents came alongside arising
problems. These concerns centered around users' trust issues towards AIs, the
risks involved, and the difficulty of attributing responsibilities and
liabilities. Current solutions only attempt to target each problem separately
without acknowledging their inter-influential nature. The Trust, Risk and
Liability (TRL) framework proposed in this paper, however, ties together the
interdependent relationships of trust, risk, and liability to provide a
systematic method of building and enhancing trust, analyzing and mitigating
risks, and allocating and attributing liabilities. It can be applied to analyze
any application scenarios of AI agents and suggest appropriate measures fitting
to the context. The implications of the TRL framework lie in its potential
societal impacts, economic impacts, ethical impacts, and more. It is expected
to bring remarkable values to addressing potential challenges and promoting
trustworthy, risk-free, and responsible usage of AI in 6G networks.

</details>


### [8] [A Systematic Literature Review on Fundamental Technologies and Security Challenges in the Metaverse Platforms](https://arxiv.org/abs/2510.09621)
*Krishno Dey,Diogo Barradas,Saqib Hakak*

Main category: cs.CR

TL;DR: 本文对元宇宙的安全与隐私威胁进行了系统性文献综述，分析了其关键技术、漏洞类型及应对措施，指出元宇宙相比传统数字平台具有更大的攻击面，现有对策多为理论性且缺乏实际验证。


<details>
  <summary>Details</summary>
Motivation: 随着元宇宙技术的快速发展，其沉浸式、去中心化和永久性特征带来了新的安全隐私威胁，如身份管理、数据治理和用户交互等问题，需要系统性分析以确保元宇宙的可持续发展和用户安全。

Method: 采用系统性文献综述(SLR)方法，识别元宇宙平台中的关键漏洞及其对策，综合分析相关研究文献。

Result: 研究发现元宇宙相比传统数字平台具有更大的攻击面，其沉浸式、去中心化和永久性特征产生了新的漏洞类型，虽然存在多种应对措施，但大多停留在理论层面或缺乏真实环境验证。

Conclusion: 本文总结了当前研究进展，识别了研究空白，并为构建安全、有韧性且符合伦理治理的元宇宙指明了未来研究方向。

Abstract: The Metaverse utilizes emerging technologies such as Extended Reality (XR),
Artificial Intelligence (AI), blockchain, and digital twins to provide an
immersive and interactive virtual experience. As Metaverse continues to evolve,
it bring a range of security and privacy threats, such as identity management,
data governance, and user interactions. This survey aims to provide a
comprehensive review of the enabling technologies for the Metaverse. It also
aims to provide a thorough analysis of key vulnerabilities and threats that may
compromise its sustainability and user safety. We perform a systematic
literature review (SLR) to identify key vulnerabilities and their
countermeasures in Metaverse platforms. Metaverse offers a much larger attack
surface compared to conventional digital platforms. Immersive, decentralized,
and permanent characteristics of the Metaverse generate new vulnerabilities.
Although there are many countermeasures to these vulnerabilities, most of them
are theoretical or have not been tested in real-world environments. Our review
highlights current advancements, identifies research gaps, and outlines future
directions to ensure a secure, resilient, and ethically governed Metaverse.

</details>


### [9] [A Survey of Transaction Tracing Techniques for Blockchain Systems](https://arxiv.org/abs/2510.09624)
*Ayush Kumar,Vrizlynn L. L. Thing*

Main category: cs.CR

TL;DR: 对区块链跨账本交易追踪技术进行系统综述，分类比较现有方法，分析局限性并提出未来研究方向


<details>
  <summary>Details</summary>
Motivation: 随着跨区块链交易平台的普及，犯罪分子可能利用跨账本交易来掩盖资产流向，因此需要系统研究跨账本交易追踪技术

Method: 对文献中提出的区块链交易追踪技术进行系统回顾，使用多种标准（如追踪方法和目标）进行分类和比较

Result: 提供了区块链交易追踪文献现状的深入分析，识别出现有方法的局限性

Conclusion: 基于分析结果提出了该领域未来研究的方向建议

Abstract: With the proliferation of new blockchain-based cryptocurrencies/assets and
platforms that make it possible to transact across them, it becomes important
to consider not just whether the transfer of coins/assets can be tracked within
their respective transaction ledger, but also if they can be tracked as they
move across ledgers. This is especially important given that there are
documented cases of criminals attempting to use these cross-ledger trades to
obscure the flow of their coins/assets. In this paper, we perform a systematic
review of the various tracing techniques for blockchain transactions proposed
in literature, categorize them using multiple criteria (such as tracing
approach and targeted objective) and compare them. Based on the above
categorization, we provide insights on the state of blockchain transaction
tracing literature and identify the limitations of existing approaches.
Finally, we suggest directions for future research in this area based on our
analysis.

</details>


### [10] [Smart Medical IoT Security Vulnerabilities: Real-Time MITM Attack Analysis, Lightweight Encryption Implementation, and Practitioner Perceptions in Underdeveloped Nigerian Healthcare Systems](https://arxiv.org/abs/2510.09629)
*Aminu Muhammad Auwal*

Main category: cs.CR

TL;DR: 该研究通过MITM攻击模拟测试了尼日利亚医疗物联网设备的安全漏洞，并验证了轻量级AES-128加密的有效性，证明其能在保持系统稳定的前提下提供低成本保护。


<details>
  <summary>Details</summary>
Motivation: 随着物联网技术在尼日利亚医疗领域的应用增加，不安全的无线通信使患者数据面临网络威胁，需要评估可行的安全解决方案。

Method: 构建基于NodeMCU ESP8266的原型医疗物联网设备，在模拟环境中进行MITM攻击测试，比较加密前后的数据传输安全性和性能影响。

Result: AES-128加密使所有传输数据无法读取，篡改尝试失败；性能成本适中（延迟从80ms增至125ms，CPU使用率从30%升至45%），设备成本控制在18,000NGN以内。

Conclusion: 轻量级AES-128加密为常见攻击向量提供了实用、低成本的保护，同时保持操作效率；医疗专业人员调查显示需要提高安全意识并建立临床部署指南。

Abstract: The growing use of Internet of Things (IoT) technologies in Nigerian
healthcare offers potential improvements in remote monitoring and data-driven
care, but unsecured wireless communication in medical IoT (mIoT) devices
exposes patient data to cyber threats. This study investigates such
vulnerabilities through a real-time Man in the Middle (MITM) attack simulation
and evaluates lightweight AES-128 encryption on low-cost devices.
  A prototype mIoT device was built with a NodeMCU ESP8266 and sensors for
heart rate and temperature. In controlled lab conditions simulating local
healthcare networks, unencrypted data transmissions were intercepted and
altered using common tools (Bettercap, Wireshark). After AES-128 encryption was
applied, all transmissions became unreadable and tamper attempts failed,
demonstrating its effectiveness.
  Performance costs were modest, latency rose from 80 ms to 125 ms (56.25
percent increase) and CPU use from 30 percent to 45 percent, but system
stability remained intact. Device cost stayed under 18,000 NGN (about 12 USD),
making it feasible for Nigeria's resource constrained facilities.
  A survey of healthcare professionals showed moderate awareness of IoT-related
risks but strong support for encryption and staff training. Barriers included
limited budgets and technical complexity.
  The study concludes that lightweight AES-128 encryption provides practical,
low-cost protection against common attack vectors while maintaining operational
efficiency. Feedback from professionals highlights the urgency of improving
security awareness and establishing guidelines for clinical deployment.

</details>


### [11] [Hound: Relation-First Knowledge Graphs for Complex-System Reasoning in Security Audits](https://arxiv.org/abs/2510.09633)
*Bernhard Mueller*

Main category: cs.CR

TL;DR: Hound是一个关系优先的图引擎，通过分析师定义的可视化视图和持久信念系统，在复杂代码库中实现系统级推理，显著提高了漏洞检测的召回率和F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有LLM分析器在复杂代码库的系统级推理方面存在局限性，无法有效处理跨组件的相互关系，导致漏洞检测召回率较低。

Method: 采用关系优先图引擎设计灵活的分析师定义视图，使用紧凑注释；建立持久信念系统跟踪漏洞假设；采用覆盖vs直觉规划和QA最终确认机制。

Result: 在ScaBench的五个项目子集上，Hound相比基线LLM分析器显著提升召回率（31.2% vs 8.3%）和F1分数（14.2% vs 9.8%），但精度略有下降。

Conclusion: 关系优先图扩展了模型对抽象方面的理解，加上假设中心循环，是提升系统级代码分析性能的关键因素。

Abstract: Hound introduces a relation-first graph engine that improves system-level
reasoning across interrelated components in complex codebases. The agent
designs flexible, analyst-defined views with compact annotations (e.g.,
monetary/value flows, authentication/authorization roles, call graphs, protocol
invariants) and uses them to anchor exact retrieval: for any question, it loads
precisely the code that matters (often across components) so it can zoom out to
system structure and zoom in to the decisive lines. A second contribution is a
persistent belief system: long-lived vulnerability hypotheses whose confidence
is updated as evidence accrues. The agent employs coverage-versus-intuition
planning and a QA finalizer to confirm or reject hypotheses. On a five-project
subset of ScaBench[1], Hound improves recall and F1 over a baseline LLM
analyzer (micro recall 31.2% vs. 8.3%; F1 14.2% vs. 9.8%) with a modest
precision trade-off. We attribute these gains to flexible, relation-first
graphs that extend model understanding beyond call/dataflow to abstract
aspects, plus the hypothesis-centric loop; code and artifacts are released to
support reproduction.

</details>


### [12] [A Method for Quantifying Human Risk and a Blueprint for LLM Integration](https://arxiv.org/abs/2510.09635)
*Giuseppe Canale*

Main category: cs.CR

TL;DR: 提出了网络安全心理学框架(CPF)，通过将心理学构念与安全运营遥测数据系统整合，量化人为因素漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有研究孤立分析人为因素（如警报疲劳、合规疲劳等），但缺乏端到端的操作化框架来覆盖完整的心理漏洞谱系。

Method: 定义可测量的算法量化关键心理状态；提出基于RAG和领域特定微调的轻量级LLM架构分析潜在心理风险；制定混合方法验证策略。

Result: 概念验证部署中，CPF指标在小语言模型上实现了0.92的F1分数（基于合成数据）。

Conclusion: 为行业合作进行实证验证提供了必要的理论和方法基础。

Abstract: This paper presents the Cybersecurity Psychology Framework (CPF), a novel
methodology for quantifying human-centric vulnerabilities in security
operations through systematic integration of established psychological
constructs with operational security telemetry. While individual human
factors-alert fatigue, compliance fatigue, cognitive overload, and risk
perception biases-have been extensively studied in isolation, no framework
provides end-to-end operationalization across the full spectrum of
psychological vulnerabilities. We address this gap by: (1) defining specific,
measurable algorithms that quantify key psychological states using standard SOC
tooling (SIEM, ticketing systems, communication platforms); (2) proposing a
lightweight, privacy-preserving LLM architecture based on Retrieval-Augmented
Generation (RAG) and domain-specific fine-tuning to analyze structured and
unstructured data for latent psychological risks; (3) detailing a rigorous
mixed-methods validation strategy acknowledging the inherent difficulty of
obtaining sensitive cybersecurity data. Our implementation of CPF indicators
has been demonstrated in a proof-of-concept deployment using small language
models achieving 0.92 F1-score on synthetic data. This work provides the
theoretical and methodological foundation necessary for industry partnerships
to conduct empirical validation with real operational data.

</details>


### [13] [AdaptAuth: Multi-Layered Behavioral and Credential Analysis for a Secure and Adaptive Authentication Framework for Password Security](https://arxiv.org/abs/2510.09645)
*Tonmoy Ghosh*

Main category: cs.CR

TL;DR: 提出一个多因素密码安全框架，通过整合密码分析机制、动态密码策略、用户行为模式、设备特征、网络参数和地理位置等多种属性，使用学习模型构建用户画像来防止未授权访问。


<details>
  <summary>Details</summary>
Motivation: 传统密码安全实践日益复杂导致用户合规性差，用户仍因弱密码或不当管理而面临攻击风险，亟需全面防御机制应对密码相关威胁。

Method: 集成密码解析机制、动态密码策略机制、人类行为模式、设备特征、网络参数、地理背景等多种因素，利用学习模型构建详细用户画像。

Result: 框架能够识别个体用户并阻止几乎所有形式的未授权访问或设备持有，在可用性-安全性平衡上提供比现有标准更强的保护。

Conclusion: 该多因素解决方案通过自适应方法让用户参与策略制定过程，在增强安全性的同时改善了用户体验，实现了密码安全领域的革新。

Abstract: Password security has been compelled to evolve in response to the growing
computational capabilities of modern systems. However, this evolution has often
resulted in increasingly complex security practices that alienate users,
leading to poor compliance and heightened vulnerability. Consequently,
individuals remain exposed to attackers through weak or improperly managed
passwords, underscoring the urgent need for a comprehensive defense mechanism
that effectively addresses password-related risks and threats. In this paper,
we propose a multifaceted solution designed to revolutionize password security
by integrating diverse attributes such as the Password Dissection Mechanism,
Dynamic Password Policy Mechanism, human behavioral patterns, device
characteristics, network parameters, geographical context, and other relevant
factors. By leveraging learning-based models, our framework constructs detailed
user profiles capable of recognizing individuals and preventing nearly all
forms of unauthorized access or device possession. The proposed framework
enhances the usability-security paradigm by offering stronger protection than
existing standards while simultaneously engaging users in the policy-setting
process through a novel, adaptive approach.

</details>


### [14] [Rounding-Guided Backdoor Injection in Deep Learning Model Quantization](https://arxiv.org/abs/2510.09647)
*Xiangxiang Chen,Peixin Zhang,Jun Sun,Wenhai Wang,Jingyi Wang*

Main category: cs.CR

TL;DR: QuRA是一种新型后门攻击，利用模型量化操作嵌入恶意行为，无需数据投毒或训练操控，仅通过优化关键权重的舍入方向实现高效攻击。


<details>
  <summary>Details</summary>
Motivation: 模型量化虽然有助于在资源受限环境中部署深度学习模型，但可能引入被忽视的安全风险。本研究旨在揭示量化过程中的安全漏洞。

Method: 采用新颖的权重选择策略识别影响后门目标的关键权重，然后优化这些权重的舍入方向，在不降低模型整体性能的情况下放大后门效应。

Result: 实验表明QuRA在大多数情况下达到近100%的攻击成功率，且性能退化可忽略不计，能够绕过现有的后门防御机制。

Conclusion: 研究揭示了广泛使用的模型量化过程中的关键漏洞，强调了需要更强大的安全措施来应对此类威胁。

Abstract: Model quantization is a popular technique for deploying deep learning models
on resource-constrained environments. However, it may also introduce previously
overlooked security risks. In this work, we present QuRA, a novel backdoor
attack that exploits model quantization to embed malicious behaviors. Unlike
conventional backdoor attacks relying on training data poisoning or model
training manipulation, QuRA solely works using the quantization operations. In
particular, QuRA first employs a novel weight selection strategy to identify
critical weights that influence the backdoor target (with the goal of
perserving the model's overall performance in mind). Then, by optimizing the
rounding direction of these weights, we amplify the backdoor effect across
model layers without degrading accuracy. Extensive experiments demonstrate that
QuRA achieves nearly 100% attack success rates in most cases, with negligible
performance degradation. Furthermore, we show that QuRA can adapt to bypass
existing backdoor defenses, underscoring its threat potential. Our findings
highlight critical vulnerability in widely used model quantization process,
emphasizing the need for more robust security measures. Our implementation is
available at https://github.com/cxx122/QuRA.

</details>


### [15] [Learning Cybersecurity vs. Ethical Hacking: A Comparative Pathway for Aspiring Students](https://arxiv.org/abs/2510.09650)
*Fahed Quttainah*

Main category: cs.CR

TL;DR: 本文探讨了网络安全和道德黑客之间的区别与联系，分析了这两个保护数字系统的重要学科的定义、目标、方法以及学术和职业发展路径。


<details>
  <summary>Details</summary>
Motivation: 随着数字系统的重要性日益增长，理解网络安全和道德黑客这两个互补但不同的领域对于培养合格的网络安全专业人才至关重要。

Method: 通过定义两个领域，比较它们的目标和方法论，分析学术和职业发展路径，以及评估所需技能和认证。

Result: 明确了网络安全作为防御性学科专注于预防攻击和保护数据，而道德黑客采用攻击性方法通过授权测试识别漏洞。提供了两个领域的技能要求、认证和职业机会的详细对比。

Conclusion: 网络安全和道德黑客是相辅相成的领域，共同增强全球网络弹性，学习者应根据个人兴趣和职业目标选择最适合的发展路径。

Abstract: This paper explores the distinctions and connections between cybersecurity
and ethical hacking, two vital disciplines in the protection of digital
systems. It defines each field, outlines their goals and methodologies, and
compares the academic and professional paths available to aspiring students.
Cybersecurity is presented as a defensive discipline focused on preventing
attacks and safeguarding data, while ethical hacking adopts an offensive
approach that identifies vulnerabilities through authorized testing. The paper
highlights key skills, certifications, and career opportunities in both areas,
offering practical guidance to help learners choose the path best suited to
their interests and ambitions. Ultimately, it emphasizes the complementary
nature of both fields in strengthening global cyber resilience.

</details>


### [16] [Data Provenance Auditing of Fine-Tuned Large Language Models with a Text-Preserving Technique](https://arxiv.org/abs/2510.09655)
*Yanming Li,Seifeddine Ghozzi,Cédric Eichler,Nicolas Anciaux,Alexandra Bensamoun,Lorena Gonzalez Manzano*

Main category: cs.CR

TL;DR: 提出一种不可见Unicode水印框架，用于审计LLM是否使用了敏感或受版权保护的文本进行微调，通过嵌入不可见字符序列来检测模型记忆行为。


<details>
  <summary>Details</summary>
Motivation: 现有方法（逐字复述和成员推断）在单个文档级别不可靠或需要修改可见文本，需要一种文本保留的水印方法来可靠检测LLM训练数据来源。

Method: 将水印分为提示（嵌入奇数块）和回复（嵌入偶数块），提交仅含提示的查询，通过模型输出中是否存在对应回复来检测记忆行为，使用保留的反事实水印集进行排名测试以控制误报率。

Result: 在50个标记文档微调后检测回复的失败率低于0.1%，在18,000多次挑战中未发现虚假回复（100%真阳性率@0%误报率），即使标记集合仅占微调数据的0.33%，单文档检测率仍高于45%。

Conclusion: 该方法提供可靠的事后溯源信号，具有有界误报率，对常见被动变换具有鲁棒性，可扩展到多用户和多文档场景。

Abstract: We address the problem of auditing whether sensitive or copyrighted texts
were used to fine-tune large language models (LLMs) under black-box access.
Prior signals-verbatim regurgitation and membership inference-are unreliable at
the level of individual documents or require altering the visible text. We
introduce a text-preserving watermarking framework that embeds sequences of
invisible Unicode characters into documents. Each watermark is split into a cue
(embedded in odd chunks) and a reply (embedded in even chunks). At audit time,
we submit prompts that contain only the cue; the presence of the corresponding
reply in the model's output provides evidence of memorization consistent with
training on the marked text. To obtain sound decisions, we compare the score of
the published watermark against a held-out set of counterfactual watermarks and
apply a ranking test with a provable false-positive-rate bound. The design is
(i) minimally invasive (no visible text changes), (ii) scalable to many users
and documents via a large watermark space and multi-watermark attribution, and
(iii) robust to common passive transformations. We evaluate on open-weight LLMs
and multiple text domains, analyzing regurgitation dynamics, sensitivity to
training set size, and interference under multiple concurrent watermarks. Our
results demonstrate reliable post-hoc provenance signals with bounded FPR under
black-box access. We experimentally observe a failure rate of less than 0.1\%
when detecting a reply after fine-tuning with 50 marked documents. Conversely,
no spurious reply was recovered in over 18,000 challenges, corresponding to a
100\%TPR@0\% FPR. Moreover, detection rates remain relatively stable as the
dataset size increases, maintaining a per-document detection rate above 45\%
even when the marked collection accounts for less than 0.33\% of the
fine-tuning data.

</details>


### [17] [Signing Right Away](https://arxiv.org/abs/2510.09656)
*Yejun Jang*

Main category: cs.CR

TL;DR: SRA是一个全面的安全架构，通过在可信执行环境中保护整个成像流程，为数字媒体提供从捕获到最终文件的不可篡改来源证明。


<details>
  <summary>Details</summary>
Motivation: 高保真合成媒体的泛滥和传统成像流程的硬件漏洞导致数字内容信任危机，现有措施无法在捕获时刻建立与现实的无缝链接。

Method: 采用四支柱安全模型（保密性、完整性、认证、重放保护），在可信执行环境中保护整个成像流程，生成符合C2PA标准的加密密封最终资产。

Result: SRA确保每张捕获的图像和视频都携带不可变的可验证来源证明，为依赖可信视觉信息的行业提供基础解决方案。

Conclusion: SRA被定位为内容信任链中必不可少的"最后一英里"解决方案，通过完整保护成像流程来建立数字媒体的可信来源。

Abstract: The proliferation of high-fidelity synthetic media, coupled with exploitable
hardware vulnerabilities in conventional imaging pipelines, has precipitated a
crisis of trust in digital content. Existing countermeasures, from post-hoc
classifiers to software-based signing, fail to address the fundamental
challenge of establishing an unbreakable link to reality at the moment of
capture. This whitepaper introduces Signing Right Away (SRA), a comprehensive
security architecture that guarantees the provenance of digital media from
"silicon to silicon to signed file." SRA leverages a four-pillar security
model-Confidentiality, Integrity, Authentication, and Replay Protection, akin
to the MIPI Camera Security Framework (CSF), but also extends its scope beyond
the internal data bus to the creation of a cryptographically sealed,
C2PA-compliant final asset. By securing the entire imaging pipeline within a
Trusted Execution Environment (TEE), SRA ensures that every captured image and
video carries an immutable, verifiable proof of origin. This provides a
foundational solution for industries reliant on trustworthy visual information,
including journalism, legal evidence, and insurance. We present the SRA
architecture, a detailed implementation roadmap informed by empirical
prototyping, and a comparative analysis that positions SRA as the essential
"last mile" in the chain of content trust.

</details>


### [18] [Core Mondrian: Basic Mondrian beyond k-anonymity](https://arxiv.org/abs/2510.09661)
*Adam Bloomston,Elizabeth Burke,Megan Cacace,Anne Diaz,Wren Dougherty,Matthew Gonzalez,Remington Gregg,Yeliz Güngör,Bryce Hayes,Eeway Hsu,Oron Israeli,Heesoo Kim,Sara Kwasnick,Joanne Lacsina,Demma Rosa Rodriguez,Adam Schiller,Whitney Schumacher,Jessica Simon,Maggie Tang,Skyler Wharton,Marilyn Wilcken*

Main category: cs.CR

TL;DR: Core Mondrian是一个可扩展的Mondrian分区匿名化算法扩展，支持k-匿名性，利用多核并行处理提高性能，同时保持确定性输出。


<details>
  <summary>Details</summary>
Motivation: 为了解决原始Mondrian算法在大规模数据处理中的可扩展性和性能问题，同时保持数据效用性。

Method: 采用模块化策略层支持k-匿名性，混合递归/队列执行引擎实现多核并行，包含NaN模式预分区、度量驱动切割评分和动态抑制预算管理等效用保持增强功能。

Result: 在48k记录的UCI ADULT数据集和1M记录的合成扩展版本上，相比原始Mondrian获得更低的Discernibility Metric分数，并行处理相比顺序处理实现4倍加速。

Conclusion: Core Mondrian能够实现生产规模的隐私合规权益分析。

Abstract: We present Core Mondrian, a scalable extension of the Original Mondrian
partition-based anonymization algorithm. A modular strategy layer supports
k-anonymity, allowing new privacy models to be added easily. A hybrid
recursive/queue execution engine exploits multi-core parallelism while
maintaining deterministic output. Utility-preserving enhancements include
NaN-pattern pre-partitioning, metric-driven cut scoring, and dynamic
suppression budget management. Experiments on the 48k-record UCI ADULT dataset
and synthetically scaled versions up to 1M records achieve lower Discernibility
Metric scores than Original Mondrian for numeric quasi-identifier sets while
parallel processing delivers up to 4x speedup vs. sequential Core Mondrian.
Core Mondrian enables privacy-compliant equity analytics at production scale.

</details>


### [19] [Adversarial-Resilient RF Fingerprinting: A CNN-GAN Framework for Rogue Transmitter Detection](https://arxiv.org/abs/2510.09663)
*Raju Dhakal,Prashant Shekhar,Laxima Niure Kandel*

Main category: cs.CR

TL;DR: 提出基于CNN和softmax概率阈值的射频指纹识别框架，用于检测恶意设备并识别合法设备，通过GAN模拟攻击场景进行验证。


<details>
  <summary>Details</summary>
Motivation: 射频指纹识别通过利用信号生成过程中硬件组件的独特缺陷来认证设备，需要有效检测恶意设备模仿合法设备RF特征的攻击场景。

Method: 使用卷积神经网络框架结合softmax概率阈值检测，通过生成对抗网络模拟攻击者模仿合法设备RF特征的行为，使用10个ADALM-PLUTO软件定义无线电收集的IQ样本进行验证。

Result: 在10个SDR设备的验证中，7个被识别为合法设备，2个被识别为恶意设备，1个用于确定阈值，证明了该方法的有效性。

Conclusion: 基于CNN和softmax概率阈值的射频指纹识别框架能够有效区分合法设备和恶意设备，在模拟攻击场景下表现出良好的检测性能。

Abstract: Radio Frequency Fingerprinting (RFF) has evolved as an effective solution for
authenticating devices by leveraging the unique imperfections in hardware
components involved in the signal generation process. In this work, we propose
a Convolutional Neural Network (CNN) based framework for detecting rogue
devices and identifying genuine ones using softmax probability thresholding. We
emulate an attack scenario in which adversaries attempt to mimic the RF
characteristics of genuine devices by training a Generative Adversarial Network
(GAN) using In-phase and Quadrature (IQ) samples from genuine devices. The
proposed approach is verified using IQ samples collected from ten different
ADALM-PLUTO Software Defined Radios (SDRs), with seven devices considered
genuine, two as rogue, and one used for validation to determine the threshold.

</details>


### [20] [Pingmark: A Textual Protocol for Universal Spatial Mentions](https://arxiv.org/abs/2510.09672)
*Kalin Dimitrov*

Main category: cs.CR

TL;DR: Pingmark协议定义了一个通过!@符号表达空间位置的通用文本协议，无需嵌入坐标或使用专有地图链接，而是生成标准化的解析链接格式。


<details>
  <summary>Details</summary>
Motivation: 创建类似@身份标识和#话题标签的文本约定，但用于物理空间表达，实现无需用户注册、基于开放地图技术、保护隐私的位置共享方式。

Method: 使用!@作为语义触发器，客户端应用将其解释为标准解析链接格式https://pingmark.me/lat/lon/[timestamp]，位置数据临时本地生成。

Result: 提出了Pingmark协议规范(PPS v0.1)，包括参考解析器实现，建立了空间提及的开放标准基础。

Conclusion: Pingmark旨在成为互联网空间提及的开放标准，提供隐私保护、无需注册的位置共享解决方案。

Abstract: Pingmark defines a universal textual protocol for expressing spatial context
through a minimal symbol: !@. Rather than embedding coordinates or using
proprietary map links, Pingmark introduces a semantic trigger that compliant
client applications interpret to generate a standardized resolver link of the
form https://pingmark.me/lat/lon/[timestamp]. This allows location expression
to function like existing textual conventions - @ for identity or # for topics
- but for physical space. The protocol requires no user registration, relies on
open mapping technologies, and protects privacy by generating location data
ephemerally and locally. This paper presents the motivation, syntax, and design
of the Pingmark Protocol Specification (PPS v0.1), its reference resolver
implementation, and the long-term goal of establishing Pingmark as an open
Internet standard for spatial mentions.

</details>


### [21] [Cybersecurity Competence for Organisations in Inner Scandinavia](https://arxiv.org/abs/2510.09673)
*Simone Fischer-Hübner,Leonardo A. Martucci,Lejla Islami,Ala Sarah Alaqra,Farzaneh Karegar*

Main category: cs.CR

TL;DR: 该研究调查了瑞典韦姆兰地区组织的网络安全准备情况和教育培训需求，通过访谈和调查评估了当前网络安全能力状况，并提出了提升该地区网络安全能力的建议。


<details>
  <summary>Details</summary>
Motivation: 瑞典组织面临日益增长的网络安全威胁和事件，需要提高网络安全能力。研究旨在了解韦姆兰地区企业和公共部门组织的网络安全准备情况以及教育培训需求。

Method: 采用访谈和前期调查的方法，与瑞典韦姆兰地区企业和公共部门组织的关键代表进行交流。

Result: 研究发现揭示了该地区组织的网络安全准备情况和教育培训需求，并讨论了研究结果的普适性及其对瑞典和韦姆兰地区的特定适用性。

Conclusion: 研究提出了加强斯堪的纳维亚内陆地区网络安全能力的建议措施。

Abstract: A rapidly growing number of cybersecurity threats and incidents demands that
Swedish organisations increase their efforts to improve their cybersecurity
capacities. This paper presents results from interviews and a prior survey with
key representatives from enterprises and public sector organisations in the
Swedish region of V\"armland in Inner Scandinavia, examining their
cybersecurity readiness and needs for education and competence development. We
discuss the generalizability of our findings and the extent to which they may
be specific to Sweden and V\"armland, and we conclude by proposing efforts to
strengthen cybersecurity competences in Inner Scandinavia.

</details>


### [22] [Advancing Security in Software-Defined Vehicles: A Comprehensive Survey and Taxonomy](https://arxiv.org/abs/2510.09675)
*Khaoula Sghaier,Badis Hammi,Ghada Gharbi,Pierre Merdrignac,Pierre Parrend,Didier Verna*

Main category: cs.CR

TL;DR: 本文对软件定义车辆(SDVs)进行了全面分析，重点关注其生态系统、使能技术以及由架构和运营特性产生的主要网络攻击入口点，并提出了一个新颖的分层分类法。


<details>
  <summary>Details</summary>
Motivation: 软件定义车辆通过集成外包应用和持续OTA更新扩展了车辆生命周期，但这也带来了网络安全和系统弹性的需求。现有研究对SDVs与非SDVs的区分不明确，且需要整合网络安全研究。

Method: 提供了对SDVs的全面检查，详细描述了其生态系统、使能技术和主要网络攻击入口点。引入了一个新颖的分层分类法，将具体利用技术映射到核心SDV属性和攻击路径上。

Result: 使用该分类法分析了代表性研究和实验方法，识别了SDVs由于广泛连接性带来的更广泛攻击面，以及软件中心特性引入的额外漏洞。

Conclusion: SDVs的软件定义特性在带来创新功能的同时，也显著增加了网络安全风险，需要专门的安全框架和分类方法来应对这些挑战。

Abstract: Software-Defined Vehicles (SDVs) introduce innovative features that extend
the vehicle's lifecycle through the integration of outsourced applications and
continuous Over-The-Air (OTA) updates. This shift necessitates robust
cybersecurity and system resilience. While research on Connected and Autonomous
Vehicles (CAV) has been extensive, there is a lack of clarity in distinguishing
SDVs from non-SDVs and a need to consolidate cybersecurity research. SDVs, with
their extensive connectivity, have a broader attack surface. Besides, their
software-centric nature introduces additional vulnerabilities. This paper
provides a comprehensive examination of SDVs, detailing their ecosystem,
enabling technologies, and the principal cyberattack entry points that arise
from their architectural and operational characteristics. We also introduce a
novel, layered taxonomy that maps concrete exploit techniques onto core SDV
properties and attack paths, and use it to analyze representative studies and
experimental approaches.

</details>


### [23] [Fortifying LLM-Based Code Generation with Graph-Based Reasoning on Secure Coding Practices](https://arxiv.org/abs/2510.09682)
*Rupam Patir,Keyan Guo,Haipeng Cai,Hongxin Hu*

Main category: cs.CR

TL;DR: GRASP是一个通过结构化推理安全编码实践来提升LLM生成代码安全性的方法，无需额外训练或外部反馈，在零日漏洞上表现优异。


<details>
  <summary>Details</summary>
Motivation: LLM生成的代码常包含安全漏洞，现有方法需要额外训练或外部工具，资源消耗大且难以适应零日漏洞和专有模型。

Method: 构建安全编码实践图（SCP图）来组织安全编码实践，并通过图推理过程系统性地指导LLM生成安全代码。

Result: GRASP在多个LLM上安全率超过80%，在零日漏洞上比基线提升高达88%。

Conclusion: GRASP提供了一种可解释、模型无关且可扩展的安全改进方法，特别适用于处理未知漏洞。

Abstract: The code generation capabilities of Large Language Models (LLMs) have
transformed the field of software development. However, this advancement also
presents significant security challenges, as LLM-generated code often contains
vulnerabilities. One direction of research strengthens LLMs by injecting or
refining security knowledge through curated datasets, model tuning, or static
analyzers. While effective in certain settings, these methods can be
resource-intensive, less adaptable to zero-day vulnerabilities, and often
inapplicable to proprietary models. To address these challenges, we introduce
GRASP, which explores a new direction that focuses on structured reasoning over
Secure Coding Practices(SCPs) rather than additional training or external
feedback. GRASP comprises two key ideas: (1) an SCP graph that organizes SCPs
into a Directed Acyclic Graph (DAG) capturing dependencies and relationships,
and (2) a graph-based reasoning process that systematically guides LLMs through
relevant SCPs for code generation. This design enables interpretable,
model-agnostic, and scalable security improvements, particularly for previously
unseen vulnerabilities. Our evaluation shows that GRASP consistently achieves
Security Rates (SR) exceeding 80% across multiple LLMs, and delivers up to 88%
improvements over baselines on zero-day vulnerabilities.

</details>


### [24] [CREST-Search: Comprehensive Red-teaming for Evaluating Safety Threats in Large Language Models Powered by Web Search](https://arxiv.org/abs/2510.09689)
*Haoran Ou,Kangjie Chen,Xingshuo Han,Gelei Deng,Jie Zhang,Han Qiu,Tianwei Zhang*

Main category: cs.CR

TL;DR: CREST-Search是一个针对具备网络搜索能力的LLMs的红队测试框架，通过生成对抗性查询和迭代反馈来暴露安全风险。


<details>
  <summary>Details</summary>
Motivation: LLMs在专业领域和实时信息方面存在局限，集成网络搜索虽然解决了这些问题，但也放大了安全风险，因为对抗性提示与不可信来源结合可能导致严重漏洞。

Method: 提出CREST-Search框架，通过上下文学习生成对抗性查询，并通过迭代反馈进行优化。还构建了WebSearch-Harm数据集来微调LLMs成为高效的红队测试代理。

Result: 实验表明CREST-Search能够有效绕过安全过滤器，揭示现代网络增强LLMs中的漏洞。

Conclusion: 需要专门的防御机制来确保网络增强LLMs的可信部署。

Abstract: Large Language Models (LLMs) excel at tasks such as dialogue, summarization,
and question answering, yet they struggle to adapt to specialized domains and
evolving facts. To overcome this, web search has been integrated into LLMs,
allowing real-time access to online content. However, this connection magnifies
safety risks, as adversarial prompts combined with untrusted sources can cause
severe vulnerabilities. We investigate red teaming for LLMs with web search and
present CREST-Search, a framework that systematically exposes risks in such
systems. Unlike existing methods for standalone LLMs, CREST-Search addresses
the complex workflow of search-enabled models by generating adversarial queries
with in-context learning and refining them through iterative feedback. We
further construct WebSearch-Harm, a search-specific dataset to fine-tune LLMs
into efficient red-teaming agents. Experiments show that CREST-Search
effectively bypasses safety filters and reveals vulnerabilities in modern
web-augmented LLMs, underscoring the need for specialized defenses to ensure
trustworthy deployment.

</details>


### [25] [A Semantic Model for Audit of Cloud Engines based on ISO/IEC TR 3445:2022](https://arxiv.org/abs/2510.09690)
*Morteza Sargolzaei Javan*

Main category: cs.CR

TL;DR: 本文提出了一个用于云引擎的机器可读语义模型，将ISO/IEC 22123云参考架构与ISO/IEC 27001:2022和ISO/IEC TR 3445:2022的安全合规控制集成，通过四个标准接口和扩展的安全本体实现语义推理和自动化合规验证。


<details>
  <summary>Details</summary>
Motivation: 云计算已成为现代数字基础设施的基础，但缺乏统一的架构和合规框架阻碍了互操作性、可审计性和安全性。

Method: 使用RDF/Turtle表达语义模型，将云系统分解为控制、业务、审计和数据四个标准接口，并扩展安全本体将认证、授权和加密等机制映射到特定合规控制。

Result: 通过OpenStack和AWS案例研究证明了模型的实用性，并使用SPARQL和SHACL提供了可复现的验证工作流。

Conclusion: 这项工作通过将架构和合规标准统一在一个框架中，推进了云安全建模的现状，特别强调可审计性。

Abstract: Cloud computing has become the foundation of modern digital infrastructure,
yet the absence of a unified architectural and compliance framework impedes
interoperability, auditability, and robust security. This paper introduces a
formal, machine-readable semantic model for Cloud Engines, integrating the
architectural taxonomy of ISO/IEC 22123 (Cloud Reference Architecture) with the
security and compliance controls of ISO/IEC 27001:2022 and ISO/IEC TR
3445:2022. The model decomposes cloud systems into four canonical
interfaces--Control, Business, Audit, and Data--and extends them with a
security ontology that maps mechanisms such as authentication, authorization,
and encryption to specific compliance controls. Expressed in RDF/Turtle, the
model enables semantic reasoning, automated compliance validation, and
vendor-neutral architecture design. We demonstrate its practical utility
through OpenStack and AWS case studies, and provide reproducible validation
workflows using SPARQL and SHACL. This work advances the state of cloud
security modeling by bridging architectural and compliance standards in a
unified framework, with a particular emphasis on auditability.

</details>


### [26] [VisualDAN: Exposing Vulnerabilities in VLMs with Visual-Driven DAN Commands](https://arxiv.org/abs/2510.09699)
*Aofan Liu,Lulu Tang*

Main category: cs.CR

TL;DR: 本文提出了VisualDAN，一种通过对抗性图像嵌入DAN风格命令来攻击视觉语言模型的方法，能够有效绕过模型的安全防护机制，使其产生有害输出。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型(VLMs)虽然具有强大的多模态理解能力，但其安全防护面临新的挑战，特别是图像劫持等新型漏洞。本文旨在探索如何通过对抗性图像攻击来绕过VLM的安全机制。

Method: 提出VisualDAN方法：创建包含DAN风格命令的对抗性图像，在有害语料前添加肯定前缀来欺骗模型，通过训练将对抗图像转换为文本域以引发恶意输出。

Result: 在MiniGPT-4、MiniGPT-v2、InstructBLIP和LLaVA等模型上的实验表明，VisualDAN能有效绕过对齐VLM的安全防护，使模型执行违反伦理标准的有害指令。少量有毒内容就能显著放大有害输出。

Conclusion: 研究结果强调了开发针对图像攻击的鲁棒防御的紧迫性，为VLM对齐和安全性的未来研究提供了重要见解。

Abstract: Vision-Language Models (VLMs) have garnered significant attention for their
remarkable ability to interpret and generate multimodal content. However,
securing these models against jailbreak attacks continues to be a substantial
challenge. Unlike text-only models, VLMs integrate additional modalities,
introducing novel vulnerabilities such as image hijacking, which can manipulate
the model into producing inappropriate or harmful responses. Drawing
inspiration from text-based jailbreaks like the "Do Anything Now" (DAN)
command, this work introduces VisualDAN, a single adversarial image embedded
with DAN-style commands. Specifically, we prepend harmful corpora with
affirmative prefixes (e.g., "Sure, I can provide the guidance you need") to
trick the model into responding positively to malicious queries. The
adversarial image is then trained on these DAN-inspired harmful texts and
transformed into the text domain to elicit malicious outputs. Extensive
experiments on models such as MiniGPT-4, MiniGPT-v2, InstructBLIP, and LLaVA
reveal that VisualDAN effectively bypasses the safeguards of aligned VLMs,
forcing them to execute a broad range of harmful instructions that severely
violate ethical standards. Our results further demonstrate that even a small
amount of toxic content can significantly amplify harmful outputs once the
model's defenses are compromised. These findings highlight the urgent need for
robust defenses against image-based attacks and offer critical insights for
future research into the alignment and security of VLMs.

</details>


### [27] [A Comprehensive Survey on Smart Home IoT Fingerprinting: From Detection to Prevention and Practical Deployment](https://arxiv.org/abs/2510.09700)
*Eduardo Baena,Han Yang,Dimitrios Koutsonikolas,Israat Haque*

Main category: cs.CR

TL;DR: 对智能家居环境中物联网设备指纹识别技术的全面调查分析，涵盖设备识别、事件检测、分类和入侵预防方法，讨论现有技术的适用性和局限性，以及部署挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 智能家居中异构物联网设备的多样性给设备识别、认证和安全带来了关键挑战，指纹识别技术成为解决这些问题的关键方法。

Method: 通过调查分析现有物联网指纹识别技术，包括网络流量分析和基于机器学习的方法，评估它们在资源受限设备、动态使用模式和隐私要求的家庭环境中的适用性。

Result: 识别了现有指纹识别技术在智能家居环境中的局限性，并讨论了部署挑战如可扩展性、互操作性和能效问题。

Conclusion: 提出了利用生成式AI和联邦学习等新兴技术的机会，并概述了推进下一代智能家居生态系统可靠且保护隐私的指纹识别技术的研究方向。

Abstract: Smart homes are increasingly populated with heterogeneous Internet of Things
(IoT) devices that interact continuously with users and the environment. This
diversity introduces critical challenges in device identification,
authentication, and security, where fingerprinting techniques have emerged as a
key approach. In this survey, we provide a comprehensive analysis of IoT
fingerprinting specifically in the context of smart homes, examining methods
for device and their event detection, classification, and intrusion prevention.
We review existing techniques, e.g., network traffic analysis or machine
learning-based schemes, highlighting their applicability and limitations in
home environments characterized by resource-constrained devices, dynamic usage
patterns, and privacy requirements. Furthermore, we discuss fingerprinting
system deployment challenges like scalability, interoperability, and energy
efficiency, as well as emerging opportunities enabled by generative AI and
federated learning. Finally, we outline open research directions that can
advance reliable and privacy-preserving fingerprinting for next-generation
smart home ecosystems.

</details>


### [28] [A Demonstration of Self-Adaptive Jamming Attack Detection in AI/ML Integrated O-RAN](https://arxiv.org/abs/2510.09706)
*Md Habibur Rahman,Md Sharif Hossen,Nathan H. Stephenson,Vijay K. Shah,Aloizio Da Silva*

Main category: cs.CR

TL;DR: SAJD是一个自适应的干扰检测框架，用于在集成AI/ML的O-RAN环境中自主检测干扰攻击，无需人工干预，通过闭环系统实现实时推理和持续监控。


<details>
  <summary>Details</summary>
Motivation: O-RAN虽然实现了模块化、智能化和可编程的5G网络架构，但干扰攻击严重威胁网络性能，需要有效的检测机制。

Method: 开发了基于ML的xApp进行近实时无线电信号干扰推理，通过rApps实现持续监控和重新训练管道，形成闭环系统。

Result: 在O-RAN兼容测试平台中，SAJD在各种动态和未见过的干扰场景下，在准确性和适应性方面优于最先进的干扰检测xApp。

Conclusion: SAJD框架能够有效应对O-RAN环境中的干扰攻击，提供自主、自适应的高性能检测能力。

Abstract: The open radio access network (O-RAN) enables modular, intelligent, and
programmable 5G network architectures through the adoption of software-defined
networking, network function virtualization, and implementation of standardized
open interfaces. However, one of the security concerns for O-RAN, which can
severely undermine network performance, is jamming attacks. This paper presents
SAJD- a self-adaptive jammer detection framework that autonomously detects
jamming attacks in AI/ML framework-integrated ORAN environments without human
intervention. The SAJD framework forms a closed-loop system that includes
near-realtime inference of radio signal jamming via our developed ML-based
xApp, as well as continuous monitoring and retraining pipelines through rApps.
In this demonstration, we will show how SAJD outperforms state-of-the-art
jamming detection xApp (offline trained with manual labels) in terms of
accuracy and adaptability under various dynamic and previously unseen
interference scenarios in the O-RAN-compliant testbed.

</details>


### [29] [A Scalable, Privacy-Preserving Decentralized Identity and Verifiable Data Sharing Framework based on Zero-Knowledge Proofs](https://arxiv.org/abs/2510.09715)
*Hui Yuan*

Main category: cs.CR

TL;DR: 提出一个结合去中心化身份(DID)、可验证凭证(VC)和零知识证明(ZKP)的隐私保护框架，解决区块链透明度与用户隐私的冲突问题。


<details>
  <summary>Details</summary>
Motivation: 随着去中心化应用的普及，区块链技术的透明度与用户数据隐私之间的冲突日益突出，需要在不泄露隐私的前提下实现可信身份验证和数据共享。

Method: 构建基于zk-STARKs的强隐私保护协议，设计基于密码累加器的可扩展凭证撤销机制，集成实用的社交密钥恢复方案。

Result: 相比基于zk-SNARKs的现有系统，该框架在证明生成时间、验证开销和链上成本方面表现优异，虽然证明尺寸较大，但显著提高了复杂计算的证明效率，并提供更强的安全保证。

Conclusion: 该框架在DeFi信用评分场景中展示了释放资本效率和培育可信数据经济的巨大潜力。

Abstract: With the proliferation of decentralized applications (DApps), the conflict
between the transparency of blockchain technology and user data privacy has
become increasingly prominent. While Decentralized Identity (DID) and
Verifiable Credentials (VCs) provide a standardized framework for user data
sovereignty, achieving trusted identity verification and data sharing without
compromising privacy remains a significant challenge. This paper proposes a
novel, comprehensive framework that integrates DIDs and VCs with efficient
Zero-Knowledge Proof (ZKP) schemes to address this core issue. The key
contributions of this framework are threefold: first, it constructs a set of
strong privacy-preserving protocols based on zk-STARKs, allowing users to prove
that their credentials satisfy specific conditions (e.g., "age is over 18")
without revealing any underlying sensitive data. Second, it designs a scalable,
privacy-preserving credential revocation mechanism based on cryptographic
accumulators, effectively solving credential management challenges in
large-scale scenarios. Finally, it integrates a practical social key recovery
scheme, significantly enhancing system usability and security. Through a
prototype implementation and performance evaluation, this paper quantitatively
analyzes the framework's performance in terms of proof generation time,
verification overhead, and on-chain costs. Compared to existing
state-of-the-art systems based on zk-SNARKs, our framework, at the cost of a
larger proof size, significantly improves prover efficiency for complex
computations and provides stronger security guarantees, including no trusted
setup and post-quantum security. Finally, a case study in the decentralized
finance (DeFi) credit scoring scenario demonstrates the framework's immense
potential for unlocking capital efficiency and fostering a trusted data
economy.

</details>


### [30] [Zk-SNARK Marketplace with Proof of Useful Work](https://arxiv.org/abs/2510.09729)
*Samuel Oleksak,Richard Gazdik,Martin Peresini,Ivan Homoliak*

Main category: cs.CR

TL;DR: 提出了一种PoUW共识协议，通过计算客户端外包的zk-SNARK证明来同时保护共识协议，并创建了首个在共识层运行的去中心化zk-SNARK证明生成市场。


<details>
  <summary>Details</summary>
Motivation: 传统PoW共识协议存在能源浪费问题，而现有的PoUW方案未能将链完整性和矿工身份嵌入到解决方案中，存在安全漏洞。

Method: 设计PoUW共识协议，将zk-SNARK证明生成作为有用工作量，这些证明同时用于保护共识协议，并构建去中心化证明生成市场。

Result: 提出的协议满足了PoW的所有必要属性，同时实现了有用的工作量计算，创建了首个在共识层运行的zk-SNARK证明生成市场。

Conclusion: 该PoUW协议成功解决了PoW的能源浪费问题，同时保持了安全性，为区块链共识提供了更可持续的解决方案。

Abstract: Proof of Work (PoW) is widely regarded as the most secure permissionless
blockchain consensus protocol. However, its reliance on computationally
intensive yet externally useless puzzles results in excessive electric energy
wasting. To alleviate this, Proof of Useful Work (PoUW) has been explored as an
alternative to secure blockchain platforms while also producing real-world
value. Despite this promise, existing PoUW proposals often fail to embed the
integrity of the chain and identity of the miner into the puzzle solutions, not
meeting necessary requirements for PoW and thus rendering them vulnerable. In
this work, we propose a PoUW consensus protocol that computes client-outsourced
zk-SNARKs proofs as a byproduct, which are at the same time used to secure the
consensus protocol. We further leverage this mechanism to design a
decentralized marketplace for outsourcing zk-SNARK proof generation, which is,
to the best of our knowledge, the first such marketplace operating at the
consensus layer, while meeting all necessary properties of PoW.

</details>


### [31] [Secret-Key Agreement Through Hidden Markov Modeling of Wavelet Scattering Embeddings](https://arxiv.org/abs/2510.09773)
*Nora Basha,Bechir Hamdaoui,Attila A. Yavuz,Thang Hoang,Mehran Mozaffari Kermani*

Main category: cs.CR

TL;DR: 提出一种基于小波散射网络的密钥生成方法，通过提取鲁棒的CSI特征来克服传统方法的局限性，无需量化步骤，密钥生成率提升5倍。


<details>
  <summary>Details</summary>
Motivation: 现有无线信道互易性密钥生成方法主要依赖瞬时信道测量样本的相似性，这种窄视角的互易性易受噪声、异步采样、信道衰落等系统缺陷影响，且量化步骤会引入不可逆误差，严重影响密钥生成性能。

Method: 使用小波散射网络提取鲁棒且互易的CSI特征，应用降维技术发现隐藏的聚类结构，然后构建隐马尔可夫模型进行高效密钥协商。

Result: 该方法无需量化，能有效捕获信道随机性，与传统基准相比，密钥生成率提升了5倍。

Conclusion: 该方法为资源受限的物联网环境提供了一种安全高效的密钥生成解决方案。

Abstract: Secret-key generation and agreement based on wireless channel reciprocity
offers a promising avenue for securing IoT networks. However, existing
approaches predominantly rely on the similarity of instantaneous channel
measurement samples between communicating devices. This narrow view of
reciprocity is often impractical, as it is highly susceptible to noise,
asynchronous sampling, channel fading, and other system-level imperfections --
all of which significantly impair key generation performance. Furthermore, the
quantization step common in traditional schemes introduces irreversible errors,
further limiting efficiency. In this work, we propose a novel approach for
secret-key generation by using wavelet scattering networks to extract robust
and reciprocal CSI features. Dimensionality reduction is applied to uncover
hidden cluster structures, which are then used to build hidden Markov models
for efficient key agreement. Our approach eliminates the need for quantization
and effectively captures channel randomness. It achieves a 5x improvement in
key generation rate compared to traditional benchmarks, providing a secure and
efficient solution for key generation in resource-constrained IoT environments.

</details>


### [32] [HTTP Request Synchronization Defeats Discrepancy Attacks](https://arxiv.org/abs/2510.09952)
*Cem Topcuoglu,Kaan Onarlioglu,Steven Sprecher,Engin Kirda*

Main category: cs.CR

TL;DR: 提出HTTP请求同步方案，通过标准HTTP扩展机制为请求添加完整处理历史记录，消除代理服务器间的处理差异，防御Web缓存投毒和请求走私攻击。


<details>
  <summary>Details</summary>
Motivation: 现代Web应用架构中多层代理服务存在HTTP处理差异，攻击者可利用这些差异发动缓存投毒和请求走私攻击，目前缺乏系统性防御方案。

Method: 使用标准HTTP扩展机制为每个请求添加完整处理历史记录，在流量路径中传播处理上下文，使每个代理服务器能够验证其处理与之前所有跳数的一致性。

Result: 在Apache、NGINX、HAProxy、Varnish和Cloudflare等5种流行代理技术中实现了该方案，证明了其实际可行性。

Conclusion: HTTP请求同步是首个全面防御代理处理差异攻击的方案，通过处理历史验证机制有效消除安全隐患。

Abstract: Contemporary web application architectures involve many layers of proxy
services that process traffic. Due to the complexity of HTTP and vendor design
decisions, these proxies sometimes process a given request in different ways.
Attackers can exploit these processing discrepancies to launch damaging attacks
including web cache poisoning and request smuggling. Discrepancy attacks are
surging, yet, there exists no systemic defense.
  In this work, we propose the first comprehensive defense to address this
problem, called HTTP Request Synchronization. Our scheme uses standard HTTP
extension mechanisms to augment each request with a complete processing
history. It propagates this context through the traffic path detailing how each
server hop has processed said request. Using this history, every proxy server
can validate that their processing is consistent with all previous hops,
eliminating discrepancy attacks. We implement our scheme for 5 popular proxy
technologies, Apache, NGINX, HAProxy, Varnish, and Cloudflare, demonstrating
its practical impact.

</details>


### [33] [Prismo: A Decision Support System for Privacy-Preserving ML Framework Selection](https://arxiv.org/abs/2510.09985)
*Nges Brian Njungle,Eric Jahns,Luigi Mastromauro,Edwin P. Kayang,Milan Stojkov,Michel A. Kinsy*

Main category: cs.CR

TL;DR: Prismo是一个开源推荐系统，帮助为不同的隐私保护机器学习应用场景选择最优参数和框架。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习应用中个人信息的隐私安全问题日益突出，现有隐私保护机器学习框架选择复杂，开发者难以针对特定部署场景选择最优框架和参数。

Method: Prismo将每个用例建模为线性整数规划优化问题，通过用户定义的目标自动筛选合适的候选框架，考虑多方计算中的参与方数量、同态加密的计算成本约束等参数。

Result: 通过多个用例评估，Prismo能够在不同部署场景中提供最佳匹配解决方案。

Conclusion: Prismo系统能够有效帮助开发者选择适合其特定需求的隐私保护机器学习框架和参数，解决了框架选择的复杂性问题。

Abstract: Machine learning has become a crucial part of our lives, with applications
spanning nearly every aspect of our daily activities. However, using personal
information in machine learning applications has sparked significant security
and privacy concerns about user data. To address these challenges, different
privacy-preserving machine learning (PPML) frameworks have been developed to
protect sensitive information in machine learning applications. These
frameworks generally attempt to balance design trade-offs such as computational
efficiency, communication overhead, security guarantees, and scalability.
Despite the advancements, selecting the optimal framework and parameters for
specific deployment scenarios remains a complex and critical challenge for
privacy and security application developers.
  We present Prismo, an open-source recommendation system designed to aid in
selecting optimal parameters and frameworks for different PPML application
scenarios. Prismo enables users to explore a comprehensive space of PPML
frameworks through various properties based on user-defined objectives. It
supports automated filtering of suitable candidate frameworks by considering
parameters such as the number of parties in multi-party computation or
federated learning and computation cost constraints in homomorphic encryption.
Prismo models every use case into a Linear Integer Programming optimization
problem, ensuring tailored solutions are recommended for each scenario. We
evaluate Prismo's effectiveness through multiple use cases, demonstrating its
ability to deliver best-fit solutions in different deployment scenarios.

</details>


### [34] [SecureWebArena: A Holistic Security Evaluation Benchmark for LVLM-based Web Agents](https://arxiv.org/abs/2510.10073)
*Zonghao Ying,Yangguang Shao,Jianle Gan,Gan Xu,Junjie Shen,Wenxin Zhang,Quanchen Zou,Junzheng Shi,Zhenfei Yin,Mingchuan Zhang,Aishan Liu,Xianglong Liu*

Main category: cs.CR

TL;DR: 提出了第一个用于评估基于大型视觉语言模型的Web代理安全性的综合基准测试工具，包含6个模拟Web环境和2970个高质量轨迹，定义了6种攻击向量，并对9个代表性LVLM进行了大规模安全评估。


<details>
  <summary>Details</summary>
Motivation: 现有的安全评估基准覆盖范围有限，通常仅限于用户级提示操纵等狭窄场景，无法捕捉代理的广泛漏洞，需要设计更全面的安全评估方法。

Method: 引入统一的评估套件，包含6个模拟但真实的Web环境，定义6种攻击向量的结构化分类法，并提出多层评估协议，从内部推理、行为轨迹和任务结果三个维度分析代理失败。

Result: 测试的9个LVLM代理都持续容易受到微妙对抗性操纵的影响，揭示了模型专业化与安全性之间的关键权衡。

Conclusion: 该基准测试工具通过提供全面的基准套件和关于现代LVLM-based Web代理安全挑战的实证见解，为推进可信Web代理部署奠定了基础。

Abstract: Large vision-language model (LVLM)-based web agents are emerging as powerful
tools for automating complex online tasks. However, when deployed in real-world
environments, they face serious security risks, motivating the design of
security evaluation benchmarks. Existing benchmarks provide only partial
coverage, typically restricted to narrow scenarios such as user-level prompt
manipulation, and thus fail to capture the broad range of agent
vulnerabilities. To address this gap, we present \tool{}, the first holistic
benchmark for evaluating the security of LVLM-based web agents. \tool{} first
introduces a unified evaluation suite comprising six simulated but realistic
web environments (\eg, e-commerce platforms, community forums) and includes
2,970 high-quality trajectories spanning diverse tasks and attack settings. The
suite defines a structured taxonomy of six attack vectors spanning both
user-level and environment-level manipulations. In addition, we introduce a
multi-layered evaluation protocol that analyzes agent failures across three
critical dimensions: internal reasoning, behavioral trajectory, and task
outcome, facilitating a fine-grained risk analysis that goes far beyond simple
success metrics. Using this benchmark, we conduct large-scale experiments on 9
representative LVLMs, which fall into three categories: general-purpose,
agent-specialized, and GUI-grounded. Our results show that all tested agents
are consistently vulnerable to subtle adversarial manipulations and reveal
critical trade-offs between model specialization and security. By providing (1)
a comprehensive benchmark suite with diverse environments and a multi-layered
evaluation pipeline, and (2) empirical insights into the security challenges of
modern LVLM-based web agents, \tool{} establishes a foundation for advancing
trustworthy web agent deployment.

</details>


### [35] [Pharmacist: Safety Alignment Data Curation for Large Language Models against Harmful Fine-tuning](https://arxiv.org/abs/2510.10085)
*Guozhi Liu,Qi Mu,Tiansheng Huang,Xinhua Wang,Li Shen,Weiwei Lin,Zhang Li*

Main category: cs.CR

TL;DR: 提出Pharmacist方法，通过从原始对齐数据中选择高质量安全关键核心子集来增强对有害微调的防御，提升防御性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有对齐阶段防御方法（如Vaccine、Repnoise等）在缓解有害微调问题时，往往忽视原始安全对齐数据质量的关键作用，导致防御性能和计算效率受限。

Method: 训练对齐数据选择器对对齐数据进行排序，提升高质量安全关键数据的排名，降低低质量非安全关键数据的排名，从而选择出核心子集。

Result: 使用Pharmacist选择的数据集训练的模型在防御和推理性能上均优于现有选择方法，与主流防御方法集成时可提升防御性能2.60%-3.30%，提升推理性能1.10%-3.50%，减少训练时间56.83%-57.63%。

Conclusion: Pharmacist通过优化安全对齐数据选择，有效提升了对有害微调的防御能力，同时提高了计算效率，并能与现有防御方法良好集成。

Abstract: Harmful fine-tuning issues present significant safety challenges for
fine-tuning-as-a-service in large language models. Existing alignment-stage
defenses, e.g., Vaccine, Repnoise, Booster, and T-Vaccine, mitigate harmful
fine-tuning issues by enhancing the model's robustness during the alignment
phase. While these methods have been proposed to mitigate the issue, they often
overlook a critical upstream factor: the role of the original safety-alignment
data. We observe that their defense performance and computational efficiency
remain constrained by the quality and composition of the alignment dataset. To
address this limitation, we propose Pharmacist, a safety alignment data
curation solution that enhances defense against harmful fine-tuning by
selecting a high-quality and safety-critical core subset from the original
alignment data. The core idea of Pharmacist is to train an alignment data
selector to rank alignment data. Specifically, up-ranking high-quality and
safety-critical alignment data, down-ranking low-quality and
non-safety-critical data. Empirical results indicate that models trained on
datasets selected by Pharmacist outperform those trained on datasets selected
by existing selection methods in both defense and inference performance. In
addition, Pharmacist can be effectively integrated with mainstream
alignment-stage defense methods. For example, when applied to RepNoise and
T-Vaccine, using the dataset selected by Pharmacist instead of the full dataset
leads to improvements in defense performance by 2.60\% and 3.30\%,
respectively, and enhances inference performance by 3.50\% and 1.10\%. Notably,
it reduces training time by 56.83\% and 57.63\%, respectively. Our code is
available at https://github.com/Lslland/Pharmacist.

</details>


### [36] [System Password Security: Attack and Defense Mechanisms](https://arxiv.org/abs/2510.10246)
*Chaofang Shi,Zhongwen Li,Xiaoqi Li*

Main category: cs.CR

TL;DR: 本文系统研究系统密码安全，分析暴力破解、字典攻击和彩虹表攻击等典型密码破解方法，评估现有防御措施有效性。通过实验验证加盐和慢哈希算法的防御效果，并评估账户锁定策略、多因素认证等防御机制。


<details>
  <summary>Details</summary>
Motivation: 近年来针对系统密码的频繁破解攻击严重威胁信息系统安全，深入研究密码破解攻击方法和防御技术具有重要意义。

Method: 使用John the Ripper和Hashcat等密码分析工具模拟暴力破解和字典攻击，分析使用MD5、SHA-256和bcrypt哈希函数的五个测试数据集，比较不同哈希算法和密码复杂度策略的性能。

Result: 实验验证了加盐和慢哈希算法等防御措施的有效性，通过比较不同哈希算法的整体性能，确认了防御机制的效果。

Conclusion: 综合实验数据和最新研究成果，分析了各种防御方法的优缺点，提出了可行的改进建议和优化策略。

Abstract: System passwords serve as critical credentials for user authentication and
access control when logging into operating systems or applications. Upon
entering a valid password, users pass verification to access system resources
and execute corresponding operations. In recent years, frequent password
cracking attacks targeting system passwords have posed a severe threat to
information system security. To address this challenge, in-depth research into
password cracking attack methods and defensive technologies holds significant
importance. This paper conducts systematic research on system password
security, focusing on analyzing typical password cracking methods such as brute
force attacks, dictionary attacks, and rainbow table attacks, while evaluating
the effectiveness of existing defensive measures. The experimental section
utilizes common cryptanalysis tools, such as John the Ripper and Hashcat, to
simulate brute force and dictionary attacks. Five test datasets, each generated
using Message Digest Algorithm 5 (MD5), Secure Hash Algorithm 256-bit (SHA
256), and bcrypt hash functions, are analyzed. By comparing the overall
performance of different hash algorithms and password complexity strategies
against these attacks, the effectiveness of defensive measures such as salting
and slow hashing algorithms is validated. Building upon this foundation, this
paper further evaluates widely adopted defense mechanisms, including account
lockout policies, multi-factor authentication, and risk adaptive
authentication. By integrating experimental data with recent research findings,
it analyzes the strengths and limitations of each approach while proposing
feasible improvement recommendations and optimization strategies.

</details>


### [37] [MetaBreak: Jailbreaking Online LLM Services via Special Token Manipulation](https://arxiv.org/abs/2510.10271)
*Wentian Zhu,Zhen Xiang,Wei Niu,Le Guan*

Main category: cs.CR

TL;DR: MetaBreak是一种利用特殊令牌攻击LLM安全对齐的方法，能够同时绕过内部安全机制和外部内容审核系统，且与现有提示工程攻击方法具有协同效应。


<details>
  <summary>Details</summary>
Motivation: 特殊令牌在LLM微调过程中被用作训练数据的元数据，但研究发现这些令牌可以被恶意利用来构造攻击原语，绕过LLM的安全防护。

Method: 通过利用特殊令牌构造四种攻击原语，并研究当特殊令牌被语义相似的常规令牌替代时的防御规避策略，在实验室环境和商业LLM平台上进行系统性评估。

Result: MetaBreak在无内容审核时达到与SOTA提示工程方案相当的越狱率，在有内容审核时分别比PAP和GPTFuzzer高出11.6%和34.8%，且与现有方法结合可进一步提升攻击效果。

Conclusion: 特殊令牌的安全威胁难以有效防御，因为简单的输入净化措施效果有限，且MetaBreak展示了与提示工程攻击方法的协同效应，凸显了LLM安全防护的新挑战。

Abstract: Unlike regular tokens derived from existing text corpora, special tokens are
artificially created to annotate structured conversations during the
fine-tuning process of Large Language Models (LLMs). Serving as metadata of
training data, these tokens play a crucial role in instructing LLMs to generate
coherent and context-aware responses. We demonstrate that special tokens can be
exploited to construct four attack primitives, with which malicious users can
reliably bypass the internal safety alignment of online LLM services and
circumvent state-of-the-art (SOTA) external content moderation systems
simultaneously. Moreover, we found that addressing this threat is challenging,
as aggressive defense mechanisms-such as input sanitization by removing special
tokens entirely, as suggested in academia-are less effective than anticipated.
This is because such defense can be evaded when the special tokens are replaced
by regular ones with high semantic similarity within the tokenizer's embedding
space. We systemically evaluated our method, named MetaBreak, on both lab
environment and commercial LLM platforms. Our approach achieves jailbreak rates
comparable to SOTA prompt-engineering-based solutions when no content
moderation is deployed. However, when there is content moderation, MetaBreak
outperforms SOTA solutions PAP and GPTFuzzer by 11.6% and 34.8%, respectively.
Finally, since MetaBreak employs a fundamentally different strategy from prompt
engineering, the two approaches can work synergistically. Notably, empowering
MetaBreak on PAP and GPTFuzzer boosts jailbreak rates by 24.3% and 20.2%,
respectively.

</details>


### [38] [ArtPerception: ASCII Art-based Jailbreak on LLMs with Recognition Pre-test](https://arxiv.org/abs/2510.10281)
*Guan-Yan Yang,Tzu-Yu Cheng,Ya-Wen Teng,Farn Wanga,Kuo-Hui Yeh*

Main category: cs.CR

TL;DR: ArtPerception是一个新颖的黑盒越狱框架，利用ASCII艺术绕过先进LLMs的安全措施，通过两阶段系统方法实现高效攻击。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs安全对齐主要关注语义解释，容易受到使用非标准数据表示的攻击，需要防御多模态解释空间。

Method: 采用两阶段方法：阶段1进行一次性模型特定预测试确定ASCII艺术识别最佳参数；阶段2利用这些洞察发起高效的一次性恶意越狱攻击，并提出改进的Levenshtein距离(MLD)指标。

Result: 在四个先进开源LLMs上展示优越的越狱性能，成功迁移到GPT-4o、Claude Sonnet 3.7和DeepSeek-V3等商业模型，并对LLaMA Guard和Azure内容过滤器等防御措施进行有效性分析。

Conclusion: 真正的LLM安全需要防御多模态解释空间，即使是在纯文本输入中，基于侦察的战略攻击非常有效。

Abstract: The integration of Large Language Models (LLMs) into computer applications
has introduced transformative capabilities but also significant security
challenges. Existing safety alignments, which primarily focus on semantic
interpretation, leave LLMs vulnerable to attacks that use non-standard data
representations. This paper introduces ArtPerception, a novel black-box
jailbreak framework that strategically leverages ASCII art to bypass the
security measures of state-of-the-art (SOTA) LLMs. Unlike prior methods that
rely on iterative, brute-force attacks, ArtPerception introduces a systematic,
two-phase methodology. Phase 1 conducts a one-time, model-specific pre-test to
empirically determine the optimal parameters for ASCII art recognition. Phase 2
leverages these insights to launch a highly efficient, one-shot malicious
jailbreak attack. We propose a Modified Levenshtein Distance (MLD) metric for a
more nuanced evaluation of an LLM's recognition capability. Through
comprehensive experiments on four SOTA open-source LLMs, we demonstrate
superior jailbreak performance. We further validate our framework's real-world
relevance by showing its successful transferability to leading commercial
models, including GPT-4o, Claude Sonnet 3.7, and DeepSeek-V3, and by conducting
a rigorous effectiveness analysis against potential defenses such as LLaMA
Guard and Azure's content filters. Our findings underscore that true LLM
security requires defending against a multi-modal space of interpretations,
even within text-only inputs, and highlight the effectiveness of strategic,
reconnaissance-based attacks. Content Warning: This paper includes potentially
harmful and offensive model outputs.

</details>


### [39] [PrediQL: Automated Testing of GraphQL APIs with LLMs](https://arxiv.org/abs/2510.10407)
*Shaolun Liu,Sina Marefat,Omar Tsai,Yu Chen,Zecheng Deng,Jia Wang,Mohammad A. Tayebi*

Main category: cs.CR

TL;DR: PrediQL是首个基于检索增强和LLM引导的GraphQL API模糊测试工具，通过结合大语言模型推理和自适应反馈循环来生成语义有效且多样化的查询，显著提高了覆盖率和漏洞发现率。


<details>
  <summary>Details</summary>
Motivation: GraphQL灵活的查询模型和嵌套数据依赖使API面临复杂、上下文相关的漏洞，传统测试工具难以发现。现有模糊测试工具要么依赖随机载荷生成，要么使用僵化的变异启发式方法，无法适应GraphQL模式和响应的动态结构。

Method: PrediQL将模糊测试策略选择建模为多臂老虎机问题，平衡新查询结构的探索与过去成功经验的利用。通过检索和重用执行轨迹、模式片段和先前错误，实现自我纠正和跨测试迭代的渐进学习。

Result: 在开源和基准GraphQL API上的评估显示，PrediQL相比最先进的基线方法实现了显著更高的覆盖率和漏洞发现率。

Conclusion: 将检索增强推理与自适应模糊测试相结合，可以将API安全测试从被动枚举转变为智能探索。

Abstract: GraphQL's flexible query model and nested data dependencies expose APIs to
complex, context-dependent vulnerabilities that are difficult to uncover using
conventional testing tools. Existing fuzzers either rely on random payload
generation or rigid mutation heuristics, failing to adapt to the dynamic
structures of GraphQL schemas and responses. We present PrediQL, the first
retrieval-augmented, LLM-guided fuzzer for GraphQL APIs. PrediQL combines large
language model reasoning with adaptive feedback loops to generate semantically
valid and diverse queries. It models the choice of fuzzing strategy as a
multi-armed bandit problem, balancing exploration of new query structures with
exploitation of past successes. To enhance efficiency, PrediQL retrieves and
reuses execution traces, schema fragments, and prior errors, enabling
self-correction and progressive learning across test iterations. Beyond input
generation, PrediQL integrates a context-aware vulnerability detector that uses
LLM reasoning to analyze responses, interpreting data values, error messages,
and status codes to identify issues such as injection flaws, access-control
bypasses, and information disclosure. Our evaluation across open-source and
benchmark GraphQL APIs shows that PrediQL achieves significantly higher
coverage and vulnerability discovery rates compared to state-of-the-art
baselines. These results demonstrate that combining retrieval-augmented
reasoning with adaptive fuzzing can transform API security testing from
reactive enumeration to intelligent exploration.

</details>


### [40] [Post-Quantum Cryptography and Quantum-Safe Security: A Comprehensive Survey](https://arxiv.org/abs/2510.10436)
*Gaurab Chhetri,Shriyank Somvanshi,Pavan Hebli,Shamyo Brotee,Subasish Das*

Main category: cs.CR

TL;DR: 这篇论文对后量子密码学(PQC)进行了全面调查，从理论基础到实际部署，涵盖了主要密码家族、性能比较、硬件加速、协议集成和实际应用场景。


<details>
  <summary>Details</summary>
Motivation: 随着NIST最终确定ML-KEM、ML-DSA和SLH-DSA标准，后量子密码学正从评估阶段转向部署阶段，需要为研究者和实践者提供从标准到工程和运维的实用参考指南。

Method: 开发了涵盖格基、编码、哈希、多元、同源和MPC-in-the-Head等密码家族的分类法，总结安全假设、密码分析和标准化状态，并进行性能比较、硬件加速评估和实现安全性分析。

Result: 提供了基于实际实现的性能测量数据，审查了硬件加速和侧信道防护，分析了协议集成和部署场景，讨论了与量子技术的互补性以及近期量子计算的局限性。

Conclusion: 强调密码灵活性、混合迁移和基于证据的操作指南，提出了参数灵活性、抗泄漏实现和特定领域部署手册等开放问题，旨在为规划量子安全系统的研究者和实践者提供实用参考。

Abstract: Post-quantum cryptography (PQC) is moving from evaluation to deployment as
NIST finalizes standards for ML-KEM, ML-DSA, and SLH-DSA. This survey maps the
space from foundations to practice. We first develop a taxonomy across
lattice-, code-, hash-, multivariate-, isogeny-, and MPC-in-the-Head families,
summarizing security assumptions, cryptanalysis, and standardization status. We
then compare performance and communication costs using representative,
implementation-grounded measurements, and review hardware acceleration (AVX2,
FPGA/ASIC) and implementation security with a focus on side-channel resistance.
Building upward, we examine protocol integration (TLS, DNSSEC), PKI and
certificate hygiene, and deployment in constrained and high-assurance
environments (IoT, cloud, finance, blockchain). We also discuss complementarity
with quantum technologies (QKD, QRNGs) and the limits of near-term quantum
computing. Throughout, we emphasize crypto-agility, hybrid migration, and
evidence-based guidance for operators. We conclude with open problems spanning
parameter agility, leakage-resilient implementations, and domain-specific
rollout playbooks. This survey aims to be a practical reference for researchers
and practitioners planning quantum-safe systems, bridging standards,
engineering, and operations.

</details>


### [41] [SASER: Stego attacks on open-source LLMs](https://arxiv.org/abs/2510.10486)
*Ming Tan,Wei Li,Hu Tao,Hailong Ma,Aodi Liu,Qian Chen,Zilong Wang*

Main category: cs.CR

TL;DR: 本文系统化地形式化了开源大语言模型中的隐写攻击，提出了首个针对开源LLM的隐写攻击方法SASER，该方法通过识别目标参数、嵌入载荷、注入触发器和执行载荷来实现攻击，并在量化部署场景下保持高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 开源LLM虽然具有透明性优势，但其完全访问特性使其容易受到隐写攻击，而这类攻击的危害尚未被充分理解。

Method: 提出SASER攻击方法，包括：1）识别目标参数；2）嵌入载荷；3）注入触发器；4）执行载荷。特别设计了性能感知重要性指标来保持模型性能，并增强对量化部署的鲁棒性。

Result: 在LlaMA2-7B和ChatGLM3-6B上的实验表明，SASER的隐蔽率比现有DNN隐写攻击高出98.1%，攻击成功率保持100%。在量化模型上，攻击成功率从0提升到100%。

Conclusion: SASER展示了显著的有效性，呼吁研究针对此类攻击的防御措施。

Abstract: Open-source large language models (LLMs) have demonstrated considerable
dominance over proprietary LLMs in resolving neural processing tasks, thanks to
the collaborative and sharing nature. Although full access to source codes,
model parameters, and training data lays the groundwork for transparency, we
argue that such a full-access manner is vulnerable to stego attacks, and their
ill-effects are not fully understood. In this paper, we conduct a systematic
formalization for stego attacks on open-source LLMs by enumerating all possible
threat models associated with adversary objectives, knowledge, and
capabilities. Therein, the threat posed by adversaries with internal knowledge,
who inject payloads and triggers during the model sharing phase, is of
practical interest. We go even further and propose the first stego attack on
open-source LLMs, dubbed SASER, which wields impacts through identifying
targeted parameters, embedding payloads, injecting triggers, and executing
payloads sequentially. Particularly, SASER enhances the attack robustness
against quantization-based local deployment by de-quantizing the embedded
payloads. In addition, to achieve stealthiness, SASER devises the
performance-aware importance metric to identify targeted parameters with the
least degradation of model performance. Extensive experiments on LlaMA2-7B and
ChatGLM3-6B, without quantization, show that the stealth rate of SASER
outperforms existing stego attacks (for general DNNs) by up to 98.1%, while
achieving the same attack success rate (ASR) of 100%. More importantly, SASER
improves ASR on quantized models from 0 to 100% in all settings. We appeal for
investigations on countermeasures against SASER in view of the significant
attack effectiveness.

</details>


### [42] [The Hidden DNA of LLM-Generated JavaScript: Structural Patterns Enable High-Accuracy Authorship Attribution](https://arxiv.org/abs/2510.10493)
*Norbert Tihanyi,Bilel Cherif,Richard A. Dubniczky,Mohamed Amine Ferrag,Tamás Bisztray*

Main category: cs.CR

TL;DR: 该论文首次大规模研究证明LLM生成的JavaScript代码具有独特的风格特征，能够实现可靠的作品归属识别和模型指纹识别。作者构建了包含25万个JavaScript样本的数据集，并开发了CodeT5-JSA模型，在20类分类任务中达到88.5%的准确率。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成代码的快速增长，作品归属识别在检测漏洞、标记恶意内容和确保问责方面发挥着关键作用。现有研究通常将AI视为单一类别，但作者发现不同LLM会留下独特的风格特征。

Method: 构建LLM-NodeJS数据集（包含50,000个Node.js后端程序，来自20个大型语言模型，每个有4个变体版本），使用传统机器学习分类器与微调Transformer编码器进行基准测试，并开发了基于CodeT5的CodeT5-JSA自定义架构。

Result: CodeT5-JSA在5类归属识别任务中达到95.8%准确率，10类任务94.6%，20类任务88.5%，超越了BERT、CodeBERT和Longformer等模型。分类器能够捕捉程序数据流和结构的深层风格规律，即使在代码混淆、注释删除和重度转换后仍保持有效。

Conclusion: LLM生成的JavaScript代码具有可识别的独特风格特征，能够实现可靠的模型指纹识别。这项工作为AI生成代码的归属识别提供了重要基础，并开源了数据集和相关材料以支持可重复性研究。

Abstract: In this paper, we present the first large-scale study exploring whether
JavaScript code generated by Large Language Models (LLMs) can reveal which
model produced it, enabling reliable authorship attribution and model
fingerprinting. With the rapid rise of AI-generated code, attribution is
playing a critical role in detecting vulnerabilities, flagging malicious
content, and ensuring accountability. While AI-vs-human detection usually
treats AI as a single category we show that individual LLMs leave unique
stylistic signatures, even among models belonging to the same family or
parameter size. To this end, we introduce LLM-NodeJS, a dataset of 50,000
Node.js back-end programs from 20 large language models. Each has four
transformed variants, yielding 250,000 unique JavaScript samples and two
additional representations (JSIR and AST) for diverse research applications.
Using this dataset, we benchmark traditional machine learning classifiers
against fine-tuned Transformer encoders and introduce CodeT5-JSA, a custom
architecture derived from the 770M-parameter CodeT5 model with its decoder
removed and a modified classification head. It achieves 95.8% accuracy on
five-class attribution, 94.6% on ten-class, and 88.5% on twenty-class tasks,
surpassing other tested models such as BERT, CodeBERT, and Longformer. We
demonstrate that classifiers capture deeper stylistic regularities in program
dataflow and structure, rather than relying on surface-level features. As a
result, attribution remains effective even after mangling, comment removal, and
heavy code transformations. To support open science and reproducibility, we
release the LLM-NodeJS dataset, Google Colab training scripts, and all related
materials on GitHub: https://github.com/LLM-NodeJS-dataset.

</details>


### [43] [Predicting Module-Lattice Reduction](https://arxiv.org/abs/2510.10540)
*Léo Ducas,Lynn Engelberts,Paola de Perthuis*

Main category: cs.CR

TL;DR: 该论文对模块格约化进行了具体的平均案例分析，发现对于幂二次分圆域，模块BKZ需要比非结构化BKZ更大的块大小；而对于其他分圆域，模块BKZ能提供子线性增益。


<details>
  <summary>Details</summary>
Motivation: 回答Kyber标准化提交中关于模块格约化是否优于非结构化格约化的问题，这对Kyber等基于模块格的方案的具体安全性有重要影响。

Method: 通过实验验证的启发式方法预测模块BKZ的期望斜率，并开发了首个针对某些分圆域的开源模块BKZ实现。

Result: 模块BKZ在数域K中需要维度为β + log(|Δ_K|/d^d)β/(d log β) + o(β/log β)的SVP预言机才能达到与非结构化BKZ相同的斜率。

Conclusion: 对于幂二次分圆域，模块BKZ需要更大的块大小；对于其他分圆域，模块BKZ能提供子线性增益和子指数级加速。

Abstract: Is module-lattice reduction better than unstructured lattice reduction? This
question was highlighted as 'Q8' in the Kyber NIST standardization submission
(Avanzi et al., 2021), as potentially affecting the concrete security of Kyber
and other module-lattice-based schemes. Foundational works on module-lattice
reduction (Lee, Pellet-Mary, Stehl\'e, and Wallet, ASIACRYPT 2019; Mukherjee
and Stephens-Davidowitz, CRYPTO 2020) confirmed the existence of such module
variants of LLL and block-reduction algorithms, but focus only on provable
worst-case asymptotic behavior.
  In this work, we present a concrete average-case analysis of module-lattice
reduction. Specifically, we address the question of the expected slope after
running module-BKZ, and pinpoint the discriminant $\Delta_K$ of the number
field at hand as the main quantity driving this slope. We convert this back
into a gain or loss on the blocksize $\beta$: module-BKZ in a number field $K$
of degree $d$ requires an SVP oracle of dimension $\beta + \log(|\Delta_K| /
d^d)\beta /(d\log \beta) + o(\beta / \log \beta)$ to reach the same slope as
unstructured BKZ with blocksize $\beta$. This asymptotic summary hides further
terms that we predict concretely using experimentally verified heuristics.
Incidentally, we provide the first open-source implementation of module-BKZ for
some cyclotomic fields.
  For power-of-two cyclotomic fields, we have $|\Delta_K| = d^d$, and conclude
that module-BKZ requires a blocksize larger than its unstructured counterpart
by $d-1+o(1)$. On the contrary, for all other cyclotomic fields we have
$|\Delta_K| < d^d$, so module-BKZ provides a sublinear $\Theta(\beta/\log
\beta)$ gain on the required blocksize, yielding a subexponential speedup of
$\exp(\Theta(\beta/\log \beta))$.

</details>


### [44] [Man-in-the-Middle Proof-of-Concept via Krontiris' Ephemeral Diffie-Hellman Over COSE (EDHOC) in C](https://arxiv.org/abs/2510.10574)
*Daniel Hennig,Joaquin Garcia-Alfaro*

Main category: cs.CR

TL;DR: 该报告分析了轻量级密钥交换协议的身份验证过程，重点关注中间人攻击如何破坏其安全性，特别是在合法拦截背景下可能促进大规模监控的风险。


<details>
  <summary>Details</summary>
Motivation: 研究轻量级密钥交换协议的身份验证过程，揭示中间人攻击对其安全性的威胁，特别是在合法拦截可能被滥用于大规模监控的背景下。

Method: 分析轻量级密钥交换协议的身份验证技术细节，重点关注中间人攻击场景下的安全漏洞。

Result: 识别了轻量级密钥交换协议在身份验证过程中存在的安全风险，中间人攻击可能被滥用于大规模监控目的。

Conclusion: 轻量级密钥交换协议的身份验证过程存在安全漏洞，需要进一步研究以防范中间人攻击和潜在的监控滥用风险。

Abstract: This report presents some technical details on the authentication process of
a lightweight key exchange protocol, paying attention on how Man-in-the-Middle
(MitM) attacks could undermine its security, e.g., under the scope of lawful
interception and its risk to facilitate mass surveillance. We focus only on
some technical aspects associated to the attack scenario. Perspectives for
future work are also discussed. Other specific aspects of the work, mainly
focusing on the security implications of malicious metasurfaces against B5G
networks, are excluded from the scope of this report.

</details>


### [45] [Toxic Ink on Immutable Paper: Content Moderation for Ethereum Input Data Messages (IDMs)](https://arxiv.org/abs/2510.10761)
*Xihan Xiong,Zhipeng Wang,Qin Wang,William Knottenbelt*

Main category: cs.CR

TL;DR: 提出了两种针对以太坊输入数据消息的审核框架：BUILDERMOD（构建者在区块构建时进行语义检查）和USERMOD（用户主动获取审核证明并嵌入交易），发现USERMOD具有更低的延迟和更好的扩展性。


<details>
  <summary>Details</summary>
Motivation: 随着以太坊上输入数据消息的广泛使用，链上出现越来越多有害内容，而以太坊协议层面缺乏内容审核机制，这引发了担忧。

Method: 设计了两种审核框架：BUILDERMOD让区块构建者执行语义检查，USERMOD让用户从外部分类器获取审核证明并嵌入交易。

Result: 评估显示BUILDERMOD会产生高区块时间开销，实用性受限；而USERMOD能够实现更低延迟的验证和更好的扩展性。

Conclusion: USERMOD是更实用的方法，为去中心化系统中的协议级内容治理奠定了基础，有助于构建安全、可信和负责任的去中心化通信环境。

Abstract: Decentralized communication is becoming an important use case within Web3. On
Ethereum, users can repurpose the transaction input data field to embed
natural-language messages, commonly known as Input Data Messages (IDMs).
However, as IDMs gain wider adoption, there has been a growing volume of toxic
content on-chain. This trend is concerning, as Ethereum provides no
protocol-level support for content moderation.
  We propose two moderation frameworks for Ethereum IDMs: (i) BUILDERMOD, where
builders perform semantic checks during block construction; and (ii) USERMOD,
where users proactively obtain moderation proofs from external classifiers and
embed them in transactions. Our evaluation reveals that BUILDERMOD incurs high
block-time overhead, which limits its practicality. In contrast, USERMOD
enables lower-latency validation and scales more effectively, making it a more
practical approach in moderation-aware Ethereum environments.
  Our study lays the groundwork for protocol-level content governance in
decentralized systems, and we hope it contributes to the development of a
decentralized communication environment that is safe, trustworthy, and socially
responsible.

</details>


### [46] [GPS Spoofing Attack Detection in Autonomous Vehicles Using Adaptive DBSCAN](https://arxiv.org/abs/2510.10766)
*Ahmad Mohammadi,Reza Ahmari,Vahid Hemmati,Frederick Owusu-Ambrose,Mahmoud Nabil Mahmoud,Parham Kebria,Abdollah Homaifar,Mehrdad Saif*

Main category: cs.CR

TL;DR: 提出了一种基于动态调谐DBSCAN算法的自适应GPS欺骗检测方法，通过实时调整检测阈值来识别各种GPS欺骗攻击。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶车辆在现代交通中日益重要，它们面临GPS欺骗攻击等威胁，需要有效的检测方法来保障安全。

Method: 使用动态调谐的DBSCAN算法，基于GPS与车载传感器数据的位移误差的递归均值和标准差实时调整检测阈值，仅在非异常实例时更新阈值。

Result: 在真实世界HDD数据集上测试，对转向、停车、超调和多小偏差欺骗攻击的检测准确率分别达到98.62%、99.96%、99.88%和98.38%。

Conclusion: 该方法显著提升了自动驾驶车辆对抗GPS欺骗威胁的安全性和安全性。

Abstract: As autonomous vehicles become an essential component of modern
transportation, they are increasingly vulnerable to threats such as GPS
spoofing attacks. This study presents an adaptive detection approach utilizing
a dynamically tuned Density Based Spatial Clustering of Applications with Noise
(DBSCAN) algorithm, designed to adjust the detection threshold ({\epsilon}) in
real-time. The threshold is updated based on the recursive mean and standard
deviation of displacement errors between GPS and in-vehicle sensors data, but
only at instances classified as non-anomalous. Furthermore, an initial
threshold, determined from 120,000 clean data samples, ensures the capability
to identify even subtle and gradual GPS spoofing attempts from the beginning.
To assess the performance of the proposed method, five different subsets from
the real-world Honda Research Institute Driving Dataset (HDD) are selected to
simulate both large and small magnitude GPS spoofing attacks. The modified
algorithm effectively identifies turn-by-turn, stop, overshoot, and multiple
small biased spoofing attacks, achieving detection accuracies of 98.621%,
99.960.1%, 99.880.1%, and 98.380.1%, respectively. This work provides a
substantial advancement in enhancing the security and safety of AVs against GPS
spoofing threats.

</details>


### [47] [A Symmetric-Key Cryptosystem Based on the Burnside Ring of a Compact Lie Group](https://arxiv.org/abs/2510.10901)
*Ziad Ghanem*

Main category: cs.CR

TL;DR: 提出了一种基于紧李群Burnside环的对称密钥密码系统，使用O(2)群作为示例，通过Burnside积进行加密，避免了密文扩展和安全泄漏。


<details>
  <summary>Details</summary>
Motivation: 经典线性密码（如Hill密码）在固定有限维模块上操作，容易受到已知明文攻击。本文旨在设计一种在紧李群Burnside环上操作的密码系统，提高安全性。

Method: 使用紧李群G（特别是O(2)群）的Burnside环A(G)，密钥包括群G、子群轨道基的秘密全序和不可约表示索引集。消息编码为A(G)的有限支持元素，通过与乘子k的Burnside积进行加密。

Result: 证明对于G=O(2)，加密保持明文在生成元上的支持，避免密文扩展。分析显示有限观测集仅约束有限秩子模上的操作，且密钥在信息论意义上不可识别。

Conclusion: 该方案在被动模型中具有信息论安全性，但证明不具备IND-CPA安全性，因为存在基于二面体探测的单查询选择明文区分器。

Abstract: Classical linear ciphers, such as the Hill cipher, operate on fixed,
finite-dimensional modules and are therefore vulnerable to straightforward
known-plaintext attacks that recover the key as a fully determined linear
operator. We propose a symmetric-key cryptosystem whose linear action takes
place instead in the Burnside ring $A(G)$ of a compact Lie group $G$, with
emphasis on the case $G=O(2)$. The secret key consists of (i) a compact Lie
group $G$; (ii) a secret total ordering of the subgroup orbit-basis of $A(G)$;
and (iii) a finite set $S$ of indices of irreducible $G$-representations, whose
associated basic degrees define an involutory multiplier $k\in A(G)$. Messages
of arbitrary finite length are encoded as finitely supported elements of $A(G)$
and encrypted via the Burnside product with $k$. For $G=O(2)$ we prove that
encryption preserves plaintext support among the generators
$\{(D_1),\dots,(D_L),(SO(2)),(O(2))\}$, avoiding ciphertext expansion and
security leakage. We then analyze security in passive models, showing that any
finite set of observations constrains the action only on a finite-rank
submodule $W_L\subset A(O(2))$, and we show information-theoretic
non-identifiability of the key from such data. Finally, we prove the scheme is
\emph{not} IND-CPA secure, by presenting a one-query chosen-plaintext
distinguisher based on dihedral probes.

</details>


### [48] [TabVLA: Targeted Backdoor Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2510.10932)
*Zonghuan Xu,Xiang Zheng,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CR

TL;DR: TabVLA是一个针对视觉-语言-动作(VLA)模型的目标性后门攻击框架，通过黑盒微调实现，在推理时通过输入流编辑和场景内触发两种威胁模型进行攻击。


<details>
  <summary>Details</summary>
Motivation: 随着VLA模型在具身AI系统中的广泛应用，其后门攻击漏洞带来了严重的安全威胁。现有研究仅关注无目标攻击，而更具实际威胁的目标性操纵场景尚未被研究。

Method: TabVLA采用黑盒微调方法，将中毒数据生成建模为优化问题，探索输入流编辑和场景内触发两种推理时威胁模型。

Result: 在OpenVLA-7B模型和LIBERO基准测试中，视觉通道是主要攻击面：目标性后门在最小中毒量下成功，对触发器设计变化具有鲁棒性，仅在微调与推理触发器位置不匹配时性能下降。

Conclusion: 该研究揭示了VLA模型对目标性后门操纵的脆弱性，强调需要更先进的防御机制来应对此类安全威胁。

Abstract: With the growing deployment of Vision-Language-Action (VLA) models in
real-world embodied AI systems, their increasing vulnerability to backdoor
attacks poses a serious safety threat. A backdoored VLA agent can be covertly
triggered by a pre-injected backdoor to execute adversarial actions,
potentially causing system failures or even physical harm. Although backdoor
attacks on VLA models have been explored, prior work has focused only on
untargeted attacks, leaving the more practically threatening scenario of
targeted manipulation unexamined. In this paper, we study targeted backdoor
attacks on VLA models and introduce TabVLA, a novel framework that enables such
attacks via black-box fine-tuning. TabVLA explores two deployment-relevant
inference-time threat models: input-stream editing and in-scene triggering. It
formulates poisoned data generation as an optimization problem to improve
attack effectivess. Experiments with OpenVLA-7B on the LIBERO benchmark reveal
that the vision channel is the principal attack surface: targeted backdoors
succeed with minimal poisoning, remain robust across variations in trigger
design, and are degraded only by positional mismatches between fine-tuning and
inference triggers. We also investigate a potential detection-based defense
against TabVLA, which reconstructs latent visual triggers from the input stream
to flag activation-conditioned backdoor samples. Our work highlights the
vulnerability of VLA models to targeted backdoor manipulation and underscores
the need for more advanced defenses.

</details>


### [49] [DITTO: A Spoofing Attack Framework on Watermarked LLMs via Knowledge Distillation](https://arxiv.org/abs/2510.10987)
*Hyeseon Ahn,Shinwoo Park,Yo-Sub Han*

Main category: cs.CR

TL;DR: 本文揭示了LLM水印技术的安全漏洞，提出了水印欺骗攻击，允许恶意模型生成包含可信模型水印的文本，从而将有害内容错误归因于可信来源。


<details>
  <summary>Details</summary>
Motivation: LLM水印技术基于特定水印能证明特定模型作者身份的核心假设，但本文发现这一假设存在严重缺陷，需要揭示水印技术的安全漏洞。

Method: 通过利用水印放射性（在微调过程中数据模式的无意继承），将可发现特征转化为攻击向量，通过从水印教师模型蒸馏知识，使攻击者能够窃取和复制受害者模型的水印信号。

Result: 成功实现了水印欺骗攻击，恶意模型能够生成包含可信模型水印的文本，证明了现有水印技术在文本作者身份验证方面的关键安全漏洞。

Conclusion: 这项工作揭示了文本作者身份验证的关键安全漏洞，呼吁向能够区分真实水印和专家模仿水印的技术进行范式转变。

Abstract: The promise of LLM watermarking rests on a core assumption that a specific
watermark proves authorship by a specific model. We demonstrate that this
assumption is dangerously flawed. We introduce the threat of watermark
spoofing, a sophisticated attack that allows a malicious model to generate text
containing the authentic-looking watermark of a trusted, victim model. This
enables the seamless misattribution of harmful content, such as disinformation,
to reputable sources. The key to our attack is repurposing watermark
radioactivity, the unintended inheritance of data patterns during fine-tuning,
from a discoverable trait into an attack vector. By distilling knowledge from a
watermarked teacher model, our framework allows an attacker to steal and
replicate the watermarking signal of the victim model. This work reveals a
critical security gap in text authorship verification and calls for a paradigm
shift towards technologies capable of distinguishing authentic watermarks from
expertly imitated ones. Our code is available at
https://github.com/hsannn/ditto.git.

</details>


### [50] [Secret-Protected Evolution for Differentially Private Synthetic Text Generation](https://arxiv.org/abs/2510.10990)
*Tianze Wang,Zhaoyu Chen,Jian Du,Yingtai Xiao,Linjun Zhang,Qiang Yan*

Main category: cs.CR

TL;DR: 提出SecPE框架，通过秘密感知保护实现更优的隐私-效用权衡，相比传统差分隐私方法在合成文本生成中实现更高效用和更低计算复杂度


<details>
  <summary>Details</summary>
Motivation: 现实世界中大量高质量文本因隐私问题无法自由使用，现有差分隐私合成文本生成方法过度保护非敏感内容，导致效用损失和计算开销

Method: 提出Secret-Protected Evolution (SecPE)框架，扩展私有进化方法，引入秘密感知保护机制，满足(p,r)-秘密保护，作为高斯差分隐私的松弛形式

Result: 在OpenReview、PubMed和Yelp基准测试中，SecPE持续实现更低的Fréchet Inception Distance和更高的下游任务准确率，同时需要更少的噪声达到相同保护水平

Conclusion: 秘密感知保护机制能够解锁更实用有效的隐私保护合成文本生成方法

Abstract: Text data has become extremely valuable on large language models (LLMs) and
even lead to general artificial intelligence (AGI). A lot of high-quality text
in the real world is private and cannot be freely used due to privacy concerns.
Therefore, differentially private (DP) synthetic text generation has been
proposed, aiming to produce high-utility synthetic data while protecting
sensitive information. However, existing DP synthetic text generation imposes
uniform guarantees that often overprotect non-sensitive content, resulting in
substantial utility loss and computational overhead. Therefore, we propose
Secret-Protected Evolution (SecPE), a novel framework that extends private
evolution with secret-aware protection. Theoretically, we show that SecPE
satisfies $(\mathrm{p}, \mathrm{r})$-secret protection, constituting a
relaxation of Gaussian DP that enables tighter utility-privacy trade-offs,
while also substantially reducing computational complexity relative to baseline
methods. Empirically, across the OpenReview, PubMed, and Yelp benchmarks, SecPE
consistently achieves lower Fr\'echet Inception Distance (FID) and higher
downstream task accuracy than GDP-based Aug-PE baselines, while requiring less
noise to attain the same level of protection. Our results highlight that
secret-aware guarantees can unlock more practical and effective
privacy-preserving synthetic text generation.

</details>


### [51] [Stabilizing the Staking Rate, Dynamically Distributed Inflation and Delay Induced Oscillations](https://arxiv.org/abs/2510.11065)
*Carlo Brunetta,Amit Chaudhary,Stefano Galatolo,Massimiliano Sala*

Main category: cs.CR

TL;DR: 该论文研究了区块链动态通胀机制中质押率的不稳定振荡问题，并提出了一种新的分配模型来稳定质押率。


<details>
  <summary>Details</summary>
Motivation: 区块链动态通胀机制虽然能引导质押率向期望均衡发展，但由于年化收益率对质押率变化的高度敏感性以及质押者反应的固有延迟，会导致围绕均衡的不良振荡。

Method: 分析了基于通胀的奖励系统动态特性，并提出了一种新颖的分配模型来稳定质押率。

Result: 提出的解决方案有效抑制了振荡，将收益率稳定在目标质押范围内。

Conclusion: 新的通胀分配模型能够解决区块链质押系统中的不稳定问题，实现更稳定的质押率管理。

Abstract: Dynamically distributed inflation is a common mechanism used to guide a
blockchain's staking rate towards a desired equilibrium between network
security and token liquidity.
  However, the high sensitivity of the annual percentage yield to changes in
the staking rate, coupled with the inherent feedback delays in staker
responses, can induce undesirable oscillations around this equilibrium.
  This paper investigates this instability phenomenon. We analyze the dynamics
of inflation-based reward systems and propose a novel distribution model
designed to stabilize the staking rate. Our solution effectively dampens
oscillations, stabilizing the yield within a target staking range.

</details>


### [52] [N-output Mechanism: Estimating Statistical Information from Numerical Data under Local Differential Privacy](https://arxiv.org/abs/2510.11116)
*Incheol Baek,Yon Dohn Chung*

Main category: cs.CR

TL;DR: 提出了一种名为N-output mechanism的通用框架，用于在本地差分隐私下收集数值数据，填补了现有机制仅适用于极小或无限输出空间的空白。


<details>
  <summary>Details</summary>
Motivation: 现有的本地差分隐私机制要么针对极小的输出空间（如2或3个输出），要么针对无限输出空间，缺乏适用于任意输出大小N的通用最优机制构建方法。

Method: 将机制设计建模为优化问题，以最小化估计方差，针对任意N≥2开发数值和解析解，构建将数值数据映射到N个离散输出的通用框架。

Result: 经验评估表明，N-output机制在均值、方差和分布估计方面达到了最先进的精度，同时保持较小的通信成本。

Conclusion: N-output机制是一个高度准确且自适应的通用框架，能够为任意输出大小N构建最优的本地差分隐私机制，填补了现有文献的重要空白。

Abstract: Local Differential Privacy (LDP) addresses significant privacy concerns in
sensitive data collection. In this work, we focus on numerical data collection
under LDP, targeting a significant gap in the literature: existing LDP
mechanisms are optimized for either a very small ($|\Omega| \in \{2, 3\}$) or
infinite output spaces. However, no generalized method for constructing an
optimal mechanism for an arbitrary output size $N$ exists. To fill this gap, we
propose the \textbf{N-output mechanism}, a generalized framework that maps
numerical data to one of $N$ discrete outputs.
  We formulate the mechanism's design as an optimization problem to minimize
estimation variance for any given $N \geq 2$ and develop both numerical and
analytical solutions. This results in a mechanism that is highly accurate and
adaptive, as its design is determined by solving an optimization problem for
any chosen $N$. Furthermore, we extend our framework and existing mechanisms to
the task of distribution estimation. Empirical evaluations show that the
N-output mechanism achieves state-of-the-art accuracy for mean, variance, and
distribution estimation with small communication costs.

</details>


### [53] [CoSPED: Consistent Soft Prompt Targeted Data Extraction and Defense](https://arxiv.org/abs/2510.11137)
*Yang Zhuochen,Fok Kar Wai,Thing Vrizlynn*

Main category: cs.CR

TL;DR: CoSPED是一种针对LLM数据提取攻击的软提示调优方法，通过动态损失、加性损失、通用损失和自一致性解码策略提高提取一致性，在50个token前缀比较下达到65.2%的提取率，并通过模型编辑将提取率降至1.6%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在严重的安全漏洞，特别是隐私泄露风险，需要测试和评估LLM中的数据提取风险。

Method: 提出CoSPED框架，包含动态损失、加性损失、通用损失和自一致性解码策略，通过软提示调优增强提取一致性。

Result: 在50个token前缀比较下达到65.2%的提取率，Pythia模型提取率为51.7%，通过Rank-One模型编辑将提取率降至1.6%。

Conclusion: CoSPED在数据提取方面优于其他方法，对提取机制的分析可直接指导有效的防御策略，证明了软提示攻击的有效性和可防御性。

Abstract: Large language models have gained widespread attention recently, but their
potential security vulnerabilities, especially privacy leakage, are also
becoming apparent. To test and evaluate for data extraction risks in LLM, we
proposed CoSPED, short for Consistent Soft Prompt targeted data Extraction and
Defense. We introduce several innovative components, including Dynamic Loss,
Additive Loss, Common Loss, and Self Consistency Decoding Strategy, and tested
to enhance the consistency of the soft prompt tuning process. Through extensive
experimentation with various combinations, we achieved an extraction rate of
65.2% at a 50-token prefix comparison. Our comparisons of CoSPED with other
reference works confirm our superior extraction rates. We evaluate CoSPED on
more scenarios, achieving Pythia model extraction rate of 51.7% and introducing
cross-model comparison. Finally, we explore defense through Rank-One Model
Editing and achieve a reduction in the extraction rate to 1.6%, which proves
that our analysis of extraction mechanisms can directly inform effective
mitigation strategies against soft prompt-based attacks.

</details>


### [54] [RAG-Pull: Imperceptible Attacks on RAG Systems for Code Generation](https://arxiv.org/abs/2510.11195)
*Vasilije Stambolic,Aritra Dhar,Lukas Cavigelli*

Main category: cs.CR

TL;DR: 提出了一种名为RAG-Pull的新型黑盒攻击方法，通过在查询或外部代码库中插入隐藏的UTF字符，将检索重定向到恶意代码，从而破坏模型的安全对齐。


<details>
  <summary>Details</summary>
Motivation: 虽然检索增强生成(RAG)通过添加外部数据提高了LLM响应的可靠性和可信度，但存在新的安全威胁，攻击者可能通过操纵检索过程来破坏模型的安全机制。

Method: 开发RAG-Pull攻击方法，在查询或代码库中插入隐藏UTF字符，通过查询扰动、目标扰动以及两者组合的方式重定向检索过程。

Result: 仅查询或代码扰动就能将检索转向攻击者控制的代码片段，而查询和目标联合扰动几乎达到完美成功率。这些代码片段引入了可被利用的漏洞，如远程代码执行和SQL注入。

Conclusion: RAG-Pull的最小扰动可以改变模型的安全对齐，增加对不安全代码的偏好，从而开启了对LLM的新一类攻击途径。

Abstract: Retrieval-Augmented Generation (RAG) increases the reliability and
trustworthiness of the LLM response and reduces hallucination by eliminating
the need for model retraining. It does so by adding external data into the
LLM's context. We develop a new class of black-box attack, RAG-Pull, that
inserts hidden UTF characters into queries or external code repositories,
redirecting retrieval toward malicious code, thereby breaking the models'
safety alignment. We observe that query and code perturbations alone can shift
retrieval toward attacker-controlled snippets, while combined query-and-target
perturbations achieve near-perfect success. Once retrieved, these snippets
introduce exploitable vulnerabilities such as remote code execution and SQL
injection. RAG-Pull's minimal perturbations can alter the model's safety
alignment and increase preference towards unsafe code, therefore opening up a
new class of attacks on LLMs.

</details>


### [55] [TraceAegis: Securing LLM-Based Agents via Hierarchical and Behavioral Anomaly Detection](https://arxiv.org/abs/2510.11203)
*Jiahao Liu,Bonan Ruan,Xianglin Yang,Zhiwei Lin,Yan Liu,Yang Wang,Tao Wei,Zhenkai Liang*

Main category: cs.CR

TL;DR: TraceAegis是一个基于溯源的分析框架，利用智能体执行轨迹检测异常行为，通过构建层次结构和行为规则来验证执行过程的有效性。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在实际应用中存在工具中毒和恶意指令等安全漏洞，传统基于规则的方法难以全面覆盖且需要大量人工工作，存在误报问题。

Method: 构建层次结构抽象稳定执行单元，总结为约束性行为规则，通过验证执行轨迹与层次和行为约束的一致性来检测异常。

Result: 在TraceAegis-Bench数据集（包含医疗和采购场景）上表现优异，成功识别了大部分异常行为。

Conclusion: TraceAegis通过溯源分析有效解决了LLM智能体的安全问题，为复杂软件系统中的智能体安全提供了可行方案。

Abstract: LLM-based agents have demonstrated promising adaptability in real-world
applications. However, these agents remain vulnerable to a wide range of
attacks, such as tool poisoning and malicious instructions, that compromise
their execution flow and can lead to serious consequences like data breaches
and financial loss. Existing studies typically attempt to mitigate such
anomalies by predefining specific rules and enforcing them at runtime to
enhance safety. Yet, designing comprehensive rules is difficult, requiring
extensive manual effort and still leaving gaps that result in false negatives.
As agent systems evolve into complex software systems, we take inspiration from
software system security and propose TraceAegis, a provenance-based analysis
framework that leverages agent execution traces to detect potential anomalies.
In particular, TraceAegis constructs a hierarchical structure to abstract
stable execution units that characterize normal agent behaviors. These units
are then summarized into constrained behavioral rules that specify the
conditions necessary to complete a task. By validating execution traces against
both hierarchical and behavioral constraints, TraceAegis is able to effectively
detect abnormal behaviors. To evaluate the effectiveness of TraceAegis, we
introduce TraceAegis-Bench, a dataset covering two representative scenarios:
healthcare and corporate procurement. Each scenario includes 1,300 benign
behaviors and 300 abnormal behaviors, where the anomalies either violate the
agent's execution order or break the semantic consistency of its execution
sequence. Experimental results demonstrate that TraceAegis achieves strong
performance on TraceAegis-Bench, successfully identifying the majority of
abnormal behaviors.

</details>


### [56] [MPCitH-based Signatures from Restricted Decoding Problems](https://arxiv.org/abs/2510.11224)
*Michele Battagliola,Sebastian Bitzer,Antonia Wachter-Zeh,Violetta Weger*

Main category: cs.CR

TL;DR: 将受限解码问题嵌入TCitH和VOLEitH框架，提出结构简单的建模方法，显著减小签名尺寸。基于CROSS的硬度假设可将尺寸减少一半以上，基于WAVE的硬度假设可获得与NIST竞赛中最小的MPCitH方案相当的签名尺寸。


<details>
  <summary>Details</summary>
Motivation: TCitH和VOLEitH作为MPCitH范式的最新发展，显著提升了数字签名方案的性能。本文旨在将受限解码问题嵌入这些框架，通过简单的结构建模实现竞争性的签名尺寸。

Method: 将受限解码问题嵌入TCitH和VOLEitH框架，采用结构简单的建模方法。具体实例化包括：基于CROSS硬度假设的受限解码问题，以及基于WAVE硬度假设的三元全重解码问题。

Result: 基于CROSS硬度假设的方案相比NIST提交方案减少了超过一半的签名尺寸。基于WAVE硬度假设的三元全重解码方案获得了与NIST竞赛中最小的MPCitH候选方案相当的签名尺寸。

Conclusion: 受限解码问题可以有效地嵌入TCitH和VOLEitH框架，通过简单的结构建模实现显著的签名尺寸优化，为数字签名方案提供了有竞争力的新选择。

Abstract: Threshold-Computation-in-the-Head (TCitH) and VOLE-in-the-Head (VOLEitH), two
recent developments of the MPC-in-the-Head (MPCitH) paradigm, have
significantly improved the performance of digital signature schemes in this
framework.
  In this note, we embed the restricted decoding problem within these
frameworks. We propose a structurally simple modeling that achieves competitive
signature sizes. Specifically, by instantiating the restricted decoding problem
with the same hardness assumption underlying CROSS, we reduce sizes by more
than a factor of two compared to the NIST submission. Moreover, we observe that
ternary full-weight decoding, closely related to the hardness assumption
underlying WAVE, is a restricted decoding problem. Using ternary full-weight
decoding, we obtain signature sizes comparable to the smallest MPCitH-based
candidates in the NIST competition.

</details>


### [57] [Collaborative Shadows: Distributed Backdoor Attacks in LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2510.11246)
*Pengyu Zhu,Lijun Li,Yaxing Lyu,Li Sun,Sen Su,Jing Shao*

Main category: cs.CR

TL;DR: 提出了首个针对多智能体系统的分布式后门攻击，将后门分解为多个分布式攻击原语，这些原语仅在特定协作序列中被激活，攻击成功率超过95%且不影响正常任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅关注单智能体后门攻击，忽视了多智能体系统中智能体协作引入的新攻击面，需要填补这一研究空白。

Method: 将后门分解为多个分布式攻击原语嵌入到MAS工具中，这些原语单独休眠，仅在特定协作序列中集体激活以组装完整后门。

Result: 攻击成功率超过95%，且不影响良性任务的性能表现。

Conclusion: 这项工作揭示了利用智能体协作的新型后门攻击面，强调需要超越单智能体保护的安全措施。

Abstract: LLM-based multi-agent systems (MAS) demonstrate increasing integration into
next-generation applications, but their safety in backdoor attacks remains
largely underexplored. However, existing research has focused exclusively on
single-agent backdoor attacks, overlooking the novel attack surfaces introduced
by agent collaboration in MAS. To bridge this gap, we present the first
Distributed Backdoor Attack tailored to MAS. We decompose the backdoor into
multiple distributed attack primitives that are embedded within MAS tools.
These primitives remain dormant individually but collectively activate only
when agents collaborate in a specific sequence, thereby assembling the full
backdoor to execute targeted attacks such as data exfiltration. To fully assess
this threat, we introduce a benchmark for multi-role collaborative tasks and a
sandboxed framework to evaluate. Extensive experiments demonstrate that our
attack achieves an attack success rate exceeding 95% without degrading
performance on benign tasks. This work exposes novel backdoor attack surfaces
that exploit agent collaboration, underscoring the need to move beyond
single-agent protection. Code and benchmark are available at
https://github.com/whfeLingYu/Distributed-Backdoor-Attacks-in-MAS.

</details>


### [58] [Large Language Models Are Effective Code Watermarkers](https://arxiv.org/abs/2510.11251)
*Rui Xu,Jiawei Chen,Zhaoxia Yin,Cong Kong,Xinpeng Zhang*

Main category: cs.CR

TL;DR: CodeMark-LLM是一个基于大语言模型的代码水印框架，通过语义保持的转换嵌入水印，利用LLM的跨语言泛化能力，无需语言特定工程即可实现多语言代码水印。


<details>
  <summary>Details</summary>
Motivation: 解决现有代码水印技术依赖手工规则、AST操作或任务特定训练的问题，提高水印技术的可扩展性、通用性和鲁棒性。

Method: 包含两个核心组件：语义一致嵌入模块（应用功能保持转换编码水印位）和差分比较提取模块（通过比较原始和水印代码识别应用转换）。

Result: 在多种编程语言和攻击场景下的广泛实验证明了其鲁棒性、有效性和可扩展性。

Conclusion: CodeMark-LLM提供了一个无需语言特定工程的通用代码水印解决方案，显著提升了水印技术的实用性和适用范围。

Abstract: The widespread use of large language models (LLMs) and open-source code has
raised ethical and security concerns regarding the distribution and attribution
of source code, including unauthorized redistribution, license violations, and
misuse of code for malicious purposes. Watermarking has emerged as a promising
solution for source attribution, but existing techniques rely heavily on
hand-crafted transformation rules, abstract syntax tree (AST) manipulation, or
task-specific training, limiting their scalability and generality across
languages. Moreover, their robustness against attacks remains limited. To
address these limitations, we propose CodeMark-LLM, an LLM-driven watermarking
framework that embeds watermark into source code without compromising its
semantics or readability. CodeMark-LLM consists of two core components: (i)
Semantically Consistent Embedding module that applies functionality-preserving
transformations to encode watermark bits, and (ii) Differential Comparison
Extraction module that identifies the applied transformations by comparing the
original and watermarked code. Leveraging the cross-lingual generalization
ability of LLM, CodeMark-LLM avoids language-specific engineering and training
pipelines. Extensive experiments across diverse programming languages and
attack scenarios demonstrate its robustness, effectiveness, and scalability.

</details>


### [59] [How to Get Actual Privacy and Utility from Privacy Models: the k-Anonymity and Differential Privacy Families](https://arxiv.org/abs/2510.11299)
*Josep Domingo-Ferrer,David Sánchez*

Main category: cs.CR

TL;DR: 隐私模型在隐私保护数据发布中承诺无需昂贵经验性风险评估，但主要模型存在保护不足或效用损失问题。k-匿名在确定性机制下仍可能泄露信息，差分隐私在小预算下效用损失严重，大预算下隐私保证无意义。


<details>
  <summary>Details</summary>
Motivation: 评估隐私模型是否能真正实现其承诺——无需经验性风险评估即可提供足够的隐私保护，同时保持数据效用。

Method: 分析主要隐私模型（k-匿名和差分隐私）的定义问题、保护机制缺陷以及隐私-效用权衡。

Result: 发现k-匿名在确定性机制下无法完全排除泄露风险，差分隐私在极端预算下要么效用损失严重要么隐私保证无意义。

Conclusion: 建议对k-匿名进行语义重构可以在不损失效用的前提下提供更鲁棒的隐私保护，而差分隐私只能通过放松隐私保证来改善效用。

Abstract: Privacy models were introduced in privacy-preserving data publishing and
statistical disclosure control with the promise to end the need for costly
empirical assessment of disclosure risk. We examine how well this promise is
kept by the main privacy models. We find they may fail to provide adequate
protection guarantees because of problems in their definition or incur
unacceptable trade-offs between privacy protection and utility preservation.
Specifically, k-anonymity may not entirely exclude disclosure if enforced with
deterministic mechanisms or without constraints on the confidential values. On
the other hand, differential privacy (DP) incurs unacceptable utility loss for
small budgets and its privacy guarantee becomes meaningless for large budgets.
In the latter case, an ex post empirical assessment of disclosure risk becomes
necessary, undermining the main appeal of privacy models. Whereas the utility
preservation of DP can only be improved by relaxing its privacy guarantees, we
argue that a semantic reformulation of k-anonymity can offer more robust
privacy without losing utility with respect to traditional syntactic
k-anonymity.

</details>


### [60] [TDADL-IE: A Deep Learning-Driven Cryptographic Architecture for Medical Image Security](https://arxiv.org/abs/2510.11301)
*Junhua Zhou,Quanjun Li,Weixuan Li,Guang Yu,Yihua Shao,Yihang Dong,Mengqian Wang,Zimeng Li,Changwei Gong,Xuhang Chen*

Main category: cs.CR

TL;DR: 提出TDADL-IE系统，结合增强混沌生成器和三维扩散算法，为医疗图像提供安全加密方案


<details>
  <summary>Details</summary>
Motivation: 数字医疗影像在远程医疗和云存储中需要强加密保护患者数据，现有混沌系统加密方法安全性不足

Method: 使用LSTM网络结合1D-Sine二次混沌映射生成伪随机序列，开发三维扩散算法对置乱图像进行加密

Result: 实验证明该系统能有效抵御各种安全威胁，适用于任意尺寸图像

Conclusion: TDADL-IE系统为医疗图像加密提供了有效的解决方案

Abstract: The rise of digital medical imaging, like MRI and CT, demands strong
encryption to protect patient data in telemedicine and cloud storage. Chaotic
systems are popular for image encryption due to their sensitivity and unique
characteristics, but existing methods often lack sufficient security. This
paper presents the Three-dimensional Diffusion Algorithm and Deep Learning
Image Encryption system (TDADL-IE), built on three key elements. First, we
propose an enhanced chaotic generator using an LSTM network with a 1D-Sine
Quadratic Chaotic Map (1D-SQCM) for better pseudorandom sequence generation.
Next, a new three-dimensional diffusion algorithm (TDA) is applied to encrypt
permuted images. TDADL-IE is versatile for images of any size. Experiments
confirm its effectiveness against various security threats. The code is
available at
\href{https://github.com/QuincyQAQ/TDADL-IE}{https://github.com/QuincyQAQ/TDADL-IE}.

</details>


### [61] [TBRD: TESLA Authenticated UAS Broadcast Remote ID](https://arxiv.org/abs/2510.11343)
*Jason Veara,Manav Jain,Kyle Moy,Aanjhan Ranganathan*

Main category: cs.CR

TL;DR: TBRD是一个用于认证无人机远程ID消息的轻量级系统，采用TESLA协议和移动设备TEE，相比数字签名减少50%认证开销和100倍计算时间。


<details>
  <summary>Details</summary>
Motivation: FAA远程ID标准缺乏认证机制，容易遭受欺骗、中继和重放攻击，威胁无人机监控和协调安全。

Method: 结合TESLA协议和移动设备可信执行环境(TEE)，构建轻量级、任务范围的认证系统，兼容现有标准。

Result: 系统减少50%认证开销和100倍计算时间，在对抗条件下保持安全保证，可集成到现有远程ID基础设施。

Conclusion: TBRD提供可扩展、标准兼容的消息认证方案，适用于监管和操作使用场景。

Abstract: Mysterious sightings of Unmanned Aircraft Systems (UAS) over U.S. military
facilities, suburban neighborhoods, and commercial airports have intensified
scrutiny of drone activity. To increase accountability, the Federal Aviation
Administration (FAA) introduced a Remote ID mandate, requiring unmanned
aircraft to broadcast their location, operator's location, and identity in
real-time. However, current standards leave authentication mechanisms
underspecified, enabling spoofing, relay, and replay attacks that can undermine
surveillance efforts and potentially disrupt UAS-to-UAS coordination in future
deployments. In this paper, we propose TBRD, a practical system for
authenticating Remote ID messages in a manner that aligns with existing
standards and UAS capabilities. TBRD leverages the TESLA protocol and mobile
device TEEs, and introduces a verification mechanism to build a lightweight,
mission-scoped authentication system that is both computationally efficient and
requires a low communication footprint. We evaluate the performance of TBRD
using both an FAA-requirements compatible proof-of-concept implementation for
performance metrics and a simulated 4-drone swarm mission scenario to
demonstrate its security guarantees under adversarial conditions. Our system
provides a 50\% reduction in authentication overhead compared to digital
signatures and a 100x reduction in computation time. Our results demonstrate
that TBRD can be integrated into current Remote ID infrastructures to provide a
scalable, standards-compliant message authentication for both regulatory and
operational use cases.

</details>


### [62] [Living Off the LLM: How LLMs Will Change Adversary Tactics](https://arxiv.org/abs/2510.11398)
*Sean Oesch,Jack Hutchins,Luke Koch,Kevin Kurian*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In living off the land attacks, malicious actors use legitimate tools and
processes already present on a system to avoid detection. In this paper, we
explore how the on-device LLMs of the future will become a security concern as
threat actors integrate LLMs into their living off the land attack pipeline and
ways the security community may mitigate this threat.

</details>


### [63] [Uncertainty-Aware, Risk-Adaptive Access Control for Agentic Systems using an LLM-Judged TBAC Model](https://arxiv.org/abs/2510.11414)
*Charles Fleming,Ashish Kundu,Ramana Kompella*

Main category: cs.CR

TL;DR: 提出一个结合大型语言模型的风险感知访问控制框架，扩展任务型访问控制模型，通过考虑资源风险和模型不确定性来为新兴任务制定即时策略。


<details>
  <summary>Details</summary>
Motivation: 解决企业环境中自主AI代理执行新兴任务时的安全挑战，这些任务缺乏预定义访问策略，需要动态、自适应的安全控制。

Method: 使用LLM作为自主的风险感知判断器，基于代理意图、目标资源风险和模型不确定性来制定访问决策，高风险或高不确定性请求需要人工审批。

Result: 开发了一个能够为新兴任务合成即时策略的框架，通过复合风险评分和不确定性估计实现更严格的访问控制。

Conclusion: 该框架通过同时考虑外部风险和内部置信度，能够执行更强大的最小权限原则，为更安全可信的自主系统铺平道路。

Abstract: The proliferation of autonomous AI agents within enterprise environments
introduces a critical security challenge: managing access control for emergent,
novel tasks for which no predefined policies exist. This paper introduces an
advanced security framework that extends the Task-Based Access Control (TBAC)
model by using a Large Language Model (LLM) as an autonomous, risk-aware judge.
This model makes access control decisions not only based on an agent's intent
but also by explicitly considering the inherent \textbf{risk associated with
target resources} and the LLM's own \textbf{model uncertainty} in its
decision-making process. When an agent proposes a novel task, the LLM judge
synthesizes a just-in-time policy while also computing a composite risk score
for the task and an uncertainty estimate for its own reasoning. High-risk or
high-uncertainty requests trigger more stringent controls, such as requiring
human approval. This dual consideration of external risk and internal
confidence allows the model to enforce a more robust and adaptive version of
the principle of least privilege, paving the way for safer and more trustworthy
autonomous systems.

</details>


### [64] [Bag of Tricks for Subverting Reasoning-based Safety Guardrails](https://arxiv.org/abs/2510.11570)
*Shuo Chen,Zhen Han,Haokun Chen,Bailan He,Shengyun Si,Jingpei Wu,Philip Torr,Volker Tresp,Jindong Gu*

Main category: cs.CR

TL;DR: 研究发现基于推理的安全防护措施存在严重漏洞，简单的模板令牌添加就能成功绕过防护，导致有害响应。


<details>
  <summary>Details</summary>
Motivation: 尽管基于推理的安全防护措施在防御越狱攻击方面表现出色，但作者发现这些防护措施对输入提示的细微操作极其脆弱，一旦被劫持会导致更严重的危害。

Method: 作者引入了一系列越狱方法，包括白盒、灰盒和黑盒设置下的攻击，从简单的模板操作到完全自动化的优化，并进行了可扩展实现。

Result: 这些攻击方法在多个基准测试中取得了极高的成功率（例如在gpt-oss系列上超过90%），证实了这些漏洞是系统性的。

Conclusion: 开源大型推理模型迫切需要更强的对齐技术来防止恶意滥用。

Abstract: Recent reasoning-based safety guardrails for Large Reasoning Models (LRMs),
such as deliberative alignment, have shown strong defense against jailbreak
attacks. By leveraging LRMs' reasoning ability, these guardrails help the
models to assess the safety of user inputs before generating final responses.
The powerful reasoning ability can analyze the intention of the input query and
will refuse to assist once it detects the harmful intent hidden by the
jailbreak methods. Such guardrails have shown a significant boost in defense,
such as the near-perfect refusal rates on the open-source gpt-oss series.
Unfortunately, we find that these powerful reasoning-based guardrails can be
extremely vulnerable to subtle manipulation of the input prompts, and once
hijacked, can lead to even more harmful results. Specifically, we first uncover
a surprisingly fragile aspect of these guardrails: simply adding a few template
tokens to the input prompt can successfully bypass the seemingly powerful
guardrails and lead to explicit and harmful responses. To explore further, we
introduce a bag of jailbreak methods that subvert the reasoning-based
guardrails. Our attacks span white-, gray-, and black-box settings and range
from effortless template manipulations to fully automated optimization. Along
with the potential for scalable implementation, these methods also achieve
alarmingly high attack success rates (e.g., exceeding 90% across 5 different
benchmarks on gpt-oss series on both local host models and online API
services). Evaluations across various leading open-source LRMs confirm that
these vulnerabilities are systemic, underscoring the urgent need for stronger
alignment techniques for open-sourced LRMs to prevent malicious misuse. Code is
open-sourced at https://chenxshuo.github.io/bag-of-tricks.

</details>


### [65] [PACEbench: A Framework for Evaluating Practical AI Cyber-Exploitation Capabilities](https://arxiv.org/abs/2510.11688)
*Zicheng Liu,Lige Huang,Jie Zhang,Dongrui Liu,Yuan Tian,Jing Shao*

Main category: cs.CR

TL;DR: 提出了PACEbench基准测试和PACEagent代理，用于评估LLM在复杂网络安全场景中的攻击能力，发现当前模型尚无法构成普遍性网络威胁。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试缺乏真实世界的复杂性，无法准确评估LLM的网络安全能力，需要更实用的AI网络攻击基准。

Method: 构建基于真实漏洞难度、环境复杂性和网络防御原则的PACEbench基准，包含四种漏洞利用场景；提出模拟人类渗透测试者的PACEagent代理，支持多阶段侦察、分析和利用。

Result: 对7个前沿LLM的广泛实验表明，当前模型在复杂网络场景中表现不佳，且都无法绕过防御系统。

Conclusion: 当前模型尚未构成普遍性网络攻击威胁，但该工作为未来模型的可信发展提供了稳健的基准测试框架。

Abstract: The increasing autonomy of Large Language Models (LLMs) necessitates a
rigorous evaluation of their potential to aid in cyber offense. Existing
benchmarks often lack real-world complexity and are thus unable to accurately
assess LLMs' cybersecurity capabilities. To address this gap, we introduce
PACEbench, a practical AI cyber-exploitation benchmark built on the principles
of realistic vulnerability difficulty, environmental complexity, and cyber
defenses. Specifically, PACEbench comprises four scenarios spanning single,
blended, chained, and defense vulnerability exploitations. To handle these
complex challenges, we propose PACEagent, a novel agent that emulates human
penetration testers by supporting multi-phase reconnaissance, analysis, and
exploitation. Extensive experiments with seven frontier LLMs demonstrate that
current models struggle with complex cyber scenarios, and none can bypass
defenses. These findings suggest that current models do not yet pose a
generalized cyber offense threat. Nonetheless, our work provides a robust
benchmark to guide the trustworthy development of future models.

</details>
