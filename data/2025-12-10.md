<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 25]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Agentic Artificial Intelligence for Ethical Cybersecurity in Uganda: A Reinforcement Learning Framework for Threat Detection in Resource-Constrained Environments](https://arxiv.org/abs/2512.07909)
*Ibrahim Adabara,Bashir Olaniyi Sadiq,Aliyu Nuhu Shuaibu,Yale Ibrahim Danjuma,Venkateswarlu Maninti,Mutebi Joe*

Main category: cs.CR

TL;DR: 本研究提出了一种基于强化学习的智能体人工智能框架，结合伦理治理层和人工监督，在资源受限环境下实现自适应、可信赖的网络安全防护，相比传统规则系统显著提升了检测率和公平性。


<details>
  <summary>Details</summary>
Motivation: 乌干达快速数字化转型增加了对网络服务的依赖，同时也面临更复杂的网络威胁。资源受限环境下，传统基于规则的入侵检测系统缺乏适应性和伦理保障，导致未检测到的漏洞和过度拦截合法流量。

Method: 提出智能体人工智能框架，集成强化学习、明确的伦理治理层和人工监督。开发了CPU优化的五节点网络拓扑仿真环境，模拟乌干达关键数字基础设施，生成良性及恶意流量。训练Q-learning智能体在明确定义的伦理约束和人工可审计条件下运行。

Result: AAI框架实现了100%检测率、零误报和完全伦理合规，而传统规则基线系统只有70%检测率和15%误报率。结果表明在CPU-only、资源受限环境中，基于伦理治理的强化学习能显著提升网络安全效果和公平性。

Conclusion: 智能体、伦理治理的强化学习框架为乌干达国家网络安全战略中负责任AI的实际应用提供了可行路径，在资源受限环境下能大幅提升网络安全效能和公平性。

Abstract: Uganda's rapid digital transformation, supported by national strategies such as Vision 2040 and the Digital Transformation Roadmap, has expanded reliance on networked services while simultaneously increasing exposure to sophisticated cyber threats. In resource-constrained settings, commonly deployed rule-based intrusion detection systems lack the adaptability and ethical safeguards needed to address evolving attack patterns, leading to undetected breaches and excessive blocking of legitimate traffic. This study proposes an Agentic Artificial Intelligence (AAI) framework that integrates reinforcement learning, an explicit ethical governance layer, and human oversight to deliver adaptive and trustworthy cybersecurity. A CPU-optimized simulation environment was developed using a five-node network topology that mirrors key elements of Uganda's critical digital infrastructure and generates both benign and malicious traffic, including phishing, ransomware, and distributed denial-of-service attacks. A Q-learning agent, operating within clearly defined ethical constraints and subject to human auditability, was trained and evaluated against a traditional rule-based baseline. The AAI framework achieved a 100 percent detection rate, zero false positives, and full ethical compliance, compared with 70 percent detection and 15 percent false positives for the baseline system. These results demonstrate that agentic, ethically governed reinforcement learning can substantially improve cybersecurity effectiveness and fairness in CPU-only, resource-constrained environments, offering a practical pathway for operationalizing responsible AI in Uganda's national cybersecurity strategy.

</details>


### [2] [AgentCrypt: Advancing Privacy and (Secure) Computation in AI Agent Collaboration](https://arxiv.org/abs/2512.08104)
*Harish Karthikeyan,Yue Guo,Leo de Castro,Antigoni Polychroniadou,Leo Ardon,Udari Madhushani Sehwag,Sumitra Ganesh,Manuela Veloso*

Main category: cs.CR

TL;DR: AgentCrypt是一个四层加密框架，用于AI代理间的细粒度隐私保护通信，确保标记数据的隐私始终优先于正确性，支持从无限制数据交换到全同态加密的计算。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在现实多代理环境中运行，确保可靠和上下文感知的隐私通信变得至关重要。传统访问控制不足，因为隐私风险通常在访问授予后出现，现有方法将隐私视为二元约束，忽视了细粒度、角色特定和计算依赖的隐私需求。

Method: 提出AgentCrypt四层框架：第1层无限制数据交换，第4层使用同态加密等技术实现完全加密计算。该框架在任何AI代理平台上添加保护层，确保标记数据的隐私始终优先于正确性。

Result: 在Langgraph和Google ADK上实现并测试，展示了跨平台的通用性。引入了一个模拟所有隐私级别关键任务的基准数据集，支持系统评估和可监管机器学习系统的发展。

Conclusion: AgentCrypt解决了AI代理通信中的隐私保护挑战，支持多样化交互中的隐私保护，并能在原本无法访问的数据上进行计算，克服了数据孤岛等障碍，为安全代理通信和计算提供了可监管的解决方案。

Abstract: As AI agents increasingly operate in real-world, multi-agent environments, ensuring reliable and context-aware privacy in agent communication is critical, especially to comply with evolving regulatory requirements. Traditional access controls are insufficient, as privacy risks often arise after access is granted; agents may use information in ways that compromise privacy, such as messaging humans, sharing context with other agents, making tool calls, persisting data, or generating derived private information. Existing approaches often treat privacy as a binary constraint, whether data is shareable or not, overlooking nuanced, role-specific, and computation-dependent privacy needs essential for regulatory compliance.
  Agents, including those based on large language models, are inherently probabilistic and heuristic. There is no formal guarantee of how an agent will behave for any query, making them ill-suited for operations critical to security. To address this, we introduce AgentCrypt, a four-tiered framework for fine-grained, encrypted agent communication that adds a protection layer atop any AI agent platform. AgentCrypt spans unrestricted data exchange (Level 1) to fully encrypted computation using techniques such as homomorphic encryption (Level 4). Crucially, it guarantees the privacy of tagged data is always maintained, prioritizing privacy above correctness.
  AgentCrypt ensures privacy across diverse interactions and enables computation on otherwise inaccessible data, overcoming barriers such as data silos. We implemented and tested it with Langgraph and Google ADK, demonstrating versatility across platforms. We also introduce a benchmark dataset simulating privacy-critical tasks at all privacy levels, enabling systematic evaluation and fostering the development of regulatable machine learning systems for secure agent communication and computation.

</details>


### [3] [Detecting Ambiguity Aversion in Cyberattack Behavior to Inform Cognitive Defense Strategies](https://arxiv.org/abs/2512.08107)
*Stephan Carney,Soham Hans,Sofia Hirschmann,Stacey Marsella,Yvonne Fonken,Peggy Wu,Nikolos Gurney*

Main category: cs.CR

TL;DR: 该研究开发了一个框架，利用多模态数据和LLM管道来检测网络攻击者在面临不确定性时表现出的模糊厌恶认知偏差，为自适应认知防御策略提供基础。


<details>
  <summary>Details</summary>
Motivation: 网络攻击者在操作环境中经常面临不确定性，本研究旨在探索如何建模和检测他们表现出的模糊厌恶认知偏差（即对已知概率的偏好超过未知概率），从而为更有效的网络安全防御提供认知层面的洞察。

Method: 1) 利用人类红队实验的多模态数据；2) 使用大型语言模型(LLM)管道将非结构化日志解析为MITRE ATT&CK映射的行动序列；3) 应用新的计算模型近乎实时地推断攻击者的模糊厌恶水平。

Result: 开发了一个能够操作化攻击者认知特征的方法论框架，能够检测攻击者在网络攻击过程中表现出的模糊厌恶偏差，为自适应防御策略提供了基础组件。

Conclusion: 通过将攻击者的模糊厌恶认知偏差操作化，本研究为开发自适应认知防御策略奠定了重要基础，有助于网络安全防御从被动响应转向主动预测和适应攻击者的认知特征。

Abstract: Adversaries (hackers) attempting to infiltrate networks frequently face uncertainty in their operational environments. This research explores the ability to model and detect when they exhibit ambiguity aversion, a cognitive bias reflecting a preference for known (versus unknown) probabilities. We introduce a novel methodological framework that (1) leverages rich, multi-modal data from human-subjects red-team experiments, (2) employs a large language model (LLM) pipeline to parse unstructured logs into MITRE ATT&CK-mapped action sequences, and (3) applies a new computational model to infer an attacker's ambiguity aversion level in near-real time. By operationalizing this cognitive trait, our work provides a foundational component for developing adaptive cognitive defense strategies.

</details>


### [4] [Information-Dense Reasoning for Efficient and Auditable Security Alert Triage](https://arxiv.org/abs/2512.08169)
*Guangze Zhao,Yongzheng Zhang,Changbo Tian,Dan Xie,Hongri Liu,Bailing Wang*

Main category: cs.CR

TL;DR: AIDR是一个混合云边框架，通过梯度压缩推理链来优化警报分类，在保持准确性和可审计性的同时大幅降低延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 安全运营中心面临海量异构警报流，存在"警报分类延迟悖论"：详细推理链确保准确性和合规性但延迟和成本过高，而简化推理链则牺牲透明度和可审计性。现有方案（签名系统、异常检测、云端LLM）各有缺陷。

Method: 提出AIDR混合云边框架，核心创新是梯度压缩推理链，保留决策关键步骤。具体方法：1) 将警报提炼为3-5个高信息密度要点（减少68%token）；2) 通过LoRA训练领域专家模型；3) 云边架构：云端LLM路由警报到本地专家生成SOAR就绪的JSON。

Result: 实验显示AIDR相比Chain-of-Thought实现更高准确性和40.6%延迟降低，对数据损坏具有鲁棒性，支持分布外泛化，实现可审计且高效的安全运营中心分类，完全符合数据驻留合规要求。

Conclusion: AIDR通过约束信息密度优化解决了警报分类延迟悖论，在保持决策相关信息和可审计性的同时最小化复杂性，为安全运营中心提供了高效、合规的分类解决方案。

Abstract: Security Operations Centers face massive, heterogeneous alert streams under minute-level service windows, creating the Alert Triage Latency Paradox: verbose reasoning chains ensure accuracy and compliance but incur prohibitive latency and token costs, while minimal chains sacrifice transparency and auditability. Existing solutions fail: signature systems are brittle, anomaly methods lack actionability, and fully cloud-hosted LLMs raise latency, cost, and privacy concerns. We propose AIDR, a hybrid cloud-edge framework that addresses this trade-off through constrained information-density optimization. The core innovation is gradient-based compression of reasoning chains to retain only decision-critical steps--minimal evidence sufficient to justify predictions while respecting token and latency budgets. We demonstrate that this approach preserves decision-relevant information while minimizing complexity. We construct compact datasets by distilling alerts into 3-5 high-information bullets (68% token reduction), train domain-specialized experts via LoRA, and deploy a cloud-edge architecture: a cloud LLM routes alerts to on-premises experts generating SOAR-ready JSON. Experiments demonstrate AIDR achieves higher accuracy and 40.6% latency reduction versus Chain-of-Thought, with robustness to data corruption and out-of-distribution generalization, enabling auditable and efficient SOC triage with full data residency compliance.

</details>


### [5] [Security Analysis of Integer Learning with Errors with Rejection Sampling](https://arxiv.org/abs/2512.08172)
*Kyle Yates,Antsa Pierrottet,Abdullah Al Mamun,Ryann Cartor,Mashrur Chowdhury,Shuhong Gao*

Main category: cs.CR

TL;DR: 本文研究线性最小二乘法攻击在ILWE问题上的有效性，特别是针对CRYSTALS-Dilithium等使用拒绝采样的签名方案。与基于侧信道攻击的研究不同，本文直接从签名构造ILWE实例，并通过实验验证了这些签名方案的安全性。


<details>
  <summary>Details</summary>
Motivation: ASIACRYPT 2018提出的基于线性最小二乘法的数字攻击针对ILWE问题，但该攻击在实际签名方案（如CRYSTALS-Dilithium）中的有效性尚未充分研究。本文旨在直接评估该攻击对实际部署的签名方案的影响，而不依赖侧信道信息。

Method: 1. 直接从获得的签名构造ILWE实例（而非依赖侧信道信息）
2. 引入新颖的模拟技术：通过$\mathbb{R}$中的矩阵进行模多项式算术
3. 开发处理大样本量的高效算法
4. 对CRYSTALS-Dilithium等使用拒绝采样的签名方案进行理论和实验分析

Result: 实验结果表明，基于ILWE的签名方案（如CRYSTALS-Dilithium）具有宣称的安全性。线性最小二乘法攻击在实际参数设置下效果有限，验证了这些方案的安全性保证。

Conclusion: 本文通过直接分析签名构造的ILWE实例，证实了基于ILWE的签名方案在实际部署中的安全性。研究还讨论了在智能交通系统等实际应用中的意义，为后量子密码方案的现实安全性评估提供了参考。

Abstract: At ASIACRYPT 2018, a digital attack based on linear least squares was introduced for a variant of the learning with errors (LWE) problem which omits modular reduction known as the integer learning with errors problem (ILWE). In this paper, we present a theoretical and experimental study of the effectiveness of the attack when applied directly to small parameter ILWE instances found in popular digital signature schemes such as CRYSTALS-Dilithium which utilize rejection sampling. Unlike other studies which form ILWE instances based on additional information obtained from side-channel attacks, we take a more direct approach to the problem by constructing our ILWE instance from only the obtained signatures. We outline and introduce novel techniques in our simulation designs such as modular polynomial arithmetic via matrices in $\mathbb{R}$, as well as algorithms for handling large sample sizes efficiently. Our experimental results reinforce the proclaimed security of signature schemes based on ILWE. We additionally discuss the implications of our work and digital signatures as a whole in regards to real-world applications such as in Intelligent Transportation Systems (ITS).

</details>


### [6] [A Practical Framework for Evaluating Medical AI Security: Reproducible Assessment of Jailbreaking and Privacy Vulnerabilities Across Clinical Specialties](https://arxiv.org/abs/2512.08185)
*Jinghao Wang,Ping Zhang,Carter Yagemann*

Main category: cs.CR

TL;DR: 提出一个完全可复现的医疗AI安全评估框架，可在消费级CPU硬件上运行，使用合成患者记录评估医疗大语言模型对越狱攻击和隐私提取攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前医疗大语言模型在临床决策支持中广泛应用，但对其对抗性滥用和隐私泄露的鲁棒性评估存在障碍：现有安全基准需要GPU集群、商业API访问或受保护的健康数据，限制了社区参与这一关键研究领域。

Method: 设计一个实用、完全可复现的框架，覆盖多个按临床风险分层的医疗专业领域（从急诊医学、精神病学等高危领域到全科医学），使用合成患者记录（无需IRB批准），在消费级CPU硬件上运行免费可用模型，评估越狱攻击（角色扮演、权威冒充、多轮操纵）和隐私提取攻击。

Result: 提出了完整的框架规范，包括威胁模型、数据生成方法、评估协议和评分标准，为医疗专科模型和防御机制的对比安全评估奠定基础。

Conclusion: 该框架消除了成本障碍，使更广泛的研究社区能够参与医疗AI安全评估，推进确保医疗AI系统安全可信的总体目标。

Abstract: Medical Large Language Models (LLMs) are increasingly deployed for clinical decision support across diverse specialties, yet systematic evaluation of their robustness to adversarial misuse and privacy leakage remains inaccessible to most researchers. Existing security benchmarks require GPU clusters, commercial API access, or protected health data -- barriers that limit community participation in this critical research area. We propose a practical, fully reproducible framework for evaluating medical AI security under realistic resource constraints. Our framework design covers multiple medical specialties stratified by clinical risk -- from high-risk domains such as emergency medicine and psychiatry to general practice -- addressing jailbreaking attacks (role-playing, authority impersonation, multi-turn manipulation) and privacy extraction attacks. All evaluation utilizes synthetic patient records requiring no IRB approval. The framework is designed to run entirely on consumer CPU hardware using freely available models, eliminating cost barriers. We present the framework specification including threat models, data generation methodology, evaluation protocols, and scoring rubrics. This proposal establishes a foundation for comparative security assessment of medical-specialist models and defense mechanisms, advancing the broader goal of ensuring safe and trustworthy medical AI systems.

</details>


### [7] [Evaluating Vulnerabilities of Connected Vehicles Under Cyber Attacks by Attack-Defense Tree](https://arxiv.org/abs/2512.08204)
*Muhammad Baqer Mollah,Honggang Wang,Hua Fang*

Main category: cs.CR

TL;DR: 提出基于攻击树的CAV网络安全漏洞评估方法，通过攻击防御树系统分析漏洞，定义基于现有威胁和防御措施的漏洞度量指标。


<details>
  <summary>Details</summary>
Motivation: 联网自动驾驶车辆(CAV)结合了通信、感知和计算技术，能提升交通安全和效率，但连接性也带来了网络安全风险，恶意攻击者可能通过破坏系统组件来危害CAV。

Method: 采用基于攻击树的方法论，利用攻击防御树公式系统评估攻击叶漏洞，在分析漏洞指数前，定义了基于现有网络安全威胁和相应防御对策的漏洞度量指标。

Result: 论文提出了一种系统化的CAV网络安全漏洞评估框架，能够识别和分析攻击路径，量化漏洞风险，为CAV安全防护提供理论基础。

Conclusion: 攻击树方法为CAV网络安全漏洞评估提供了有效的系统化工具，有助于识别关键攻击路径并制定相应的防御策略，提升CAV系统的整体安全性。

Abstract: Connected vehicles represent a key enabler of intelligent transportation systems, where vehicles are equipped with advanced communication, sensing, and computing technologies to interact not only with one another but also with surrounding infrastructures and the environment. Through continuous data exchange, such vehicles are capable of enhancing road safety, improving traffic efficiency, and ensuring more reliable mobility services. Further, when these capabilities are integrated with advanced automation technologies, the concept essentially evolves into connected and autonomous vehicles (CAVs). While connected vehicles primarily focus on seamless information sharing, autonomous vehicles are mainly dependent on advanced perception, decision-making, and control mechanisms to operate with minimal or without human intervention. However, as a result of connectivity, an adversary with malicious intentions might be able to compromise successfully by breaching the system components of CAVs. In this paper, we present an attack-tree based methodology for evaluating cyber security vulnerabilities in CAVs. In particular, we utilize the attack-defense tree formulation to systematically assess attack-leaf vulnerabilities, and before analyzing the vulnerability indices, we also define a measure of vulnerabilities, which is based on existing cyber security threats and corresponding defensive countermeasures.

</details>


### [8] [MIRAGE: Misleading Retrieval-Augmented Generation via Black-box and Query-agnostic Poisoning Attacks](https://arxiv.org/abs/2512.08289)
*Tailun Chen,Yu He,Yan Wang,Shuo Shao,Haolun Zheng,Zhihao Liu,Jinfeng Li,Yuefeng Chen,Zhixuan Chu,Zhan Qin*

Main category: cs.CR

TL;DR: MIRAGE：一种在严格黑盒和查询无关环境下工作的多阶段投毒攻击框架，通过替代模型反馈、查询合成和语义锚定等技术，显著提升了RAG系统投毒攻击的效果和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统投毒攻击研究通常依赖不切实际的假设（如白盒访问或已知用户查询），低估了实际攻击的难度。本文旨在填补这一空白，研究在严格黑盒和查询无关环境下的实际可行攻击方法。

Method: 提出MIRAGE多阶段投毒管道：1）使用角色驱动查询合成来近似潜在用户搜索分布；2）通过语义锚定将意图不可察觉地嵌入文档以提高检索可见性；3）利用对抗性测试时间偏好优化（TPO）来最大化说服效果。整个框架基于替代模型反馈进行自动化优化。

Result: 在三个长格式、领域特定数据集构建的新基准上，MIRAGE在攻击效果和隐蔽性方面显著优于现有基线方法，并展现出跨不同检索器-LLM配置的显著可迁移性。

Conclusion: MIRAGE揭示了RAG系统在严格黑盒和查询无关环境下的实际安全威胁，其攻击效果和可迁移性凸显了开发鲁棒防御策略的紧迫需求。

Abstract: Retrieval-Augmented Generation (RAG) systems enhance LLMs with external knowledge but introduce a critical attack surface: corpus poisoning. While recent studies have demonstrated the potential of such attacks, they typically rely on impractical assumptions, such as white-box access or known user queries, thereby underestimating the difficulty of real-world exploitation. In this paper, we bridge this gap by proposing MIRAGE, a novel multi-stage poisoning pipeline designed for strict black-box and query-agnostic environments. Operating on surrogate model feedback, MIRAGE functions as an automated optimization framework that integrates three key mechanisms: it utilizes persona-driven query synthesis to approximate latent user search distributions, employs semantic anchoring to imperceptibly embed these intents for high retrieval visibility, and leverages an adversarial variant of Test-Time Preference Optimization (TPO) to maximize persuasion. To rigorously evaluate this threat, we construct a new benchmark derived from three long-form, domain-specific datasets. Extensive experiments demonstrate that MIRAGE significantly outperforms existing baselines in both attack efficacy and stealthiness, exhibiting remarkable transferability across diverse retriever-LLM configurations and highlighting the urgent need for robust defense strategies.

</details>


### [9] [Systematization of Knowledge: Security and Safety in the Model Context Protocol Ecosystem](https://arxiv.org/abs/2512.08290)
*Shiva Gaire,Srijan Gyawali,Saroj Mishra,Suman Niroula,Dilip Thakur,Umesh Yadav*

Main category: cs.CR

TL;DR: 本文系统分析了Model Context Protocol（MCP）生态系统的安全风险，区分了对抗性安全威胁和认知性安全危害，并提出了防御方案路线图。


<details>
  <summary>Details</summary>
Motivation: MCP已成为连接LLM与外部数据和工具的事实标准，但它在解决互操作性挑战的同时，引入了新的威胁格局，其中认知错误（幻觉）与安全漏洞（未授权操作）的界限变得模糊。需要系统化分析这些风险。

Method: 采用系统化知识（SoK）方法，建立MCP生态系统的风险分类学，分析MCP原语（资源、提示、工具）的结构性漏洞，并调查最先进的防御技术。

Result: 识别出对抗性安全威胁（如间接提示注入、工具中毒）和认知性安全危害（如分布式工具委托中的对齐失败），展示了"上下文"如何在多智能体环境中被武器化以触发未授权操作。

Conclusion: 需要综合防御策略来保护从对话式聊天机器人到自主智能体操作系统的过渡，提出了包含加密溯源和运行时意图验证等技术的路线图。

Abstract: The Model Context Protocol (MCP) has emerged as the de facto standard for connecting Large Language Models (LLMs) to external data and tools, effectively functioning as the "USB-C for Agentic AI." While this decoupling of context and execution solves critical interoperability challenges, it introduces a profound new threat landscape where the boundary between epistemic errors (hallucinations) and security breaches (unauthorized actions) dissolves. This Systematization of Knowledge (SoK) aims to provide a comprehensive taxonomy of risks in the MCP ecosystem, distinguishing between adversarial security threats (e.g., indirect prompt injection, tool poisoning) and epistemic safety hazards (e.g., alignment failures in distributed tool delegation). We analyze the structural vulnerabilities of MCP primitives, specifically Resources, Prompts, and Tools, and demonstrate how "context" can be weaponized to trigger unauthorized operations in multi-agent environments. Furthermore, we survey state-of-the-art defenses, ranging from cryptographic provenance (ETDI) to runtime intent verification, and conclude with a roadmap for securing the transition from conversational chatbots to autonomous agentic operating systems.

</details>


### [10] [Exposing and Defending Membership Leakage in Vulnerability Prediction Models](https://arxiv.org/abs/2512.08291)
*Yihan Liao,Jacky Keung,Xiaoxue Ma,Jingyu Zhang,Yicheng Sun*

Main category: cs.CR

TL;DR: 首次全面分析漏洞预测模型的成员推理攻击风险，发现logits和loss最易泄露隐私，并提出轻量级噪声防御方法NMID，显著降低攻击成功率


<details>
  <summary>Details</summary>
Motivation: 漏洞预测模型在安全关键代码分析任务中面临隐私风险，成员推理攻击在NLP和视觉领域已有研究，但在代码分析领域尚未充分探索，需要评估其影响并提供防御方案

Method: 采用黑盒和灰盒威胁模型，评估LSTM、BiGRU和CodeBERT等架构在不同特征组合（嵌入、logits、loss、置信度）下的攻击成功率，并提出基于输出掩码和高斯噪声注入的NMID防御方法

Result: 实验发现logits和loss是最易泄露成员信息的关键输出，NMID防御能将攻击AUC从接近1.0降至0.65以下，同时保持模型预测性能

Conclusion: 代码分析模型存在严重隐私风险，logits和loss是主要泄露渠道，提出的NMID防御方法能有效保护隐私而不影响模型效用，为AI驱动的软件系统安全提供可行方案

Abstract: Neural models for vulnerability prediction (VP) have achieved impressive performance by learning from large-scale code repositories. However, their susceptibility to Membership Inference Attacks (MIAs), where adversaries aim to infer whether a particular code sample was used during training, poses serious privacy concerns. While MIA has been widely investigated in NLP and vision domains, its effects on security-critical code analysis tasks remain underexplored. In this work, we conduct the first comprehensive analysis of MIA on VP models, evaluating the attack success across various architectures (LSTM, BiGRU, and CodeBERT) and feature combinations, including embeddings, logits, loss, and confidence. Our threat model aligns with black-box and gray-box settings where prediction outputs are observable, allowing adversaries to infer membership by analyzing output discrepancies between training and non-training samples. The empirical findings reveal that logits and loss are the most informative and vulnerable outputs for membership leakage. Motivated by these observations, we propose a Noise-based Membership Inference Defense (NMID), which is a lightweight defense module that applies output masking and Gaussian noise injection to disrupt adversarial inference. Extensive experiments demonstrate that NMID significantly reduces MIA effectiveness, lowering the attack AUC from nearly 1.0 to below 0.65, while preserving the predictive utility of VP models. Our study highlights critical privacy risks in code analysis and offers actionable defense strategies for securing AI-powered software systems.

</details>


### [11] [Secure Audio Embedding in Images using Nature-Inspired Optimization](https://arxiv.org/abs/2512.08299)
*Aman Kumar,Ankit Chaudhary*

Main category: cs.CR

TL;DR: 提出一种基于哈里斯鹰优化算法的LSB音频隐写方法，将音频文件隐藏在图像中，通过优化像素位置提升图像质量、鲁棒性和嵌入容量。


<details>
  <summary>Details</summary>
Motivation: 在数字世界中保护敏感数据至关重要。隐写术通过隐藏秘密数据的存在而非内容，为多媒体通信提供更好的安全性。

Method: 使用最小有效位（LSB）方法将音频文件隐藏在图像中，并通过哈里斯鹰优化（HHO）算法优化像素位置选择。HHO是一种模仿哈里斯鹰狩猎行为的自然启发元启发式算法。

Result: 实验使用峰值信噪比（PSNR）、结构相似性指数（SSIM）和均方误差（MSE）进行评估。结果显示，与现有方法相比，HHO算法实现了更好的图像质量、鲁棒性和嵌入容量。

Conclusion: 提出的基于HHO优化的LSB音频隐写方法在图像质量、鲁棒性和嵌入容量方面优于现有方法，为多媒体通信中的数据保护提供了有效解决方案。

Abstract: In todays digital world, protecting sensitive data is very essential. Steganography hides the existence of secret data instead of its content, providing better security for multimedia communication. This paper proposes a new technique for hiding audio files inside images using the Least Significant Bit (LSB) method optimized by the Harris Hawks Optimization (HHO) algorithm. HHO is a nature-inspired metaheuristic that imitates the hunting behavior of Harris hawks to find optimal pixel positions for embedding data. The proposed method is evaluated using Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Mean Square Error (MSE). Experimental results show that HHO achieves better image quality, robustness, and embedding capacity compared to existing methods.

</details>


### [12] [Privacy-Preserving Identifier Checking in 5G](https://arxiv.org/abs/2512.08310)
*Marcel D. S. K. Gräfenstein,Stefan Köpsell,Maryam Zarezadeh*

Main category: cs.CR

TL;DR: 提出一种基于同态加密的5G设备标识符隐私保护验证协议，能在不暴露标识符给运营商的情况下验证设备是否在黑名单/灰名单中，平衡隐私与监管需求。


<details>
  <summary>Details</summary>
Motivation: 4G/5G网络中，设备标识符（如IMEI）的共享给移动网络运营商带来了严重的隐私风险，可能导致长期跟踪和用户活动跨会话关联。现有3GPP定义的设备标识寄存器（EIR）过程会暴露设备标识符，需要一种既能验证设备完整性又保护隐私的解决方案。

Method: 修改PEPSI协议用于私有集合成员（PSM）场景，采用BFV同态加密方案。用户设备（UE）能证明其标识符不在运营商的黑名单或灰名单中，同时运营商只能得知验证结果。协议还包含经授权的执法机构钩子，实现可控的去匿名化。

Result: 系统能在5秒内完成在线验证，每会话通信量约15-16MB。该方案符合后量子安全标准，证明其在实际应用中的可行性。

Conclusion: 同态加密在5G标识符隐私保护管理中具有良好前景，为未来6G网络中可扩展且合规的验证系统奠定了基础，在隐私保护和监管问责之间取得了平衡。

Abstract: Device identifiers like the International Mobile Equipment Identity (IMEI) are crucial for ensuring device integrity and meeting regulations in 4G and 5G networks. However, sharing these identifiers with Mobile Network Operators (MNOs) brings significant privacy risks by enabling long-term tracking and linking of user activities across sessions. In this work, we propose a privacy-preserving identifier checking method in 5G. This paper introduces a protocol for verifying device identifiers without exposing them to the network while maintaining the same functions as the 3GPP-defined Equipment Identity Register (EIR) process. The proposed solution modifies the PEPSI protocol for a Private Set Membership (PSM) setting using the BFV homomorphic encryption scheme. This lets User Equipment (UE) prove that its identifier is not on an operator's blacklist or greylist while ensuring that the MNO only learns the outcome of the verification. The protocol allows controlled deanonymization through an authorized Law Enforcement (LE) hook, striking a balance between privacy and accountability. Implementation results show that the system can perform online verification within five seconds and requires about 15 to 16 MB of communication per session. This confirms its practical use under post-quantum security standards. The findings highlight the promise of homomorphic encryption for managing identifiers while preserving privacy in 5G, laying the groundwork for scalable and compliant verification systems in future 6G networks.

</details>


### [13] [Developing a Strong CPS Defender: An Evolutionary Approach](https://arxiv.org/abs/2512.08320)
*Qingyuan Hu,Christopher M. Poskitt,Jun Sun,Yuqi Chen*

Main category: cs.CR

TL;DR: Evo-Defender：一种通过动态攻防交互迭代增强CPS防御的进化框架，在未见攻击场景中比现有方法性能提升2.7%


<details>
  <summary>Details</summary>
Motivation: 传统CPS异常检测方法通常基于专家或模糊测试生成的数据集进行一次性训练，难以泛化到未见和更隐蔽的攻击策略。防御者可以通过主动挑战攻击者来发现更细微的攻击，从而提升检测能力。

Method: 提出Evo-Defender进化框架，包含智能攻击者（使用引导模糊测试探索多样化非冗余攻击策略）和自进化防御者（使用增量学习适应新攻击模式），通过动态攻防交互迭代增强防御。

Result: 在两个现实CPS测试平台（田纳西伊士曼过程和机器人装配工作站）上实现，注入600多个攻击场景。在端到端攻击检测实验中，在未见场景中比最先进基线性能提升高达2.7%，训练数据使用更高效，检测更快更鲁棒。

Conclusion: Evo-Defender通过动态攻防交互有效提升了CPS异常检测能力，能够更好地适应新攻击模式，为关键基础设施提供更强大的安全保障。

Abstract: Cyber-physical systems (CPSs) are used extensively in critical infrastructure, underscoring the need for anomaly detection systems that are able to catch even the most motivated attackers. Traditional anomaly detection techniques typically do `one-off' training on datasets crafted by experts or generated by fuzzers, potentially limiting their ability to generalize to unseen and more subtle attack strategies. Stopping at this point misses a key opportunity: a defender can actively challenge the attacker to find more nuanced attacks, which in turn can lead to more effective detection capabilities. Building on this concept, we propose Evo-Defender, an evolutionary framework that iteratively strengthens CPS defenses through a dynamic attacker-defender interaction. Evo-Defender includes a smart attacker that employs guided fuzzing to explore diverse, non-redundant attack strategies, while the self-evolving defender uses incremental learning to adapt to new attack patterns. We implement Evo-Defender on two realistic CPS testbeds: the Tennessee Eastman process and a Robotic Arm Assembly Workstation, injecting over 600 attack scenarios. In end-to-end attack detection experiments, Evo-Defender achieves up to 2.7% higher performance than state-of-the-art baselines on unseen scenarios, while utilizing training data more efficiently for faster and more robust detection.

</details>


### [14] [Argus: A Multi-Agent Sensitive Information Leakage Detection Framework Based on Hierarchical Reference Relationships](https://arxiv.org/abs/2512.08326)
*Bin Wang,Hui Li,Liyang Zhang,Qijia Zhuang,Ao Yang,Dong Zhang,Xijun Luo,Bing Lin*

Main category: cs.CR

TL;DR: Argus是一个基于多智能体协作的敏感信息检测框架，通过三层检测机制显著降低误报率，在真实仓库环境中达到94.86%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 代码仓库中的敏感信息泄露已成为关键安全挑战。传统检测方法（正则表达式、指纹特征、高熵计算）存在高误报率问题，不仅降低检测效率，还显著增加开发人员的手动筛选负担。大语言模型和多智能体协作架构为敏感信息检测提供了新的技术视角。

Method: 提出Argus多智能体协作框架，采用三层检测机制：1) 关键内容检测，2) 文件上下文分析，3) 项目参考关系分析。开发了两个新基准测试：一个评估真实泄露检测能力，另一个评估误报过滤性能。

Result: 实验结果显示：Argus在泄露检测中达到94.86%准确率，精确率96.36%，召回率94.64%，F1分数0.955。分析97个真实仓库的总成本仅为2.2美元。

Conclusion: Argus通过多智能体协作和三层检测机制，有效解决了传统敏感信息检测方法的高误报率问题，在真实环境中表现出色且成本效益高。所有代码实现和相关数据集已开源供进一步研究和应用。

Abstract: Sensitive information leakage in code repositories has emerged as a critical security challenge. Traditional detection methods that rely on regular expressions, fingerprint features, and high-entropy calculations often suffer from high false-positive rates. This not only reduces detection efficiency but also significantly increases the manual screening burden on developers. Recent advances in large language models (LLMs) and multi-agent collaborative architectures have demonstrated remarkable potential for tackling complex tasks, offering a novel technological perspective for sensitive information detection. In response to these challenges, we propose Argus, a multi-agent collaborative framework for detecting sensitive information. Argus employs a three-tier detection mechanism that integrates key content, file context, and project reference relationships to effectively reduce false positives and enhance overall detection accuracy. To comprehensively evaluate Argus in real-world repository environments, we developed two new benchmarks, one to assess genuine leak detection capabilities and another to evaluate false-positive filtering performance. Experimental results show that Argus achieves up to 94.86% accuracy in leak detection, with a precision of 96.36%, recall of 94.64%, and an F1 score of 0.955. Moreover, the analysis of 97 real repositories incurred a total cost of only 2.2$. All code implementations and related datasets are publicly available at https://github.com/TheBinKing/Argus-Guard for further research and application.

</details>


### [15] [USCSA: Evolution-Aware Security Analysis for Proxy-Based Upgradeable Smart Contracts](https://arxiv.org/abs/2512.08372)
*Xiaoqi Li,Lei Xie,Wenkai Li,Zongwei Li*

Main category: cs.CR

TL;DR: USCSA是一个可升级智能合约安全分析器，通过AST差异分析评估升级过程中的风险，在检测升级引入的漏洞方面达到92.3%准确率和89.7%召回率。


<details>
  <summary>Details</summary>
Motivation: 区块链系统中智能合约升级需要考虑升级的连续性和后续维护，但实际升级操作经常引入新的安全漏洞，需要专门的工具来评估升级风险。

Method: 提出USCSA（可升级智能合约安全分析器），使用抽象语法树（AST）差异分析来评估升级过程的风险，分析了3,546个可升级合约的漏洞案例，覆盖重入、访问控制缺陷、整数溢出等常见漏洞类别。

Result: 实验结果显示USCSA在检测升级引入的漏洞方面达到92.3%准确率、89.7%召回率和91.0%的F1分数，高风险变更映射效率比传统方法提升30%。

Conclusion: USCSA为可升级智能合约的安全性和完整性提供了显著优势，为区块链应用的安全审计提供了新颖高效的解决方案。

Abstract: In the case of upgrading smart contracts on blockchain systems, it is essential to consider the continuity of upgrade and subsequent maintenance. In practice, upgrade operations often introduce new vulnerabilities. To address this, we propose an Upgradable Smart Contract Security Analyzer, USCSA, which evaluates the risks associated with the upgrade process using the Abstract Syntax Tree (AST) differential analysis. We collected and analyzed 3,546 cases of vulnerabilities in upgradable contracts,covering common vulnerability categories such as reentrancy, access control flaws, and integer overflow. Experimental results show that USCSA achieves an accuracy of 92.3%, recall of 89.7%, and F1-score of 91.0% in detecting upgrade-induced vulnerabilities.
  In addition, the efficiency of mapping high-risk changes has achieved a 30% improvement over the conventional approach. As a result, USCSA provides a significant advantage to improve the security and integrity of upgradable smart contracts, providing a novel and efficient solution to secure audits on blockchain applications.

</details>


### [16] [Attention is All You Need to Defend Against Indirect Prompt Injection Attacks in LLMs](https://arxiv.org/abs/2512.08417)
*Yinan Zhong,Qianhao Miao,Yanjiao Chen,Jiangyi Deng,Yushi Cheng,Wenyuan Xu*

Main category: cs.CR

TL;DR: Rennervate是一个防御间接提示注入攻击的框架，利用注意力特征进行细粒度token级检测，通过两步注意力池机制实现精确清洗，在5个LLM和6个数据集上优于15种现有防御方法。


<details>
  <summary>Details</summary>
Motivation: LLM应用容易受到间接提示注入攻击，攻击者通过不可信的外部数据源注入恶意指令，现有防御方法不够有效，需要更精确的检测和清洗机制。

Method: 提出Rennervate框架，利用LLM的注意力特征进行细粒度token级检测，采用两步注意力池机制聚合注意力头和响应token，实现精确的IPI检测和清洗。

Result: 在5个LLM和6个数据集上优于15种商业和学术防御方法，能够检测未知攻击并抵抗自适应攻击，建立了FIPI细粒度IPI数据集并开源。

Conclusion: Rennervate通过细粒度token级检测有效防御间接提示注入攻击，在保持LLM功能的同时提供精确清洗，为IPI防御研究提供了新方法和数据集。

Abstract: Large Language Models (LLMs) have been integrated into many applications (e.g., web agents) to perform more sophisticated tasks. However, LLM-empowered applications are vulnerable to Indirect Prompt Injection (IPI) attacks, where instructions are injected via untrustworthy external data sources. This paper presents Rennervate, a defense framework to detect and prevent IPI attacks. Rennervate leverages attention features to detect the covert injection at a fine-grained token level, enabling precise sanitization that neutralizes IPI attacks while maintaining LLM functionalities. Specifically, the token-level detector is materialized with a 2-step attentive pooling mechanism, which aggregates attention heads and response tokens for IPI detection and sanitization. Moreover, we establish a fine-grained IPI dataset, FIPI, to be open-sourced to support further research. Extensive experiments verify that Rennervate outperforms 15 commercial and academic IPI defense methods, achieving high precision on 5 LLMs and 6 datasets. We also demonstrate that Rennervate is transferable to unseen attacks and robust against adaptive adversaries.

</details>


### [17] [LLM-based Vulnerable Code Augmentation: Generate or Refactor?](https://arxiv.org/abs/2512.08493)
*Dyna Soumhane Ouchebara,Stéphane Dupont*

Main category: cs.CR

TL;DR: LLM-based数据增强能有效缓解漏洞代码库的类别不平衡问题，混合策略能最好地提升漏洞分类器性能


<details>
  <summary>Details</summary>
Motivation: 漏洞代码库通常存在严重的类别不平衡问题，限制了基于深度学习的漏洞分类器的效果。数据增强可以通过缓解代表性不足的CWE（常见弱点枚举）的稀缺性来帮助解决这个问题。

Method: 研究基于LLM的漏洞函数增强方法，比较两种策略：1）控制生成新的漏洞样本；2）对现有样本进行语义保持的重构。使用Qwen2.5-Coder生成增强数据，CodeBERT作为漏洞分类器在SVEN数据集上进行评估。

Result: 研究发现LLM增强方法确实能通过简单流程和合理质量来丰富漏洞代码库，并且混合策略（结合新样本生成和现有样本重构）最能提升漏洞分类器的性能。

Conclusion: LLM-based数据增强是解决漏洞代码库不平衡问题的有效方法，混合增强策略在提升漏洞检测性能方面表现最佳。

Abstract: Vulnerability code-bases often suffer from severe imbalance, limiting the effectiveness of Deep Learning-based vulnerability classifiers. Data Augmentation could help solve this by mitigating the scarcity of under-represented CWEs. In this context, we investigate LLM-based augmentation for vulnerable functions, comparing controlled generation of new vulnerable samples with semantics-preserving refactoring of existing ones. Using Qwen2.5-Coder to produce augmented data and CodeBERT as a vulnerability classifier on the SVEN dataset, we find that our approaches are indeed effective in enriching vulnerable code-bases through a simple process and with reasonable quality, and that a hybrid strategy best boosts vulnerability classifiers' performance.

</details>


### [18] [Labeled Delegated PSI and its Applications in the Public Sector](https://arxiv.org/abs/2512.08558)
*Kristof Verslype,Florian Kerschbaum,Cyprien Delpech de Saint Guilhem,Bart De Decker,Jorn Lapon*

Main category: cs.CR

TL;DR: 本文提出了一种具有可组合输出功能的新型多方委托私有集合交集协议，支持加密载荷和伪名化标识符，适用于公共部门数据链接应用。


<details>
  <summary>Details</summary>
Motivation: 公民敏感数据（如社会、医疗、财政数据）分散在公共机构和私人领域，挖掘这些数据可以带来新的洞察，如改善医疗保健、欺诈检测和循证决策。现有的多方委托私有集合交集技术缺乏必要的附加功能，如安全交付交集元素的载荷，阻碍了实际部署。

Method: 基于与政府组织的合作需求，设计了一种新的D-PSI协议，具有可组合的输出功能，包括加密载荷和伪名化标识符。协议在标准模型中证明安全，能够抵抗半诚实数据提供者的共谋攻击，以及非共谋但可能恶意的独立数据收集者。

Result: 开发了一种安全的多方委托私有集合交集协议，支持加密载荷传递和伪名化标识符生成，满足公共部门数据链接的实际需求。

Conclusion: 该协议能够安全地从多个数据提供者私有地链接和收集数据，适用于公共部门的实际部署，解决了现有D-PSI技术在实际应用中的功能限制问题。

Abstract: Sensitive citizen data, such as social, medical, and fiscal data, is heavily fragmented across
  public bodies and the private domain. Mining the combined data sets allows for new insights that otherwise remain hidden.
  Examples are improved healthcare, fraud detection, and evidence-based policy making.
  (Multi-party) delegated private set intersection (D-PSI) is a privacy-enhancing technology to link data across multiple data providers using a data collector.
  However, before it can be deployed in these use cases, it needs to be enhanced with additional functions, e.g., securely delivering payload only for elements in the intersection.
  Although there has been recent progress in the communication and computation requirements of D-PSI, these practical obstacles have not yet been addressed.
  This paper is the result of a collaboration with a governmental organization responsible for collecting, linking, and pseudonymizing data.
  Based on their requirements, we design a new D-PSI protocol with composable output functions, including encrypted payload and pseudonymized identifiers.
  We show that our protocol is secure in the standard model against colluding semi-honest data providers and against a non-colluding, possibly malicious independent party, the data collector.
  It, hence, allows to privately link and collect data from multiple data providers suitable for deployment in these use cases in the public sector.

</details>


### [19] [Integrating Public Input and Technical Expertise for Effective Cybersecurity Policy Formulation](https://arxiv.org/abs/2512.08575)
*Hlekane Ngobeni,Mike Wa Nkongolo*

Main category: cs.CR

TL;DR: 该论文通过系统文献综述和文献计量分析，探讨了如何平衡技术专长与公众参与来制定有效的网络安全政策，识别了五个关键主题，并强调需要包容性、灵活的治理策略。


<details>
  <summary>Details</summary>
Motivation: 随着数字化转型和技术使用的增加，网络漏洞和威胁日益复杂，危及国家安全。当前网络安全政策制定缺乏整体方法和协作努力，需要平衡技术专长与公众参与来制定更有效的政策。

Method: 采用PRISMA方法的系统文献综述和文献计量分析，通过主题分析识别关键主题。

Result: 识别出五个关键主题：多利益相关者参与和以人为本的方法、治理与政策框架、技术基础设施、评估与合规、法律权利与主权。研究发现协作努力不足，削弱了网络安全政策的有效性。

Conclusion: 未来的网络安全政策研究和实践需要包容性、灵活的治理策略，将公众参与融入每个阶段，从主要的技术和法律视角转向更全面的方法。

Abstract: The evolving of digital transformation and increased use of technology comes with increased cyber vulnerabilities, which compromise national security. Cyber-threats become more sophisticated as the technology advances. This emphasises the need for strong risk mitigation strategies. To define strong and robust cybersecurity, policies requires an integrated approach of balancing technical expertise with public input. This paper aims to explore strategies used to balance technical expertise and public input to develop effective and robust cybersecurity policies. It also studied how the effective integration of technical expertise with public input is critical to developing effective strategies and resilient cybersecurity frameworks that strengthens national security. A lack of a holistic approach and collaborative efforts to cybersecurity can hinder the effectiveness of cybersecurity policies. This paper followed a systematic literature review with bibliometric analysis using the PRISMA methodology to explore how technical expertise and public input can be integrated to guide cybersecurity policy making. The thematic analysis identified five important themes in developing effective cybersecurity policies, these key themes are: Multi-Stakeholder Involvement and Human Centric Approaches (MSI & HCA), Governance and Policy Frameworks (GPF), Technical Infrastructure (TI), Evaluation and Compliance (EC), and Legal Rights and Sovereignty (LRS). The synthesis shows that there is no adequate exploration of collaborative efforts which undermines the effectiveness of the cybersecurity policies. The findings suggest that inclusive, flexible governance strategies that integrate public input at every stage are necessary for future cybersecurity policy research and practice, which must shift away from a primarily technical and legal perspective.

</details>


### [20] [An Explainable AI Model for the Detecting Malicious Smart Contracts Based on EVM Opcode Based Features](https://arxiv.org/abs/2512.08782)
*Roopak Surendran*

Main category: cs.CR

TL;DR: 提出基于机器学习检测恶意智能合约的方法，通过分析EVM操作码，使用SMOTE平衡数据集和基于熵的监督分箱方法，实现99%检测率且误报率仅0.01，并整合LIME算法提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 黑客可能在以太坊区块链上部署恶意Solidity程序，利用重入攻击、tx.origin攻击、随机数漏洞等漏洞攻击合法程序，导致资金流失和拒绝服务等问题。因此需要在部署前识别和预防恶意智能合约。

Method: 1) 分析EVM操作码频率；2) 使用SMOTE算法平衡操作码频率数据集；3) 采用基于熵的监督分箱方法将操作码频率转换为二进制值(0,1)；4) 训练可解释AI模型；5) 整合LIME算法解释预测结果。

Result: 提出的机制能够检测99%的恶意智能合约，误报率仅为0.01。LIME算法能够解释ML分类器为何将特定智能合约声明为恶意，基于EVM操作码的二进制值提供可解释性。

Conclusion: 基于EVM操作码的机器学习方法能有效检测恶意智能合约，结合可解释AI技术提高了检测系统的透明度和可信度，为区块链安全提供了实用解决方案。

Abstract: Hackers may create malicious solidity programs and deploy it in the Ethereum block chain. These malicious smart contracts try to attack legitimate programs by exploiting its vulnerabilities such as reentrancy, tx.origin attack, bad randomness, deligatecall and so on. This may lead to drain of the funds, denial of service and so on . Hence, it is necessary to identify and prevent the malicious smart contract before deploying it into the blockchain. In this paper, we propose an ML based malicious smart contract detection mechanism by analyzing the EVM opcodes. After balancing the opcode frequency dataset with SMOTE algorithm, we transformed opcode frequencies to the binary values (0,1) using an entropy based supervised binning method. Then, an explainable AI model is trained with the proposed binary opcode based features. From the implementations, we found that the proposed mechanism can detect 99% of malicious smart contracts with a false positive rate of only 0.01. Finally, we incorporated LIME algorithm in our classifier to justify its predictions. We found that, LIME algorithm can explain why a particular smart contract app is declared as malicious by our ML classifier based on the binary value of EVM opcodes.

</details>


### [21] [Democratizing ML for Enterprise Security: A Self-Sustained Attack Detection Framework](https://arxiv.org/abs/2512.08802)
*Sadegh Momeni,Ge Zhang,Birkett Huber,Hamza Harkous,Sam Lipton,Benoit Seguin,Yanis Pavlidis*

Main category: cs.CR

TL;DR: 提出两阶段混合框架：第一阶段用宽松YARA规则进行粗粒度过滤（高召回率），第二阶段用ML分类器过滤误报，结合Simula合成数据生成和主动学习持续优化。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习在安全领域有进展，但安全运营中心仍普遍使用基于规则的检测，因为ML方案资源密集且存在技能差距。传统规则方法虽高效，但僵化导致高误报/漏报且需要持续人工维护。

Method: 两阶段混合框架：1) 宽松YARA规则进行粗粒度过滤（优化高召回率）；2) ML分类器过滤第一阶段输出的误报。使用Simula无种子合成数据生成框架解决数据稀缺问题，通过主动学习持续反馈机制自适应调整模型。

Result: 在数万台系统的生产环境中长期测试，处理每日2500亿原始日志事件，通过过滤和ML推理减少到每日少量待调查工单。长期实验显示主动学习使模型精度随时间持续提升。

Conclusion: 该框架提供自持续、低开销、低维护的解决方案，使安全专业人员能够作为专家"教师"指导模型学习，民主化ML威胁检测。

Abstract: Despite advancements in machine learning for security, rule-based detection remains prevalent in Security Operations Centers due to the resource intensiveness and skill gap associated with ML solutions. While traditional rule-based methods offer efficiency, their rigidity leads to high false positives or negatives and requires continuous manual maintenance. This paper proposes a novel, two-stage hybrid framework to democratize ML-based threat detection. The first stage employs intentionally loose YARA rules for coarse-grained filtering, optimized for high recall. The second stage utilizes an ML classifier to filter out false positives from the first stage's output. To overcome data scarcity, the system leverages Simula, a seedless synthetic data generation framework, enabling security analysts to create high-quality training datasets without extensive data science expertise or pre-labeled examples. A continuous feedback loop incorporates real-time investigation results to adaptively tune the ML model, preventing rule degradation.
  This proposed model with active learning has been rigorously tested for a prolonged time in a production environment spanning tens of thousands of systems. The system handles initial raw log volumes often reaching 250 billion events per day, significantly reducing them through filtering and ML inference to a handful of daily tickets for human investigation. Live experiments over an extended timeline demonstrate a general improvement in the model's precision over time due to the active learning feature. This approach offers a self-sustained, low-overhead, and low-maintenance solution, allowing security professionals to guide model learning as expert ``teachers''.

</details>


### [22] [PrivTune: Efficient and Privacy-Preserving Fine-Tuning of Large Language Models via Device-Cloud Collaboration](https://arxiv.org/abs/2512.08809)
*Yi Liu,Weixiang Han,Chengjun Cai,Xingliang Yuan,Cong Wang*

Main category: cs.CR

TL;DR: PrivTune：基于Split Learning的高效隐私保护微调框架，通过向token表示注入优化噪声来抵御推理攻击，在保持模型性能的同时显著降低攻击成功率


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型即服务的兴起，用户通过上传私有数据集进行模型微调，但存在敏感数据泄露风险。现有基于差分隐私的方法难以平衡隐私保护与模型效用，容易遭受推理攻击或导致性能下降。

Method: 提出PrivTune框架，基于Split Learning在底部模型向token表示注入优化噪声，使每个token类似于n跳间接邻居。将问题形式化为优化问题，计算最优噪声向量，调整dχ-隐私噪声分布的参数，并根据token重要性缩放噪声以最小化失真。

Result: 在五个数据集（涵盖分类和生成任务）上测试，针对三种嵌入反演和三种属性推理攻击，使用RoBERTa在Stanford Sentiment Treebank数据集上，PrivTune将攻击成功率降至10%，效用性能仅下降3.33%，优于现有基线方法。

Conclusion: PrivTune通过创新的噪声注入机制有效平衡了隐私保护与模型性能，为语言模型即服务中的隐私保护微调提供了实用解决方案。

Abstract: With the rise of large language models, service providers offer language models as a service, enabling users to fine-tune customized models via uploaded private datasets. However, this raises concerns about sensitive data leakage. Prior methods, relying on differential privacy within device-cloud collaboration frameworks, struggle to balance privacy and utility, exposing users to inference attacks or degrading fine-tuning performance. To address this, we propose PrivTune, an efficient and privacy-preserving fine-tuning framework via Split Learning (SL). The key idea of PrivTune is to inject crafted noise into token representations from the SL bottom model, making each token resemble the $n$-hop indirect neighbors. PrivTune formulates this as an optimization problem to compute the optimal noise vector, aligning with defense-utility goals. On this basis, it then adjusts the parameters (i.e., mean) of the $d_χ$-Privacy noise distribution to align with the optimization direction and scales the noise according to token importance to minimize distortion. Experiments on five datasets (covering both classification and generation tasks) against three embedding inversion and three attribute inference attacks show that, using RoBERTa on the Stanford Sentiment Treebank dataset, PrivTune reduces the attack success rate to 10% with only a 3.33% drop in utility performance, outperforming state-of-the-art baselines.

</details>


### [23] [Secure and Privacy-Preserving Federated Learning for Next-Generation Underground Mine Safety](https://arxiv.org/abs/2512.08862)
*Mohamed Elmahallawy,Sanjay Madria,Samuel Frimpong*

Main category: cs.CR

TL;DR: FedMining：针对地下采矿的隐私保护联邦学习框架，通过去中心化功能加密和平衡聚合机制解决隐私安全和数据异构问题


<details>
  <summary>Details</summary>
Motivation: 地下采矿依赖传感器网络监测安全参数，但集中式ML训练存在隐私安全风险。联邦学习虽能实现去中心化训练，但在采矿环境中面临模型更新窃听攻击和非独立同分布数据分布的挑战

Method: 提出FedMining框架，包含两个核心创新：1) 去中心化功能加密方案，保持本地模型加密状态，防止未授权访问和推理攻击；2) 平衡聚合机制，缓解数据异构性并提升收敛性

Result: 在真实采矿数据集上的评估表明，FedMining能够在保护隐私的同时保持高模型精度，实现快速收敛，并减少通信和计算开销

Conclusion: FedMining兼具安全性和实用性，适用于实时地下安全监测，为采矿行业提供隐私保护的联邦学习解决方案

Abstract: Underground mining operations depend on sensor networks to monitor critical parameters such as temperature, gas concentration, and miner movement, enabling timely hazard detection and safety decisions. However, transmitting raw sensor data to a centralized server for machine learning (ML) model training raises serious privacy and security concerns. Federated Learning (FL) offers a promising alternative by enabling decentralized model training without exposing sensitive local data. Yet, applying FL in underground mining presents unique challenges: (i) Adversaries may eavesdrop on shared model updates to launch model inversion or membership inference attacks, compromising data privacy and operational safety; (ii) Non-IID data distributions across mines and sensor noise can hinder model convergence. To address these issues, we propose FedMining--a privacy-preserving FL framework tailored for underground mining. FedMining introduces two core innovations: (1) a Decentralized Functional Encryption (DFE) scheme that keeps local models encrypted, thwarting unauthorized access and inference attacks; and (2) a balancing aggregation mechanism to mitigate data heterogeneity and enhance convergence. Evaluations on real-world mining datasets demonstrate FedMining's ability to safeguard privacy while maintaining high model accuracy and achieving rapid convergence with reduced communication and computation overhead. These advantages make FedMining both secure and practical for real-time underground safety monitoring.

</details>


### [24] [Decentralized Trust for Space AI: Blockchain-Based Federated Learning Across Multi-Vendor LEO Satellite Networks](https://arxiv.org/abs/2512.08882)
*Mohamed Elmahallawy,Asma Jodeiri Akbarfam*

Main category: cs.CR

TL;DR: OrbitChain是一个基于区块链的框架，用于在低地球轨道卫星网络中实现可信的多供应商协作，通过将共识卸载到高空平台来确保模型更新的透明可审计性，防止恶意更新影响联邦学习模型聚合。


<details>
  <summary>Details</summary>
Motivation: 随着空间AI应用的兴起，联邦卫星学习面临间歇性连接导致的收敛缓慢问题，以及跨卫星星座可能出现的偏见或伪造更新等信任挑战，特别是在卫星间或卫星-地面通信链路可能遭受网络攻击的情况下。

Method: 提出OrbitChain框架：1) 将共识卸载到计算能力更强的高空平台；2) 确保不同轨道、不同供应商模型更新的透明可审计来源；3) 防止被操纵或不完整的贡献影响全局联邦学习模型聚合。

Result: OrbitChain减少了计算和通信开销，同时提高了隐私性、安全性和全局模型准确性。其许可的权威证明账本以亚秒级延迟（0.16s、0.26s、0.35s对应不同法定人数）完成超过1000个区块。在真实卫星数据集上，相比单供应商方案，收敛时间最多减少30小时。

Conclusion: OrbitChain通过区块链技术有效解决了联邦卫星学习中的信任和收敛问题，为实时多供应商学习提供了可行解决方案，展示了在低地球轨道网络中实现可信协作的实际效果。

Abstract: The rise of space AI is reshaping government and industry through applications such as disaster detection, border surveillance, and climate monitoring, powered by massive data from commercial and governmental low Earth orbit (LEO) satellites. Federated satellite learning (FSL) enables joint model training without sharing raw data, but suffers from slow convergence due to intermittent connectivity and introduces critical trust challenges--where biased or falsified updates can arise across satellite constellations, including those injected through cyberattacks on inter-satellite or satellite-ground communication links. We propose OrbitChain, a blockchain-backed framework that empowers trustworthy multi-vendor collaboration in LEO networks. OrbitChain (i) offloads consensus to high-altitude platforms (HAPs) with greater computational capacity, (ii) ensures transparent, auditable provenance of model updates from different orbits owned by different vendors, and (iii) prevents manipulated or incomplete contributions from affecting global FSL model aggregation. Extensive simulations show that OrbitChain reduces computational and communication overhead while improving privacy, security, and global model accuracy. Its permissioned proof-of-authority ledger finalizes over 1000 blocks with sub-second latency (0.16,s, 0.26,s, 0.35,s for 1-of-5, 3-of-5, and 5-of-5 quorums). Moreover, OrbitChain reduces convergence time by up to 30 hours on real satellite datasets compared to single-vendor, demonstrating its effectiveness for real-time, multi-vendor learning. Our code is available at https://github.com/wsu-cyber-security-lab-ai/OrbitChain.git

</details>


### [25] [Improved Pseudorandom Codes from Permuted Puzzles](https://arxiv.org/abs/2512.08918)
*Miranda Christ,Noah Golowich,Sam Gunn,Ankur Moitra,Daniel Wichs*

Main category: cs.CR

TL;DR: 本文提出了首个同时具备次指数伪随机性、二进制字母表上最坏情况编辑鲁棒性、以及检测密钥泄露鲁棒性的伪随机码（PRC）构造，解决了先前PRC构造的多个缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有伪随机码（PRC）构造存在三个主要问题：1）易受拟多项式时间区分攻击；2）缺乏对常数大小字母表上编辑的鲁棒性；3）无法抵抗知道检测密钥的敌手。这些缺陷限制了PRC在实际AI水印应用中的实用性。

Method: 基于新形式化的"置换码猜想"构建PRC，该猜想认为置换后的噪声码字分布是伪随机的。作者证明该猜想可由先前用于构造双高效私有信息检索的置换谜题猜想推导，并通过证明其能抵抗包括只读分支程序在内的广泛简单区分器来提供证据支持。

Result: 成功构造了首个同时具备：1）合理的次指数伪随机性安全性；2）二进制字母表上最坏情况编辑鲁棒性；3）即使计算无界且拥有检测密钥的敌手也无法破坏的鲁棒性的PRC。

Conclusion: 本文提出的PRC构造解决了先前工作的主要缺陷，为AI生成内容的水印提供了更强的理论基础和实际可行性，其安全性基于新的置换码猜想，并提供了对该猜想的证据支持。

Abstract: Watermarks are an essential tool for identifying AI-generated content. Recently, Christ and Gunn (CRYPTO '24) introduced pseudorandom error-correcting codes (PRCs), which are equivalent to watermarks with strong robustness and quality guarantees. A PRC is a pseudorandom encryption scheme whose decryption algorithm tolerates a high rate of errors. Pseudorandomness ensures quality preservation of the watermark, and error tolerance of decryption translates to the watermark's ability to withstand modification of the content.
  In the short time since the introduction of PRCs, several works (NeurIPS '24, RANDOM '25, STOC '25) have proposed new constructions. Curiously, all of these constructions are vulnerable to quasipolynomial-time distinguishing attacks. Furthermore, all lack robustness to edits over a constant-sized alphabet, which is necessary for a meaningfully robust LLM watermark. Lastly, they lack robustness to adversaries who know the watermarking detection key. Until now, it was not clear whether any of these properties was achievable individually, let alone together.
  We construct pseudorandom codes that achieve all of the above: plausible subexponential pseudorandomness security, robustness to worst-case edits over a binary alphabet, and robustness against even computationally unbounded adversaries that have the detection key. Pseudorandomness rests on a new assumption that we formalize, the permuted codes conjecture, which states that a distribution of permuted noisy codewords is pseudorandom. We show that this conjecture is implied by the permuted puzzles conjecture used previously to construct doubly efficient private information retrieval. To give further evidence, we show that the conjecture holds against a broad class of simple distinguishers, including read-once branching programs.

</details>
