<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 18]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Quantum-Classical Hybrid Encryption Framework Based on Simulated BB84 and AES-256: Design and Experimental Evaluation](https://arxiv.org/abs/2511.02836)
*Hector E Mozo*

Main category: cs.CR

TL;DR: 提出了一种结合量子密钥分发（模拟BB84协议）与AES-256加密的混合加密框架，通过量子原理生成密钥，使用经典密码学保护数据，并包含完整性验证机制。


<details>
  <summary>Details</summary>
Motivation: 为应对量子计算威胁，开发量子感知的网络安全系统，结合量子密钥分发和经典加密的优势，确保在量子能力对手存在下的安全性。

Method: 使用Python实现模块化架构，模拟量子密钥交换、AES-256加密和安全打包，引入HMAC验证和可选的后量子数字签名机制。

Result: 通过多种攻击场景（密钥篡改、HMAC失败、文件损坏）的视觉测试，验证了该方法的有效性和鲁棒性。

Conclusion: 该解决方案为量子感知网络安全系统提供了实用的基础框架。

Abstract: This paper presents the design, implementation, and evaluation of a hybrid
encryption framework that combines quantum key distribution, specifically a
simulated BB84 protocol, with AES-256 encryption. The system enables secure
file encryption by leveraging quantum principles for key generation and
classical cryptography for data protection. It introduces integrity validation
mechanisms, including HMAC verification and optional post-quantum digital
signatures, ensuring robustness even in the presence of quantum-capable
adversaries. The entire architecture is implemented in Python, with modular
components simulating quantum key exchange, encryption, and secure packaging.
Experimental results include visual testing of various attack scenarios, such
as key tampering, HMAC failure, and file corruption, demonstrating the
effectiveness and resilience of the approach. The proposed solution serves as a
practical foundation for quantum-aware cybersecurity systems.

</details>


### [2] [AI Agents with Decentralized Identifiers and Verifiable Credentials](https://arxiv.org/abs/2511.02841)
*Sandro Rodriguez Garzon,Awid Vaziry,Enis Mert Kuzu,Dennis Enrique Gehrmann,Buse Varkan,Alexander Gaballa,Axel Küpper*

Main category: cs.CR

TL;DR: 提出一个基于自控数字身份和可验证凭证的多智能体信任建立框架，解决LLM智能体在跨域对话中缺乏自动信任建立机制的问题


<details>
  <summary>Details</summary>
Motivation: LLM智能体在对话开始时缺乏自动建立细粒度信任的技术手段，而跨域操作需要自主互操作的信任建立机制

Method: 为每个智能体配备自控数字身份，结合账本锚定的去中心化标识符(DID)和第三方颁发的可验证凭证(VC)，通过DID认证和VC交换建立跨域信任

Result: 原型系统验证了技术可行性，但也揭示了当LLM单独控制安全程序时的局限性

Conclusion: 基于DID和VC的数字身份框架为智能体间跨域信任建立提供了可行方案，但需要进一步解决LLM单独控制安全程序的问题

Abstract: LLM-based AI agents still lack the technical means to automatically build
nuanced and differentiated trust in other agents at the beginning of an
agent-to-agent dialogue. But autonomous and interoperable trust establishing
becomes a fundamental prerequisite once agents start to operate beyond isolated
environments and engage in dialogues across individual or organizational
boundaries. A promising way to fill this gap in Agentic AI is to equip agents
with long-lived digital identities and introduce tamper-proof and flexible
identity-bound attestations of agents, provisioned by commonly trusted third
parties and designed for cross-domain verifiability. This article presents a
conceptual framework and a prototypical multi-agent system, where each agent is
endowed with a self-sovereign digital identity. It combines a unique and
ledger-anchored Decentralized Identifier (DID) of an agent with a set of
third-party issued Verifiable Credentials (VCs). This enables agents at the
start of a dialog to prove ownership of their self-controlled DIDs for
authentication purposes and to establish various cross-domain trust
relationships through the spontaneous exchange of their self-hosted DID-bound
VCs. A comprehensive evaluation of the prototypical implementation demonstrates
technical feasibility but also reveals limitations once an agent's LLM is in
sole charge to control the respective security procedures.

</details>


### [3] [Proof-of-Spiking-Neurons(PoSN): Neuromorphic Consensus for Next-Generation Blockchains](https://arxiv.org/abs/2511.02868)
*M. Z. Haider,M. U Ghouri,Tayyaba Noreen,M. Salman*

Main category: cs.CR

TL;DR: 提出了一种基于脉冲神经网络的神经形态共识协议PoSN，通过将交易编码为脉冲序列、竞争性放电动态选举领导者、神经同步确认区块，实现了并行事件驱动的低能耗共识。


<details>
  <summary>Details</summary>
Motivation: 区块链系统面临可扩展性、延迟和能源效率的持续挑战，现有共识协议如PoW和PoS要么消耗过多资源，要么存在中心化风险。

Method: 在神经形态平台上实现混合系统架构，使用Nengo和PyNN等仿真框架，将交易编码为脉冲序列，通过竞争性放电动态选举领导者，利用神经同步最终确认区块。

Result: 实验结果显示在能源效率、吞吐量和收敛性方面相比PoB和PoR有显著提升。

Conclusion: PoSN为可持续、自适应的区块链奠定了基础，适用于物联网、边缘计算和大规模分布式系统。

Abstract: Blockchain systems face persistent challenges of scalability, latency, and
energy inefficiency. Existing consensus protocols such as Proof-of-Work (PoW)
and Proof-of-Stake (PoS) either consume excessive resources or risk
centralization. This paper proposes \textit{Proof-of-Spiking-Neurons (PoSN)}, a
neuromorphic consensus protocol inspired by spiking neural networks. PoSN
encodes transactions as spike trains, elects leaders through competitive firing
dynamics, and finalizes blocks via neural synchronization, enabling parallel
and event-driven consensus with minimal energy overhead. A hybrid system
architecture is implemented on neuromorphic platforms, supported by simulation
frameworks such as Nengo and PyNN. Experimental results show significant gains
in energy efficiency, throughput, and convergence compared to PoB and PoR. PoSN
establishes a foundation for sustainable, adaptive blockchains suitable for
IoT, edge, and large-scale distributed systems.

</details>


### [4] [Designing Proportionate Cybersecurity Frameworks for European Micro-Enterprises: Lessons from the Squad 2025 Case](https://arxiv.org/abs/2511.02898)
*Roberto Garrone*

Main category: cs.CR

TL;DR: 本文分析了欧盟针对微型和小型企业（SMEs）的网络安全意识政策框架，提出了一个适合微型企业采用的七维预防模型，强调提高网络安全意识是增强网络犯罪敏感性和促进保护行为的最有效短期手段。


<details>
  <summary>Details</summary>
Motivation: 欧洲大多数企业是微型和小型企业，但它们对网络威胁高度脆弱。作者参与欧盟Squad 2025团队，旨在为资源有限的微型企业设计适合的网络安全治理原则。

Method: 作者参与Squad 2025团队并最初提出了七步预防结构，后在项目中协作完善。该框架基于提高网络安全意识的设计前提，结合ENISA指南、ISO 27005和NIS2指令等背景重构概念架构。

Result: 提出了一个通用的七维预防模型，适合微型企业采用，并讨论了政策转移、意识培训和成熟度评估的影响。

Conclusion: 提高微型和小型企业的网络安全意识是增强其对网络犯罪敏感性和促进保护行为的最有效短期策略，七维预防模型为资源有限的企业提供了可行的网络安全治理框架。

Abstract: Micro and small enterprises (SMEs) account for most European businesses yet
remain highly vulnerable to cyber threats. This paper analyses the design logic
of a recent European policy initiative -- the Squad 2025 Playbook on
Cybersecurity Awareness for Micro-SMEs -- to extract general principles for
proportionate, resource-aware cybersecurity governance. The author participated
in the Squad 2025 team and originally proposed the seven-step preventive
structure that later shaped the Playbook's design, subsequently refined
collaboratively within the project. The framework was guided by the author's
design premise that raising cybersecurity awareness among micro- and
small-enterprise actors represents the most efficient short-term lever for
increasing sensitivity to cybercrime and promoting protective behaviours.
Without reproducing any proprietary material, the paper reconstructs the
conceptual architecture of that approach within the broader context of ENISA
guidance, ISO 27005, and the NIS2 Directive. It proposes a generic
seven-dimension preventive model suitable for micro-enterprise adoption and
discusses implications for policy transfer, awareness training, and maturity
assessment.

</details>


### [5] [Lightweight Session-Key Rekeying Framework for Secure IoT-Edge Communication](https://arxiv.org/abs/2511.02924)
*Haranath Rakshit,Rajkumar Bhandari,Subhasis Banerjee*

Main category: cs.CR

TL;DR: DSEKP是一种轻量级会话密钥重新协商协议，通过HMAC-KDF生成每会话AES-GCM密钥，在保持高效的同时提供前向安全性和重放攻击保护。


<details>
  <summary>Details</summary>
Motivation: 物联网设备需要轻量级安全机制，传统预共享密钥存在静态密钥重用、重放攻击和缺乏前向安全性等问题。

Method: 使用HMAC-SHA256基于密钥派生函数生成每会话密钥，通过单次init-ack交换进行HMAC认证，在ESP32和Raspberry Pi上通过MQTT实现。

Result: 与静态PSK相比，吞吐量和可靠性几乎相同，平均延迟增加27%，有效载荷大小增加10%，但实现了每会话前向安全性和内置重放保护。

Conclusion: 动态对称重新协商可显著增强物联网-边缘链路安全性，计算和带宽成本最小，为从静态PSK迁移提供了实用路径。

Abstract: The proliferation of Internet of Things (IoT) networks demands security
mechanisms that protect constrained devices without the computational cost of
public-key cryptography. Conventional Pre-Shared Key (PSK) encryption, while
efficient, remains vulnerable due to static key reuse, replay attacks, and the
lack of forward secrecy. This paper presents the Dynamic Session Enhanced Key
Protocol (DSEKP) - a lightweight session-key rekeying framework, a fully
symmetric extension to PSK that derives per-session AES-GCM keys using the
HMAC-based Key Derivation Function (HKDF-SHA256) and authenticates session
establishment through an HMAC proof in a single init-ack exchange. DSEKP was
implemented on an ESP32 IoT sensor node and a Raspberry Pi 5 edge server
communicating through a Mosquitto MQTT broker, and benchmarked against a static
PSK baseline over more than 6,500 encrypted packets per configuration. The
results demonstrate nearly identical throughput and reliability, with moderate
overhead - mean latency increased by 27% and payload size by 10% - while
delivering per-session forward secrecy and built-in replay protection. These
findings confirm that dynamic symmetric rekeying can substantially strengthen
IoT-Edge links with minimal computational and bandwidth cost, offering a
practical migration path from static PSK to session-aware, scalable, and
reproducible IoT security.

</details>


### [6] [PrivyWave: Privacy-Aware Wireless Sensing of Heartbeat](https://arxiv.org/abs/2511.02993)
*Yixuan Gao,Tanvir Ahmed,Zekun Chang,Thijs Roumen,Rajalakshmi Nandakumar*

Main category: cs.CR

TL;DR: PrivyWave是一个基于密钥的物理混淆系统，通过生成加密控制的伪心跳信号来保护无线感知隐私，允许授权设备准确监测而阻止未授权设备。


<details>
  <summary>Details</summary>
Motivation: 现有隐私解决方案要么完全阻止所有感知系统（丧失实用性），要么在数据收集后操作（无法实现选择性访问），需要一种能在物理层实现选择性隐私保护的方法。

Method: 使用密钥控制的物理混淆技术，生成加密频率的伪心跳信号。未授权传感器接收真实和伪信号的混合，无法区分；授权传感器使用密钥过滤伪信号恢复准确测量。

Result: 评估显示：毫米波雷达下，未授权传感器平均绝对误差21.3 BPM，授权传感器5.8 BPM；声学感知下，未授权误差42.0 BPM，授权传感器9.7 BPM。系统在不同距离、方向和环境下均表现稳健。

Conclusion: 物理层混淆是实现普遍健康监测中选择性隐私保护的可行方法，支持多种感知模式且无需针对每种模式定制。

Abstract: Wireless sensing technologies can now detect heartbeats using radio frequency
and acoustic signals, raising significant privacy concerns. Existing privacy
solutions either protect from all sensing systems indiscriminately preventing
any utility or operate post-data collection, failing to enable selective access
where authorized devices can monitor while unauthorized ones cannot. We present
a key-based physical obfuscation system, PrivyWave, that addresses this
challenge by generating controlled decoy heartbeat signals at
cryptographically-determined frequencies. Unauthorized sensors receive a
mixture of real and decoy signals that are indistinguishable without the secret
key, while authorized sensors use the key to filter out decoys and recover
accurate measurements. Our evaluation with 13 participants demonstrates
effective protection across both sensing modalities: for mmWave radar,
unauthorized sensors show 21.3 BPM mean absolute error while authorized sensors
maintain a much smaller 5.8 BPM; for acoustic sensing, unauthorized error
increases to 42.0 BPM while authorized sensors achieve 9.7 BPM. The system
operates across multiple sensing modalities without per-modality customization
and provides cryptographic obfuscation guarantees. Performance benchmarks show
robust protection across different distances (30-150 cm), orientations
(120{\deg} field of view), and diverse indoor environments, establishing
physical-layer obfuscation as a viable approach for selective privacy in
pervasive health monitoring.

</details>


### [7] [Exploratory Analysis of Cyberattack Patterns on E-Commerce Platforms Using Statistical Methods](https://arxiv.org/abs/2511.03020)
*Fatimo Adenike Adeniya*

Main category: cs.CR

TL;DR: 本研究提出了一个结合统计建模和机器学习的混合分析框架，用于检测和预测电子商务领域的网络攻击模式，发现节假日期间网络攻击更严重，CatBoost模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 电子商务平台面临的网络攻击日益复杂，威胁消费者信任和运营连续性，需要开发有效的检测和预测方法来应对这些威胁。

Method: 使用Verizon社区数据泄露数据集，应用Auto ARIMA进行时间序列预测和显著性检验（Mann-Whitney U检验），ANOVA分析季节性变化，以及集成机器学习模型（XGBoost、LightGBM、CatBoost）进行预测分类。

Result: 发现节假日购物活动期间网络攻击显著更严重（U = 2579981.5, p = 0.0121），涉及个人身份信息的泄露显示更高的威胁指标，CatBoost模型表现最佳（准确率85.29%，F1分数0.2254，ROC AUC 0.8247）。

Conclusion: 该框架独特地结合了季节性预测和可解释的集成学习，能够进行时间风险预测和泄露类型分类，为主动网络安全资源分配提供见解，并为未来实时威胁检测研究指明方向。

Abstract: Cyberattacks on e-commerce platforms have grown in sophistication,
threatening consumer trust and operational continuity. This research presents a
hybrid analytical framework that integrates statistical modelling and machine
learning for detecting and forecasting cyberattack patterns in the e-commerce
domain. Using the Verizon Community Data Breach (VCDB) dataset, the study
applies Auto ARIMA for temporal forecasting and significance testing, including
a Mann-Whitney U test (U = 2579981.5, p = 0.0121), which confirmed that holiday
shopping events experienced significantly more severe cyberattacks than
non-holiday periods. ANOVA was also used to examine seasonal variation in
threat severity, while ensemble machine learning models (XGBoost, LightGBM, and
CatBoost) were employed for predictive classification. Results reveal recurrent
attack spikes during high-risk periods such as Black Friday and holiday
seasons, with breaches involving Personally Identifiable Information (PII)
exhibiting elevated threat indicators. Among the models, CatBoost achieved the
highest performance (accuracy = 85.29%, F1 score = 0.2254, ROC AUC = 0.8247).
The framework uniquely combines seasonal forecasting with interpretable
ensemble learning, enabling temporal risk anticipation and breach-type
classification. Ethical considerations, including responsible use of sensitive
data and bias assessment, were incorporated. Despite class imbalance and
reliance on historical data, the study provides insights for proactive
cybersecurity resource allocation and outlines directions for future real-time
threat detection research.

</details>


### [8] [Bayesian Advantage of Re-Identification Attack in the Shuffle Model](https://arxiv.org/abs/2511.03213)
*Pengcheng Su,Haibo Cheng,Ping Wang*

Main category: cs.CR

TL;DR: 本文首次系统研究了洗牌模型下的贝叶斯优势，分析了攻击者在匿名化数据中重新识别用户消息的能力，并给出了洗牌差分隐私中贝叶斯攻击成功概率的上界。


<details>
  <summary>Details</summary>
Motivation: 洗牌模型在密码学和差分隐私中广泛应用，但缺乏对贝叶斯攻击者重新识别用户消息能力的系统性研究。本文旨在填补这一空白，量化攻击者在洗牌数据中的优势。

Method: 定义了贝叶斯最优攻击者的成功概率β_n(P,Q)，以及加性和乘性贝叶斯优势。推导了β_n(P,Q)的精确解析表达式和渐近特性，并在多个代表性场景中进行了评估。建立了加性贝叶斯优势与总变差距离之间的紧密互界。

Result: 获得了β_n(P,Q)的精确表达式和渐近特性，证明了加性贝叶斯优势与总变差距离之间的紧密关系。对于洗牌差分隐私，证明了攻击者成功重新识别任何目标用户消息的概率最多为e^ε/n。

Conclusion: 本文为洗牌模型中的贝叶斯重新识别攻击提供了首个系统性分析框架，建立了严格的理论界限，对理解洗牌机制的安全性具有重要意义。

Abstract: The shuffle model, which anonymizes data by randomly permuting user messages,
has been widely adopted in both cryptography and differential privacy. In this
work, we present the first systematic study of the Bayesian advantage in
re-identifying a user's message under the shuffle model. We begin with a basic
setting: one sample is drawn from a distribution $P$, and $n - 1$ samples are
drawn from a distribution $Q$, after which all $n$ samples are randomly
shuffled. We define $\beta_n(P, Q)$ as the success probability of a
Bayes-optimal adversary in identifying the sample from $P$, and define the
additive and multiplicative Bayesian advantages as $\mathsf{Adv}_n^{+}(P, Q) =
\beta_n(P,Q) - \frac{1}{n}$ and $\mathsf{Adv}_n^{\times}(P, Q) = n \cdot
\beta_n(P,Q)$, respectively. We derive exact analytical expressions and
asymptotic characterizations of $\beta_n(P, Q)$, along with evaluations in
several representative scenarios. Furthermore, we establish (nearly) tight
mutual bounds between the additive Bayesian advantage and the total variation
distance. Finally, we extend our analysis beyond the basic setting and present,
for the first time, an upper bound on the success probability of Bayesian
attacks in shuffle differential privacy. Specifically, when the outputs of $n$
users -- each processed through an $\varepsilon$-differentially private local
randomizer -- are shuffled, the probability that an attacker successfully
re-identifies any target user's message is at most $e^{\varepsilon}/n$.

</details>


### [9] [Smartphone User Fingerprinting on Wireless Traffic](https://arxiv.org/abs/2511.03229)
*Yong Huang,Zhibo Dong,Xiaoguang Yang,Dalong Zhang,Qingxian Wang,Zhihua Wang*

Main category: cs.CR

TL;DR: U-Print是一种新型攻击系统，能够从Wi-Fi MAC层帧中被动识别智能手机应用、操作和用户身份，通过分析用户偏好导致的Wi-Fi流量模式差异来实现用户推断。


<details>
  <summary>Details</summary>
Motivation: 由于无线媒介的开放性，智能手机用户面临隐私攻击风险。现有攻击只能识别移动应用及其操作，无法推断用户身份这一基本隐私信息。

Method: U-Print提取多级流量特征，使用定制时序卷积网络识别应用和操作生成用户行为序列，然后利用轮廓系数方法确定用户数量，应用k-means聚类分析和识别用户。

Result: 在三个真实环境中评估，U-Print用户推断总体准确率达98.4%，F1分数0.983。在封闭世界中正确识别96%的应用和操作，开放世界中超过86%。

Conclusion: U-Print能够有效从Wi-Fi流量中推断用户身份，揭示了无线通信中新的隐私威胁。

Abstract: Due to the openness of the wireless medium, smartphone users are susceptible
to user privacy attacks, where user privacy information is inferred from
encrypted Wi-Fi wireless traffic. Existing attacks are limited to recognizing
mobile apps and their actions and cannot infer the smartphone user identity, a
fundamental part of user privacy. To overcome this limitation, we propose
U-Print, a novel attack system that can passively recognize smartphone apps,
actions, and users from over-the-air MAC-layer frames. We observe that
smartphone users usually prefer different add-on apps and in-app actions,
yielding different changing patterns in Wi-Fi traffic. U-Print first extracts
multi-level traffic features and exploits customized temporal convolutional
networks to recognize smartphone apps and actions, thus producing users'
behavior sequences. Then, it leverages the silhouette coefficient method to
determine the number of users and applies the k-means clustering to profile and
identify smartphone users. We implement U-Print using a laptop with a Kali
dual-band wireless network card and evaluate it in three real-world
environments. U-Print achieves an overall accuracy of 98.4% and an F1 score of
0.983 for user inference. Moreover, it can correctly recognize up to 96% of
apps and actions in the closed world and more than 86% in the open world.

</details>


### [10] [Death by a Thousand Prompts: Open Model Vulnerability Analysis](https://arxiv.org/abs/2511.03247)
*Amy Chang,Nicholas Conley,Harish Santhanalakshmi Ganesan,Adam Swanda*

Main category: cs.CR

TL;DR: 测试了8个开源大语言模型的安全性和安全性状况，发现所有模型都存在普遍漏洞，多轮攻击成功率比单轮攻击高2-10倍，表明当前开源模型在扩展交互中无法维持安全防护。


<details>
  <summary>Details</summary>
Motivation: 评估开源权重模型在下游应用中的安全性和安全性状况，识别可能影响后续微调和部署的漏洞。

Method: 使用自动化对抗测试，测量每个模型对单轮和多轮提示注入和越狱攻击的抵抗力。

Result: 所有测试模型都存在普遍漏洞，多轮攻击成功率在25.86%到92.78%之间，比单轮基线高2-10倍。对齐策略和实验室优先级显著影响抵抗力。

Conclusion: 开源模型虽然对创新至关重要，但在没有分层安全控制的情况下部署会带来实际运营和道德风险。建议采用安全优先的设计理念和分层保护措施。

Abstract: Open-weight models provide researchers and developers with accessible
foundations for diverse downstream applications. We tested the safety and
security postures of eight open-weight large language models (LLMs) to identify
vulnerabilities that may impact subsequent fine-tuning and deployment. Using
automated adversarial testing, we measured each model's resilience against
single-turn and multi-turn prompt injection and jailbreak attacks. Our findings
reveal pervasive vulnerabilities across all tested models, with multi-turn
attacks achieving success rates between 25.86\% and 92.78\% -- representing a
$2\times$ to $10\times$ increase over single-turn baselines. These results
underscore a systemic inability of current open-weight models to maintain
safety guardrails across extended interactions. We assess that alignment
strategies and lab priorities significantly influence resilience:
capability-focused models such as Llama 3.3 and Qwen 3 demonstrate higher
multi-turn susceptibility, whereas safety-oriented designs such as Google Gemma
3 exhibit more balanced performance.
  The analysis concludes that open-weight models, while crucial for innovation,
pose tangible operational and ethical risks when deployed without layered
security controls. These findings are intended to inform practitioners and
developers of the potential risks and the value of professional AI security
solutions to mitigate exposure. Addressing multi-turn vulnerabilities is
essential to ensure the safe, reliable, and responsible deployment of
open-weight LLMs in enterprise and public domains. We recommend adopting a
security-first design philosophy and layered protections to ensure resilient
deployments of open-weight models.

</details>


### [11] [Auditing M-LLMs for Privacy Risks: A Synthetic Benchmark and Evaluation Framework](https://arxiv.org/abs/2511.03248)
*Junhao Li,Jiahao Chen,Zhou Feng,Chunyi Zhou*

Main category: cs.CR

TL;DR: PRISM是首个多模态、多维度、细粒度的合成数据集，用于评估多模态大语言模型在社交媒体上推断敏感个人属性的隐私风险。研究评估了6个领先的M-LLM，发现它们在准确性和效率上显著优于人类，凸显了隐私风险的威胁。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的多模态内容带来了显著的隐私风险：从看似日常的媒体内容中推断敏感个人属性。然而，缺乏基准测试和全面评估阻碍了相关研究。

Method: 提出PRISM数据集和高效评估框架。PRISM是一个大规模合成基准，包含12个敏感属性标签，通过复杂的LLM代理工作流程生成。还提出了多代理推理框架来增强评估能力。

Result: 评估了6个领先的M-LLM（Qwen、Gemini、GPT-4o、GLM、Doubao、Grok），发现这些模型在准确性和效率上显著优于人类表现。

Conclusion: 多模态大语言模型在推断个人敏感属性方面表现出强大的能力，这凸显了潜在的隐私风险，迫切需要开发强大的防御机制。

Abstract: Recent advances in multi-modal Large Language Models (M-LLMs) have
demonstrated a powerful ability to synthesize implicit information from
disparate sources, including images and text. These resourceful data from
social media also introduce a significant and underexplored privacy risk: the
inference of sensitive personal attributes from seemingly daily media content.
However, the lack of benchmarks and comprehensive evaluations of
state-of-the-art M-LLM capabilities hinders the research of private attribute
profiling on social media. Accordingly, we propose (1) PRISM, the first
multi-modal, multi-dimensional and fine-grained synthesized dataset
incorporating a comprehensive privacy landscape and dynamic user history; (2)
an Efficient evaluation framework that measures the cross-modal privacy
inference capabilities of advanced M-LLM. Specifically, PRISM is a large-scale
synthetic benchmark designed to evaluate cross-modal privacy risks. Its key
feature is 12 sensitive attribute labels across a diverse set of multi-modal
profiles, which enables targeted privacy analysis. These profiles are generated
via a sophisticated LLM agentic workflow, governed by a prior distribution to
ensure they realistically mimic social media users. Additionally, we propose a
Multi-Agent Inference Framework that leverages a pipeline of specialized LLMs
to enhance evaluation capabilities. We evaluate the inference capabilities of
six leading M-LLMs (Qwen, Gemini, GPT-4o, GLM, Doubao, and Grok) on PRISM. The
comparison with human performance reveals that these MLLMs significantly
outperform in accuracy and efficiency, highlighting the threat of potential
privacy risks and the urgent need for robust defenses.

</details>


### [12] [Let the Bees Find the Weak Spots: A Path Planning Perspective on Multi-Turn Jailbreak Attacks against LLMs](https://arxiv.org/abs/2511.03271)
*Yize Liu,Yunyun Hou,Aina Sui*

Main category: cs.CR

TL;DR: 提出基于动态加权图拓扑的ABC算法，用于高效多轮越狱攻击，显著降低查询次数并提高成功率


<details>
  <summary>Details</summary>
Motivation: 现有红队评估方法缺乏对攻击空间中成功对话轨迹的探索，且忽视攻击过程的高昂开销

Method: 将多轮攻击过程建模为路径规划问题，提出基于人工蜂群算法的协作搜索机制（雇佣蜂、观察蜂、侦察蜂）

Result: 在3个开源和2个专有模型上实现90%以上攻击成功率，GPT-3.5-Turbo达98%，平均仅需26次查询

Conclusion: ABC算法在保持高攻击成功率的同时显著降低红队评估开销，具有优越效率

Abstract: Large Language Models (LLMs) have been widely deployed across various
applications, yet their potential security and ethical risks have raised
increasing concerns. Existing research employs red teaming evaluations,
utilizing multi-turn jailbreaks to identify potential vulnerabilities in LLMs.
However, these approaches often lack exploration of successful dialogue
trajectories within the attack space, and they tend to overlook the
considerable overhead associated with the attack process. To address these
limitations, this paper first introduces a theoretical model based on
dynamically weighted graph topology, abstracting the multi-turn attack process
as a path planning problem. Based on this framework, we propose ABC, an
enhanced Artificial Bee Colony algorithm for multi-turn jailbreaks, featuring a
collaborative search mechanism with employed, onlooker, and scout bees. This
algorithm significantly improves the efficiency of optimal attack path search
while substantially reducing the average number of queries required. Empirical
evaluations on three open-source and two proprietary language models
demonstrate the effectiveness of our approach, achieving attack success rates
above 90\% across the board, with a peak of 98\% on GPT-3.5-Turbo, and
outperforming existing baselines. Furthermore, it achieves comparable success
with only 26 queries on average, significantly reducing red teaming overhead
and highlighting its superior efficiency.

</details>


### [13] [Two thousand years of the oracle problem. Insights from Ancient Delphi on the future of blockchain oracles](https://arxiv.org/abs/2511.03319)
*Giulio Caldarelli,Massimiliano Ornaghi*

Main category: cs.CR

TL;DR: 本文通过比较古代德尔斐神谕与现代区块链预言机的概念框架，分析预言机问题的共同挑战，并提出基于德尔斐神谕经验的区块链预言机可靠性改进策略。


<details>
  <summary>Details</summary>
Motivation: 解决预言机问题（信息真实性和无偏性的验证挑战）在古今都存在，特别是在区块链去中心化环境下更加复杂，需要从历史经验中寻找解决方案。

Method: 建立德尔斐神谕与区块链预言机的比较框架，利用区块链预言机分类法，对167个德尔斐查询进行词汇分析，研究问题类型与答案质量的关系。

Result: 揭示了古典预言机与计算预言机之间的共同特征，为两个领域提供了相互借鉴的分析框架。

Conclusion: 该研究为计算机科学提供了基于德尔斐经验的区块链预言机可靠性改进策略，同时为古典文献研究提供了可应用于其他古代神谕机制的解释和分类框架。

Abstract: The oracle problem refers to the inability of an agent to know if the
information coming from an oracle is authentic and unbiased. In ancient times,
philosophers and historians debated on how to evaluate, increase, and secure
the reliability of oracle predictions, particularly those from Delphi, which
pertained to matters of state. Today, we refer to data carriers for automatic
machines as oracles, but establishing a secure channel between these oracles
and the real world still represents a challenge. Despite numerous efforts, this
problem remains mostly unsolved, and the recent advent of blockchain oracles
has added a layer of complexity because of the decentralization of blockchains.
This paper conceptually connects Delphic and modern blockchain oracles,
developing a comparative framework. Leveraging blockchain oracle taxonomy,
lexical analysis is also performed on 167 Delphic queries to shed light on the
relationship between oracle answer quality and question type. The presented
framework aims first at revealing commonalities between classical and
computational oracles and then at enriching the oracle analysis within each
field. This study contributes to the computer science literature by proposing
strategies to improve the reliability of blockchain oracles based on insights
from Delphi and to classical literature by introducing a framework that can
also be applied to interpret and classify other ancient oracular mechanisms.

</details>


### [14] [LaMoS: Enabling Efficient Large Number Modular Multiplication through SRAM-based CiM Acceleration](https://arxiv.org/abs/2511.03341)
*Haomin Li,Fangxin Liu,Chenyang Guan,Zongwu Wang,Li Jiang,Haibing Guan*

Main category: cs.CR

TL;DR: 提出LaMoS，一种高效的基于SRAM的内存计算设计，用于大数模乘运算，具有高可扩展性和面积效率。


<details>
  <summary>Details</summary>
Motivation: 现有内存计算方案存在两个主要问题：1) 大多数工作专注于低比特宽模乘，不适用于ECC和RSA等主流密码算法；2) 针对大数模乘的方案依赖低效的内存逻辑操作，导致高扩展成本和延迟。

Method: 分析Barrett模乘方法，将高比特宽情况下的工作负载映射到SRAM内存计算宏单元；开发高效的内存计算架构和数据流来优化大数模乘；通过工作负载分组改进映射方案以提高可扩展性。

Result: 实验结果显示，LaMoS相比现有基于SRAM的内存计算设计实现了7.02倍的加速，并降低了高比特宽扩展成本。

Conclusion: LaMoS通过高效的内存计算设计成功解决了大数模乘的性能瓶颈，为同态加密和零知识证明等隐私计算应用提供了高性能解决方案。

Abstract: Barrett's algorithm is one of the most widely used methods for performing
modular multiplication, a critical nonlinear operation in modern privacy
computing techniques such as homomorphic encryption (HE) and zero-knowledge
proofs (ZKP). Since modular multiplication dominates the processing time in
these applications, computational complexity and memory limitations
significantly impact performance. Computing-in-Memory (CiM) is a promising
approach to tackle this problem. However, existing schemes currently suffer
from two main problems: 1) Most works focus on low bit-width modular
multiplication, which is inadequate for mainstream cryptographic algorithms
such as elliptic curve cryptography (ECC) and the RSA algorithm, both of which
require high bit-width operations; 2) Recent efforts targeting large number
modular multiplication rely on inefficient in-memory logic operations,
resulting in high scaling costs for larger bit-widths and increased latency. To
address these issues, we propose LaMoS, an efficient SRAM-based CiM design for
large-number modular multiplication, offering high scalability and area
efficiency. First, we analyze the Barrett's modular multiplication method and
map the workload onto SRAM CiM macros for high bit-width cases. Additionally,
we develop an efficient CiM architecture and dataflow to optimize large-number
modular multiplication. Finally, we refine the mapping scheme for better
scalability in high bit-width scenarios using workload grouping. Experimental
results show that LaMoS achieves a $7.02\times$ speedup and reduces high
bit-width scaling costs compared to existing SRAM-based CiM designs.

</details>


### [15] [Federated Anonymous Blocklisting across Service Providers and its Application to Group Messaging](https://arxiv.org/abs/2511.03486)
*David Soler,Carlos Dafonte,Manuel Fernández-Veiga,Ana Fernández Vilas,Francisco J. Nóvoa*

Main category: cs.CR

TL;DR: 提出了一种联邦匿名黑名单方案，用分布式领域替代集中式服务提供商，每个领域有自己的黑名单，用户认证时需要证明自己不在任何信任领域的黑名单中。


<details>
  <summary>Details</summary>
Motivation: 即时通讯中需要防止不良用户通过创建新假名来规避封禁，现有匿名黑名单方案存在性能依赖黑名单大小的问题。

Method: 设计联邦匿名黑名单架构，建立领域间信任关系，用户认证时提供不在信任领域黑名单中的证明。

Result: 实现了该方案，性能不依赖黑名单当前大小，也不需要处理黑名单新增条目，并成功集成到消息层安全协议中。

Conclusion: 联邦匿名黑名单方案为匿名环境下的消息群组提供了有效的用户管理机制，解决了现有方案的性能瓶颈问题。

Abstract: Instant messaging has become one of the most used methods of communication
online, which has attracted significant attention to its underlying
cryptographic protocols and security guarantees. Techniques to increase privacy
such as End-to-End Encryption and pseudonyms have been introduced. However,
online spaces such as messaging groups still require moderation to prevent
misbehaving users from participating in them, particularly in anonymous
contexts.. In Anonymous Blocklisting (AB) schemes, users must prove during
authentication that none of their previous pseudonyms has been blocked,
preventing misbehaving users from creating new pseudonyms. In this work we
propose an alternative \textit{Federated Anonymous Blocklisting} (FAB) in which
the centralised Service Provider is replaced by small distributed Realms, each
with its own blocklist. Realms can establish trust relationships between each
other, such that when users authenticate to a realm, they must prove that they
are not banned in any of its trusted realms. We provide an implementation of
our proposed scheme; unlike existing AB constructions, the performance of ours
does not depend on the current size of the blocklist nor requires processing
new additions to the blocklist. We also demonstrate its applicability to
real-world messaging groups by integrating our FAB scheme into the Messaging
Layer Security protocol.

</details>


### [16] [Security and Privacy Management of IoT Using Quantum Computing](https://arxiv.org/abs/2511.03538)
*Jaydip Sen*

Main category: cs.CR

TL;DR: 量子计算对物联网安全构成威胁，传统加密算法面临风险，需要开发量子安全替代方案。本章探讨了后量子密码学和量子方法在物联网中的应用。


<details>
  <summary>Details</summary>
Motivation: 量子计算的发展（如Shor和Grover算法）威胁传统加密算法（RSA、ECC、AES）的安全性，物联网系统需要量子安全解决方案。

Method: 分析后量子密码学家族（基于格、代码、哈希、多变量）和量子方法（量子密钥分发、量子随机数生成器）在资源受限物联网环境中的适用性。

Result: 提供了构建量子安全物联网系统的综合策略，包括密码学替代方案和物理安全保证方法。

Conclusion: 需要学术界、产业界和监管机构的合作，通过标准化和隐私管理确保下一代智能网络的量子安全韧性。

Abstract: The convergence of the Internet of Things (IoT) and quantum computing is
redefining the security paradigm of interconnected digital systems. Classical
cryptographic algorithms such as RSA, Elliptic Curve Cryptography (ECC), and
Advanced Encryption Standard (AES) have long provided the foundation for
securing IoT communication. However, the emergence of quantum algorithms such
as Shor's and Grover's threatens to render these techniques vulnerable,
necessitating the development of quantum-resilient alternatives. This chapter
examines the implications of quantum computing for IoT security and explores
strategies for building cryptographically robust systems in the post-quantum
era. It presents an overview of Post-Quantum Cryptographic (PQC) families,
including lattice-based, code-based, hash-based, and multivariate approaches,
analyzing their potential for deployment in resource-constrained IoT
environments. In addition, quantum-based methods such as Quantum Key
Distribution (QKD) and Quantum Random Number Generators (QRNGs) are discussed
for their ability to enhance confidentiality and privacy through physics-based
security guarantees. The chapter also highlights issues of privacy management,
regulatory compliance, and standardization, emphasizing the need for
collaborative efforts across academia, industry, and governance. Overall, it
provides a comprehensive perspective on security IoT ecosystems against quantum
threats and ensures resilience in the next generation of intelligent networks.

</details>


### [17] [Watermarking Large Language Models in Europe: Interpreting the AI Act in Light of Technology](https://arxiv.org/abs/2511.03641)
*Thomas Souverain*

Main category: cs.CR

TL;DR: 本文为应对欧盟AI法案对通用AI模型输出标记的要求，提出了一个评估LLM水印技术的框架，包括分类、评估和比较现有方法，发现目前尚无方法能完全满足所有四个标准。


<details>
  <summary>Details</summary>
Motivation: 欧盟AI法案要求通用AI模型提供者标记和检测其输出，但现有水印技术多样且快速发展，难以将法案的可靠性、互操作性、有效性和鲁棒性标准转化为具体可衡量的评估。

Method: 提出三方面贡献：(1)按LLM生命周期阶段对水印方法进行分类；(2)将欧盟标准映射到现有评估指标，并为互操作性提出三个规范性维度；(3)比较现有水印方法与欧盟标准。

Result: 比较显示目前尚无水印方法能同时满足所有四个欧盟标准，建议进一步研究嵌入LLM底层架构的水印技术。

Conclusion: 需要建立统一的评估框架来验证水印技术是否符合欧盟AI法案要求，并鼓励开发更集成的水印方法。

Abstract: To foster trustworthy Artificial Intelligence (AI) within the European Union,
the AI Act requires providers to mark and detect the outputs of their
general-purpose models. The Article 50 and Recital 133 call for marking methods
that are ''sufficiently reliable, interoperable, effective and robust''. Yet,
the rapidly evolving and heterogeneous landscape of watermarks for Large
Language Models (LLMs) makes it difficult to determine how these four standards
can be translated into concrete and measurable evaluations. Our paper addresses
this challenge, anchoring the normativity of European requirements in the
multiplicity of watermarking techniques. Introducing clear and distinct
concepts on LLM watermarking, our contribution is threefold. (1) Watermarking
Categorisation: We propose an accessible taxonomy of watermarking methods
according to the stage of the LLM lifecycle at which they are applied - before,
during, or after training, and during next-token distribution or sampling. (2)
Watermarking Evaluation: We interpret the EU AI Act's requirements by mapping
each criterion with state-of-the-art evaluations on robustness and
detectability of the watermark, and of quality of the LLM. Since
interoperability remains largely untheorised in LLM watermarking research, we
propose three normative dimensions to frame its assessment. (3) Watermarking
Comparison: We compare current watermarking methods for LLMs against the
operationalised European criteria and show that no approach yet satisfies all
four standards. Encouraged by emerging empirical tests, we recommend further
research into watermarking directly embedded within the low-level architecture
of LLMs.

</details>


### [18] [Whisper Leak: a side-channel attack on Large Language Models](https://arxiv.org/abs/2511.03675)
*Geoff McDonald,Jonathan Bar Or*

Main category: cs.CR

TL;DR: Whisper Leak是一种侧信道攻击，通过分析LLM流式响应中的数据包大小和时间模式，即使内容经过TLS加密，也能推断用户提示的主题。该攻击在28个主流LLM上效果显著，对敏感话题识别精度高达100%。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在医疗、法律等敏感领域的部署增多，隐私保护变得至关重要。尽管内容加密，但元数据泄露仍可能暴露用户对话主题。

Method: 通过分析流式响应中的数据包大小和时间模式，利用这些元数据模式进行主题分类攻击。

Result: 在28个主流LLM上实现近乎完美的分类效果（通常>98% AUPRC），在极端类别不平衡下仍保持高精度，对"洗钱"等敏感话题实现100%精确识别，恢复5-20%的目标对话。

Conclusion: 这种行业范围内的漏洞对处于网络监控下的用户构成重大风险。现有缓解措施（随机填充、令牌批处理、数据包注入）虽能降低攻击效果，但无法提供完全保护，需要LLM提供商解决元数据泄露问题。

Abstract: Large Language Models (LLMs) are increasingly deployed in sensitive domains
including healthcare, legal services, and confidential communications, where
privacy is paramount. This paper introduces Whisper Leak, a side-channel attack
that infers user prompt topics from encrypted LLM traffic by analyzing packet
size and timing patterns in streaming responses. Despite TLS encryption
protecting content, these metadata patterns leak sufficient information to
enable topic classification. We demonstrate the attack across 28 popular LLMs
from major providers, achieving near-perfect classification (often >98% AUPRC)
and high precision even at extreme class imbalance (10,000:1 noise-to-target
ratio). For many models, we achieve 100% precision in identifying sensitive
topics like "money laundering" while recovering 5-20% of target conversations.
This industry-wide vulnerability poses significant risks for users under
network surveillance by ISPs, governments, or local adversaries. We evaluate
three mitigation strategies - random padding, token batching, and packet
injection - finding that while each reduces attack effectiveness, none provides
complete protection. Through responsible disclosure, we have collaborated with
providers to implement initial countermeasures. Our findings underscore the
need for LLM providers to address metadata leakage as AI systems handle
increasingly sensitive information.

</details>
