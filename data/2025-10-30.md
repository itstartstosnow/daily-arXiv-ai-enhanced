<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 15]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Learning to Attack: Uncovering Privacy Risks in Sequential Data Releases](https://arxiv.org/abs/2510.24807)
*Ziyao Cui,Minxing Zhang,Jian Pei*

Main category: cs.CR

TL;DR: 本文研究了序列数据发布中的隐私风险，提出了一种结合隐马尔可夫模型和强化学习的双向推理攻击模型，证明即使单个数据发布满足隐私保护标准，时序相关性仍可能导致敏感信息泄露。


<details>
  <summary>Details</summary>
Motivation: 现实世界中许多系统采用序列或连续数据发布方式，而现有隐私保护研究主要关注单次数据发布。时序相关性可能让攻击者通过分析连续发布的数据推断出单次发布中隐藏的敏感信息。

Method: 提出了一种新颖的攻击模型，将隐马尔可夫模型与基于强化学习的双向推理机制相结合，使攻击者能够利用序列中前后观察值来推断私有信息。在轨迹数据背景下实例化该框架。

Result: 在Geolife、Porto Taxi和SynMob数据集上的实验表明，该模型始终优于将每次发布视为独立的基线方法，揭示了序列数据发布中固有的基本隐私风险。

Conclusion: 研究结果表明需要开发新的隐私保护框架，如时间感知差分隐私或序列数据混淆策略，以显式建模时序依赖性。

Abstract: Privacy concerns have become increasingly critical in modern AI and data
science applications, where sensitive information is collected, analyzed, and
shared across diverse domains such as healthcare, finance, and mobility. While
prior research has focused on protecting privacy in a single data release, many
real-world systems operate under sequential or continuous data publishing,
where the same or related data are released over time. Such sequential
disclosures introduce new vulnerabilities, as temporal correlations across
releases may enable adversaries to infer sensitive information that remains
hidden in any individual release. In this paper, we investigate whether an
attacker can compromise privacy in sequential data releases by exploiting
dependencies between consecutive publications, even when each individual
release satisfies standard privacy guarantees. To this end, we propose a novel
attack model that captures these sequential dependencies by integrating a
Hidden Markov Model with a reinforcement learning-based bi-directional
inference mechanism. This enables the attacker to leverage both earlier and
later observations in the sequence to infer private information. We instantiate
our framework in the context of trajectory data, demonstrating how an adversary
can recover sensitive locations from sequential mobility datasets. Extensive
experiments on Geolife, Porto Taxi, and SynMob datasets show that our model
consistently outperforms baseline approaches that treat each release
independently. The results reveal a fundamental privacy risk inherent to
sequential data publishing, where individually protected releases can
collectively leak sensitive information when analyzed temporally. These
findings underscore the need for new privacy-preserving frameworks that
explicitly model temporal dependencies, such as time-aware differential privacy
or sequential data obfuscation strategies.

</details>


### [2] [S3C2 Summit 2025-03: Industry Secure Supply Chain Summit](https://arxiv.org/abs/2510.24920)
*Elizabeth Lin,Jonah Ghebremichael,William Enck,Yasemin Acar,Michel Cukier,Alexandros Kapravelos,Christian Kastner,Laurie Williams*

Main category: cs.CR

TL;DR: 该论文报告了2025年3月6日举行的软件供应链安全峰会，汇集了来自17个组织的18名从业者，讨论了SBOM、合规性、恶意提交、构建基础设施、文化和LLM安全等六个关键主题。


<details>
  <summary>Details</summary>
Motivation: 软件供应链攻击呈指数级增长，威胁着从大型企业到开源开发者的互联网安全，需要行业和政府共同应对。

Method: 通过举办多方参与的峰会，组织六个主题讨论，使用预设问题引导对话，收集实践经验和挑战。

Result: 形成了各主题的讨论总结，识别了未解决的关键问题和挑战，为未来研究方向提供参考。

Conclusion: 峰会促进了跨行业经验分享和合作，明确了软件供应链安全面临的现实挑战，为后续研究奠定了基础。

Abstract: Software supply chains, while providing immense economic and software
development value, are only as strong as their weakest link. Over the past
several years, there has been an exponential increase in cyberattacks
specifically targeting vulnerable links in critical software supply chains.
These attacks disrupt the day-to-day functioning and threaten the security of
nearly everyone on the internet, from billion-dollar companies and government
agencies to hobbyist open-source developers. The ever-evolving threat of
software supply chain attacks has garnered interest from both the software
industry and US government in improving software supply chain security. On
Thursday, March 6th, 2025, four researchers from the NSF-backed Secure Software
Supply Chain Center (S3C2) conducted a Secure Software Supply Chain Summit with
a diverse set of 18 practitioners from 17 organizations. The goals of the
Summit were: (1) to enable sharing between participants from different
industries regarding practical experiences and challenges with software supply
chain security; (2) to help form new collaborations; and (3) to learn about the
challenges facing participants to inform our future research directions. The
summit consisted of discussions of six topics relevant to the government
agencies represented, including software bill of materials (SBOMs); compliance;
malicious commits; build infrastructure; culture; and large language models
(LLMs) and security. For each topic of discussion, we presented a list of
questions to participants to spark conversation. In this report, we provide a
summary of the summit. The open questions and challenges that remained after
each topic are listed at the end of each topic's section, and the initial
discussion questions for each topic are provided in the appendix.

</details>


### [3] [Hammering the Diagnosis: Rowhammer-Induced Stealthy Trojan Attacks on ViT-Based Medical Imaging](https://arxiv.org/abs/2510.24976)
*Banafsheh Saber Latibari,Najmeh Nazari,Hossein Sayadi,Houman Homayoun,Abhijit Mahalanobis*

Main category: cs.CR

TL;DR: 提出了一种名为Med-Hammer的新型威胁模型，结合Rowhammer硬件故障注入和神经木马攻击，破坏基于ViT的医学影像系统的完整性。


<details>
  <summary>Details</summary>
Motivation: ViT在医学影像分析中表现出色，但其对大型注意力驱动模型的依赖使其容易受到硬件级攻击。

Method: 通过Rowhammer诱导的恶意比特翻转触发植入的神经木马，导致医学扫描中的定向误分类或关键诊断（如肿瘤或病变）抑制。

Result: 在ISIC、脑肿瘤和MedMNIST等基准医学影像数据集上的实验显示，该攻击在MobileViT和SwinTransformer中分别达到约82.51%和92.56%的高攻击成功率，同时保持隐蔽性。

Conclusion: 研究揭示了医疗应用中硬件级故障与深度学习安全之间关键且未被充分探索的交集，强调了在模型架构和底层硬件平台上都需要强大防御的紧迫性。

Abstract: Vision Transformers (ViTs) have emerged as powerful architectures in medical
image analysis, excelling in tasks such as disease detection, segmentation, and
classification. However, their reliance on large, attention-driven models makes
them vulnerable to hardware-level attacks. In this paper, we propose a novel
threat model referred to as Med-Hammer that combines the Rowhammer hardware
fault injection with neural Trojan attacks to compromise the integrity of
ViT-based medical imaging systems. Specifically, we demonstrate how malicious
bit flips induced via Rowhammer can trigger implanted neural Trojans, leading
to targeted misclassification or suppression of critical diagnoses (e.g.,
tumors or lesions) in medical scans. Through extensive experiments on benchmark
medical imaging datasets such as ISIC, Brain Tumor, and MedMNIST, we show that
such attacks can remain stealthy while achieving high attack success rates
about 82.51% and 92.56% in MobileViT and SwinTransformer, respectively. We
further investigate how architectural properties, such as model sparsity,
attention weight distribution, and the number of features of the layer, impact
attack effectiveness. Our findings highlight a critical and underexplored
intersection between hardware-level faults and deep learning security in
healthcare applications, underscoring the urgent need for robust defenses
spanning both model architectures and underlying hardware platforms.

</details>


### [4] [FaRAccel: FPGA-Accelerated Defense Architecture for Efficient Bit-Flip Attack Resilience in Transformer Models](https://arxiv.org/abs/2510.24985)
*Najmeh Nazari,Banafsheh Saber Latibari,Elahe Hosseini,Fatemeh Movafagh,Chongzhou Fang,Hosein Mohammadi Makrani,Kevin Immanuel Gubbi,Abhijit Mahalanobis,Setareh Rafatirad,Hossein Sayadi,Houman Homayoun*

Main category: cs.CR

TL;DR: FaRAccel是一种针对Transformer模型的硬件加速器架构，通过FPGA实现，专门用于优化FaR（Forget and Rewire）防御方法，显著降低推理延迟并提高能效，同时保持对位翻转攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: FaR方法虽然能有效防御位翻转攻击，但会带来显著的性能和内存开销，主要原因是运行时激活路径修改和缺乏硬件级优化。

Method: 提出FaRAccel硬件加速器架构，集成可重构逻辑用于动态激活重路由，以及轻量级存储重连配置，实现低延迟推理。

Result: 在多个Transformer模型上评估显示，FaRAccel显著降低了FaR推理延迟并提高了能效，同时保持了原始FaR方法的鲁棒性增益。

Conclusion: 这是首个针对Transformer中位翻转攻击的硬件加速防御方案，有效弥合了算法鲁棒性与实际AI平台高效部署之间的差距。

Abstract: Forget and Rewire (FaR) methodology has demonstrated strong resilience
against Bit-Flip Attacks (BFAs) on Transformer-based models by obfuscating
critical parameters through dynamic rewiring of linear layers. However, the
application of FaR introduces non-negligible performance and memory overheads,
primarily due to the runtime modification of activation pathways and the lack
of hardware-level optimization. To overcome these limitations, we propose
FaRAccel, a novel hardware accelerator architecture implemented on FPGA,
specifically designed to offload and optimize FaR operations. FaRAccel
integrates reconfigurable logic for dynamic activation rerouting, and
lightweight storage of rewiring configurations, enabling low-latency inference
with minimal energy overhead. We evaluate FaRAccel across a suite of
Transformer models and demonstrate substantial reductions in FaR inference
latency and improvement in energy efficiency, while maintaining the robustness
gains of the original FaR methodology. To the best of our knowledge, this is
the first hardware-accelerated defense against BFAs in Transformers,
effectively bridging the gap between algorithmic resilience and efficient
deployment on real-world AI platforms.

</details>


### [5] [SLIP-SEC: Formalizing Secure Protocols for Model IP Protection](https://arxiv.org/abs/2510.24999)
*Racchit Jain,Satya Lokam,Yehonathan Refael,Adam Hakim,Lev Greenberg,Jay Tenenbaum*

Main category: cs.CR

TL;DR: SLIP是一个混合推理协议，通过在可信和不可信资源之间分割模型计算来保护LLM知识产权，提供信息论安全性和恶意攻击鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LLMs代表重要知识产权，部署在部分可信或不安全设备上存在模型被盗风险，需要设计具有可证明安全保证的推理协议。

Method: 基于权重矩阵的加法分解，结合掩码和概率验证技术构建安全推理协议，在可信和不可信资源间分割计算。

Result: 协议实现了对诚实但好奇对手的信息论安全性，并对恶意对手提供可忽略的可靠性错误鲁棒性。

Conclusion: SLIP提供了保护LLM知识产权的理论基础，包括精确定义、形式化协议和安全证明，与配套论文共同构建了混合推理安全的完整框架。

Abstract: Large Language Models (LLMs) represent valuable intellectual property (IP),
reflecting significant investments in training data, compute, and expertise.
Deploying these models on partially trusted or insecure devices introduces
substantial risk of model theft, making it essential to design inference
protocols with provable security guarantees.
  We present the formal framework and security foundations of SLIP, a hybrid
inference protocol that splits model computation between a trusted and an
untrusted resource. We define and analyze the key notions of model
decomposition and hybrid inference protocols, and introduce formal properties
including safety, correctness, efficiency, and t-soundness. We construct secure
inference protocols based on additive decompositions of weight matrices,
combined with masking and probabilistic verification techniques. We prove that
these protocols achieve information-theoretic security against
honest-but-curious adversaries, and provide robustness against malicious
adversaries with negligible soundness error.
  This paper focuses on the theoretical underpinnings of SLIP: precise
definitions, formal protocols, and proofs of security. Empirical validation and
decomposition heuristics appear in the companion SLIP paper. Together, the two
works provide a complete account of securing LLM IP via hybrid inference,
bridging both practice and theory.

</details>


### [6] [Secure Retrieval-Augmented Generation against Poisoning Attacks](https://arxiv.org/abs/2510.25025)
*Zirui Cheng,Jikai Sun,Anjun Gao,Yueyang Quan,Zhuqing Liu,Xiaohua Hu,Minghong Fang*

Main category: cs.CR

TL;DR: RAGuard是一个检测框架，通过扩展检索范围、分块困惑度过滤和文本相似性过滤来识别RAG系统中的数据中毒攻击，提高系统安全性。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成(RAG)系统虽然提升了LLM性能，但引入了数据中毒的安全风险，现有防御措施难以应对高级攻击。

Method: RAGuard采用三步法：扩展检索范围增加干净文本比例；分块困惑度过滤检测异常变化；文本相似性过滤标记高度相似文本。

Result: 在大规模数据集上的实验表明，RAGuard能有效检测和缓解中毒攻击，包括强自适应攻击。

Conclusion: RAGuard作为一种非参数方法，显著增强了RAG系统的安全性，能够可靠地防御数据中毒威胁。

Abstract: Large language models (LLMs) have transformed natural language processing
(NLP), enabling applications from content generation to decision support.
Retrieval-Augmented Generation (RAG) improves LLMs by incorporating external
knowledge but also introduces security risks, particularly from data poisoning,
where the attacker injects poisoned texts into the knowledge database to
manipulate system outputs. While various defenses have been proposed, they
often struggle against advanced attacks. To address this, we introduce RAGuard,
a detection framework designed to identify poisoned texts. RAGuard first
expands the retrieval scope to increase the proportion of clean texts, reducing
the likelihood of retrieving poisoned content. It then applies chunk-wise
perplexity filtering to detect abnormal variations and text similarity
filtering to flag highly similar texts. This non-parametric approach enhances
RAG security, and experiments on large-scale datasets demonstrate its
effectiveness in detecting and mitigating poisoning attacks, including strong
adaptive attacks.

</details>


### [7] [AgentCyTE: Leveraging Agentic AI to Generate Cybersecurity Training & Experimentation Scenarios](https://arxiv.org/abs/2510.25189)
*Ana M. Rodriguez,Jaime Acosta,Anantaa Kotal,Aritran Piplai*

Main category: cs.CR

TL;DR: AgentCyTE是一个结合LLM推理与确定性网络仿真的框架，用于生成可执行的网络威胁环境，通过反馈循环验证和增强场景的真实性与一致性。


<details>
  <summary>Details</summary>
Motivation: 网络安全研究和培训中设计真实且自适应的网络威胁场景仍需要大量人工努力，现有LLM生成方法往往无法通过验证或执行。

Method: 集成基于LLM的推理与确定性、模式约束的网络仿真，通过智能反馈循环观察场景结果、验证正确性，并迭代提升真实性和一致性。

Result: 该混合方法在保持LLM灵活性的同时强制执行结构有效性，实现了可扩展的数据驱动实验和可靠的威胁建模场景生成。

Conclusion: AgentCyTE框架能够为威胁建模和自适应网络安全培训提供可靠且可扩展的场景生成能力。

Abstract: Designing realistic and adaptive networked threat scenarios remains a core
challenge in cybersecurity research and training, still requiring substantial
manual effort. While large language models (LLMs) show promise for automated
synthesis, unconstrained generation often yields configurations that fail
validation or execution. We present AgentCyTE, a framework integrating
LLM-based reasoning with deterministic, schema-constrained network emulation to
generate and refine executable threat environments. Through an agentic feedback
loop, AgentCyTE observes scenario outcomes, validates correctness, and
iteratively enhances realism and consistency. This hybrid approach preserves
LLM flexibility while enforcing structural validity, enabling scalable,
data-driven experimentation and reliable scenario generation for threat
modeling and adaptive cybersecurity training. Our framework can be accessed at:
https://github.com/AnantaaKotal/AgentCyTE

</details>


### [8] [Is Protective DNS Blocking the Wild West?](https://arxiv.org/abs/2510.25352)
*David Plonka,Branden Palacio,Debbie Perouli*

Main category: cs.CR

TL;DR: 对研究教育网络中保护性DNS服务的性能进行被动测量研究，发现DNS阻止列表在命名、目标、透明度和来源方面混乱无序，难以比较，缺乏组织监督，在大规模运营中面临挑战和风险。


<details>
  <summary>Details</summary>
Motivation: 研究保护性DNS服务在研究教育网络中的实际表现，该网络服务于数百个成员机构，旨在了解现有DNS阻止列表的有效性和可操作性。

Method: 利用免费可用的DNS阻止列表（包含被视为威胁的域名），对一周内观察到的数亿用户真实DNS查询进行测试，分析哪些查询会被阻止。

Result: 发现DNS阻止列表在命名、目标、透明度和来源方面混乱无序，难以进行比较，这些保护性DNS的基础设施缺乏组织监督。

Conclusion: 保护性DNS的基础设施缺乏有序监督，在大规模运营中面临挑战和风险，需要更好的组织和管理。

Abstract: We perform a passive measurement study investigating how a Protective DNS
service might perform in a Research & Education Network serving hundreds of
member institutions. Utilizing freely-available DNS blocklists consisting of
domain names deemed to be threats, we test hundreds of millions of users' real
DNS queries, observed over a week's time, to find which answers would be
blocked because they involve domain names that are potential threats. We find
the blocklists disorderly regarding their names, goals, transparency, and
provenance making them quite difficult to compare. Consequently, these
Protective DNS underpinnings lack organized oversight, presenting challenges
and risks in operation at scale.

</details>


### [9] [From ECU to VSOC: UDS Security Monitoring Strategies](https://arxiv.org/abs/2510.25375)
*Ali Recai Yekta,Nicolas Loza,Jens Gramm,Michael Peter Schneider,Stefan Katzenbeisser*

Main category: cs.CR

TL;DR: 提出了针对汽车UDS协议的安全监控策略，包括车内日志记录和远程VSOC分析，能够检测广泛的UDS攻击向量，并评估了AUTOSAR标准在支持UDS攻击检测方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现代车辆复杂性和连接性的增加使其更容易受到网络攻击，特别是UDS协议作为车辆诊断的关键通信框架存在安全挑战。

Method: 通过制定安全事件日志记录要求、上下文数据收集以及开发检测策略来识别UDS攻击场景，并将这些策略应用于全面的UDS攻击技术分类。

Result: 检测方法覆盖了广泛的潜在攻击向量，同时识别出当前AUTOSAR标准化安全事件在支持UDS攻击检测方面存在不足。

Conclusion: 这项工作增强了对车辆安全监控的理解，并为开发汽车通信协议中稳健的网络安全措施提供了范例。

Abstract: Increasing complexity and connectivity of modern vehicles have heightened
their vulnerability to cyberattacks. This paper addresses security challenges
associated with the Unified Diagnostic Services (UDS) protocol, a critical
communication framework for vehicle diagnostics in the automotive industry. We
present security monitoring strategies for the UDS protocol that leverage
in-vehicle logging and remote analysis through a Vehicle Security Operations
Center (VSOC). Our approach involves specifying security event logging
requirements, contextual data collection, and the development of detection
strategies aimed at identifying UDS attack scenarios. By applying these
strategies to a comprehensive taxonomy of UDS attack techniques, we demonstrate
that our detection methods cover a wide range of potential attack vectors.
Furthermore, we assess the adequacy of current AUTOSAR standardized security
events in supporting UDS attack detection, identifying gaps in the current
standard. This work enhances the understanding of vehicle security monitoring
and provides an example for developing robust cybersecurity measures in
automotive communication protocols.

</details>


### [10] [An In-Depth Analysis of Cyber Attacks in Secured Platforms](https://arxiv.org/abs/2510.25470)
*Parick Ozoh,John K Omoniyi,Bukola Ibitoye*

Main category: cs.CR

TL;DR: 该论文调查了Android系统上恶意威胁检测的机器学习技术，比较了不同方法的性能，并分析了当前研究面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 全球恶意软件威胁增加，特别是Android系统上的加密型勒索软件，对移动通信和用户隐私构成严重威胁。

Method: 使用机器学习技术检测恶意威胁，对Android应用程序数据集进行分析，通过准确率等指标评估不同方法的性能。

Result: 研究发现大多数现有研究依赖用户反馈和评论，但这些可能包含虚假信息，机器学习方法需要大量数据来开发健壮的自动化反恶意软件系统。

Conclusion: 虽然机器学习在恶意威胁检测方面有潜力，但需要大量信息来开发专门化的自动化系统，当前方法在数据需求方面面临挑战。

Abstract: There is an increase in global malware threats. To address this, an
encryption-type ransomware has been introduced on the Android operating system.
The challenges associated with malicious threats in phone use have become a
pressing issue in mobile communication, disrupting user experiences and posing
significant privacy threats. This study surveys commonly used machine learning
techniques for detecting malicious threats in phones and examines their
performance. The majority of past research focuses on customer feedback and
reviews, with concerns that people might create false reviews to promote or
devalue products and services for personal gain. Hence, the development of
techniques for detecting malicious threats using machine learning has been a
key focus. This paper presents a comprehensive comparative study of current
research on the issue of malicious threats and methods for tackling these
challenges. Nevertheless, a huge amount of information is required by these
methods, presenting a challenge for developing robust, specialized automated
anti-malware systems. This research describes the Android Applications dataset,
and the accuracy of the techniques is measured using the accuracy levels of the
metrics employed in this study.

</details>


### [11] [NetEcho: From Real-World Streaming Side-Channels to Full LLM Conversation Recovery](https://arxiv.org/abs/2510.25472)
*Zheng Zhang,Guanlong Wu,Sen Deng,Shuai Wang,Yinqian Zhang*

Main category: cs.CR

TL;DR: NetEcho是一个基于LLM的框架，能够从加密网络流量中恢复LLM对话内容，揭示了当前防御机制在防止网络侧信道攻击方面的严重不足。


<details>
  <summary>Details</summary>
Motivation: 随着LLM应用中实时输出流成为主流交互模式，网络侧信道攻击暴露了严重的安全漏洞。尽管已有防御措施，但仍有敏感信息泄露风险。

Method: 首先系统分析主流LLM应用的侧信道防御机制，然后开发NetEcho框架，利用LLM从加密流量中恢复对话内容，确保高保真文本恢复和跨场景可迁移性。

Result: 在医疗和法律应用评估中，NetEcho平均能恢复约70%的对话信息，表明当前防御机制存在关键局限性。

Conclusion: 当前LLM应用的网络流量安全防御不足，需要加强安全机制以应对侧信道攻击威胁。

Abstract: In the rapidly expanding landscape of Large Language Model (LLM)
applications, real-time output streaming has become the dominant interaction
paradigm. While this enhances user experience, recent research reveals that it
exposes a non-trivial attack surface through network side-channels. Adversaries
can exploit patterns in encrypted traffic to infer sensitive information and
reconstruct private conversations. In response, LLM providers and third-party
services are deploying defenses such as traffic padding and obfuscation to
mitigate these vulnerabilities.
  This paper starts by presenting a systematic analysis of contemporary
side-channel defenses in mainstream LLM applications, with a focus on services
from vendors like OpenAI and DeepSeek. We identify and examine seven
representative deployment scenarios, each incorporating active/passive
mitigation techniques. Despite these enhanced security measures, our
investigation uncovers significant residual information that remains vulnerable
to leakage within the network traffic.
  Building on this discovery, we introduce NetEcho, a novel, LLM-based
framework that comprehensively unleashes the network side-channel risks of
today's LLM applications. NetEcho is designed to recover entire conversations
-- including both user prompts and LLM responses -- directly from encrypted
network traffic. It features a deliberate design that ensures high-fidelity
text recovery, transferability across different deployment scenarios, and
moderate operational cost. In our evaluations on medical and legal applications
built upon leading models like DeepSeek-v3 and GPT-4o, NetEcho can recover avg
$\sim$70\% information of each conversation, demonstrating a critical
limitation in current defense mechanisms. We conclude by discussing the
implications of our findings and proposing future directions for augmenting
network traffic security.

</details>


### [12] [A Study on Privacy-Preserving Scholarship Evaluation Based on Decentralized Identity and Zero-Knowledge Proofs](https://arxiv.org/abs/2510.25477)
*Yi Chen,Bin Chen,Peichang Zhang,Da Che*

Main category: cs.CR

TL;DR: 提出基于去中心化身份和零知识证明的奖学金评估系统，解决传统集中式评估中的隐私泄露和透明度问题


<details>
  <summary>Details</summary>
Motivation: 传统集中式奖学金评估需要学生提交详细学术记录，存在数据泄露和滥用的风险，难以同时保证隐私保护和透明审计

Method: 使用去中心化身份和零知识证明技术，在链下聚合多维零知识证明，智能合约验证评估标准符合性而不泄露原始分数或计算细节

Result: 实验结果表明该方案不仅能高效自动化评估，还能最大程度保护学生隐私和数据完整性

Conclusion: 为高等教育奖学金项目提供了一个实用且可信赖的技术范式

Abstract: Traditional centralized scholarship evaluation processes typically require
students to submit detailed academic records and qualification information,
which exposes them to risks of data leakage and misuse, making it difficult to
simultaneously ensure privacy protection and transparent auditability. To
address these challenges, this paper proposes a scholarship evaluation system
based on Decentralized Identity (DID) and Zero-Knowledge Proofs (ZKP). The
system aggregates multidimensional ZKPs off-chain, and smart contracts verify
compliance with evaluation criteria without revealing raw scores or
computational details. Experimental results demonstrate that the proposed
solution not only automates the evaluation efficiently but also maximally
preserves student privacy and data integrity, offering a practical and
trustworthy technical paradigm for higher education scholarship programs.

</details>


### [13] [ZK-SenseLM: Verifiable Large-Model Wireless Sensing with Selective Abstention and Zero-Knowledge Attestation](https://arxiv.org/abs/2510.25677)
*Hasan Akgul,Mari Eplik,Javier Rojas,Aina Binti Abdullah,Pieter van der Merwe*

Main category: cs.CR

TL;DR: ZK-SenseLM是一个安全可审计的无线感知框架，结合大模型编码器、策略决策层和端到端零知识证明，用于Wi-Fi信道状态信息等RF数据的可信推理。


<details>
  <summary>Details</summary>
Motivation: 为了解决无线感知系统中推理过程的可信度和安全性问题，确保在分布偏移下的安全操作，并提供可验证的审计能力。

Method: 采用掩码频谱预训练和相位一致性正则化的编码器，结合跨模态对齐和校准选择性弃权头，实现四阶段证明流水线（特征承诺、阈值绑定、时间窗口绑定、PLONK证明）。

Result: 在活动检测、存在感知、呼吸监测和RF指纹识别等任务中，提升了宏F1分数和校准性能，在扰动下获得良好的覆盖-风险曲线，并能以紧凑证明和快速验证抵御篡改和重放攻击。

Conclusion: ZK-SenseLM成功构建了一个安全、可审计的无线感知系统，在保持高性能的同时提供了强大的可验证性，支持联邦学习和设备个性化而不损害安全性。

Abstract: ZK-SenseLM is a secure and auditable wireless sensing framework that pairs a
large-model encoder for Wi-Fi channel state information (and optionally mmWave
radar or RFID) with a policy-grounded decision layer and end-to-end
zero-knowledge proofs of inference. The encoder uses masked spectral
pretraining with phase-consistency regularization, plus a light cross-modal
alignment that ties RF features to compact, human-interpretable policy tokens.
To reduce unsafe actions under distribution shift, we add a calibrated
selective-abstention head; the chosen risk-coverage operating point is
registered and bound into the proof. We implement a four-stage proving
pipeline: (C1) feature sanity and commitment, (C2) threshold and version
binding, (C3) time-window binding, and (C4) PLONK-style proofs that the
quantized network, given the committed window, produced the logged action and
confidence. Micro-batched proving amortizes cost across adjacent windows, and a
gateway option offloads proofs from low-power devices. The system integrates
with differentially private federated learning and on-device personalization
without weakening verifiability: model hashes and the registered threshold are
part of each public statement. Across activity, presence or intrusion,
respiratory proxy, and RF fingerprinting tasks, ZK-SenseLM improves macro-F1
and calibration, yields favorable coverage-risk curves under perturbations, and
rejects tamper and replay with compact proofs and fast verification.

</details>


### [14] [Model Inversion Attacks Meet Cryptographic Fuzzy Extractors](https://arxiv.org/abs/2510.25687)
*Mallika Prabhakar,Louise Xu,Prateek Saxena*

Main category: cs.CR

TL;DR: 本文针对机器学习模型中的模型反演攻击，特别是人脸认证系统，提出了首个支持标准欧氏距离比较器的模糊提取器L2FE-Hash，能有效防御包括新提出的PIPE攻击在内的各种反演攻击。


<details>
  <summary>Details</summary>
Motivation: 模型反演攻击对使用机器学习模型的隐私敏感应用构成严重威胁。现有防御措施缺乏系统性的特性定义，且现有模糊提取器在基于ML的人脸认证中不安全。

Method: 形式化定义了模型反演防御的理想特性，将其与密码学中的模糊提取器概念关联，提出了新的PIPE攻击来证明现有方案的不安全性，然后设计了L2FE-Hash模糊提取器。

Result: PIPE攻击对现有方案的成功率超过89%；L2FE-Hash在计算安全性方面有形式化保证，在实用人脸分布下保持可用精度，能完全防御现有最先进攻击和PIPE攻击。

Conclusion: L2FE-Hash是首个支持标准欧氏距离比较器的安全模糊提取器，提供攻击无关的安全性，无需重新训练受保护的ML模型，能有效解决模型反演攻击问题。

Abstract: Model inversion attacks pose an open challenge to privacy-sensitive
applications that use machine learning (ML) models. For example, face
authentication systems use modern ML models to compute embedding vectors from
face images of the enrolled users and store them. If leaked, inversion attacks
can accurately reconstruct user faces from the leaked vectors. There is no
systematic characterization of properties needed in an ideal defense against
model inversion, even for the canonical example application of a face
authentication system susceptible to data breaches, despite a decade of
best-effort solutions.
  In this paper, we formalize the desired properties of a provably strong
defense against model inversion and connect it, for the first time, to the
cryptographic concept of fuzzy extractors. We further show that existing fuzzy
extractors are insecure for use in ML-based face authentication. We do so
through a new model inversion attack called PIPE, which achieves a success rate
of over 89% in most cases against prior schemes. We then propose L2FE-Hash, the
first candidate fuzzy extractor which supports standard Euclidean distance
comparators as needed in many ML-based applications, including face
authentication. We formally characterize its computational security guarantees,
even in the extreme threat model of full breach of stored secrets, and
empirically show its usable accuracy in face authentication for practical face
distributions. It offers attack-agnostic security without requiring any
re-training of the ML model it protects. Empirically, it nullifies both prior
state-of-the-art inversion attacks as well as our new PIPE attack.

</details>


### [15] [Exact zCDP Characterizations for Fundamental Differentially Private Mechanisms](https://arxiv.org/abs/2510.25746)
*Charlie Harrison,Pasin Manurangsi*

Main category: cs.CR

TL;DR: 本文为几个基本差分隐私机制提供了紧致的零集中差分隐私(zCDP)表征，包括拉普拉斯机制、离散拉普拉斯机制、k-随机响应和RAPPOR，并证明了拉普拉斯机制的紧致zCDP边界为ε + e^{-ε} - 1。


<details>
  <summary>Details</summary>
Motivation: 虽然存在从ε-DP到zCDP的最坏情况转换，但许多常见算法满足更强的保证，需要为基本机制提供更精确的zCDP表征。

Method: 通过数学推导和分析，为拉普拉斯机制、离散拉普拉斯机制、k-随机响应和RAPPOR等基本机制提供紧致的zCDP边界证明。

Result: 证明了拉普拉斯机制的紧致zCDP边界为ε + e^{-ε} - 1，确认了Wang(2022)的猜想，并为其他机制提供了相应的紧致边界。

Conclusion: 这项工作为基本差分隐私机制提供了精确的zCDP表征，有助于更准确地分析和组合这些机制的隐私保证。

Abstract: Zero-concentrated differential privacy (zCDP) is a variant of differential
privacy (DP) that is widely used partly thanks to its nice composition
property. While a tight conversion from $\epsilon$-DP to zCDP exists for the
worst-case mechanism, many common algorithms satisfy stronger guarantees. In
this work, we derive tight zCDP characterizations for several fundamental
mechanisms. We prove that the tight zCDP bound for the $\epsilon$-DP Laplace
mechanism is exactly $\epsilon + e^{-\epsilon} - 1$, confirming a recent
conjecture by Wang (2022). We further provide tight bounds for the discrete
Laplace mechanism, $k$-Randomized Response (for $k \leq 6$), and RAPPOR.
Lastly, we also provide a tight zCDP bound for the worst case bounded range
mechanism.

</details>
