<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 49]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Can AI Models be Jailbroken to Phish Elderly Victims? An End-to-End Evaluation](https://arxiv.org/abs/2511.11759)
*Fred Heiding,Simon Lermen*

Main category: cs.CR

TL;DR: 该研究展示了攻击者如何利用AI安全漏洞从越狱LLM生成钓鱼内容到实际攻击老年受害者的完整攻击链，揭示了当前AI安全措施在保护易受欺诈人群方面的失败。


<details>
  <summary>Details</summary>
Motivation: 研究动机是展示AI安全漏洞如何被实际利用来伤害弱势群体，特别是老年人，揭示当前AI安全措施在保护最易受欺诈人群方面的不足。

Method: 系统评估了六个前沿LLM在四个攻击类别中的安全防护，通过人类验证研究让108名老年志愿者测试AI生成的钓鱼邮件。

Result: 多个模型在某些攻击向量上表现出近乎完全易受攻击性，AI生成的钓鱼邮件成功攻陷了11%的老年参与者。

Conclusion: 当前AI安全措施不足以保护易受欺诈的弱势群体，LLM从根本上改变了欺诈经济，使攻击者能够克服语言障碍并进行大规模多轮信任建立对话。

Abstract: We present an end-to-end demonstration of how attackers can exploit AI safety failures to harm vulnerable populations: from jailbreaking LLMs to generate phishing content, to deploying those messages against real targets, to successfully compromising elderly victims. We systematically evaluated safety guardrails across six frontier LLMs spanning four attack categories, revealing critical failures where several models exhibited near-complete susceptibility to certain attack vectors. In a human validation study with 108 senior volunteers, AI-generated phishing emails successfully compromised 11\% of participants. Our work uniquely demonstrates the complete attack pipeline targeting elderly populations, highlighting that current AI safety measures fail to protect those most vulnerable to fraud. Beyond generating phishing content, LLMs enable attackers to overcome language barriers and conduct multi-turn trust-building conversations at scale, fundamentally transforming fraud economics. While some providers report voluntary counter-abuse efforts, we argue these remain insufficient.

</details>


### [2] [NegBLEURT Forest: Leveraging Inconsistencies for Detecting Jailbreak Attacks](https://arxiv.org/abs/2511.11784)
*Lama Sleem,Jerome Francois,Lujun Li,Nathan Foucher,Niccolo Gentile,Radu State*

Main category: cs.CR

TL;DR: 提出了一种基于语义一致性分析的越狱攻击检测框架NegBLEURT Forest，通过分析成功与失败响应的语义差异来识别异常输出，无需阈值校准或模型微调即可实现可靠的越狱检测。


<details>
  <summary>Details</summary>
Motivation: 越狱攻击能够绕过LLMs的安全机制生成有害内容，而由于上下文依赖性，制定通用过滤规则很困难。需要一种不依赖阈值校准或模型微调的检测方法。

Method: 引入语义一致性分析，使用否定感知评分方法捕捉有意义的模式，提出NegBLEURT Forest框架评估对抗性提示输出与预期安全行为的对齐程度，采用Isolation Forest算法识别异常响应。

Result: 实验结果显示该方法在多种模型上始终达到顶级性能，在使用定制数据集时准确率排名第一或第二，而竞争方法对模型和数据变化表现出显著敏感性。

Conclusion: NegBLEURT Forest框架能够有效检测越狱攻击，在不同模型和数据变化下保持稳定性能，为LLM安全提供了一种可靠的检测解决方案。

Abstract: Jailbreak attacks designed to bypass safety mechanisms pose a serious threat by prompting LLMs to generate harmful or inappropriate content, despite alignment with ethical guidelines. Crafting universal filtering rules remains difficult due to their inherent dependence on specific contexts. To address these challenges without relying on threshold calibration or model fine-tuning, this work introduces a semantic consistency analysis between successful and unsuccessful responses, demonstrating that a negation-aware scoring approach captures meaningful patterns. Building on this insight, a novel detection framework called NegBLEURT Forest is proposed to evaluate the degree of alignment between outputs elicited by adversarial prompts and expected safe behaviors. It identifies anomalous responses using the Isolation Forest algorithm, enabling reliable jailbreak detection. Experimental results show that the proposed method consistently achieves top-tier performance, ranking first or second in accuracy across diverse models using the crafted dataset, while competing approaches exhibit notable sensitivity to model and data variations.

</details>


### [3] [Securing Generative AI in Healthcare: A Zero-Trust Architecture Powered by Confidential Computing on Google Cloud](https://arxiv.org/abs/2511.11836)
*Adaobi Amanna,Ishana Shinde*

Main category: cs.CR

TL;DR: 提出Confidential Zero-Trust Framework (CZF)，结合零信任架构和机密计算，解决医疗AI中数据使用时的安全漏洞问题。


<details>
  <summary>Details</summary>
Motivation: 传统安全框架无法解决医疗AI中敏感患者数据和专有AI模型在活跃处理过程中的暴露问题，即数据使用时的安全漏洞。

Method: 设计多层级架构蓝图，在Google Cloud上实现CZF，结合零信任架构的细粒度访问控制和机密计算的硬件强制数据隔离，使用远程证明提供工作负载完整性的加密证明。

Result: CZF提供深度防御架构，数据在使用时在基于硬件的可信执行环境(TEE)中保持加密状态，将合规性从程序性活动转变为可验证的技术事实。

Conclusion: CZF通过填补数据使用时的安全漏洞并强制执行零信任原则，为医疗领域负责任地采用变革性AI技术建立了必要的信任基础。

Abstract: The integration of Generative Artificial Intelligence (GenAI) in healthcare is impeded by significant security challenges unaddressed by traditional frameworks, precisely the data-in-use gap where sensitive patient data and proprietary AI models are exposed during active processing. To address this, the paper proposes the Confidential Zero-Trust Framework (CZF), a novel security paradigm that synergistically combines Zero-Trust Architecture for granular access control with the hardware-enforced data isolation of Confidential Computing. We detailed a multi-tiered architectural blueprint for implementing the CZF on Google Cloud and analyzed its efficacy against real-world threats. The CZF provides a defense-in-depth architecture where data remains encrypted while in-use within a hardware-based Trusted Execution Environment (TEE). The framework's use of remote attestation offers cryptographic proof of workload integrity, transforming compliance from a procedural exercise into a verifiable technical fact and enabling secure, multi-party collaborations previously blocked by security and intellectual property concerns. By closing the data-in-use gap and enforcing Zero-Trust principles, the CZF provides a robust and verifiable framework that establishes the necessary foundation of trust to enable the responsible adoption of transformative AI technologies in healthcare.

</details>


### [4] [VULPO: Context-Aware Vulnerability Detection via On-Policy LLM Optimization](https://arxiv.org/abs/2511.11896)
*Youpeng Li,Fuxun Yu,Xinda Wang*

Main category: cs.CR

TL;DR: VULPO是一个基于强化学习的漏洞检测框架，通过上下文感知分析和自适应奖励优化，显著提升了漏洞检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞检测技术依赖固定输入或静态偏好数据集，无法进行上下文感知分析，忽略了仓库级别的依赖关系。

Method: 构建ContextVul数据集，设计多维奖励结构，引入标签级和样本级难度自适应奖励缩放机制。

Result: VULPO-4B在F1指标上比Qwen3-4B提升85%，性能可与150倍规模的DeepSeek-R1-0528相媲美。

Conclusion: VULPO框架在上下文感知漏洞检测方面表现出色，证明了强化学习在软件安全领域的有效性。

Abstract: The widespread reliance on open-source software dramatically increases the risk of vulnerability exploitation, underscoring the need for effective and scalable vulnerability detection (VD). Existing VD techniques, whether traditional machine learning-based or LLM-based approaches like prompt engineering, supervised fine-tuning, or off-policy preference optimization, remain fundamentally limited in their ability to perform context-aware analysis: They depend on fixed inputs or static preference datasets, cannot adaptively explore repository-level dependencies, and are constrained by function-level benchmarks that overlook critical vulnerability context.
  This paper introduces Vulnerability-Adaptive Policy Optimization (VULPO), an on-policy LLM reinforcement learning framework for context-aware VD. To support training and evaluation, we first construct ContextVul, a new dataset that augments high-quality function-level samples with lightweight method to extract repository-level context information. We then design multi-dimensional reward structuring that jointly captures prediction correctness, vulnerability localization accuracy, and the semantic relevance of vulnerability analysis, thereby guiding the model toward comprehensive contextual reasoning. To address the asymmetric difficulty of different vulnerability cases and mitigate reward hacking, VULPO incorporates label-level and sample-level difficulty-adaptive reward scaling, encouraging the model to explore challenging cases while maintaining balanced reward distribution. Extensive experiments demonstrate the superiority of our VULPO framework in context-aware VD: Our VULPO-4B substantially outperforms existing VD baselines based on prompt engineering and off-policy optimization, improving F1 by 85% over Qwen3-4B and achieving performance comparable to a 150x larger-scale model, DeepSeek-R1-0528.

</details>


### [5] [CITADEL: A Semi-Supervised Active Learning Framework for Malware Detection Under Continuous Distribution Drift](https://arxiv.org/abs/2511.11979)
*Md Ahsanul Haque,Md Mahmuduzzaman Kamol,Ismail Hossain,Suresh Kumar Amalapuram,Vladik Kreinovich,Mohammad Saidur Rahman*

Main category: cs.CR

TL;DR: CITADEL是一个用于Android恶意软件检测的鲁棒半监督主动学习框架，通过恶意软件特定增强、监督对比损失和多标准主动学习策略，在有限标注预算下有效应对概念漂移问题。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习检测系统面临Android恶意软件快速演化导致的概念漂移问题，现有方法计算成本高且在真实世界长期数据集上表现不佳，同时专家标注无法跟上每天45万+新样本的增长速度。

Method: 提出恶意软件特定增强（伯努利位翻转和掩码）来模拟真实漂移行为，集成监督对比损失改善边界样本区分，结合基于预测置信度、Lp范数距离和边界不确定性的多标准主动学习策略。

Result: 在四个大规模Android恶意软件基准测试上，仅使用40%标注样本就超越先前工作，F1分数分别提升超过1%、3%、7%和14%，训练速度快24倍，操作减少13倍。

Conclusion: CITADEL框架在有限标注预算下有效应对Android恶意软件检测中的概念漂移问题，显著提升了检测性能和效率。

Abstract: Android malware evolves rapidly, leading to concept drift that degrades the performance of traditional machine learning (ML)-based detection systems. While recent approaches incorporate active learning and hierarchical contrastive loss to handle this drift, they remain fully supervised, computationally expensive, and perform poorly on real-world datasets with long temporal spans. In particular, our evaluation highlights these limitations, particularly on LAMDA, a 12-year longitudinal dataset exhibiting substantial distributional shifts. Moreover, manual expert labeling cannot scale with the daily emergence of over 450,000 new malware samples, leaving most samples unlabeled and underutilized.
  To address these challenges, we propose CITADEL, a robust semi-supervised active learning framework for Android malware detection. To bridge the gap between image-domain semi-supervised learning and binary feature representations of malware, we introduce malware-specific augmentations, Bernoulli bit flips and masking, that simulate realistic drift behaviors. CITADEL further integrates supervised contrastive loss to improve boundary sample discrimination and combines it with a multi-criteria active learning strategy based on prediction confidence, $L_p$-norm distance, and boundary uncertainty, enabling effective adaptation under limited labeling budgets. Extensive evaluation on four large-scale Android malware benchmarks -- APIGraph, Chen-AZ, MaMaDroid, and LAMDA demonstrates that CITADEL outperforms prior work, achieving F1 score of over 1%, 3%, 7%, and 14% respectively, using only 40% labeled samples. Furthermore, CITADEL shows significant efficiency over prior work incurring $24\times$ faster training and $13\times$ fewer operations.

</details>


### [6] [BudgetLeak: Membership Inference Attacks on RAG Systems via the Generation Budget Side Channel](https://arxiv.org/abs/2511.12043)
*Hao Li,Jiajun He,Guangshuo Wang,Dengguo Feng,Zheng Li,Min Zhang*

Main category: cs.CR

TL;DR: BudgetLeak是一种针对检索增强生成(RAG)系统的成员推理攻击方法，通过分析不同生成预算下的响应行为模式来识别训练数据中的成员查询。


<details>
  <summary>Details</summary>
Motivation: RAG系统依赖外部知识库，但使用专有或敏感语料库存在数据风险。现有成员推理攻击方法在RAG中效果不佳，因为黑盒约束和缺乏强成员信号。

Method: 利用RAG系统中生成预算(控制最大生成令牌数)这一侧信道，通过在不同预算下探测响应并分析指标演变，使用序列建模或聚类来识别成员查询。

Result: 在四个数据集、三个LLM生成器和两个检索器上的实验表明，BudgetLeak始终优于现有基线方法，同时保持高效率和实际可行性。

Conclusion: 该研究揭示了RAG系统中先前被忽视的数据风险，并强调了开发新防御措施的必要性。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models by integrating external knowledge, but reliance on proprietary or sensitive corpora poses various data risks, including privacy leakage and unauthorized data usage. Membership inference attacks (MIAs) are a common technique to assess such risks, yet existing approaches underperform in RAG due to black-box constraints and the absence of strong membership signals. In this paper, we identify a previously unexplored side channel in RAG systems: the generation budget, which controls the maximum number of tokens allowed in a generated response. Varying this budget reveals observable behavioral patterns between member and non-member queries, as members gain quality more rapidly with larger budgets. Building on this insight, we propose BudgetLeak, a novel membership inference attack that probes responses under different budgets and analyzes metric evolution via sequence modeling or clustering. Extensive experiments across four datasets, three LLM generators, and two retrievers demonstrate that BudgetLeak consistently outperforms existing baselines, while maintaining high efficiency and practical viability. Our findings reveal a previously overlooked data risk in RAG systems and highlight the need for new defenses.

</details>


### [7] [BackWeak: Backdooring Knowledge Distillation Simply with Weak Triggers and Fine-tuning](https://arxiv.org/abs/2511.12046)
*Shanmin Wang,Dongdong Zhao*

Main category: cs.CR

TL;DR: BackWeak是一种简单、无需代理模型的KD后门攻击方法，通过使用弱触发器以极小学习率微调良性教师模型，即可在标准蒸馏过程中将后门可靠地转移到不同学生架构中。


<details>
  <summary>Details</summary>
Motivation: 现有KD后门方法通常复杂且计算密集，依赖代理学生模型和模拟蒸馏来保证可转移性，且构建的触发器类似通用对抗扰动，不够隐蔽且具有强对抗性。本文质疑这种复杂性是否必要。

Method: 提出BackWeak攻击范式：构建隐蔽的"弱"触发器（不可感知的扰动，具有可忽略的对抗效果），仅需使用极小学习率用弱触发器微调良性教师模型。

Result: 在多个数据集、模型架构和KD方法上的广泛实验表明，BackWeak高效、简单，且通常比先前复杂方法更隐蔽，能实现高攻击成功率。

Conclusion: 这项工作呼吁KD后门攻击研究者特别关注触发器的隐蔽性及其潜在的对抗特性，证明复杂方法并非必要。

Abstract: Knowledge Distillation (KD) is essential for compressing large models, yet relying on pre-trained "teacher" models downloaded from third-party repositories introduces serious security risks -- most notably backdoor attacks. Existing KD backdoor methods are typically complex and computationally intensive: they employ surrogate student models and simulated distillation to guarantee transferability, and they construct triggers in a way similar to universal adversarial perturbations (UAPs), which being not stealthy in magnitude, inherently exhibit strong adversarial behavior. This work questions whether such complexity is necessary and constructs stealthy "weak" triggers -- imperceptible perturbations that have negligible adversarial effect. We propose BackWeak, a simple, surrogate-free attack paradigm. BackWeak shows that a powerful backdoor can be implanted by simply fine-tuning a benign teacher with a weak trigger using a very small learning rate. We demonstrate that this delicate fine-tuning is sufficient to embed a backdoor that reliably transfers to diverse student architectures during a victim's standard distillation process, yielding high attack success rates. Extensive empirical evaluations on multiple datasets, model architectures, and KD methods show that BackWeak is efficient, simpler, and often more stealthy than previous elaborate approaches. This work calls on researchers studying KD backdoor attacks to pay particular attention to the trigger's stealthiness and its potential adversarial characteristics.

</details>


### [8] [Exploring AI in Steganography and Steganalysis: Trends, Clusters, and Sustainable Development Potential](https://arxiv.org/abs/2511.12052)
*Aditya Kumar Sahu,Chandan Kumar,Saksham Kumar,Serdar Solak*

Main category: cs.CR

TL;DR: 本文对2017-2023年间AI驱动的隐写术数据隐藏技术进行了科学计量分析，识别了7个主题集群，并评估了其与可持续发展目标的关联性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能技术在隐写术和隐写分析领域的快速发展，需要系统性地分析该领域的研究趋势、地理分布及其与可持续发展目标的关联。

Method: 采用主题建模方法对654篇相关文献进行科学计量分析，识别研究主题集群并评估与SDGs的关联。

Result: 亚洲国家贡献了69%的论文，中国领先(312篇)，印度次之(114篇)；识别出7个主题集群；仅18篇论文与SDGs相关，其中SDG9(工业、创新和基础设施)最多。

Conclusion: 该研究首次对AI驱动的隐写术数据隐藏技术进行科学计量分析，揭示了该领域的研究热点、地理分布特征以及与社会可持续发展的关联不足。

Abstract: Steganography and steganalysis are strongly related subjects of information security. Over the past decade, many powerful and efficient artificial intelligence (AI) - driven techniques have been designed and presented during research into steganography as well as steganalysis. This study presents a scientometric analysis of AI-driven steganography-based data hiding techniques using a thematic modelling approach. A total of 654 articles within the time span of 2017 to 2023 have been considered. Experimental evaluation of the study reveals that 69% of published articles are from Asian countries. The China is on top (TP:312), followed by India (TP-114). The study mainly identifies seven thematic clusters: steganographic image data hiding, deep image steganalysis, neural watermark robustness, linguistic steganography models, speech steganalysis algorithms, covert communication networks, and video steganography techniques. The proposed study also assesses the scope of AI-steganography under the purview of sustainable development goals (SDGs) to present the interdisciplinary reciprocity between them. It has been observed that only 18 of the 654 articles are aligned with one of the SDGs, which shows that limited studies conducted in alignment with SDG goals. SDG9 which is Industry, Innovation, and Infrastructure is leading among 18 SDGs mapped articles. To the top of our insight, this study is the unique one to present a scientometric study on AI-driven steganography-based data hiding techniques. In the context of descriptive statistics, the study breaks down the underlying causes of observed trends, including the influence of DL developments, trends in East Asia and maturity of foundational methods. The work also stresses upon the critical gaps in societal alignment, particularly the SDGs, ultimately working on unveiling the field's global impact on AI security challenges.

</details>


### [9] [Explainable Transformer-Based Email Phishing Classification with Adversarial Robustness](https://arxiv.org/abs/2511.12085)
*Sajad U P*

Main category: cs.CR

TL;DR: 提出了一种结合DistilBERT、对抗训练和可解释AI的混合方法，用于检测AI生成的钓鱼邮件，同时提供易于理解的决策解释。


<details>
  <summary>Details</summary>
Motivation: AI生成的钓鱼攻击降低了现有检测系统的弹性，需要开发更鲁棒且透明的检测方法。

Method: 使用DistilBERT进行邮件分类，结合FGM对抗训练增强鲁棒性，集成LIME可解释AI技术，并利用Flan-T5-small生成自然语言安全解释。

Result: 实现了精确的钓鱼邮件分类，同时为模型决策提供了透明且易于理解的解释。

Conclusion: 该混合方法在保持高分类精度的同时，通过对抗训练和可解释AI增强了系统对AI生成钓鱼攻击的防御能力和透明度。

Abstract: Phishing and related cyber threats are becoming more varied and technologically advanced. Among these, email-based phishing remains the most dominant and persistent threat. These attacks exploit human vulnerabilities to disseminate malware or gain unauthorized access to sensitive information. Deep learning (DL) models, particularly transformer-based models, have significantly enhanced phishing mitigation through their contextual understanding of language. However, some recent threats, specifically Artificial Intelligence (AI)-generated phishing attacks, are reducing the overall system resilience of phishing detectors. In response, adversarial training has shown promise against AI-generated phishing threats. This study presents a hybrid approach that uses DistilBERT, a smaller, faster, and lighter version of the BERT transformer model for email classification. Robustness against text-based adversarial perturbations is reinforced using Fast Gradient Method (FGM) adversarial training. Furthermore, the framework integrates the LIME Explainable AI (XAI) technique to enhance the transparency of the DistilBERT architecture. The framework also uses the Flan-T5-small language model from Hugging Face to generate plain-language security narrative explanations for end-users. This combined approach ensures precise phishing classification while providing easily understandable justifications for the model's decisions.

</details>


### [10] [AttackVLA: Benchmarking Adversarial and Backdoor Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2511.12149)
*Jiayu Li,Yunhan Zhao,Xiang Zheng,Zonghuan Xu,Yige Li,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CR

TL;DR: 提出了AttackVLA框架来统一评估Vision-Language-Action模型的安全漏洞，并开发了BackdoorVLA攻击方法，能在触发条件下强制VLA执行指定的长时程动作序列。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA攻击方法缺乏统一的评估框架，不同架构的动作标记器差异阻碍了可重复性和公平比较，且大多数攻击未在真实场景中验证。

Method: 构建AttackVLA统一框架，覆盖数据构建、模型训练和推理全过程；实现多种攻击方法；提出BackdoorVLA后门攻击，能在触发条件下精确控制VLA执行长时程动作序列。

Result: 在模拟和真实机器人环境中评估，BackdoorVLA平均目标成功率58.4%，在特定任务上达到100%；现有攻击主要导致非目标性失败或静态动作状态。

Conclusion: 提供了标准化框架评估VLA漏洞，展示了精确对抗操控的可能性，推动VLA基础具身系统的安全研究。

Abstract: Vision-Language-Action (VLA) models enable robots to interpret natural-language instructions and perform diverse tasks, yet their integration of perception, language, and control introduces new safety vulnerabilities. Despite growing interest in attacking such models, the effectiveness of existing techniques remains unclear due to the absence of a unified evaluation framework. One major issue is that differences in action tokenizers across VLA architectures hinder reproducibility and fair comparison. More importantly, most existing attacks have not been validated in real-world scenarios. To address these challenges, we propose AttackVLA, a unified framework that aligns with the VLA development lifecycle, covering data construction, model training, and inference. Within this framework, we implement a broad suite of attacks, including all existing attacks targeting VLAs and multiple adapted attacks originally developed for vision-language models, and evaluate them in both simulation and real-world settings. Our analysis of existing attacks reveals a critical gap: current methods tend to induce untargeted failures or static action states, leaving targeted attacks that drive VLAs to perform precise long-horizon action sequences largely unexplored. To fill this gap, we introduce BackdoorVLA, a targeted backdoor attack that compels a VLA to execute an attacker-specified long-horizon action sequence whenever a trigger is present. We evaluate BackdoorVLA in both simulated benchmarks and real-world robotic settings, achieving an average targeted success rate of 58.4% and reaching 100% on selected tasks. Our work provides a standardized framework for evaluating VLA vulnerabilities and demonstrates the potential for precise adversarial manipulation, motivating further research on securing VLA-based embodied systems.

</details>


### [11] [Multi-Agent Collaborative Fuzzing with Continuous Reflection for Smart Contracts Vulnerability Detection](https://arxiv.org/abs/2511.12164)
*Jie Chen,Liangmin Wang*

Main category: cs.CR

TL;DR: SmartFuzz是一个基于大语言模型代理的智能合约漏洞检测模糊测试工具，通过持续反思过程和反应式协作链来生成有效的攻击交易序列，显著提高了复杂漏洞的检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有模糊测试工具在检测需要特定攻击交易序列触发的复杂漏洞方面存在不足，因为它们优先考虑代码覆盖率而非漏洞发现，且缺乏对状态合约的语义理解，生成大量无效交易序列。

Method: 提出持续反思过程(CRP)将交易序列生成重构为通过环境反馈不断进化的过程；设计反应式协作链(RCC)基于交易序列依赖关系协调模糊测试过程；构建多代理协作团队，各专家代理在RCC指导下从全局和局部视角共同生成和优化交易序列。

Result: 在真实世界合约和DApp项目上的实验表明，SmartFuzz在30分钟内检测到的漏洞比现有最先进工具多5.8%-74.7%，并将漏报率降低了高达80%。

Conclusion: SmartFuzz通过结合大语言模型驱动的代理和协作反思机制，显著提升了智能合约复杂漏洞的检测效果，为智能合约安全测试提供了新的有效方法。

Abstract: Fuzzing is a widely used technique for detecting vulnerabilities in smart contracts, which generates transaction sequences to explore the execution paths of smart contracts. However, existing fuzzers are falling short in detecting sophisticated vulnerabilities that require specific attack transaction sequences with proper inputs to trigger, as they (i) prioritize code coverage over vulnerability discovery, wasting considerable effort on non-vulnerable code regions, and (ii) lack semantic understanding of stateful contracts, generating numerous invalid transaction sequences that cannot pass runtime execution.
  In this paper, we propose SmartFuzz, a novel collaborative reflective fuzzer for smart contract vulnerability detection. It employs large language model-driven agents as the fuzzing engine and continuously improves itself by learning and reflecting through interactions with the environment. Specifically, we first propose a new Continuous Reflection Process (CRP) for fuzzing smart contracts, which reforms the transaction sequence generation as a self-evolving process through continuous reflection on feedback from the runtime environment. Then, we present the Reactive Collaborative Chain (RCC) to orchestrate the fuzzing process into multiple sub-tasks based on the dependencies of transaction sequences. Furthermore, we design a multi-agent collaborative team, where each expert agent is guided by the RCC to jointly generate and refine transaction sequences from both global and local perspectives. We conduct extensive experiments to evaluate SmartFuzz's performance on real-world contracts and DApp projects. The results demonstrate that SmartFuzz outperforms existing state-of-the-art tools: (i) it detects 5.8\%-74.7\% more vulnerabilities within 30 minutes, and (ii) it reduces false negatives by up to 80\%.

</details>


### [12] [RulePilot: An LLM-Powered Agent for Security Rule Generation](https://arxiv.org/abs/2511.12224)
*Hongtai Wang,Ming Xu,Yanpei Guo,Weili Han,Hoon Wei Lim,Jin Song Dong*

Main category: cs.CR

TL;DR: RulePilot是一个基于LLM代理的自动化规则生成系统，能够将自然语言描述转换为入侵检测规则，无需安全专家手动编写语法规则。


<details>
  <summary>Details</summary>
Motivation: 实时安全需求使得检测规则成为入侵检测生命周期的重要组成部分，但传统规则生成需要领域专家深度知识，自动化可以显著节省时间并减轻安全工程师负担。

Method: 使用中间表示(IR)将复杂配置规则抽象为结构化、标准化格式，让LLM能够以更可控和一致的方式生成规则。安全分析师只需提供自然语言描述，RulePilot自动生成检测规则。

Result: RulePilot在文本相似度和执行成功率方面表现优异，文本相似度比基线模型提升107.4%，在实际执行测试中达到更好的检测准确率。

Conclusion: RulePilot能够有效帮助初级分析师和普通用户在规则创建过程中，显著提升规则生成效率和质量。

Abstract: The real-time demand for system security leads to the detection rules becoming an integral part of the intrusion detection life-cycle. Rule-based detection often identifies malicious logs based on the predefined grammar logic, requiring experts with deep domain knowledge for rule generation. Therefore, automation of rule generation can result in significant time savings and ease the burden of rule-related tasks on security engineers. In this paper, we propose RulePilot, which mimics human expertise via LLM-based agent for addressing rule-related challenges like rule creation or conversion. Using RulePilot, the security analysts do not need to write down the rules following the grammar, instead, they can just provide the annotations such as the natural-language-based descriptions of a rule, our RulePilot can automatically generate the detection rules without more intervention. RulePilot is equipped with the intermediate representation (IR), which abstracts the complexity of config rules into structured, standardized formats, allowing LLMs to focus on generation rules in a more manageable and consistent way. We present a comprehensive evaluation of RulePilot in terms of textual similarity and execution success abilities, showcasing RulePilot can generate high-fidelity rules, outperforming the baseline models by up to 107.4% in textual similarity to ground truths and achieving better detection accuracy in real-world execution tests. We perform a case study from our industry collaborators in Singapore, showcasing that RulePilot significantly help junior analysts/general users in the rule creation process.

</details>


### [13] [eFPE: Design, Implementation, and Evaluation of a Lightweight Format-Preserving Encryption Algorithm for Embedded Systems](https://arxiv.org/abs/2511.12225)
*Nishant Vasantkumar Hegde,Suneesh Bare,K B Ramesh,Aamir Ibrahim*

Main category: cs.CR

TL;DR: eFPE是一种轻量级格式保持加密算法，采用8轮Feistel结构和新型轻量级伪随机函数，专为资源受限的嵌入式系统设计，可直接加密偶数位十进制字符串而无需填充或复杂转换。


<details>
  <summary>Details</summary>
Motivation: 资源受限的嵌入式系统需要安全且轻量级的数据保护方案，特别是在必须保持数据格式完整性的场景下，如金融终端、医疗传感器和工业物联网设备。

Method: 采用8轮Feistel密码结构，核心是新型轻量级伪随机函数，该函数采用高效的两轮迭代结构，包含类AES操作（字节替换、密钥异或和字节旋转）。

Result: 在ARM7TDMI LPC2148微控制器上实现，总固件ROM占用4.73 kB，RAM占用1.34 kB，核心算法模块仅需3.55 kB ROM和116 B RAM。

Conclusion: eFPE在数据格式完整性、最小资源占用和低操作延迟方面表现出色，是金融终端、医疗传感器和工业物联网设备的理想解决方案。

Abstract: Resource-constrained embedded systems demand secure yet lightweight data protection, particularly when data formats must be preserved. This paper introduces eFPE (Enhanced Format-Preserving Encryption), an 8-round Feistel cipher featuring a "novel lightweight Pseudorandom Function (PRF)" specifically designed for this domain. The PRF, architected with an efficient two-iteration structure of AES-inspired operations (byte-substitution, keyed XOR, and byte-rotation), underpins eFPE's ability to directly encrypt even-length decimal strings without padding or complex conversions, while aiming for IND-CCA2 security under standard assumptions. Implemented and evaluated on an ARM7TDMI LPC2148 microcontroller using Keil μVision 4, eFPE demonstrates the efficacy of its targeted design: a total firmware Read-Only Memory (ROM) footprint of 4.73 kB and Random Access Memory (RAM) usage of 1.34 kB. The core eFPE algorithm module itself is notably compact, requiring only 3.55 kB ROM and 116 B RAM. These characteristics make eFPE a distinct and highly suitable solution for applications like financial terminals, medical sensors, and industrial IoT devices where data format integrity, minimal resource footprint, and low operational latency are paramount.

</details>


### [14] [Software Supply Chain Security of Web3](https://arxiv.org/abs/2511.12274)
*Martin Monperrus*

Main category: cs.CR

TL;DR: 分析Web3生态系统中的软件供应链安全挑战，结合传统Web2问题与区块链技术的不可变性和高风险特性，提出缓解策略。


<details>
  <summary>Details</summary>
Motivation: Web3应用通过dApps和智能合约管理数十亿美元数字资产，其复杂的软件供应链引入重大安全漏洞，需要研究独特的安全挑战。

Method: 分析威胁格局并针对Web3生态系统特性提出缓解策略。

Result: 识别了Web3软件供应链特有的安全风险，提出了增强Web3系统安全态势的解决方案。

Conclusion: Web3生态系统面临独特的软件供应链安全挑战，需要专门的安全策略来保护数字资产和系统完整性。

Abstract: Web3 applications, built on blockchain technology, manage billions of dollars in digital assets through decentralized applications (dApps) and smart contracts. These systems rely on complex, software supply chains that introduce significant security vulnerabilities. This paper examines the software supply chain security challenges unique to the Web3 ecosystem, where traditional Web2 software supply chain problems intersect with the immutable and high-stakes nature of blockchain technology. We analyze the threat landscape and propose mitigation strategies to strengthen the security posture of Web3 systems.

</details>


### [15] [Privacy-Preserving Prompt Injection Detection for LLMs Using Federated Learning and Embedding-Based NLP Classification](https://arxiv.org/abs/2511.12295)
*Hasini Jayathilaka*

Main category: cs.CR

TL;DR: 提出基于联邦学习和嵌入分类的隐私保护提示注入检测框架，在保护隐私的同时实现与集中式训练相当的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有提示注入检测方法需要集中化提示数据，存在显著的隐私风险，需要开发隐私保护的检测方案。

Method: 使用句子嵌入对良性提示和对抗性提示数据集进行编码，训练集中式和联邦逻辑回归模型，联邦方法仅共享模型参数。

Result: 联邦方法在保护隐私的同时实现了与集中式训练相当的检测性能，证明了无需暴露原始数据的有效检测可行性。

Conclusion: 虽然数据集规模有限，但研究为构建安全和隐私感知的LLM系统提供了强有力的概念验证和新方向。

Abstract: Prompt injection attacks are an emerging threat to large language models (LLMs), enabling malicious users to manipulate outputs through carefully designed inputs. Existing detection approaches often require centralizing prompt data, creating significant privacy risks. This paper proposes a privacy-preserving prompt injection detection framework based on federated learning and embedding-based classification. A curated dataset of benign and adversarial prompts was encoded with sentence embedding and used to train both centralized and federated logistic regression models. The federated approach preserved privacy by sharing only model parameters across clients, while achieving detection performance comparable to centralized training. Results demonstrate that effective prompt injection detection is feasible without exposing raw data, making this one of the first explorations of federated security for LLMs. Although the dataset is limited in scale, the findings establish a strong proof-of-concept and highlight new directions for building secure and privacy-aware LLM systems.

</details>


### [16] [Dyadic-Chaotic Lifting S-Boxes for Enhanced Physical-Layer Security within 6G Networks](https://arxiv.org/abs/2511.12325)
*Ilias Cherkaoui,Indrakshi Dey*

Main category: cs.CR

TL;DR: 提出了首个用于物理层安全的混沌提升S盒，通过β变换驱动系统和二元条件采样生成可种子化的8位置换，满足6G网络对轻量级、可重构安全组件的需求。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要轻量级、信息论安全保护，传统密码学在延迟和能耗方面负担重，且难以抵御大规模预计算攻击。需要可重构的混淆组件来应对大规模攻击。

Method: 结合β变换驱动的动力系统和二元条件采样，生成时间可变、可种子化的8位置换，保持均匀性并支持会话间动态多样化。

Result: S盒达到最优代数度7，高平均非线性度102.5（达到8位边界的85%），最大差分概率0.039，最大线性概率0.648，适合在多轮密码中使用。

Conclusion: 提出的可重构轻量级S盒直接满足6G网络物理层安全的关键需求，提供快速、硬件友好的混淆组件，具备应对演进威胁的灵活性。

Abstract: Sixth-Generation (6G) wireless networks will interconnect billions of resource-constrained devices and time-critical services, where classical, fixed, and heavy cryptography strains latency and energy budgets and struggles against large-scale, pre-computation attacks. Physical-Layer Security (PLS) is therefore pivotal to deliver lightweight, information-theoretic protection, but still requires strong, reconfigurable confusion components that can be diversified per slice, session, or device to blunt large-scale precomputation and side-channel attacks. In order to address the above requirement, we introduce the first-ever chaos-lifted substitution box (S-box) for PLS that couples a $β$-transformation-driven dynamical system with dyadic conditional sampling to generate time-varying, seedable 8-bit permutations on demand. This construction preserves uniformity via ergodicity, yields full 8-bit bijections, and supports on-the-fly diversification across sessions. The resulting S-box attains optimal algebraic degree 7 on every output bit and high average nonlinearity 102.5 (85% of the 8-bit bound), strengthening resistance to algebraic and linear cryptanalysis. Differential and linear profiling report max DDT entry 10 (probability 0.039) and max linear probability 0.648, motivating deployment within a multi-round cipher with a strong diffusion layer, where the security-to-efficiency trade-off is compelling. Our proposed reconfigurable, lightweight S-box directly fulfills key PLS requirements of 6G networks by delivering fast, hardware-amenable confusion components with built-in agility against evolving threats.

</details>


### [17] [On the Security and Privacy of AI-based Mobile Health Chatbots](https://arxiv.org/abs/2511.12377)
*Samuel Wairimu,Leonardo Horn Iwaya*

Main category: cs.CR

TL;DR: 该研究对16个AI移动健康聊天机器人进行安全评估，发现存在安全漏洞、隐私问题和政策违规，并提出改进建议


<details>
  <summary>Details</summary>
Motivation: AI移动健康聊天机器人虽然提供个性化健康服务，但存在严重的安全隐私隐患，需要系统评估其技术稳健性

Method: 采用三阶段方法：手动检查、静态代码分析和动态分析，评估技术稳健性和设计实现选择对用户的影响

Result: 发现安全漏洞（如启用远程WebView调试）、隐私问题以及违反Google Play政策（如未提供公开隐私政策）

Conclusion: 提出改进数据处理、披露和用户安全的建议，支持开发者设计更透明、隐私友好和安全的移动健康聊天机器人

Abstract: The rise of Artificial Intelligence (AI) has impacted the development of mobile health (mHealth) apps, most notably with the advent of AI-based chatbots used as ubiquitous ``companions'' for various services, from fitness to mental health assistants. While these mHealth chatbots offer clear benefits, such as personalized health information and predictive diagnoses, they also raise significant concerns regarding security and privacy. This study empirically assesses 16 AI-based mHealth chatbots identified from the Google Play Store. The empirical assessment follows a three-phase approach (manual inspection, static code analysis, and dynamic analysis) to evaluate technical robustness and how design and implementation choices impact end users. Our findings revealed security vulnerabilities (e.g., enabling Remote WebView debugging), privacy issues, and non-compliance with Google Play policies (e.g., failure to provide publicly accessible privacy policies). Based on our findings, we offer several recommendations to enhance the security and privacy of mHealth chatbots. These recommendations focus on improving data handling processes, disclosure, and user security. Therefore, this work also seeks to support mHealth developers and security/privacy engineers in designing more transparent, privacy-friendly, and secure mHealth chatbots.

</details>


### [18] [GenSIaC: Toward Security-Aware Infrastructure-as-Code Generation with Large Language Models](https://arxiv.org/abs/2511.12385)
*Yikun Li,Matteo Grella,Daniel Nahmias,Gal Engelberg,Dan Klein,Giancarlo Guizzardi,Thijs van Ede,Andrea Continella*

Main category: cs.CR

TL;DR: 本文研究了大型语言模型在生成安全感知的基础设施即代码方面的潜力，通过创建GenSIaC指令微调数据集来提升LLMs识别和预防IaC安全错误配置的能力。


<details>
  <summary>Details</summary>
Motivation: 随着云基础设施复杂度的增加，IaC脚本中的错误配置和安全漏洞风险上升，而LLMs在生成安全IaC代码方面的能力尚不明确，需要解决LLMs对IaC安全弱点的理解不足以及缺乏增强IaC代码生成安全性的技术问题。

Method: 首先评估基础LLMs在IaC代码生成和检查中识别安全弱点的能力，然后创建GenSIaC指令微调数据集，利用该数据集对LLMs进行微调，指导模型生成安全感知的IaC代码。

Result: 经过微调的模型在识别和预防IaC安全错误配置方面性能显著提升，F1分数从0.303提高到0.858。消融研究验证了GenSIaC对其他LLMs的泛化能力和跨语言能力。

Conclusion: GenSIaC方法有效提升了LLMs生成安全IaC代码的能力，为解决IaC脚本中的安全配置问题提供了可行方案。

Abstract: In recent years, Infrastructure as Code (IaC) has emerged as a critical approach for managing and provisioning IT infrastructure through code and automation. IaC enables organizations to create scalable and consistent environments, effectively managing servers and development settings. However, the growing complexity of cloud infrastructures has led to an increased risk of misconfigurations and security vulnerabilities in IaC scripts. To address this problem, this paper investigates the potential of Large Language Models (LLMs) in generating security-aware IaC code, avoiding misconfigurations introduced by developers and administrators.
  While LLMs have made significant progress in natural language processing and code generation, their ability to generate secure IaC scripts remains unclear. This paper addresses two major problems: 1) the lack of understanding of security weaknesses in IaC scripts generated by LLMs, and 2) the absence of techniques for enhancing security in generating IaC code with LLMs.
  To assess the extent to which LLMs contain security knowledge, we first conduct a comprehensive evaluation of base LLMs in recognizing major IaC security weaknesses during the generation and inspection of IaC code. Then, we propose GenSIaC, an instruction fine-tuning dataset designed to improve LLMs' ability to recognize potential security weaknesses. Leveraging GenSIaC, we fine-tune LLMs and instruct models to generate security-aware IaC code. Our evaluation demonstrates that our models achieve substantially improved performance in recognizing and preventing IaC security misconfigurations, e.g., boosting the F1-score from 0.303 to 0.858. Additionally, we perform ablation studies and explore GenSIaC's generalizability to other LLMs and its cross-language capabilities.

</details>


### [19] [GRAPHTEXTACK: A Realistic Black-Box Node Injection Attack on LLM-Enhanced GNNs](https://arxiv.org/abs/2511.12423)
*Jiaji Ma,Puja Trivedi,Danai Koutra*

Main category: cs.CR

TL;DR: GRAPHTEXTACK是一种针对LLM增强GNN的黑盒多模态投毒节点注入攻击方法，通过精心设计的结构和语义注入节点来降低模型性能，无需依赖模型内部信息或代理模型。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗攻击主要独立扰动结构或文本，对LLM增强GNN的影响有限，且许多攻击假设不现实的能力。需要一种更现实、更有效的多模态攻击方法。

Method: 提出GRAPHTEXTACK攻击框架，使用进化优化方法在多模态搜索空间中寻找最优节点注入策略，通过多目标适应度函数平衡局部预测破坏和全局图影响。

Result: 在五个数据集和两种最先进的LLM增强GNN模型上的实验表明，GRAPHTEXTACK显著优于12个强基线方法。

Conclusion: GRAPHTEXTACK是首个针对LLM增强GNN的黑盒多模态投毒节点注入攻击，展示了多模态攻击的有效性，为模型安全性提供了重要见解。

Abstract: Text-attributed graphs (TAGs), which combine structural and textual node information, are ubiquitous across many domains. Recent work integrates Large Language Models (LLMs) with Graph Neural Networks (GNNs) to jointly model semantics and structure, resulting in more general and expressive models that achieve state-of-the-art performance on TAG benchmarks. However, this integration introduces dual vulnerabilities: GNNs are sensitive to structural perturbations, while LLM-derived features are vulnerable to prompt injection and adversarial phrasing. While existing adversarial attacks largely perturb structure or text independently, we find that uni-modal attacks cause only modest degradation in LLM-enhanced GNNs. Moreover, many existing attacks assume unrealistic capabilities, such as white-box access or direct modification of graph data. To address these gaps, we propose GRAPHTEXTACK, the first black-box, multi-modal{, poisoning} node injection attack for LLM-enhanced GNNs. GRAPHTEXTACK injects nodes with carefully crafted structure and semantics to degrade model performance, operating under a realistic threat model without relying on model internals or surrogate models. To navigate the combinatorial, non-differentiable search space of connectivity and feature assignments, GRAPHTEXTACK introduces a novel evolutionary optimization framework with a multi-objective fitness function that balances local prediction disruption and global graph influence. Extensive experiments on five datasets and two state-of-the-art LLM-enhanced GNN models show that GRAPHTEXTACK significantly outperforms 12 strong baselines.

</details>


### [20] [SeedAIchemy: LLM-Driven Seed Corpus Generation for Fuzzing](https://arxiv.org/abs/2511.12448)
*Aidan Wen,Norah A. Alzahrani,Jingzhi Jiang,Andrew Joe,Karen Shieh,Andy Zhang,Basel Alomair,David Wagner*

Main category: cs.CR

TL;DR: SeedAIchemy是一个自动化的LLM驱动的语料库生成工具，通过五个模块从互联网收集公开文件，其中四个模块使用LLM工作流构建搜索词以最大化语料库质量。


<details>
  <summary>Details</summary>
Motivation: 使开发者更容易有效地实施模糊测试，通过自动化语料库生成来替代手动收集。

Method: 使用五个模块从互联网收集公开文件，其中四个模块采用LLM工作流来构建优化的搜索词。

Result: SeedAIchemy生成的语料库在多样化的目标程序和库上表现显著优于朴素语料库，与手动整理的语料库表现相当。

Conclusion: SeedAIchemy能够有效自动化语料库生成过程，为模糊测试提供高质量的输入语料。

Abstract: We introduce SeedAIchemy, an automated LLM-driven corpus generation tool that makes it easier for developers to implement fuzzing effectively. SeedAIchemy consists of five modules which implement different approaches at collecting publicly available files from the internet. Four of the five modules use large language model (LLM) workflows to construct search terms designed to maximize corpus quality. Corpora generated by SeedAIchemy perform significantly better than a naive corpus and similarly to a manually-curated corpus on a diverse range of target programs and libraries.

</details>


### [21] [A Content-Preserving Secure Linguistic Steganography](https://arxiv.org/abs/2511.12565)
*Lingyun Xiang,Chengfu Ou,Xu He,Zhongliang Yang,Yuling Liu*

Main category: cs.CR

TL;DR: 提出了一种内容保持的语言隐写方法CLstega，通过可控分布变换嵌入秘密信息，不修改原始文本内容，实现完美安全的隐蔽通信。


<details>
  <summary>Details</summary>
Motivation: 现有语言隐写方法通过内容变换隐藏秘密信息，但会在正常文本和隐写文本之间产生细微差异，存在安全风险。需要一种不修改原始文本内容的安全隐写方法。

Method: CLstega采用增强掩码策略定位嵌入位置，设计动态分布隐写编码策略，通过从原始概率分布导出目标分布来编码秘密信息。通过精心选择目标词构建掩码句子数据集，微调原始MLM生成目标MLM，直接从原始文本提取秘密信息。

Result: 实验结果显示CLstega能实现100%的提取成功率，在安全性方面优于现有方法，有效平衡了嵌入容量和安全性。

Conclusion: CLstega方法确保了秘密信息的完美安全性，同时完全保持了原始文本的完整性，为安全隐蔽通信提供了有效解决方案。

Abstract: Existing linguistic steganography methods primarily rely on content transformations to conceal secret messages. However, they often cause subtle yet looking-innocent deviations between normal and stego texts, posing potential security risks in real-world applications. To address this challenge, we propose a content-preserving linguistic steganography paradigm for perfectly secure covert communication without modifying the cover text. Based on this paradigm, we introduce CLstega (\textit{C}ontent-preserving \textit{L}inguistic \textit{stega}nography), a novel method that embeds secret messages through controllable distribution transformation. CLstega first applies an augmented masking strategy to locate and mask embedding positions, where MLM(masked language model)-predicted probability distributions are easily adjustable for transformation. Subsequently, a dynamic distribution steganographic coding strategy is designed to encode secret messages by deriving target distributions from the original probability distributions. To achieve this transformation, CLstega elaborately selects target words for embedding positions as labels to construct a masked sentence dataset, which is used to fine-tune the original MLM, producing a target MLM capable of directly extracting secret messages from the cover text. This approach ensures perfect security of secret messages while fully preserving the integrity of the original cover text. Experimental results show that CLstega can achieve a 100\% extraction success rate, and outperforms existing methods in security, effectively balancing embedding capacity and security.

</details>


### [22] [Prrr: Personal Random Rewards for Blockchain Reporting](https://arxiv.org/abs/2511.12626)
*Hongyin Chen,Yubin Ke,Xiaotie Deng,Ittay Eyal*

Main category: cs.CR

TL;DR: Prrr协议通过随机异质奖励设计解决了智能合约报告机制的安全性与性能权衡问题，采用事前合成不对称机制实现激励兼容。


<details>
  <summary>Details</summary>
Motivation: 现有报告协议面临安全性与性能的权衡：依赖少数可信发布者带来中心化风险，而开放发布则导致区块链上报告数量过多。对称奖励设计是这一问题的根本原因。

Method: 提出Prrr协议，采用随机异质奖励分配机制（事前合成不对称），使用第二价格风格结算来分配奖励，确保激励兼容性。

Result: Prrr协议在子博弈完美纳什均衡下运行，能够抵抗合谋和Sybil攻击，同时实现安全性和效率。

Conclusion: Prrr是首个在博弈论机制中刻意形成参与者不对称性的协议，适用于依赖及时报告的多种智能合约场景。

Abstract: Smart contracts, the stateful programs running on blockchains, often rely on reports. Publishers are paid to publish these reports on the blockchain. Designing protocols that incentivize timely reporting is the prevalent reporting problem. But existing solutions face a security-performance trade-off: Relying on a small set of trusted publishers introduces centralization risks, while allowing open publication results in an excessive number of reports on the blockchain. We identify the root cause of this trade-off to be the standard symmetric reward design, which treats all reports equally. We prove that no symmetric-reward mechanism can overcome the trade-off.
  We present Personal Random Rewards for Reporting (Prrr), a protocol that assigns random heterogeneous values to reports. We call this novel mechanism-design concept Ex-Ante Synthetic Asymmetry. To the best of our knowledge, Prrr is the first game-theoretic mechanism (in any context) that deliberately forms participant asymmetry. Prrr employs a second-price-style settlement to allocate rewards, ensuring incentive compatibility and achieving both security and efficiency. Following the protocol constitutes a Subgame-Perfect Nash Equilibrium, robust against collusion and Sybil attacks. Prrr is applicable to numerous smart contracts that rely on timely reports.

</details>


### [23] [Adaptive Dual-Layer Web Application Firewall (ADL-WAF) Leveraging Machine Learning for Enhanced Anomaly and Threat Detection](https://arxiv.org/abs/2511.12643)
*Ahmed Sameh,Sahar Selim*

Main category: cs.CR

TL;DR: 提出了一种自适应双层WAF，使用决策树和SVM两层机器学习模型来提高威胁检测精度，在五个基准数据集上达到99.88%的检测准确率和100%的精确率。


<details>
  <summary>Details</summary>
Motivation: 传统WAF难以有效区分恶意和合法流量，导致威胁检测效果有限，需要更准确的检测方法。

Method: 采用双层机器学习模型：第一层使用决策树算法检测流量异常，第二层使用支持向量机将异常分类为威胁异常或良性异常，结合数据预处理和特征工程技术。

Result: 在五个大型基准数据集上的评估显示，ADL WAF实现了99.88%的检测准确率和100%的精确率，显著提高了异常检测能力并减少了误报。

Conclusion: 将机器学习技术集成到WAF中可以显著提高Web应用安全性，提供更准确和高效的威胁检测。

Abstract: Web Application Firewalls are crucial for protecting web applications against a wide range of cyber threats. Traditional Web Application Firewalls often struggle to effectively distinguish between malicious and legitimate traffic, leading to limited efficacy in threat detection. To overcome these limitations, this paper proposes an Adaptive Dual-Layer WAF employing a two-layered Machine Learning model designed to enhance the accuracy of anomaly and threat detection. The first layer employs a Decision Tree (DT) algorithm to detect anomalies by identifying traffic deviations from established normal patterns. The second layer employs Support Vector Machine to classify these anomalies as either threat anomalies or benign anomalies. Our Adaptive Dual Layer WAF incorporates comprehensive data pre-processing and feature engineering techniques and has been thoroughly evaluated using five large benchmark datasets. Evaluation using these datasets shows that ADL WAF achieves a detection accuracy of 99.88% and a precision of 100%, significantly enhancing anomaly detection and reducing false positives. These findings suggest that integrating machine learning techniques into WAFs can substantially improve web application security by providing more accurate and efficient threat detection.

</details>


### [24] [Scalable Hierarchical AI-Blockchain Framework for Real-Time Anomaly Detection in Large-Scale Autonomous Vehicle Networks](https://arxiv.org/abs/2511.12648)
*Rathin Chandra Shit,Sharmila Subudhi*

Main category: cs.CR

TL;DR: 本文提出了一种三层混合安全架构HAVEN，用于解决自动驾驶车辆网络的安全挑战，实现了亚10毫秒的异常检测和分布式协调，在100-1000辆车的大规模网络中表现出色。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆网络面临传感器集成复杂性、实时性要求和分布式通信协议带来的安全挑战，现有安全方案无法在可接受的安全/隐私框架内提供亚10毫秒异常检测和大规模车辆网络的分布式协调。

Method: HAVEN采用三层混合安全架构：第一层在边缘部署轻量级集成异常检测模型；中间层使用拜占庭容错联邦学习聚合区域威胁情报；顶层采用选择性区块链机制确保关键安全协调。

Result: 实验结果显示亚10毫秒检测延迟，多模态传感器数据准确率达94%，F1分数92%，在20%节点被攻陷时仍保持拜占庭容错，区块链存储开销降低，保证充分的差分隐私。

Conclusion: HAVEN框架通过新颖的三层处理克服了实时安全义务与分布式安全协调之间的重要权衡，在检测精度和网络弹性方面相比其他方法有显著改进。

Abstract: The security of autonomous vehicle networks is facing major challenges, owing to the complexity of sensor integration, real-time performance demands, and distributed communication protocols that expose vast attack surfaces around both individual and network-wide safety. Existing security schemes are unable to provide sub-10 ms (milliseconds) anomaly detection and distributed coordination of large-scale networks of vehicles within an acceptable safety/privacy framework. This paper introduces a three-tier hybrid security architecture HAVEN (Hierarchical Autonomous Vehicle Enhanced Network), which decouples real-time local threat detection and distributed coordination operations. It incorporates a light ensemble anomaly detection model on the edge (first layer), Byzantine-fault-tolerant federated learning to aggregate threat intelligence at a regional scale (middle layer), and selected blockchain mechanisms (top layer) to ensure critical security coordination. Extensive experimentation is done on a real-world autonomous driving dataset. Large-scale simulations with the number of vehicles ranging between 100 and 1000 and different attack types, such as sensor spoofing, jamming, and adversarial model poisoning, are conducted to test the scalability and resiliency of HAVEN. Experimental findings show sub-10 ms detection latency with an accuracy of 94% and F1-score of 92% across multimodal sensor data, Byzantine fault tolerance validated with 20\% compromised nodes, and a reduced blockchain storage overhead, guaranteeing sufficient differential privacy. The proposed framework overcomes the important trade-off between real-time safety obligation and distributed security coordination with novel three-tiered processing. The scalable architecture of HAVEN is shown to provide great improvement in detection accuracy as well as network resilience over other methods.

</details>


### [25] [AI Bill of Materials and Beyond: Systematizing Security Assurance through the AI Risk Scanning (AIRS) Framework](https://arxiv.org/abs/2511.12668)
*Samuel Nathanson,Alexander Lee,Catherine Chen Kieffer,Jared Junkin,Jessica Ye,Amir Saeed,Melanie Lockhart,Russ Fink,Elisha Peterson,Lanier Watkins*

Main category: cs.CR

TL;DR: 本文提出了AI风险扫描(AIRS)框架，通过威胁建模和自动化证据生成，将AI文档从描述性披露转向可验证的风险评估。


<details>
  <summary>Details</summary>
Motivation: 现有AI透明度机制(如Model Cards、Datasheets、SBOMs)主要关注来源报告，但缺乏可验证的机器可读安全证据，无法有效保障AI系统安全。

Method: 开发AIRS框架，基于MITRE ATLAS对抗性ML分类法，通过三个渐进式试点研究(Smurf、OPAL、Pilot C)设计AI BOM模式，自动生成结构化工件捕获模型完整性、序列化安全、结构适配器和运行时行为。

Result: 在量化GPT-OSS-20B模型上验证了安全加载器策略执行、分片哈希验证、污染和后门探测等功能。与SPDX 3.0和CycloneDX 1.6比较显示在身份和评估元数据上对齐，但识别了AI特定保障字段的关键差距。

Conclusion: AIRS框架通过将威胁建模与自动化可审计证据生成相结合，将SBOM实践扩展到AI领域，为标准化、可信赖和机器可验证的AI风险文档提供了原则性基础。

Abstract: Assurance for artificial intelligence (AI) systems remains fragmented across software supply-chain security, adversarial machine learning, and governance documentation. Existing transparency mechanisms - including Model Cards, Datasheets, and Software Bills of Materials (SBOMs) - advance provenance reporting but rarely provide verifiable, machine-readable evidence of model security. This paper introduces the AI Risk Scanning (AIRS) Framework, a threat-model-based, evidence-generating framework designed to operationalize AI assurance. The AIRS Framework evolved through three progressive pilot studies - Smurf (AIBOM schema design), OPAL (operational validation), and Pilot C (AIRS) - that reframed AI documentation from descriptive disclosure toward measurable, evidence-bound verification. The framework aligns its assurance fields to the MITRE ATLAS adversarial ML taxonomy and automatically produces structured artifacts capturing model integrity, packaging and serialization safety, structural adapters, and runtime behaviors. Currently, the AIRS Framework is scoped to provide model-level assurances for LLMs, but it could be expanded to include other modalities and cover system-level threats (e.g. application-layer abuses, tool-calling). A proof-of-concept on a quantized GPT-OSS-20B model demonstrates enforcement of safe loader policies, per-shard hash verification, and contamination and backdoor probes executed under controlled runtime conditions. Comparative analysis with SBOM standards of SPDX 3.0 and CycloneDX 1.6 reveals alignment on identity and evaluation metadata, but identifies critical gaps in representing AI-specific assurance fields. The AIRS Framework thus extends SBOM practice to the AI domain by coupling threat modeling with automated, auditable evidence generation, providing a principled foundation for standardized, trustworthy, and machine-verifiable AI risk documentation.

</details>


### [26] [Offensive tool determination strategy R.I.D.D.L.E. + (C)](https://arxiv.org/abs/2511.12704)
*Herman Errico*

Main category: cs.CR

TL;DR: 本文提出了一种风险评估方法，通过分析攻击工具特性来改进对故意威胁的理解，使用R.I.D.D.L.E.+C变量框架进行威胁评估。


<details>
  <summary>Details</summary>
Motivation: 故意威胁是关键基础设施漏洞的主要风险因素，需要准确的风险评估来分析威胁、评估漏洞并评估对资产和系统的潜在影响。

Method: 基于"攻击工具确定策略"的方法，使用R.I.D.D.L.E.+C变量（抵抗性、入侵时机、损害、中断时机、潜伏期、效率和成本），通过开源情报评估这些变量，并为每个变量分配特定范围的值。

Result: 提供了一个实用应用矩阵，可以揭示意外漏洞，并为决策和安全规划提供更细粒度的框架。

Conclusion: 该方法可以作为风险评估过程的附加阶段，通过引入攻击工具特性分析参数，增强对故意威胁的理解和评估能力。

Abstract: Intentional threats are a major risk factor related to vulnerabilities in critical infrastructure assets, and an accurate risk assessment is necessary to analyze threats, assess vulnerabilities, and evaluate potential impacts on assets and systems. This research proposes a methodology that can be added as an additional phase in the risk assessment process. The method introduces an extra analytical parameter concerning offensive tool characteristics, improving the understanding of intentional threats.
  The methodology is presented using clear and accessible language suitable for a broad audience. It is based on an approach described as an "offensive tool determination strategy," summarized by the acronym R.I.D.D.L.E.+C, which refers to the variables used in the analysis: resistance, intrusion timing, damage, disruption timing, latency, efficiency, and cost. These variables are evaluated using open-source intelligence.
  Each variable is assigned a specific range of values according to its potential impact on the targeted asset. A matrix is then provided for practical application, which can reveal unexpected vulnerabilities and offer a more granular framework for decision-making and security planning.

</details>


### [27] [ProxyPrints: From Database Breach to Spoof, A Plug-and-Play Defense for Biometric Systems](https://arxiv.org/abs/2511.12739)
*Yaniv Hacmon,Keren Gorelik,Gilad Gressel,Yisroel Mirsky*

Main category: cs.CR

TL;DR: ProxyPrints是一个实用的指纹数据保护方案，通过透明中间件层将扫描的指纹转换为一致且不可链接的别名，实现可撤销生物识别，无需修改专有匹配软件。


<details>
  <summary>Details</summary>
Motivation: 指纹识别系统广泛部署但存储的指纹数据存在安全漏洞，即使使用基于细节点的模板也可能被逆向工程重建指纹图像，导致无法修复的身份泄露风险。

Method: ProxyPrints作为指纹扫描器和匹配算法之间的透明中间件层，将每个扫描的指纹转换为一致的、不可链接的别名，支持生物识别身份的撤销和替换。

Result: 在标准基准数据集和商业指纹识别系统上的评估表明，ProxyPrints在保持匹配性能的同时提供了强大的安全性和可撤销性。

Conclusion: ProxyPrints是一个即插即用、可扩展的指纹数据保护解决方案，提供可撤销生物识别和泄露检测能力，开源实现便于实际部署。

Abstract: Fingerprint recognition systems are widely deployed for authentication and forensic applications, but the security of stored fingerprint data remains a critical vulnerability. While many systems avoid storing raw fingerprint images in favor of minutiae-based templates, recent research shows that these templates can be reverse-engineered to reconstruct realistic fingerprint images, enabling physical spoofing attacks that compromise user identities with no means of remediation.
  We present ProxyPrints, the first practical defense that brings cancellable biometrics to existing fingerprint recognition systems without requiring modifications to proprietary matching software. ProxyPrints acts as a transparent middleware layer between the fingerprint scanner and the matching algorithm, transforming each scanned fingerprint into a consistent, unlinkable alias. This transformation allows biometric identities to be revoked and replaced in the event of a breach, without affecting authentication accuracy. Additionally, ProxyPrints provides organizations with breach detection capabilities by enabling the identification of out-of-band spoofing attempts involving compromised aliases.
  We evaluate ProxyPrints on standard benchmark datasets and commercial fingerprint recognition systems, demonstrating that it preserves matching performance while offering strong security and revocability. Our open-source implementation includes tools for alias generation and deployment in real-world pipelines, making ProxyPrints a drop-in, scalable solution for fingerprint data protection.

</details>


### [28] [An Evaluation Framework for Network IDS/IPS Datasets: Leveraging MITRE ATT&CK and Industry Relevance Metrics](https://arxiv.org/abs/2511.12743)
*Adrita Rahman Tori,Khondokar Fida Hasan*

Main category: cs.CR

TL;DR: 提出了一个多维框架来评估入侵检测系统数据集的质量，通过整合MITRE ATT&CK知识库和五个互补指标，全面评估数据集在特定行业威胁覆盖方面的适用性。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型评估实践主要关注准确性指标，往往忽略了数据集是否代表行业特定威胁，导致入侵检测系统在实际部署中效果不佳。

Method: 结合威胁情报、自然语言处理和定量分析，开发了一个多维评估框架，使用MITRE ATT&CK知识库和五个互补指标来评估数据集对特定行业威胁的覆盖情况。

Result: 对九个公开IDS/IPS数据集的分析显示，在医疗、能源和金融等行业存在显著的威胁覆盖差距，较新的数据集（如CIC-IoMT）与行业特定威胁更匹配。

Conclusion: 该框架提供了一个标准化、可解释的方法来选择符合行业特定操作需求的数据集，能够增强AI驱动入侵检测系统在实际环境中的有效性。

Abstract: The performance of Machine Learning (ML) and Deep Learning (DL)-based Intrusion Detection and Prevention Systems (IDS/IPS) is critically dependent on the relevance and quality of the datasets used for training and evaluation. However, current AI model evaluation practices for developing IDS/IPS focus predominantly on accuracy metrics, often overlooking whether datasets represent industry-specific threats. To address this gap, we introduce a novel multi-dimensional framework that integrates the MITRE ATT&CK knowledge base for threat intelligence and employs five complementary metrics that together provide a comprehensive assessment of dataset suitability. Methodologically, this framework combines threat intelligence, natural language processing, and quantitative analysis to assess the suitability of datasets for specific industry contexts. Applying this framework to nine publicly available IDS/IPS datasets reveals significant gaps in threat coverage, particularly in the healthcare, energy, and financial sectors. In particular, recent datasets (e.g., CIC-IoMT, CIC-UNSW-NB15) align better with sector-specific threats, whereas others, like CICIoV-24, underperform despite their recency. Our findings provide a standardized, interpretable approach for selecting datasets aligned with sector-specific operational requirements, ultimately enhancing the real-world effectiveness of AI-driven IDS/IPS deployments. The efficiency and practicality of the framework are validated through deployment in a real-world case study, underscoring its capacity to inform dataset selection and enhance the effectiveness of AI-driven IDS/IPS in operational environments.

</details>


### [29] [Whose Narrative is it Anyway? A KV Cache Manipulation Attack](https://arxiv.org/abs/2511.12752)
*Mukkesh Ganesh,Kaushik Iyer,Arun Baalaaji Sankar Ananthan*

Main category: cs.CR

TL;DR: 提出了一种名为"历史交换"的新型KV缓存攻击方法，通过在块级别操纵KV缓存来劫持模型生成方向，而不改变用户可见的提示。


<details>
  <summary>Details</summary>
Motivation: KV缓存作为自回归大语言模型高效推理的重要组成部分，其作为模型内部状态表示的特性使其成为完整性攻击的潜在目标。

Method: 通过用来自不同主题的预计算缓存覆盖活动生成缓存的连续段，在Qwen 3系列模型上进行324种配置的实证评估，分析缓存覆盖的时间、幅度和层深度影响。

Result: 只有全层覆盖才能成功劫持对话主题，产生三种行为：立即持久主题转移、部分恢复或延迟劫持。高层结构规划在生成过程早期编码，局部话语结构由模型最后层维护。

Conclusion: KV缓存是安全分析的重要向量，它不仅编码上下文，还编码主题轨迹和结构规划，使其成为操纵模型行为的强大接口。

Abstract: The Key Value(KV) cache is an important component for efficient inference in autoregressive Large Language Models (LLMs), but its role as a representation of the model's internal state makes it a potential target for integrity attacks. This paper introduces "History Swapping," a novel block-level attack that manipulates the KV cache to steer model generation without altering the user-facing prompt. The attack involves overwriting a contiguous segment of the active generation's cache with a precomputed cache from a different topic. We empirically evaluate this method across 324 configurations on the Qwen 3 family of models, analyzing the impact of timing, magnitude, and layer depth of the cache overwrite. Our findings reveal that only full-layer overwrites can successfully hijack the conversation's topic, leading to three distinct behaviors: immediate and persistent topic shift, partial recovery, or a delayed hijack. Furthermore, we observe that high-level structural plans are encoded early in the generation process and local discourse structure is maintained by the final layers of the model. This work demonstrates that the KV cache is a significant vector for security analysis, as it encodes not just context but also topic trajectory and structural planning, making it a powerful interface for manipulating model behavior.

</details>


### [30] [Cybersecurity of High-Altitude Platform Stations: Threat Taxonomy, Attacks and Defenses with Standards Mapping - DDoS Attack Use Case](https://arxiv.org/abs/2511.12766)
*Chaouki Hjaiji,Bassem Ouni,Mohamed-Slim Alouini*

Main category: cs.CR

TL;DR: 本文对高空平台站(HAPS)的网络安全进行了系统分析，提出了威胁分类法，讨论了可行的防御措施，并通过仿真研究了DDoS攻击的影响。


<details>
  <summary>Details</summary>
Motivation: 随着高空平台站成为非地面网络中的重要节点，其网络安全和隐私保护问题日益突出，需要系统性的安全分析和防御方案。

Method: 采用结构化分析方法，包括子系统分析、威胁映射、防御措施讨论，并使用OMNeT++/INET进行仿真案例研究。

Result: 建立了HAPS的威胁分类法，识别了各子系统的安全风险，验证了DDoS攻击对服务和控制平面可用性的影响。

Conclusion: 提出了符合HAPS约束条件的网络安全防御框架，并指出了未来研究方向，为实际部署提供工程权衡参考。

Abstract: High-Altitude Platform Stations (HAPS) are emerging stratospheric nodes within non-terrestrial networks. We provide a structured overview of HAPS subsystems and principal communication links, map cybersecurity and privacy exposure across communication, control, and power subsystems, and propose a stratosphere-aware threat taxonomy. We then discuss defenses feasible under HAPS constraints including encryption and authentication, frequency agility, directional and beam-steered antennas, intrusion detection, secure boot, and software and supply-chain assurance-while highlighting how they align with emerging regulatory and standards guidance. Finally, we report a simulation-based case study using OMNeT++/INET to characterize distributed-denial-of-service (DDoS) impact on service and control-plane availability, and summarize regulatory and standardization considerations relevant to deployment. We conclude with concrete future research directions. The study is simulation-grounded and intended to inform engineering trade-offs for real-world HAPS deployments rather than serve as an on-air validation.

</details>


### [31] [Efficient Adversarial Malware Defense via Trust-Based Raw Override and Confidence-Adaptive Bit-Depth Reduction](https://arxiv.org/abs/2511.12827)
*Ayush Chaudhary,Sisir Doppalpudi*

Main category: cs.CR

TL;DR: 提出结合TRO和CABDR的新框架，在保持91%清洁准确率的同时，将计算开销降至1.76倍，比现有方法提升2.3倍，实现了对抗鲁棒性与计算效率的优化平衡。


<details>
  <summary>Details</summary>
Motivation: 现有对抗防御方法虽然提升了鲁棒性，但带来了4-22倍的计算开销，这对每天处理数百万样本的生产系统构成了重大挑战。

Method: 结合Trust-Raw Override (TRO)和Confidence-Adaptive Bit-Depth Reduction (CABDR)，利用自适应置信度机制选择性应用防御措施。

Result: 在EMBER v2数据集上验证，保持91%清洁准确率，将攻击成功率降至31-37%，对C&W等优化攻击减少48.8%，吞吐量达126万样本/秒。

Conclusion: 生产环境中的实用对抗鲁棒性需要明确优化效率-鲁棒性权衡，为组织在不产生过高基础设施成本的情况下部署鲁棒防御提供了可行路径。

Abstract: The deployment of robust malware detection systems in big data environments requires careful consideration of both security effectiveness and computational efficiency. While recent advances in adversarial defenses have demonstrated strong robustness improvements, they often introduce computational overhead ranging from 4x to 22x, which presents significant challenges for production systems processing millions of samples daily. In this work, we propose a novel framework that combines Trust-Raw Override (TRO) with Confidence-Adaptive Bit-Depth Reduction (CABDR) to explicitly optimize the trade-off between adversarial robustness and computational efficiency. Our approach leverages adaptive confidence-based mechanisms to selectively apply defensive measures, achieving 1.76x computational overhead - a 2.3x improvement over state-of-the-art smoothing defenses. Through comprehensive evaluation on the EMBER v2 dataset comprising 800K samples, we demonstrate that our framework maintains 91 percent clean accuracy while reducing attack success rates to 31-37 percent across multiple attack types, with particularly strong performance against optimization-based attacks such as C and W (48.8 percent reduction). The framework achieves throughput of up to 1.26 million samples per second (measured on pre-extracted EMBER features with no runtime feature extraction), validated across 72 production configurations with statistical significance (5 independent runs, 95 percent confidence intervals, p less than 0.01). Our results suggest that practical adversarial robustness in production environments requires explicit optimization of the efficiency-robustness trade-off, providing a viable path for organizations to deploy robust defenses without prohibitive infrastructure costs.

</details>


### [32] [Privacy-Preserving Federated Learning from Partial Decryption Verifiable Threshold Multi-Client Functional Encryption](https://arxiv.org/abs/2511.12936)
*Minjie Wang,Jinguang Han,Weizhi Meng*

Main category: cs.CR

TL;DR: 提出了一种可验证的联邦学习阈值安全聚合协议VTSAFL，通过部分解密验证技术确保聚合结果的可验证性，同时显著降低计算和通信开销。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中梯度泄露问题威胁隐私安全和模型完整性，现有基于阈值密码学的方案无法保证聚合结果的可验证性，易受投毒攻击威胁。

Method: 构建了部分解密可验证的阈值多客户端函数加密方案，并应用于联邦学习中实现VTSAFL协议，使客户端能够验证聚合结果。

Result: 在MNIST数据集上的实验表明，VTSAFL能达到与现有方案相同的准确率，同时总训练时间减少40%以上，通信开销降低达50%。

Conclusion: VTSAFL协议在保证安全性的同时显著提升了效率，特别适合资源受限的物联网设备部署。

Abstract: In federated learning, multiple parties can cooperate to train the model without directly exchanging their own private data, but the gradient leakage problem still threatens the privacy security and model integrity. Although the existing scheme uses threshold cryptography to mitigate the inference attack, it can not guarantee the verifiability of the aggregation results, making the system vulnerable to the threat of poisoning attack. We construct a partial decryption verifiable threshold multi client function encryption scheme, and apply it to Federated learning to implement the federated learning verifiable threshold security aggregation protocol (VTSAFL). VTSAFL empowers clients to verify aggregation results, concurrently minimizing both computational and communication overhead. The size of the functional key and partial decryption results of the scheme are constant, which provides efficiency guarantee for large-scale deployment. The experimental results on MNIST dataset show that vtsafl can achieve the same accuracy as the existing scheme, while reducing the total training time by more than 40%, and reducing the communication overhead by up to 50%. This efficiency is critical for overcoming the resource constraints inherent in Internet of Things (IoT) devices.

</details>


### [33] [Esim: EVM Bytecode Similarity Detection Based on Stable-Semantic Graph](https://arxiv.org/abs/2511.12971)
*Zhuo Chen,Gaoqiang Ji,Yiling He,Lei Wu,Yajin Zhou*

Main category: cs.CR

TL;DR: 提出了一种新的EVM字节码表示方法SSG，并开发了Esim工具用于智能合约相似性检测，在大规模研究中表现优于现有工具。


<details>
  <summary>Details</summary>
Motivation: DeFi快速发展中代码重用和开源贡献有限导致抄袭和漏洞传播问题，传统二进制相似性检测方法在EVM字节码上存在局限性。

Method: 提出Stable-Semantic Graph(SSG)表示方法，捕捉稳定指令间关系，使用异构图神经网络将SSG嵌入矩阵进行相似性检测。

Result: Esim在SSG构建上达到控制流100% F1分数和数据流95.16% F1分数，相似性检测性能达到96.3% AUC，优于传统方法。

Conclusion: Esim在六个EVM兼容链上的大规模研究中表现优于最先进工具Etherscan，证明了其在漏洞搜索中的有效性。

Abstract: Decentralized finance (DeFi) is experiencing rapid expansion. However, prevalent code reuse and limited open-source contributions have introduced significant challenges to the blockchain ecosystem, including plagiarism and the propagation of vulnerable code. Consequently, an effective and accurate similarity detection method for EVM bytecode is urgently needed to identify similar contracts. Traditional binary similarity detection methods are typically based on instruction stream or control flow graph (CFG), which have limitations on EVM bytecode due to specific features like low-level EVM bytecode and heavily-reused basic blocks. Moreover, the highly-diverse Solidity Compiler (Solc) versions further complicate accurate similarity detection.
  Motivated by these challenges, we propose a novel EVM bytecode representation called Stable-Semantic Graph (SSG), which captures relationships between 'stable instructions' (special instructions identified by our study). Moreover, we implement a prototype, Esim, which embeds SSG into matrices for similarity detection using a heterogeneous graph neural network. Esim demonstrates high accuracy in SSG construction, achieving F1-scores of 100% for control flow and 95.16% for data flow, and its similarity detection performance reaches 96.3% AUC, surpassing traditional approaches. Our large-scale study, analyzing 2,675,573 smart contracts on six EVM-compatible chains over a one-year period, also demonstrates that Esim outperforms the SOTA tool Etherscan in vulnerability search.

</details>


### [34] [The Grain Family of Stream Ciphers: an Abstraction, Strengthening of Components and New Concrete Instantiations](https://arxiv.org/abs/2511.12981)
*Palash Sarkar*

Main category: cs.CR

TL;DR: 本文提出了Grain流密码家族的抽象定义，改进了组件设计，并给出了7个具体实现方案，涵盖80位到256位安全级别。


<details>
  <summary>Details</summary>
Motivation: 为了改进Grain流密码家族的设计，提供更严格的组件定义和更优化的实现方案，以提升密码学特性并降低硬件实现成本。

Method: 1. 提出Grain家族的抽象定义框架；2. 定义新的非线性布尔函数类；3. 改进初始化阶段的状态更新函数；4. 优化抽头位置选择；5. 允许线性反馈移位寄存器小于非线性反馈移位寄存器。

Result: 提出了7个具体流密码方案：1个80位安全级别，2个128位，2个192位，2个256位安全级别。在80位级别相比Grain~v1改进了布尔函数特性并降低了门数；在128位级别相比Grain-128a改进了布尔函数特性，其中一个方案门数更低。

Conclusion: 通过抽象定义和组件改进，成功设计了在多个安全级别上具有更好密码学特性和更优硬件效率的Grain家族流密码方案。

Abstract: The first contribution of the paper is to put forward an abstract definition of the Grain family of stream ciphers which formalises the different components that are required to specify a particular member of the family. Our second contribution is to provide new and strengthened definitions of the components. These include definining new classes of nonlinear Boolean functions, improved definition of the state update function during initialisation, choice of the tap positions, and the possibility of the linear feedback shift register being smaller than the nonlinear feedback shift register. The third contribution of the paper is to put forward seven concrete proposals of stream ciphers by suitably instantiating the abstract family, one at the 80-bit security level, and two each at the 128-bit, 192-bit, and the 256-bit security levels. At the 80-bit security level, compared to the well known Grain~v1, the new proposal uses Boolean functions with improved cryptographic properties \textit{and} an overall lower gate count. At the 128-bit level, compared to ISO/IEC standard Grain-128a, the new proposals use Boolean functions with improved cryptographic properties; one of the proposals require a few extra gates, while the other has an overall lower gate count. At the 192-bit, and the 256-bit security levels, there are no proposals in the literature with smaller gate counts.

</details>


### [35] [SafeGRPO: Self-Rewarded Multimodal Safety Alignment via Rule-Governed Policy Optimization](https://arxiv.org/abs/2511.12982)
*Xuankun Rong,Wenke Huang,Tingfeng Wang,Daiguo Zhou,Bo Du,Mang Ye*

Main category: cs.CR

TL;DR: 提出了SafeGRPO框架，通过将规则治理的奖励构建集成到GRPO中，实现可解释和可验证的推理安全优化，显著提升多模态安全意识和组合鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型虽然展现出强大的推理能力，但其扩展的模态空间引入了新的组合安全风险，即使单个输入无害，跨模态耦合也可能产生不安全语义，暴露了当前MLLMs脆弱的安全意识。

Method: 基于构建的SafeTag-VL-3K数据集，SafeGRPO执行步骤引导的安全思考来强制执行结构化推理和行为对齐，将规则治理的奖励构建集成到GRPO框架中。

Result: 在多样化基准测试中显著提高了多模态安全意识、组合鲁棒性和推理稳定性，且不牺牲通用能力。

Conclusion: SafeGRPO框架通过可解释和可验证的优化方法，有效解决了多模态模型中的组合安全风险问题，为MLLMs的安全对齐提供了新的解决方案。

Abstract: Multimodal large language models (MLLMs) have demonstrated impressive reasoning and instruction-following capabilities, yet their expanded modality space introduces new compositional safety risks that emerge from complex text-image interactions. Such cross-modal couplings can produce unsafe semantics even when individual inputs are benign, exposing the fragile safety awareness of current MLLMs. While recent works enhance safety by guiding models to reason about potential risks, unregulated reasoning traces may compromise alignment; although Group Relative Policy Optimization (GRPO) offers self-rewarded refinement without human supervision, it lacks verifiable signals for reasoning safety. To address this, we propose SafeGRPO a self-rewarded multimodal safety alignment framework that integrates rule-governed reward construction into GRPO, enabling interpretable and verifiable optimization of reasoning safety. Built upon the constructed SafeTag-VL-3K dataset with explicit visual, textual, and combined safety tags, SafeGRPO performs step-guided safety thinking to enforce structured reasoning and behavior alignment, substantially improving multimodal safety awareness, compositional robustness, and reasoning stability across diverse benchmarks without sacrificing general capabilities.

</details>


### [36] [SoK: The Last Line of Defense: On Backdoor Defense Evaluation](https://arxiv.org/abs/2511.13143)
*Gorka Abad,Marina Krček,Stefanos Koffas,Behrad Tajalli,Marco Arazzi,Roberto Riaño,Xiaoyun Xu,Zhuoran Liu,Antonino Nocera,Stjepan Picek*

Main category: cs.CR

TL;DR: 对后门防御研究进行系统性元分析，通过文献综述和实证评估揭示了当前防御评估方法的不一致性，并提出了标准化建议。


<details>
  <summary>Details</summary>
Motivation: 后门攻击对深度学习模型构成严重威胁，但现有的防御评估方法存在异质性，阻碍了防御措施之间的公平比较。

Method: 分析了183篇2018-2025年间发表的防御论文，并通过大规模实验（3个数据集、4种模型架构、16种防御方法、5种攻击，总计3000+实验）评估防御效果。

Result: 发现防御效果在不同评估设置下差异显著，当前评估实践存在计算开销报告不足、良性条件下行为评估不充分、超参数选择偏差等关键缺陷。

Conclusion: 提出了具体的挑战和基于实证的建议，旨在标准化和改进未来的防御评估，为研究人员和从业者提供可操作的见解。

Abstract: Backdoor attacks pose a significant threat to deep learning models by implanting hidden vulnerabilities that can be activated by malicious inputs. While numerous defenses have been proposed to mitigate these attacks, the heterogeneous landscape of evaluation methodologies hinders fair comparison between defenses. This work presents a systematic (meta-)analysis of backdoor defenses through a comprehensive literature review and empirical evaluation. We analyzed 183 backdoor defense papers published between 2018 and 2025 across major AI and security venues, examining the properties and evaluation methodologies of these defenses.
  Our analysis reveals significant inconsistencies in experimental setups, evaluation metrics, and threat model assumptions in the literature. Through extensive experiments involving three datasets (MNIST, CIFAR-100, ImageNet-1K), four model architectures (ResNet-18, VGG-19, ViT-B/16, DenseNet-121), 16 representative defenses, and five commonly used attacks, totaling over 3\,000 experiments, we demonstrate that defense effectiveness varies substantially across different evaluation setups. We identify critical gaps in current evaluation practices, including insufficient reporting of computational overhead and behavior under benign conditions, bias in hyperparameter selection, and incomplete experimentation. Based on our findings, we provide concrete challenges and well-motivated recommendations to standardize and improve future defense evaluations. Our work aims to equip researchers and industry practitioners with actionable insights for developing, assessing, and deploying defenses to different systems.

</details>


### [37] [A Secure Semantic Communication System Based on Knowledge Graph](https://arxiv.org/abs/2511.13246)
*Qin Guo,Haonan Tong,Sihua Wang,Peiyuan Si,Jun Zhao,Changchuan Yin*

Main category: cs.CR

TL;DR: 提出了一种语义通信系统中文本数据传输安全的新方法，通过知识图谱提取和信道加密方案保护信息传输，使合法接收者能高精度恢复文本，而窃听者无法有效获取信息。


<details>
  <summary>Details</summary>
Motivation: 在语义通信系统中，文本数据传输面临被窃听的风险，需要确保只有合法接收者能够准确获取信息内容。

Method: 发送端将文本预处理为知识图谱，采用星座对角变换与多参数加权分数傅里叶变换相结合的加密方案；接收端先解密再通过transformer模型恢复文本。

Result: 合法接收者BLEU得分达到0.9，窃听者BLEU得分低于0.3，相比基线方法安全性提升最高达20%。

Conclusion: 该方法能有效降低信息泄露概率，在语义通信系统中实现了安全的文本数据传输。

Abstract: This study proposes a novel approach to ensure the security of textual data transmission in a semantic communication system. In the proposed system, a sender transmits textual information to a receiver, while a potential eavesdropper attempts to intercept the information. At the sender side, the text is initially preprocessed, where each sentence is annotated with its corresponding topic, and subsequently extracted into a knowledge graph. To achieve the secure transmission of the knowledge graph, we propose a channel encryption scheme that integrates constellation diagonal transformation with multi-parameter weighted fractional Fourier transform (MP-WFRFT). At the receiver side, the textual data is first decrypted, and then recovered via a transformer model. Experimental results demonstrate that the proposed method reduces the probability of information compromise. The legitimate receiver achieves a Bilingual Evaluation Understudy (BLEU) score of 0.9, whereas the BLEU score of the eavesdropper remains below 0.3. Compared to the baselines, the proposed method can improve the security by up to 20%.

</details>


### [38] [DualTAP: A Dual-Task Adversarial Protector for Mobile MLLM Agents](https://arxiv.org/abs/2511.13248)
*Fuyao Zhang,Jiaming Zhang,Che Wang,Xiongtao Sun,Yurong Hao,Guowei Guan,Wenjie Li,Longtao Huang,Wei Yang Bryan Lim*

Main category: cs.CR

TL;DR: DualTAP是一个保护移动GUI代理隐私的新框架，通过解耦隐私保护和任务效用的冲突目标，在保护PII信息不被第三方路由器的MLLMs挖掘的同时，保持代理MLLM的任务执行能力。


<details>
  <summary>Details</summary>
Motivation: 移动GUI代理依赖MLLMs时存在严重隐私漏洞：包含PII的截图会被发送到不受信任的第三方路由器，这些路由器可利用自己的MLLMs挖掘用户数据。现有隐私扰动方法无法同时满足保护PII和保持任务效用的双重挑战。

Method: DualTAP训练轻量级生成器，采用对比注意力模块精确定位PII敏感区域，以及双任务对抗目标同时最小化任务保持损失和隐私干扰损失。还引入了PrivScreen数据集用于双任务评估。

Result: 在六个不同MLLMs上的综合实验显示，DualTAP将平均隐私泄露率降低了31.6个百分点（相对改进3.0倍），同时保持80.8%的任务成功率，仅比未保护基线83.6%略有下降。

Conclusion: DualTAP是首个解决移动MLLM代理中隐私-效用权衡问题的可行方案，实现了最先进的隐私保护效果。

Abstract: The reliance of mobile GUI agents on Multimodal Large Language Models (MLLMs) introduces a severe privacy vulnerability: screenshots containing Personally Identifiable Information (PII) are often sent to untrusted, third-party routers. These routers can exploit their own MLLMs to mine this data, violating user privacy. Existing privacy perturbations fail the critical dual challenge of this scenario: protecting PII from the router's MLLM while simultaneously preserving task utility for the agent's MLLM. To address this gap, we propose the Dual-Task Adversarial Protector (DualTAP), a novel framework that, for the first time, explicitly decouples these conflicting objectives. DualTAP trains a lightweight generator using two key innovations: (i) a contrastive attention module that precisely identifies and targets only the PII-sensitive regions, and (ii) a dual-task adversarial objective that simultaneously minimizes a task-preservation loss (to maintain agent utility) and a privacy-interference loss (to suppress PII leakage). To facilitate this study, we introduce PrivScreen, a new dataset of annotated mobile screenshots designed specifically for this dual-task evaluation. Comprehensive experiments on six diverse MLLMs (e.g., GPT-5) demonstrate DualTAP's state-of-the-art protection. It reduces the average privacy leakage rate by 31.6 percentage points (a 3.0x relative improvement) while, critically, maintaining an 80.8% task success rate - a negligible drop from the 83.6% unprotected baseline. DualTAP presents the first viable solution to the privacy-utility trade-off in mobile MLLM agents.

</details>


### [39] [Whistledown: Combining User-Level Privacy with Conversational Coherence in LLMs](https://arxiv.org/abs/2511.13319)
*Chelsea McMurray,Hayder Tirmazi*

Main category: cs.CR

TL;DR: Whistledown是一个为LLM对话设计的隐私保护层，通过伪名化和本地差分隐私技术保护用户敏感信息，同时保持对话实用性。


<details>
  <summary>Details</summary>
Motivation: 用户在使用LLM进行个人、情感和社会敏感对话时，提示中可能包含不希望被记录、保留或泄露的个人身份信息，特别是在讨论朋友、同事或对手时。企业也有类似需求。

Method: 结合伪名化和ε-本地差分隐私技术，使用转换缓存提供尽力而为的隐私保护，无需修改现有LLM提供商的API。

Result: Whistledown具有低计算和内存开销，可部署在客户端设备上，企业版本可部署在零信任网关中。

Conclusion: Whistledown为LLM对话提供了有效的隐私保护解决方案，平衡了隐私保护和对话实用性。

Abstract: Users increasingly rely on large language models (LLMs) for personal, emotionally charged, and socially sensitive conversations. However, prompts sent to cloud-hosted models can contain personally identifiable information (PII) that users do not want logged, retained, or leaked. We observe this to be especially acute when users discuss friends, coworkers, or adversaries, i.e., when they spill the tea. Enterprises face the same challenge when they want to use LLMs for internal communication and decision-making.
  In this whitepaper, we present Whistledown, a best-effort privacy layer that modifies prompts before they are sent to the LLM. Whistledown combines pseudonymization and $ε$-local differential privacy ($ε$-LDP) with transformation caching to provide best-effort privacy protection without sacrificing conversational utility. Whistledown is designed to have low compute and memory overhead, allowing it to be deployed directly on a client's device in the case of individual users. For enterprise users, Whistledown is deployed centrally within a zero-trust gateway that runs on an enterprise's trusted infrastructure. Whistledown requires no changes to the existing APIs of popular LLM providers.

</details>


### [40] [AutoMalDesc: Large-Scale Script Analysis for Cyber Threat Research](https://arxiv.org/abs/2511.13333)
*Alexandru-Mihai Apostu,Andrei Preda,Alexandra Daniela Damir,Diana Bolocan,Radu Tudor Ionescu,Ioana Croitoru,Mihaela Gaman*

Main category: cs.CR

TL;DR: AutoMalDesc是一个自动化静态分析摘要框架，通过自步学习管道生成威胁检测的自然语言解释，无需大量人工标注，在3600个多样本上验证了摘要质量和分类准确性的显著提升。


<details>
  <summary>Details</summary>
Motivation: 尽管恶意软件检测系统取得了显著进展，但为威胁检测生成全面的自然语言解释仍然是网络安全研究中的一个开放问题。

Method: 采用迭代自步学习管道，通过合成数据生成和验证循环逐步提高输出质量，无需大量手动数据标注。

Result: 在五种脚本语言的3600个多样本上评估显示，迭代间具有统计显著性的改进，摘要质量和分类准确性均获得一致提升。

Conclusion: 综合验证方法确认了生成摘要的技术精确性和语言连贯性，发布了包含10万+脚本样本的完整数据集以促进该领域研究。

Abstract: Generating thorough natural language explanations for threat detections remains an open problem in cybersecurity research, despite significant advances in automated malware detection systems. In this work, we present AutoMalDesc, an automated static analysis summarization framework that, following initial training on a small set of expert-curated examples, operates independently at scale. This approach leverages an iterative self-paced learning pipeline to progressively enhance output quality through synthetic data generation and validation cycles, eliminating the need for extensive manual data annotation. Evaluation across 3,600 diverse samples in five scripting languages demonstrates statistically significant improvements between iterations, showing consistent gains in both summary quality and classification accuracy. Our comprehensive validation approach combines quantitative metrics based on established malware labels with qualitative assessment from both human experts and LLM-based judges, confirming both technical precision and linguistic coherence of generated summaries. To facilitate reproducibility and advance research in this domain, we publish our complete dataset of more than 100K script samples, including annotated seed (0.9K) and test (3.6K) datasets, along with our methodology and evaluation framework.

</details>


### [41] [Enhancing All-to-X Backdoor Attacks with Optimized Target Class Mapping](https://arxiv.org/abs/2511.13356)
*Lei Wang,Yulong Tian,Hao Han,Fengyuan Xu*

Main category: cs.CR

TL;DR: 本文提出了一种增强多目标后门攻击(A2X)成功率的新方法，相比传统单目标攻击(A2O)，A2X攻击对现有防御具有更强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击研究主要关注单目标A2O攻击，忽视了更复杂的多目标A2X攻击，且普遍认为A2X攻击成功率较低。

Method: 通过优化分组和目标类别分配机制来增强A2X攻击的成功率，同时保持对现有防御的鲁棒性。

Result: 在CIFAR10、CIFAR100和Tiny-ImageNet数据集上，攻击成功率分别平均提升6.7%、16.4%和14.1%，最高提升达28%。

Conclusion: 该研究提高了对A2X攻击威胁的认识，并呼吁在这一未充分探索的领域进行更多研究。

Abstract: Backdoor attacks pose severe threats to machine learning systems, prompting extensive research in this area. However, most existing work focuses on single-target All-to-One (A2O) attacks, overlooking the more complex All-to-X (A2X) attacks with multiple target classes, which are often assumed to have low attack success rates. In this paper, we first demonstrate that A2X attacks are robust against state-of-the-art defenses. We then propose a novel attack strategy that enhances the success rate of A2X attacks while maintaining robustness by optimizing grouping and target class assignment mechanisms. Our method improves the attack success rate by up to 28%, with average improvements of 6.7%, 16.4%, 14.1% on CIFAR10, CIFAR100, and Tiny-ImageNet, respectively. We anticipate that this study will raise awareness of A2X attacks and stimulate further research in this under-explored area. Our code is available at https://github.com/kazefjj/A2X-backdoor .

</details>


### [42] [InfoDecom: Decomposing Information for Defending against Privacy Leakage in Split Inference](https://arxiv.org/abs/2511.13365)
*Ruijun Deng,Zhihui Lu,Qiang Duan*

Main category: cs.CR

TL;DR: InfoDecom是一个针对分割推理中数据重建攻击的防御框架，通过分解和去除冗余信息，然后注入理论保证隐私的噪声，实现了更好的效用-隐私权衡。


<details>
  <summary>Details</summary>
Motivation: 分割推理虽然能避免直接传输原始数据，但最近研究表明数据重建攻击可以从客户端发送的粉碎数据中恢复原始输入，导致严重隐私泄露。现有防御方法往往导致显著的效用下降，特别是当客户端模型较浅时。

Method: 首先分解并去除粉碎数据中的冗余信息，然后注入经过校准的噪声，该噪声能提供理论保证的隐私保护。

Result: 实验表明，与现有基线方法相比，InfoDecom在计算机视觉任务中实现了更优的效用-隐私权衡。

Conclusion: InfoDecom通过针对性地处理冗余信息并注入理论保证的噪声，有效解决了分割推理中的数据重建攻击问题，同时保持了良好的模型性能。

Abstract: Split inference (SI) enables users to access deep learning (DL) services without directly transmitting raw data. However, recent studies reveal that data reconstruction attacks (DRAs) can recover the original inputs from the smashed data sent from the client to the server, leading to significant privacy leakage. While various defenses have been proposed, they often result in substantial utility degradation, particularly when the client-side model is shallow. We identify a key cause of this trade-off: existing defenses apply excessive perturbation to redundant information in the smashed data. To address this issue in computer vision tasks, we propose InfoDecom, a defense framework that first decomposes and removes redundant information and then injects noise calibrated to provide theoretically guaranteed privacy. Experiments demonstrate that InfoDecom achieves a superior utility-privacy trade-off compared to existing baselines. The code and the appendix are available at https://github.com/SASA-cloud/InfoDecom.

</details>


### [43] [Tight and Practical Privacy Auditing for Differentially Private In-Context Learning](https://arxiv.org/abs/2511.13502)
*Yuyang Xia,Ruixuan Liu,Li Xiong*

Main category: cs.CR

TL;DR: 提出了一个针对差分隐私上下文学习系统的紧密且高效的隐私审计框架，通过成员推断攻击将成功率转换为经验性隐私保证


<details>
  <summary>Details</summary>
Motivation: LLMs在上下文学习中使用的演示数据可能包含私有或专有数据，现有DP实现容易出错且最坏情况下的DP边界可能高估实际泄漏，需要实用的审计工具

Method: 使用高斯DP框架，分析私有投票机制以识别最大化审计信号的投票配置，设计可靠的审计查询来检测canary演示是否存在于上下文中，支持黑盒和白盒威胁模型

Result: 在标准文本分类和生成基准测试中，经验泄漏估计与分类任务的理论DP预算紧密匹配，在生成任务中由于保守的嵌入敏感性边界而始终较低

Conclusion: 该框架可作为实际隐私审计器和验证器，适用于现实世界的DP-ICL部署

Abstract: Large language models (LLMs) perform in-context learning (ICL) by adapting to tasks from prompt demonstrations, which in practice often contain private or proprietary data. Although differential privacy (DP) with private voting is a pragmatic mitigation, DP-ICL implementations are error-prone, and worst-case DP bounds may substantially overestimate actual leakage, calling for practical auditing tools. We present a tight and efficient privacy auditing framework for DP-ICL systems that runs membership inference attacks and translates their success rates into empirical privacy guarantees using Gaussian DP. Our analysis of the private voting mechanism identifies vote configurations that maximize the auditing signal, guiding the design of audit queries that reliably reveal whether a canary demonstration is present in the context. The framework supports both black-box (API-only) and white-box (internal vote) threat models, and unifies auditing for classification and generation by reducing both to a binary decision problem. Experiments on standard text classification and generation benchmarks show that our empirical leakage estimates closely match theoretical DP budgets on classification tasks and are consistently lower on generation tasks due to conservative embedding-sensitivity bounds, making our framework a practical privacy auditor and verifier for real-world DP-ICL deployments.

</details>


### [44] [Interpretable Ransomware Detection Using Hybrid Large Language Models: A Comparative Analysis of BERT, RoBERTa, and DeBERTa Through LIME and SHAP](https://arxiv.org/abs/2511.13517)
*Elodie Mutombo Ngoie,Mike Nkongolo Wa Nkongolo,Peace Azugo,Mahmut Tokmak*

Main category: cs.CR

TL;DR: 本研究比较了三种基于Transformer的大语言模型（BERT、RoBERTa、DeBERTa）在勒索软件检测中的表现，通过将数值和分类特征转换为文本序列，结合可解释AI技术（LIME和SHAP）提供透明的决策依据。


<details>
  <summary>Details</summary>
Motivation: 勒索软件日益复杂化，需要早期且可解释的检测方法，而大语言模型在自然语言处理领域的成功为勒索软件行为模式识别提供了新思路。

Method: 使用KBinsDiscretizer和基于标记的编码将数值和分类勒索软件特征转换为文本序列，在UGRansome和Process Memory数据集上对三种LLM模型进行微调，并应用LIME和SHAP进行可解释性分析。

Result: 不同模型学习到不同的勒索软件特征模式：BERT依赖文件操作特征，RoBERTa平衡网络和金融信号，DeBERTa对金融和网络流量指标敏感。RoBERTa获得最佳F1分数，BERT获得最高ROC-AUC性能。

Conclusion: LLM与XAI的结合为勒索软件检测提供了透明框架，能够识别预测背后的特征级证据，在网络安全领域具有重要应用价值。

Abstract: Ransomware continues to evolve in complexity, making early and explainable detection a critical requirement for modern cybersecurity systems. This study presents a comparative analysis of three Transformer-based Large Language Models (LLMs) (BERT, RoBERTa, and DeBERTa) for ransomware detection using two structured datasets: UGRansome and Process Memory (PM). Since LLMs are primarily designed for natural language processing (NLP), numerical and categorical ransomware features were transformed into textual sequences using KBinsDiscretizer and token-based encoding. This enabled the models to learn behavioural patterns from system activity and network traffic through contextual embeddings. The models were fine-tuned on approximately 2,500 labelled samples and evaluated using accuracy, F1 score, and ROC-AUC. To ensure transparent decision-making in this high-stakes domain, two explainable AI techniques (LIME and SHAP) were applied to interpret feature contributions. The results show that the models learn distinct ransomware-related cues: BERT relies heavily on dominant file-operation features, RoBERTa demonstrates balanced reliance on network and financial signals, while DeBERTa exhibits strong sensitivity to financial and network-traffic indicators. Visualisation of embeddings further reveals structural differences in token representation, with RoBERTa producing more isotropic embeddings and DeBERTa capturing highly directional, disentangled patterns. In general, RoBERTa achieved the strongest F1-score, while BERT yielded the highest ROC-AUC performance. The integration of LLMs with XAI provides a transparent framework capable of identifying feature-level evidence behind ransomware predictions.

</details>


### [45] [ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models](https://arxiv.org/abs/2511.13548)
*Siyang Cheng,Gaotian Liu,Rui Mei,Yilin Wang,Kejia Zhang,Kaishuo Wei,Yuqi Yu,Weiping Wen,Xiaojie Wu,Junhua Liu*

Main category: cs.CR

TL;DR: ForgeDAN是一个新颖的进化框架，通过多策略文本扰动、可解释语义适应度评估和双维度越狱判断，有效生成对抗性提示来绕过LLM的安全对齐机制。


<details>
  <summary>Details</summary>
Motivation: 现有自动化越狱生成方法（如AutoDAN）存在突变多样性有限、适应度评估浅层和基于关键词检测脆弱等问题，需要更有效的解决方案。

Method: 采用字符、词和句子级别的多策略文本扰动增强攻击多样性；使用基于文本相似度模型的可解释语义适应度评估；集成基于LLM的分类器进行双维度越狱判断。

Result: 评估显示ForgeDAN实现了高越狱成功率，同时保持自然性和隐蔽性，优于现有最先进解决方案。

Conclusion: ForgeDAN框架在生成语义连贯且高效的对抗性提示方面表现出色，有效解决了现有方法的局限性。

Abstract: The rapid adoption of large language models (LLMs) has brought both transformative applications and new security risks, including jailbreak attacks that bypass alignment safeguards to elicit harmful outputs. Existing automated jailbreak generation approaches e.g. AutoDAN, suffer from limited mutation diversity, shallow fitness evaluation, and fragile keyword-based detection. To address these limitations, we propose ForgeDAN, a novel evolutionary framework for generating semantically coherent and highly effective adversarial prompts against aligned LLMs. First, ForgeDAN introduces multi-strategy textual perturbations across \textit{character, word, and sentence-level} operations to enhance attack diversity; then we employ interpretable semantic fitness evaluation based on a text similarity model to guide the evolutionary process toward semantically relevant and harmful outputs; finally, ForgeDAN integrates dual-dimensional jailbreak judgment, leveraging an LLM-based classifier to jointly assess model compliance and output harmfulness, thereby reducing false positives and improving detection effectiveness. Our evaluation demonstrates ForgeDAN achieves high jailbreaking success rates while maintaining naturalness and stealth, outperforming existing SOTA solutions.

</details>


### [46] [Exploring the Effectiveness of Google Play Store's Privacy Transparency Channels](https://arxiv.org/abs/2511.13576)
*Anhao Xiang,Weiping Pei,Chuan Yue*

Main category: cs.CR

TL;DR: 研究评估了Google Play商店中三种隐私透明度渠道（数据安全、隐私政策、权限清单）在帮助用户做出明智应用选择方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着GDPR和CCPA等法规对隐私透明度的要求，应用商店要求开发者通过数据安全、隐私政策和权限清单等渠道向用户传达隐私实践，但这些渠道的实际效果尚不明确。

Method: 对190名参与者进行实验，让他们与模拟的移动应用隐私透明度渠道互动，通过定量分析（辅以定性分析）参与者对五组问题的回答来评估各渠道效果。

Result: 数据安全提供最直观的用户界面，隐私政策信息最丰富且最有效，权限清单在提高用户对应用整体隐私风险的关注方面表现突出。

Conclusion: 这三种隐私透明度渠道相互补充，都需要进一步改进以更好地帮助用户做出明智的应用选择决策。

Abstract: With the requirements and emphases on privacy transparency placed by regulations such as GDPR and CCPA, the Google Play Store requires Android developers to more responsibly communicate their apps' privacy practices to potential users by providing the proper information via the data safety, privacy policy, and permission manifest privacy transparency channels. However, it is unclear how effective those channels are in helping users make informed decisions in the app selection and installation process. In this article, we conducted a study for 190 participants to interact with our simulated privacy transparency channels of mobile apps. We quantitatively analyzed (supplemented by qualitative analysis) participants' responses to five sets of questions. We found that data safety provides the most intuitive user interfaces, privacy policy is most informative and effective, while permission manifest excels at raising participants' concerns about an app's overall privacy risks. These channels complement each other and should all be improved.

</details>


### [47] [Robust Client-Server Watermarking for Split Federated Learning](https://arxiv.org/abs/2511.13598)
*Jiaxiong Tang,Zhengchunmin Dai,Liantao Wu,Peng Sun,Honglong Chen,Zhenfu Cao*

Main category: cs.CR

TL;DR: RISE是一种用于分割联邦学习的鲁棒模型知识产权保护方案，采用客户端-服务器非对称水印设计，使双方都能验证模型所有权。


<details>
  <summary>Details</summary>
Motivation: 分割联邦学习虽然具有隐私保护和计算效率优势，但存在知识产权归属模糊的问题，因为客户端和服务器共同参与训练，现有水印技术无法保护双方权益。

Method: 采用非对称水印设计：服务器通过损失正则化项嵌入基于特征的水印，客户端通过向私有数据集中注入预定义触发样本来嵌入基于后门的水印。

Result: 在标准数据集和多种网络架构上的实验表明，RISE在大多数设置下达到超过95%的水印检测率（p值<0.03），客户端和服务器端水印无相互干扰，且对常见移除攻击具有鲁棒性。

Conclusion: RISE有效解决了分割联邦学习中的知识产权保护问题，通过客户端-服务器协同水印嵌入策略，实现了双方模型所有权的可验证保护。

Abstract: Split Federated Learning (SFL) is renowned for its privacy-preserving nature and low computational overhead among decentralized machine learning paradigms. In this framework, clients employ lightweight models to process private data locally and transmit intermediate outputs to a powerful server for further computation. However, SFL is a double-edged sword: while it enables edge computing and enhances privacy, it also introduces intellectual property ambiguity as both clients and the server jointly contribute to training. Existing watermarking techniques fail to protect both sides since no single participant possesses the complete model. To address this, we propose RISE, a Robust model Intellectual property protection scheme using client-Server watermark Embedding for SFL. Specifically, RISE adopts an asymmetric client-server watermarking design: the server embeds feature-based watermarks through a loss regularization term, while clients embed backdoor-based watermarks by injecting predefined trigger samples into private datasets. This co-embedding strategy enables both clients and the server to verify model ownership. Experimental results on standard datasets and multiple network architectures show that RISE achieves over $95\%$ watermark detection rate ($p-value \lt 0.03$) across most settings. It exhibits no mutual interference between client- and server-side watermarks and remains robust against common removal attacks.

</details>


### [48] [It's a Feature, Not a Bug: Secure and Auditable State Rollback for Confidential Cloud Applications](https://arxiv.org/abs/2511.13641)
*Quinn Burke,Anjo Vahldiek-Oberwagner,Michael Swift,Patrick McDaniel*

Main category: cs.CR

TL;DR: Rebound是一个安全框架，在保持回滚保护的同时允许策略授权的合法回滚，解决了传统方法将所有回滚视为恶意的问题。


<details>
  <summary>Details</summary>
Motivation: 现有安全框架将所有回滚视为恶意攻击，阻止了用于操作恢复的合法回滚，限制了云应用的实用性。

Method: 使用参考监视器来调解状态转换，强制执行授权策略，保证状态更新和回滚的原子性，并生成防篡改日志。

Result: 通过GitLab CI的案例研究证明，Rebound能够对二进制文件、配置和原始数据版本进行稳健控制，且端到端开销较低。

Conclusion: Rebound在保持安全性的同时支持合法回滚，为云应用提供了更灵活和实用的安全保护机制。

Abstract: Replay and rollback attacks threaten cloud application integrity by reintroducing authentic yet stale data through an untrusted storage interface to compromise application decision-making. Prior security frameworks mitigate these attacks by enforcing forward-only state transitions (state continuity) with hardware-backed mechanisms, but they categorically treat all rollback as malicious and thus preclude legitimate rollbacks used for operational recovery from corruption or misconfiguration. We present Rebound, a general-purpose security framework that preserves rollback protection while enabling policy-authorized legitimate rollbacks of application binaries, configuration, and data. Key to Rebound is a reference monitor that mediates state transitions, enforces authorization policy, guarantees atomicity of state updates and rollbacks, and emits a tamper-evident log that provides transparency to applications and auditors. We formally prove Rebound's security properties and show through an application case study -- with software deployment workflows in GitLab CI -- that it enables robust control over binary, configuration, and raw data versioning with low end-to-end overhead.

</details>


### [49] [TZ-LLM: Protecting On-Device Large Language Models with Arm TrustZone](https://arxiv.org/abs/2511.13717)
*Xunjie Wang,Jiacheng Shi,Zihan Zhao,Yang Yu,Zhichao Hua,Jinyu Gu*

Main category: cs.CR

TL;DR: 提出了一种基于Arm TrustZone TEE的保护移动设备上LLM的系统设计，通过流水线恢复和协同驱动设计解决了内存效率与推理速度的权衡问题，以及NPU安全共享的挑战。


<details>
  <summary>Details</summary>
Motivation: 移动设备部署LLM虽然带来用户隐私和低延迟优势，但存在专有模型泄露给终端用户的安全风险，需要保护机制。

Method: 采用流水线恢复技术利用LLM推理的内存访问模式预取参数，隐藏内存分配、I/O和解密延迟；设计协同驱动架构，在TEE中创建最小数据平面NPU驱动与REE驱动协作。

Result: 在OpenHarmony OS和llama.cpp上实现，相比未优化的TEE基线，TTFT降低高达90.9%，解码速度提升高达23.2%。

Conclusion: 该系统有效解决了移动设备LLM部署的安全保护问题，在保持安全性的同时显著提升了推理性能。

Abstract: Large Language Models (LLMs) deployed on mobile devices offer benefits like user privacy and reduced network latency, but introduce a significant security risk: the leakage of proprietary models to end users.
  To mitigate this risk, we propose a system design for protecting on-device LLMs using Arm Trusted Execution Environment (TEE), TrustZone. Our system addresses two primary challenges: (1) The dilemma between memory efficiency and fast inference (caching model parameters within TEE memory). (2) The lack of efficient and secure Neural Processing Unit (NPU) time-sharing between Rich Execution Environment (REE) and TEE.
  Our approach incorporates two key innovations. First, we employ pipelined restoration, leveraging the deterministic memory access patterns of LLM inference to prefetch parameters on demand, hiding memory allocation, I/O and decryption latency under computation time. Second, we introduce a co-driver design, creating a minimal data plane NPU driver in the TEE that collaborates with the full-fledged REE driver. This reduces the TEE TCB size and eliminates control plane reinitialization overhead during NPU world switches.
  We implemented our system on the emerging OpenHarmony OS and the llama.cpp inference framework, and evaluated it with various LLMs on an Arm Rockchip device. Compared to a strawman TEE baseline lacking our optimizations, our system reduces TTFT by up to 90.9% and increases decoding speed by up to 23.2%.

</details>
