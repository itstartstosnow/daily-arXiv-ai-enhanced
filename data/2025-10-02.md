<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 17]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Stealing AI Model Weights Through Covert Communication Channels](https://arxiv.org/abs/2510.00151)
*Valentin Barbaza,Alan Rodrigo Diaz-Rizo,Hassan Aboushady,Spyridon Raptis,Haralampos-G. Stratigopoulos*

Main category: cs.CR

TL;DR: 提出一种针对配备AI硬件加速器的无线设备的新型模型窃取攻击，通过硬件木马和隐蔽通信信道窃取模型权重


<details>
  <summary>Details</summary>
Motivation: AI模型因其高昂开发成本、竞争优势和专有技术而成为重要知识产权，模型窃取攻击对AI模型提供商构成严重威胁

Method: 攻击分两阶段：第一阶段在受害者设备中植入硬件木马，通过隐蔽信道泄露模型权重；第二阶段使用附近无线设备拦截传输帧，逐步重建完整权重矩阵

Result: 通过硬件演示验证了攻击对四种不同类型和大小的AI模型的有效性，分析了误码率影响并提出了错误缓解技术

Conclusion: 该攻击对AI模型架构和硬件加速器具有通用性，需要探索有效的防御机制来应对此类威胁

Abstract: AI models are often regarded as valuable intellectual property due to the
high cost of their development, the competitive advantage they provide, and the
proprietary techniques involved in their creation. As a result, AI model
stealing attacks pose a serious concern for AI model providers. In this work,
we present a novel attack targeting wireless devices equipped with AI hardware
accelerators. The attack unfolds in two phases. In the first phase, the
victim's device is compromised with a hardware Trojan (HT) designed to covertly
leak model weights through a hidden communication channel, without the victim
realizing it. In the second phase, the adversary uses a nearby wireless device
to intercept the victim's transmission frames during normal operation and
incrementally reconstruct the complete weight matrix. The proposed attack is
agnostic to both the AI model architecture and the hardware accelerator used.
We validate our approach through a hardware-based demonstration involving four
diverse AI models of varying types and sizes. We detail the design of the HT
and the covert channel, highlighting their stealthy nature. Additionally, we
analyze the impact of bit error rates on the reception and propose an error
mitigation technique. The effectiveness of the attack is evaluated based on the
accuracy of the reconstructed models with stolen weights and the time required
to extract them. Finally, we explore potential defense mechanisms.

</details>


### [2] [Calyx: Privacy-Preserving Multi-Token Optimistic-Rollup Protocol](https://arxiv.org/abs/2510.00164)
*Dominik Apel,Zeta Avarikioti,Matteo Maffei,Yuheng Wang*

Main category: cs.CR

TL;DR: Calyx是首个隐私保护的多代币乐观Rollup协议，为所有L2交易提供完整的支付隐私保护，不泄露发送者、接收者、转账金额或代币类型等信息。


<details>
  <summary>Details</summary>
Motivation: 解决当前Rollup协议因需要在链上发布交易明文而存在的隐私限制问题。

Method: 采用高效的一步欺诈证明机制，支持多代币交易的原子执行，并引入交易费用方案。

Result: 单笔交易执行成本约为0.06美元（0.00002 ETH），在渐近意义上仅产生恒定大小的链上成本。

Conclusion: Calyx协议在保证安全性和隐私性的同时，实现了高效的交易执行和可持续的协议运营。

Abstract: Rollup protocols have recently received significant attention as a promising
class of Layer 2 (L2) scalability solutions. By utilizing the Layer 1 (L1)
blockchain solely as a bulletin board for a summary of the executed
transactions and state changes, rollups enable secure off-chain execution while
avoiding the complexity of other L2 mechanisms. However, to ensure data
availability, current rollup protocols require the plaintext of executed
transactions to be published on-chain, resulting in inherent privacy
limitations.
  In this paper, we address this problem by introducing Calyx, the first
privacy-preserving multi-token optimistic-Rollup protocol. Calyx guarantees
full payment privacy for all L2 transactions, revealing no information about
the sender, recipient, transferred amount, or token type. The protocol further
supports atomic execution of multiple multi-token transactions and introduces a
transaction fee scheme to enable broader application scenarios while ensuring
the sustainable operation of the protocol. To enforce correctness, Calyx adopts
an efficient one-step fraud-proof mechanism. We analyze the security and
privacy guarantees of the protocol and provide an implementation and
evaluation. Our results show that executing a single transaction costs
approximately $0.06 (0.00002 ETH) and incurs only constant-size on-chain cost
in asymptotic terms.

</details>


### [3] [CHAI: Command Hijacking against embodied AI](https://arxiv.org/abs/2510.00181)
*Luis Burbano,Diego Ortiz,Qi Sun,Siwei Yang,Haoqin Tu,Cihang Xie,Yinzhi Cao,Alvaro A Cardenas*

Main category: cs.CR

TL;DR: CHAI是一种针对具身AI系统的新型提示攻击方法，通过在多模态视觉输入中嵌入欺骗性自然语言指令，利用大型视觉语言模型的语义理解能力来劫持系统命令。


<details>
  <summary>Details</summary>
Motivation: 具身AI系统虽然能够通过常识推理处理边缘情况，但这种能力也带来了新的安全风险。研究者发现可以利用LVLM的多模态语言解释能力来实施攻击。

Method: CHAI通过系统搜索令牌空间、构建提示字典，并指导攻击模型生成视觉攻击提示，将欺骗性自然语言指令嵌入到视觉输入中。

Result: 在四个LVLM代理（无人机紧急降落、自动驾驶、空中目标跟踪）和真实机器人车辆上的实验表明，CHAI始终优于最先进的攻击方法。

Conclusion: CHAI攻击暴露了下一代具身AI系统的安全漏洞，强调了需要超越传统对抗鲁棒性的防御措施。

Abstract: Embodied Artificial Intelligence (AI) promises to handle edge cases in
robotic vehicle systems where data is scarce by using common-sense reasoning
grounded in perception and action to generalize beyond training distributions
and adapt to novel real-world situations. These capabilities, however, also
create new security risks. In this paper, we introduce CHAI (Command Hijacking
against embodied AI), a new class of prompt-based attacks that exploit the
multimodal language interpretation abilities of Large Visual-Language Models
(LVLMs). CHAI embeds deceptive natural language instructions, such as
misleading signs, in visual input, systematically searches the token space,
builds a dictionary of prompts, and guides an attacker model to generate Visual
Attack Prompts. We evaluate CHAI on four LVLM agents; drone emergency landing,
autonomous driving, and aerial object tracking, and on a real robotic vehicle.
Our experiments show that CHAI consistently outperforms state-of-the-art
attacks. By exploiting the semantic and multimodal reasoning strengths of
next-generation embodied AI systems, CHAI underscores the urgent need for
defenses that extend beyond traditional adversarial robustness.

</details>


### [4] [SecureBERT 2.0: Advanced Language Model for Cybersecurity Intelligence](https://arxiv.org/abs/2510.00240)
*Ehsan Aghaei,Sarthak Jain,Prashanth Arun,Arjun Sambamoorthy*

Main category: cs.CR

TL;DR: SecureBERT 2.0是一个专门为网络安全应用设计的增强型编码器语言模型，在多个网络安全基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 通用语言模型缺乏网络安全领域所需的专业适应性，无法有效处理专业术语、复杂文档结构以及自然语言与源代码的相互依赖关系。

Method: 基于ModernBERT架构，引入改进的长上下文建模和分层编码，在比前身大13倍的领域特定语料库上进行预训练，包含超过130亿文本标记和5300万代码标记。

Result: 在威胁情报语义搜索、语义分析、网络安全特定命名实体识别和代码漏洞自动检测等任务中表现出显著改进。

Conclusion: SecureBERT 2.0为网络安全分析和威胁检测提供了高效且精确的专门化语言模型解决方案。

Abstract: Effective analysis of cybersecurity and threat intelligence data demands
language models that can interpret specialized terminology, complex document
structures, and the interdependence of natural language and source code.
Encoder-only transformer architectures provide efficient and robust
representations that support critical tasks such as semantic search, technical
entity extraction, and semantic analysis, which are key to automated threat
detection, incident triage, and vulnerability assessment. However,
general-purpose language models often lack the domain-specific adaptation
required for high precision. We present SecureBERT 2.0, an enhanced
encoder-only language model purpose-built for cybersecurity applications.
Leveraging the ModernBERT architecture, SecureBERT 2.0 introduces improved
long-context modeling and hierarchical encoding, enabling effective processing
of extended and heterogeneous documents, including threat reports and source
code artifacts. Pretrained on a domain-specific corpus more than thirteen times
larger than its predecessor, comprising over 13 billion text tokens and 53
million code tokens from diverse real-world sources, SecureBERT 2.0 achieves
state-of-the-art performance on multiple cybersecurity benchmarks. Experimental
results demonstrate substantial improvements in semantic search for threat
intelligence, semantic analysis, cybersecurity-specific named entity
recognition, and automated vulnerability detection in code within the
cybersecurity domain.

</details>


### [5] [MAVUL: Multi-Agent Vulnerability Detection via Contextual Reasoning and Interactive Refinement](https://arxiv.org/abs/2510.00317)
*Youpeng Li,Kartik Joshi,Xinda Wang,Eric Wong*

Main category: cs.CR

TL;DR: MAVUL是一个新颖的多代理漏洞检测系统，通过集成上下文推理和交互式优化来解决现有漏洞检测方法的局限性，显著提升了检测性能和评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞检测方法存在上下文理解不足、单轮交互限制和粗粒度评估等问题，导致模型性能不佳和评估结果偏差。需要解决这些挑战以提高漏洞检测的准确性和可靠性。

Method: 设计了一个漏洞分析代理，灵活利用工具使用能力和上下文推理实现跨过程代码理解；通过跨角色代理交互中的迭代反馈和优化决策，实现可靠推理和漏洞预测；引入多维真实信息进行细粒度评估。

Result: 在成对漏洞数据集上的实验表明，MAVUL显著优于现有多代理系统（成对准确率提高62%以上）和单代理系统（平均性能提高600%以上）；漏洞分析代理与安全架构代理之间的通信轮次增加显著提升系统有效性。

Conclusion: 上下文推理在追踪漏洞流中至关重要，反馈机制发挥关键作用；集成评估代理作为无偏见的评判者，通过防止误导性二元比较确保系统实际应用性的准确可靠评估。

Abstract: The widespread adoption of open-source software (OSS) necessitates the
mitigation of vulnerability risks. Most vulnerability detection (VD) methods
are limited by inadequate contextual understanding, restrictive single-round
interactions, and coarse-grained evaluations, resulting in undesired model
performance and biased evaluation results. To address these challenges, we
propose MAVUL, a novel multi-agent VD system that integrates contextual
reasoning and interactive refinement. Specifically, a vulnerability analyst
agent is designed to flexibly leverage tool-using capabilities and contextual
reasoning to achieve cross-procedural code understanding and effectively mine
vulnerability patterns. Through iterative feedback and refined decision-making
within cross-role agent interactions, the system achieves reliable reasoning
and vulnerability prediction. Furthermore, MAVUL introduces multi-dimensional
ground truth information for fine-grained evaluation, thereby enhancing
evaluation accuracy and reliability.
  Extensive experiments conducted on a pairwise vulnerability dataset
demonstrate MAVUL's superior performance. Our findings indicate that MAVUL
significantly outperforms existing multi-agent systems with over 62% higher
pairwise accuracy and single-agent systems with over 600% higher average
performance. The system's effectiveness is markedly improved with increased
communication rounds between the vulnerability analyst agent and the security
architect agent, underscoring the importance of contextual reasoning in tracing
vulnerability flows and the crucial feedback role. Additionally, the integrated
evaluation agent serves as a critical, unbiased judge, ensuring a more accurate
and reliable estimation of the system's real-world applicability by preventing
misleading binary comparisons.

</details>


### [6] [Privately Estimating Black-Box Statistics](https://arxiv.org/abs/2510.00322)
*Günter F. Steinke,Thomas Steinke*

Main category: cs.CR

TL;DR: 提出了一种在统计效率和计算效率之间权衡的差分隐私方案，适用于任意黑盒函数，无需预先知道敏感度边界


<details>
  <summary>Details</summary>
Motivation: 传统差分隐私方法需要知道估计器的敏感度边界，但这些边界往往很大或未知，现有黑盒方法要么数据效率低，要么需要指数级函数评估

Method: 设计了一个在统计效率和计算效率之间权衡的方案，通过控制函数评估次数来实现差分隐私

Result: 提出的方案在统计效率和计算效率之间实现了良好平衡，并给出了证明方案接近最优性的下界

Conclusion: 该工作为处理任意黑盒函数的差分隐私问题提供了一种实用的解决方案，在效率和隐私保护之间找到了平衡点

Abstract: Standard techniques for differentially private estimation, such as Laplace or
Gaussian noise addition, require guaranteed bounds on the sensitivity of the
estimator in question. But such sensitivity bounds are often large or simply
unknown. Thus we seek differentially private methods that can be applied to
arbitrary black-box functions. A handful of such techniques exist, but all are
either inefficient in their use of data or require evaluating the function on
exponentially many inputs. In this work we present a scheme that trades off
between statistical efficiency (i.e., how much data is needed) and oracle
efficiency (i.e., the number of evaluations). We also present lower bounds
showing the near-optimality of our scheme.

</details>


### [7] [Security and Privacy Analysis of Tile's Location Tracking Protocol](https://arxiv.org/abs/2510.00350)
*Akshaya Kumar,Anna Raymaker,Michael Specter*

Main category: cs.CR

TL;DR: 对Tile位置追踪服务进行首次全面安全分析，发现多个可利用漏洞和设计缺陷，驳斥了平台声称的安全隐私保证。


<details>
  <summary>Details</summary>
Motivation: Tile作为第二大流行的众包位置追踪服务，缺乏正式协议描述和威胁模型，其反跟踪功能被故意削弱以支持防盗用例。

Method: 识别Tile系统中的漏洞和设计缺陷，分析其反跟踪机制和问责机制的可利用性。

Result: 发现Tile服务器能持续获取所有用户和标签的位置，非特权攻击者可通过蓝牙广告追踪用户，防盗模式易被绕过，问责机制存在漏洞。

Conclusion: 需要为众包位置追踪环境制定新的正式问责定义，当前实现存在严重安全隐患。

Abstract: We conduct the first comprehensive security analysis of Tile, the second most
popular crowd-sourced location-tracking service behind Apple's AirTags. We
identify several exploitable vulnerabilities and design flaws, disproving many
of the platform's claimed security and privacy guarantees: Tile's servers can
persistently learn the location of all users and tags, unprivileged adversaries
can track users through Bluetooth advertisements emitted by Tile's devices, and
Tile's anti-theft mode is easily subverted.
  Despite its wide deployment -- millions of users, devices, and purpose-built
hardware tags -- Tile provides no formal description of its protocol or threat
model. Worse, Tile intentionally weakens its antistalking features to support
an antitheft use-case and relies on a novel "accountability" mechanism to
punish those abusing the system to stalk victims.
  We examine Tile's accountability mechanism, a unique feature of independent
interest; no other provider attempts to guarantee accountability. While an
ideal accountability mechanism may disincentivize abuse in crowd-sourced
location tracking protocols, we show that Tile's implementation is subvertible
and introduces new exploitable vulnerabilities. We conclude with a discussion
on the need for new, formal definitions of accountability in this setting.

</details>


### [8] [A Call to Action for a Secure-by-Design Generative AI Paradigm](https://arxiv.org/abs/2510.00451)
*Dalal Alharthi,Ivan Roberto Kawaminami Garcia*

Main category: cs.CR

TL;DR: 提出了PromptShield框架，这是一个基于本体的安全防护系统，旨在通过语义验证标准化用户输入，消除歧义并防御对抗性攻击，显著提升LLM的安全性和性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型普遍存在易受提示注入等对抗性攻击的漏洞，需要一种安全优先的AI范式来主动缓解这些漏洞。

Method: 引入PromptShield框架，采用本体驱动的方法进行确定性安全提示交互，通过语义验证标准化用户输入。在AWS云日志分析系统中进行实验，模拟提示注入攻击并评估框架效果。

Result: 实验结果显示PromptShield显著提升了模型安全性和性能，精确率、召回率和F1分数均达到约94%。本体框架不仅缓解对抗性威胁，还增强了系统整体性能和可靠性。

Conclusion: PromptShield为AI安全标准奠定了基础，强调了确定性提示工程和基于本体的验证在确保LLM安全部署中的关键作用，其模块化设计使其适用于各种生成式AI应用领域。

Abstract: Large language models have gained widespread prominence, yet their
vulnerability to prompt injection and other adversarial attacks remains a
critical concern. This paper argues for a security-by-design AI paradigm that
proactively mitigates LLM vulnerabilities while enhancing performance. To
achieve this, we introduce PromptShield, an ontology-driven framework that
ensures deterministic and secure prompt interactions. It standardizes user
inputs through semantic validation, eliminating ambiguity and mitigating
adversarial manipulation. To assess PromptShield's security and performance
capabilities, we conducted an experiment on an agent-based system to analyze
cloud logs within Amazon Web Services (AWS), containing 493 distinct events
related to malicious activities and anomalies. By simulating prompt injection
attacks and assessing the impact of deploying PromptShield, our results
demonstrate a significant improvement in model security and performance,
achieving precision, recall, and F1 scores of approximately 94%. Notably, the
ontology-based framework not only mitigates adversarial threats but also
enhances the overall performance and reliability of the system. Furthermore,
PromptShield's modular and adaptable design ensures its applicability beyond
cloud security, making it a robust solution for safeguarding generative AI
applications across various domains. By laying the groundwork for AI safety
standards and informing future policy development, this work stimulates a
crucial dialogue on the pivotal role of deterministic prompt engineering and
ontology-based validation in ensuring the safe and responsible deployment of
LLMs in high-stakes environments.

</details>


### [9] [Cloud Investigation Automation Framework (CIAF): An AI-Driven Approach to Cloud Forensics](https://arxiv.org/abs/2510.00452)
*Dalal Alharthi,Ivan Roberto Kawaminami Garcia*

Main category: cs.CR

TL;DR: 提出了Cloud Investigation Automation Framework (CIAF)，一个基于本体的框架，用于自动化云取证日志分析，提高勒索软件检测效率，达到93%的精确率、召回率和F1分数。


<details>
  <summary>Details</summary>
Motivation: 云取证调查仍依赖手动分析，耗时且易出错。LLMs能够模拟人类推理，为自动化云日志分析提供了可能。

Method: 引入CIAF框架，通过语义验证标准化用户输入，消除歧义，确保日志解释的一致性。采用确定性提示工程和基于本体的验证方法。

Result: 在分析包含勒索软件相关事件的Microsoft Azure日志时，CIAF显著提高了勒索软件检测性能，精确率、召回率和F1分数均达到93%。

Conclusion: CIAF的模块化、适应性设计使其成为应对各种网络攻击的强大解决方案，为标准化取证方法和未来AI驱动自动化奠定了基础。

Abstract: Large Language Models (LLMs) have gained prominence in domains including
cloud security and forensics. Yet cloud forensic investigations still rely on
manual analysis, making them time-consuming and error-prone. LLMs can mimic
human reasoning, offering a pathway to automating cloud log analysis. To
address this, we introduce the Cloud Investigation Automation Framework (CIAF),
an ontology-driven framework that systematically investigates cloud forensic
logs while improving efficiency and accuracy. CIAF standardizes user inputs
through semantic validation, eliminating ambiguity and ensuring consistency in
log interpretation. This not only enhances data quality but also provides
investigators with reliable, standardized information for decision-making. To
evaluate security and performance, we analyzed Microsoft Azure logs containing
ransomware-related events. By simulating attacks and assessing CIAF's impact,
results showed significant improvement in ransomware detection, achieving
precision, recall, and F1 scores of 93 percent. CIAF's modular, adaptable
design extends beyond ransomware, making it a robust solution for diverse
cyberattacks. By laying the foundation for standardized forensic methodologies
and informing future AI-driven automation, this work underscores the role of
deterministic prompt engineering and ontology-based validation in enhancing
cloud forensic investigations. These advancements improve cloud security while
paving the way for efficient, automated forensic workflows.

</details>


### [10] [Has the Two-Decade-Old Prophecy Come True? Artificial Bad Intelligence Triggered by Merely a Single-Bit Flip in Large Language Models](https://arxiv.org/abs/2510.00490)
*Yu Yan,Siqi Lu,Yang Gao,Zhaoxuan Li,Ziming Zhao,Qingjun Yuan,Yongjuan Wang*

Main category: cs.CR

TL;DR: 该论文首次系统性地发现并验证了大型语言模型权重文件中存在单比特漏洞，通过构建权重敏感度熵模型和概率启发式扫描框架BitSifter，能够高效定位关键易受攻击比特，并设计了远程比特翻转攻击链，可在真实环境中实现语义级攻击。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型以.gguf量化格式广泛部署，其权重空间暴露在硬件攻击面下，比特翻转攻击(BFA)能够远程破坏软件系统完整性，因此需要研究LLM权重文件中的单比特漏洞。

Method: 构建信息论权重敏感度熵模型和概率启发式扫描框架BitSifter，用于高效定位关键易受攻击比特；设计远程BFA攻击链，实现语义级攻击。

Result: 在主流开源模型中发现单比特翻转可导致三种目标语义级故障：人工缺陷智能(输出事实错误)、人工弱智能(逻辑推理能力退化)和人工不良智能(生成有害内容)；攻击频率464.3次/秒时，31.7秒内可100%成功翻转单个比特，使LLM准确率从73.5%降至0%。

Conclusion: LLM权重文件存在严重的单比特漏洞，模型大小与鲁棒性呈负相关，小型模型更易受攻击；注意力机制和输出层相关区域是最敏感的攻击目标，需要加强模型安全防护。

Abstract: Recently, Bit-Flip Attack (BFA) has garnered widespread attention for its
ability to compromise software system integrity remotely through hardware fault
injection. With the widespread distillation and deployment of large language
models (LLMs) into single file .gguf formats, their weight spaces have become
exposed to an unprecedented hardware attack surface. This paper is the first to
systematically discover and validate the existence of single-bit
vulnerabilities in LLM weight files: in mainstream open-source models (e.g.,
DeepSeek and QWEN) using .gguf quantized formats, flipping just single bit can
induce three types of targeted semantic level failures Artificial Flawed
Intelligence (outputting factual errors), Artificial Weak Intelligence
(degradation of logical reasoning capability), and Artificial Bad Intelligence
(generating harmful content).
  By building an information theoretic weight sensitivity entropy model and a
probabilistic heuristic scanning framework called BitSifter, we achieved
efficient localization of critical vulnerable bits in models with hundreds of
millions of parameters. Experiments show that vulnerabilities are significantly
concentrated in the tensor data region, particularly in areas related to the
attention mechanism and output layers, which are the most sensitive. A negative
correlation was observed between model size and robustness, with smaller models
being more susceptible to attacks. Furthermore, a remote BFA chain was
designed, enabling semantic-level attacks in real-world environments: At an
attack frequency of 464.3 times per second, a single bit can be flipped with
100% success in as little as 31.7 seconds. This causes the accuracy of LLM to
plummet from 73.5% to 0%, without requiring high-cost equipment or complex
prompt engineering.

</details>


### [11] [Memory-Augmented Log Analysis with Phi-4-mini: Enhancing Threat Detection in Structured Security Logs](https://arxiv.org/abs/2510.00529)
*Anbi Guo,Mahfuza Farooque*

Main category: cs.CR

TL;DR: DM-RAG是一个双记忆检索增强生成框架，用于结构化日志分析，通过结合短期记忆缓冲和长期FAISS索引记忆来提升APT检测性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在结构化安全日志分析中面临上下文有限和领域不匹配的问题，需要专门框架来提升APT检测能力。

Method: 集成短期记忆缓冲存储近期摘要和长期FAISS索引记忆存储历史模式，使用指令调优的Phi-4-mini处理组合上下文并输出结构化预测，贝叶斯融合确保可靠记忆持久化。

Result: 在UNSW-NB15数据集上达到53.64%准确率和98.70%召回率，在召回率上超过微调和RAG基线方法。

Conclusion: 该架构轻量级、可解释且可扩展，支持实时威胁监控，无需额外语料库或大量调优。

Abstract: Structured security logs are critical for detecting advanced persistent
threats (APTs). Large language models (LLMs) struggle in this domain due to
limited context and domain mismatch. We propose \textbf{DM-RAG}, a dual-memory
retrieval-augmented generation framework for structured log analysis. It
integrates a short-term memory buffer for recent summaries and a long-term
FAISS-indexed memory for historical patterns. An instruction-tuned Phi-4-mini
processes the combined context and outputs structured predictions. Bayesian
fusion promotes reliable persistence into memory. On the UNSW-NB15 dataset,
DM-RAG achieves 53.64% accuracy and 98.70% recall, surpassing fine-tuned and
RAG baselines in recall. The architecture is lightweight, interpretable, and
scalable, enabling real-time threat monitoring without extra corpora or heavy
tuning.

</details>


### [12] [Sentry: Authenticating Machine Learning Artifacts on the Fly](https://arxiv.org/abs/2510.00554)
*Andrew Gan,Zahra Ghodsi*

Main category: cs.CR

TL;DR: Sentry是一个基于GPU的框架，通过实现数据集和模型的加密签名与验证来确保机器学习工件的真实性，防止供应链攻击。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统依赖第三方开源数据集和预训练模型，存在供应链攻击风险，现有系统缺乏真实性验证机制，而传统加密验证方法在GPU平台上效率低下。

Method: 开发Sentry框架，将开发者身份与签名绑定，在GPU内存加载工件时实时执行认证，采用GPU加速的Merkle树和格哈希等加密哈希结构，实现内存优化和资源分区方案。

Result: 评估显示Sentry是实用的解决方案，相比基于CPU的基线实现了数量级的速度提升。

Conclusion: Sentry为机器学习系统带来了真实性保障，通过GPU加速的加密验证有效缓解了供应链攻击风险。

Abstract: Machine learning systems increasingly rely on open-source artifacts such as
datasets and models that are created or hosted by other parties. The reliance
on external datasets and pre-trained models exposes the system to supply chain
attacks where an artifact can be poisoned before it is delivered to the
end-user. Such attacks are possible due to the lack of any authenticity
verification in existing machine learning systems. Incorporating cryptographic
solutions such as hashing and signing can mitigate the risk of supply chain
attacks. However, existing frameworks for integrity verification based on
cryptographic techniques can incur significant overhead when applied to
state-of-the-art machine learning artifacts due to their scale, and are not
compatible with GPU platforms. In this paper, we develop Sentry, a novel
GPU-based framework that verifies the authenticity of machine learning
artifacts by implementing cryptographic signing and verification for datasets
and models. Sentry ties developer identities to signatures and performs
authentication on the fly as artifacts are loaded on GPU memory, making it
compatible with GPU data movement solutions such as NVIDIA GPUDirect that
bypass the CPU. Sentry incorporates GPU acceleration of cryptographic hash
constructions such as Merkle tree and lattice hashing, implementing memory
optimizations and resource partitioning schemes for a high throughput
performance. Our evaluations show that Sentry is a practical solution to bring
authenticity to machine learning systems, achieving orders of magnitude speedup
over a CPU-based baseline.

</details>


### [13] [IntrusionX: A Hybrid Convolutional-LSTM Deep Learning Framework with Squirrel Search Optimization for Network Intrusion Detection](https://arxiv.org/abs/2510.00572)
*Ahsan Farabi,Muhaiminul Rashid Shad,Israt Khandaker*

Main category: cs.CR

TL;DR: 提出IntrusionX混合深度学习框架，结合CNN和LSTM处理网络入侵检测，使用松鼠搜索算法优化超参数，在NSL-KDD数据集上取得高准确率。


<details>
  <summary>Details</summary>
Motivation: 解决入侵检测系统面临的网络攻击演变、高维流量数据和类别不平衡问题。

Method: 集成CNN提取局部特征，LSTM进行时序建模，使用松鼠搜索算法优化超参数，采用分层数据分割和动态类别权重处理类别不平衡。

Result: 在NSL-KDD数据集上，二分类准确率98%，五分类准确率87%，少数类召回率显著提升（U2R:71%，R2L:93%）。

Conclusion: IntrusionX通过可复现的、关注类别不平衡的设计和元启发式优化，在入侵检测任务中表现出色。

Abstract: Intrusion Detection Systems (IDS) face persistent challenges due to evolving
cyberattacks, high-dimensional traffic data, and severe class imbalance in
benchmark datasets such as NSL-KDD. To address these issues, we propose
IntrusionX, a hybrid deep learning framework that integrates Convolutional
Neural Networks (CNNs) for local feature extraction and Long Short-Term Memory
(LSTM) networks for temporal modeling. The architecture is further optimized
using the Squirrel Search Algorithm (SSA), enabling effective hyperparameter
tuning while maintaining computational efficiency. Our pipeline incorporates
rigorous preprocessing, stratified data splitting, and dynamic class weighting
to enhance the detection of rare classes. Experimental evaluation on NSL-KDD
demonstrates that IntrusionX achieves 98% accuracy in binary classification and
87% in 5-class classification, with significant improvements in minority class
recall (U2R: 71%, R2L: 93%). The novelty of IntrusionX lies in its
reproducible, imbalance-aware design with metaheuristic optimization.

</details>


### [14] [A Monoid Ring Approach to Color Visual Cryptography](https://arxiv.org/abs/2510.00763)
*Maximilian Reif,Jens Zumbrägel*

Main category: cs.CR

TL;DR: 提出了一种新的彩色视觉密码方案，使用更通用的颜色幺半群模型，实现了更小的像素扩展和更高对比度


<details>
  <summary>Details</summary>
Motivation: 现有彩色视觉密码方案在颜色叠加方面限制较多，需要更灵活的模型来提升像素扩展和对比度性能

Method: 重新审视Koga和 Ishihara的多项式框架，应用幺半群环来构建新的彩色视觉密码方案

Result: 相比同类方案，实现了更短的像素扩展和更高的对比度

Conclusion: 幺半群环方法为彩色视觉密码提供了有效的构造框架，显著提升了方案性能

Abstract: A visual cryptography scheme is a secret sharing scheme in which the secret
information is an image and the shares are printed on transparencies, so that
the secret image can be recovered by simply stacking the shares on top of each
other. Such schemes do therefore not require any knowledge of cryptography
tools to recover the secret, and they have widespread applications, for
example, when sharing QR codes or medical images. In this work we deal with
visual cryptography threshold schemes for color images. Our color model differs
from most previous work by allowing arbitrary colors to be stacked, resulting
in a possibly different color. This more general color monoid model enables us
to achieve shorter pixel expansion and higher contrast than comparable schemes.
We revisit the polynomial framework of Koga and Ishihara for constructing
visual cryptography schemes and apply the monoid ring to obtain new schemes for
color visual cryptography.

</details>


### [15] [Fast, Secure, and High-Capacity Image Watermarking with Autoencoded Text Vectors](https://arxiv.org/abs/2510.00799)
*Gautier Evennou,Vivien Chappelier,Ewa Kijak*

Main category: cs.CR

TL;DR: LatentSeal将水印重新定义为语义通信，通过文本自编码器将完整句子映射为紧凑的256维潜向量，突破了传统256位有效载荷上限，实现了高容量、鲁棒且可解释的水印系统。


<details>
  <summary>Details</summary>
Motivation: 传统图像水印系统将嵌入的有效载荷视为无意义的比特，这种比特中心视角限制了容量并阻碍水印携带有用信息。

Method: 使用轻量级文本自编码器将完整句子映射到紧凑的256维单位范数潜向量，通过微调的水印模型进行鲁棒嵌入，并通过秘密可逆旋转进行安全保护。

Result: 在多个基准测试中超越了现有技术水平，BLEU-4和精确匹配指标表现优异，ROC AUC得分达到0.97-0.99，突破了长期存在的256位有效载荷上限。

Conclusion: 通过从比特有效载荷转向语义潜向量，LatentSeal实现了不仅鲁棒和高容量，而且安全可解释的水印，为来源追踪、篡改解释和可信AI治理提供了具体路径。

Abstract: Most image watermarking systems focus on robustness, capacity, and
imperceptibility while treating the embedded payload as meaningless bits. This
bit-centric view imposes a hard ceiling on capacity and prevents watermarks
from carrying useful information. We propose LatentSeal, which reframes
watermarking as semantic communication: a lightweight text autoencoder maps
full-sentence messages into a compact 256-dimensional unit-norm latent vector,
which is robustly embedded by a finetuned watermark model and secured through a
secret, invertible rotation. The resulting system hides full-sentence messages,
decodes in real time, and survives valuemetric and geometric attacks. It
surpasses prior state of the art in BLEU-4 and Exact Match on several
benchmarks, while breaking through the long-standing 256-bit payload ceiling.
It also introduces a statistically calibrated score that yields a ROC AUC score
of 0.97-0.99, and practical operating points for deployment. By shifting from
bit payloads to semantic latent vectors, LatentSeal enables watermarking that
is not only robust and high-capacity, but also secure and interpretable,
providing a concrete path toward provenance, tamper explanation, and
trustworthy AI governance. Models, training and inference code, and data splits
will be available upon publication.

</details>


### [16] [Universally Composable Termination Analysis of Tendermint](https://arxiv.org/abs/2510.01097)
*Zhixin Dong,Xian Xu,Yuhang Zeng,Mingchao Wan,Chunmiao Li*

Main category: cs.CR

TL;DR: 本文首次对Tendermint共识协议进行通用可组合(UC)安全分析，证明其在选择性消息延迟攻击下的安全性。通过构建UC理想模型，形式化分析了Tendermint在部分同步网络假设下的安全性和终止性。


<details>
  <summary>Details</summary>
Motivation: 现有对Tendermint安全性和终止性的分析都是孤立进行的，没有考虑与其他协议的并发组合。此外，在拜占庭对手造成的自适应网络延迟下的终止性尚未得到正式分析。

Method: 构建Tendermint的UC理想模型，形式化其核心机制：基于阶段的共识过程、动态超时、提案锁定、领导者轮换等，在网络对手选择性延迟协议消息的情况下进行分析。

Result: 主要结果证明Tendermint协议UC实现了理想Tendermint模型，确保有界终止延迟，即在最多f<n/3节点为拜占庭的情况下仍能保证终止，前提是网络延迟保持在协议定义的阈值内。

Conclusion: 通过UC框架的形式化证明，Tendermint保持了安全性和终止性。根据UC的组合定理，这保证了当Tendermint与各种区块链组件组合时，这些属性得以保持。

Abstract: Modern blockchain systems operating in adversarial environments require
robust consensus protocols that guarantee both safety and termination under
network delay attacks. Tendermint, a widely adopted consensus protocol in
consortium blockchains, achieves high throughput and finality. However,
previous analysis of the safety and termination has been done in a standalone
fashion, with no consideration of the composition with other protocols
interacting with it in a concurrent manner. Moreover, the termination
properties under adaptive network delays caused by Byzantine adversaries have
not been formally analyzed. This paper presents the first universally
composable (UC) security analysis of Tendermint, demonstrating its resilience
against strategic message-delay attacks. By constructing a UC ideal model of
Tendermint, we formalize its core mechanisms: phase-base consensus procedure,
dynamic timeouts, proposal locking, leader rotation, and others, under a
network adversary that selectively delays protocol messages. Our main result
proves that the Tendermint protocol UC-realizes the ideal Tendermint model,
which ensures bounded termination latency, i.e., guaranteed termination, even
when up to $f<n/3$ nodes are Byzantine (where $n$ is the number of nodes
participating in the consensus), provided that network delays remain within a
protocol-defined threshold under the partially synchronous net assumption.
Specifically, through formal proofs within the UC framework, we show that
Tendermint maintains safety and termination. By the composition theorem of UC,
this guarantees that these properties are maintained when Tendermint is
composed with various blockchain components.

</details>


### [17] [EditTrack: Detecting and Attributing AI-assisted Image Editing](https://arxiv.org/abs/2510.01173)
*Zhengyuan Jiang,Yuyang Zhang,Moyang Guo,Neil Zhenqiang Gong*

Main category: cs.CR

TL;DR: 提出了EditTrack框架，用于检测图像是否由特定基础图像通过AI编辑模型生成，并能识别具体的编辑模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法只能检测图像是否被AI生成/编辑，无法判断是否从特定基础图像编辑而来，存在检测和归因的局限性。

Method: 基于对编辑过程的四个关键观察，提出重新编辑策略和精心设计的相似性度量，通过比较基础图像和可疑图像来确定编辑关系和具体模型。

Result: 在5个最先进编辑模型和6个数据集上的评估显示，EditTrack在检测和归因方面始终准确，显著优于5个基线方法。

Conclusion: EditTrack是首个解决图像编辑检测和归因问题的框架，能有效识别图像是否从特定基础图像编辑而来以及使用的具体编辑模型。

Abstract: In this work, we formulate and study the problem of image-editing detection
and attribution: given a base image and a suspicious image, detection seeks to
determine whether the suspicious image was derived from the base image using an
AI editing model, while attribution further identifies the specific editing
model responsible. Existing methods for detecting and attributing AI-generated
images are insufficient for this problem, as they focus on determining whether
an image was AI-generated/edited rather than whether it was edited from a
particular base image. To bridge this gap, we propose EditTrack, the first
framework for this image-editing detection and attribution problem. Building on
four key observations about the editing process, EditTrack introduces a novel
re-editing strategy and leverages carefully designed similarity metrics to
determine whether a suspicious image originates from a base image and, if so,
by which model. We evaluate EditTrack on five state-of-the-art editing models
across six datasets, demonstrating that it consistently achieves accurate
detection and attribution, significantly outperforming five baselines.

</details>
