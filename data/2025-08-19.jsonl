{"id": "2508.11710", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11710", "abs": "https://arxiv.org/abs/2508.11710", "authors": ["Hael Abdulhakim Ali Humran", "Ferdi Sonmez"], "title": "Code Vulnerability Detection Across Different Programming Languages with AI Models", "comment": null, "summary": "Security vulnerabilities present in a code that has been written in diverse\nprogramming languages are among the most critical yet complicated aspects of\nsource code to detect. Static analysis tools based on rule-based patterns\nusually do not work well at detecting the context-dependent bugs and lead to\nhigh false positive rates. Recent developments in artificial intelligence,\nspecifically the use of transformer-based models like CodeBERT and CodeLlama,\nprovide light to this problem, as they show potential in finding such flaws\nbetter. This paper presents the implementations of these models on various\ndatasets of code vulnerability, showing how off-the-shelf models can\nsuccessfully produce predictive capacity in models through dynamic fine-tuning\nof the models on vulnerable and safe code fragments. The methodology comprises\nthe gathering of the dataset, normalization of the language, fine-tuning of the\nmodel, and incorporation of ensemble learning and explainable AI. Experiments\nshow that a well-trained CodeBERT can be as good as or even better than some\nexisting static analyzers in terms of accuracy greater than 97%. Further study\nhas indicated that although language models can achieve close-to-perfect\nrecall, the precision can decrease. A solution to this is given by hybrid\nmodels and validation procedures, which will reduce false positives. According\nto the results, the AI-based solutions generalize to different programming\nlanguages and classes of vulnerability. Nevertheless, robustness,\ninterpretability, and deployment readiness are still being developed. The\nresults illustrate the probabilities that AI will enhance the trustworthiness\nin the usability and scalability of machine-learning-based detectors of\nvulnerabilities."}
{"id": "2508.11711", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11711", "abs": "https://arxiv.org/abs/2508.11711", "authors": ["Irash Perera", "Hiranya Abeyrathne", "Sanjeewa Malalgoda", "Arshardh Ifthikar"], "title": "Enhancing GraphQL Security by Detecting Malicious Queries Using Large Language Models, Sentence Transformers, and Convolutional Neural Networks", "comment": null, "summary": "GraphQL's flexibility, while beneficial for efficient data fetching,\nintroduces unique security vulnerabilities that traditional API security\nmechanisms often fail to address. Malicious GraphQL queries can exploit the\nlanguage's dynamic nature, leading to denial-of-service attacks, data\nexfiltration through injection, and other exploits. Existing solutions, such as\nstatic analysis, rate limiting, and general-purpose Web Application Firewalls,\noffer limited protection against sophisticated, context-aware attacks. This\npaper presents a novel, AI-driven approach for real-time detection of malicious\nGraphQL queries. Our method combines static analysis with machine learning\ntechniques, including Large Language Models (LLMs) for dynamic schema-based\nconfiguration, Sentence Transformers (SBERT and Doc2Vec) for contextual\nembedding of query payloads, and Convolutional Neural Networks (CNNs), Random\nForests, and Multilayer Perceptrons for classification. We detail the system\narchitecture, implementation strategies optimized for production environments\n(including ONNX Runtime optimization and parallel processing), and evaluate the\nperformance of our detection models and the overall system under load. Results\ndemonstrate high accuracy in detecting various threats, including SQL\ninjection, OS command injection, and XSS exploits, alongside effective\nmitigation of DoS and SSRF attempts. This research contributes a robust and\nadaptable solution for enhancing GraphQL API security."}
{"id": "2508.11716", "categories": ["cs.CR", "cs.AI", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.11716", "abs": "https://arxiv.org/abs/2508.11716", "authors": ["Javier Mu√±oz-Haro", "Ruben Tolosana", "Ruben Vera-Rodriguez", "Aythami Morales", "Julian Fierrez"], "title": "Privacy-Aware Detection of Fake Identity Documents: Methodology, Benchmark, and Improved Detection Methods (FakeIDet2)", "comment": null, "summary": "Remote user verification in Internet-based applications is becoming\nincreasingly important nowadays. A popular scenario for it consists of\nsubmitting a picture of the user's Identity Document (ID) to a service\nplatform, authenticating its veracity, and then granting access to the\nrequested digital service. An ID is well-suited to verify the identity of an\nindividual, since it is government issued, unique, and nontransferable.\nHowever, with recent advances in Artificial Intelligence (AI), attackers can\nsurpass security measures in IDs and create very realistic physical and\nsynthetic fake IDs. Researchers are now trying to develop methods to detect an\never-growing number of these AI-based fakes that are almost indistinguishable\nfrom authentic (bona fide) IDs. In this counterattack effort, researchers are\nfaced with an important challenge: the difficulty in using real data to train\nfake ID detectors. This real data scarcity for research and development is\noriginated by the sensitive nature of these documents, which are usually kept\nprivate by the ID owners (the users) and the ID Holders (e.g., government,\npolice, bank, etc.). The main contributions of our study are: 1) We propose and\ndiscuss a patch-based methodology to preserve privacy in fake ID detection\nresearch. 2) We provide a new public database, FakeIDet2-db, comprising over\n900K real/fake ID patches extracted from 2,000 ID images, acquired using\ndifferent smartphone sensors, illumination and height conditions, etc. In\naddition, three physical attacks are considered: print, screen, and composite.\n3) We present a new privacy-aware fake ID detection method, FakeIDet2. 4) We\nrelease a standard reproducible benchmark that considers physical and synthetic\nattacks from popular databases in the literature."}
{"id": "2508.11742", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.11742", "abs": "https://arxiv.org/abs/2508.11742", "authors": ["Minhao Jin", "Hongyu He", "Maria Apostolaki"], "title": "Assessing User Privacy Leakage in Synthetic Packet Traces: An Attack-Grounded Approach", "comment": null, "summary": "Current synthetic traffic generators (SynNetGens) promise privacy but lack\ncomprehensive guarantees or empirical validation, even as their fidelity\nsteadily improves. We introduce the first attack-grounded benchmark for\nassessing the privacy of SynNetGens directly from the traffic they produce. We\nframe privacy as membership inference at the traffic-source level--a realistic\nand actionable threat for data holders. To this end, we present TraceBleed, the\nfirst attack that exploits behavioral fingerprints across flows using\ncontrastive learning and temporal chunking, outperforming prior membership\ninference baselines by 172%. Our large-scale study across GAN-, diffusion-, and\nGPT-based SynNetGens uncovers critical insights: (i) SynNetGens leak user-level\ninformation; (ii) differential privacy either fails to stop these attacks or\nseverely degrades fidelity; and (iii) sharing more synthetic data amplifies\nleakage by 59% on average. Finally, we introduce TracePatch, the first\nSynNetGen-agnostic defense that combines adversarial ML with SMT constraints to\nmitigate leakage while preserving fidelity."}
{"id": "2508.11797", "categories": ["cs.CR", "cs.DB", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.11797", "abs": "https://arxiv.org/abs/2508.11797", "authors": ["Calkin Garg", "Omar Rios Cruz", "Tessa Andersen", "Gaby G. Dagher", "Donald Winiecki", "Min Long"], "title": "AegisBlock: A Privacy-Preserving Medical Research Framework using Blockchain", "comment": "Submitted to IEEE Conference on Collaboration and Internet Computing\n  2025", "summary": "Due to HIPAA and other privacy regulations, it is imperative to maintain\npatient privacy while conducting research on patient health records. In this\npaper, we propose AegisBlock, a patient-centric access controlled framework to\nshare medical records with researchers such that the anonymity of the patient\nis maintained while ensuring the trustworthiness of the data provided to\nresearchers. AegisBlock allows for patients to provide access to their medical\ndata, verified by miners. A researcher submits a time-based range query to\nrequest access to records from a certain patient, and upon patient approval,\naccess will be granted. Our experimental evaluation results show that\nAegisBlock is scalable with respect to the number of patients and hospitals in\nthe system, and efficient with up to 50% of malicious miners."}
{"id": "2508.11812", "categories": ["cs.CR", "cs.CY", "cs.NI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.11812", "abs": "https://arxiv.org/abs/2508.11812", "authors": ["Tyler Schroder", "Sohee Kim Park"], "title": "Securing Sideways: Thwarting Lateral Movement by Implementing Active Directory Tiering", "comment": "11 pages", "summary": "The advancement of computing equipment and the advances in services over the\nInternet has allowed corporations, higher education, and many other\norganizations to pursue the shared computing network environment. A requirement\nfor shared computing environments is a centralized identity system to\nauthenticate and authorize user access. An organization's digital identity\nplane is a prime target for cyber threat actors. When compromised, identities\ncan be exploited to steal credentials, create unauthorized accounts, and\nmanipulate permissions-enabling attackers to gain control of the network and\nundermine its confidentiality, availability, and integrity. Cybercrime losses\nreached a record of 16.6 B in the United States in 2024. For organizations\nusing Microsoft software, Active Directory is the on-premises identity system\nof choice. In this article, we examine the challenge of security compromises in\nActive Directory (AD) environments and present effective strategies to prevent\ncredential theft and limit lateral movement by threat actors. Our proposed\napproaches aim to confine the movement of compromised credentials, preventing\nsignificant privilege escalation and theft. We argue that through our\nillustration of real-world scenarios, tiering can halt lateral movement and\nadvanced cyber-attacks, thus reducing ransom escalation. Our work bridges a gap\nin existing literature by combining technical guidelines with theoretical\narguments in support of tiering, positioning it as a vital component of modern\ncybersecurity strategy even though it cannot function in isolation. As the\nhardware advances and the cloud sourced services along with AI is advancing\nwith unprecedented speed, we think it is important for security experts and the\nbusiness to work together and start designing and developing software and\nframeworks to classify devices automatically and accurately within the tiered\nstructure."}
{"id": "2508.11817", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.11817", "abs": "https://arxiv.org/abs/2508.11817", "authors": ["Mukesh Poudel", "Nick Rahimi"], "title": "Machine Learning-Based AES Key Recovery via Side-Channel Analysis on the ASCAD Dataset", "comment": null, "summary": "Cryptographic algorithms like AES and RSA are widely used and they are\nmathematically robust and almost unbreakable but its implementation on physical\ndevices often leak information through side channels, such as electromagnetic\n(EM) emissions, potentially compromising said theoretically secure algorithms.\nThis paper investigates the application of machine learning (ML) techniques and\nDeep Learning models to exploit such leakage for partial key recovery. We use\nthe public ASCAD `fixed' and `variable' key dataset, containing 700 and 1400 EM\ntraces respectively from an AES-128 implementation on an 8-bit microcontroller.\nThe problem is framed as a 256-class classification task where we target the\noutput of the first-round S-box operation, which is dependent on a single key\nbyte. We evaluate standard classifiers (Random Forest (RF), Support Vector\nMachine (SVM)), a Convolutional Neural Network(CNN) and a Residual Neural\nNetwork(ResNet). We also explore the utility of RF-based feature importance for\ndimensionality reduction. Crucially, we employ this domain-specific Key Rank\nmetric for evaluation, showing its necessity over standard classification\naccuracy. Our results show that SVM and RF on full features perform poorly in\nkey ranking. However, RF trained on reduced (top 100) identified via importance\nanalysis achieves Rank 0 (successful key byte recovery) using almost half the\nattack traces. The implemented CNN also achieves Rank 0 efficiently using\napproximately 65 attack traces for the fixed-key dataset. The ResNets perform\nbest on large and complex datasets but may not always be the best choice for\nsimple fixed key dataset in terms of efficiency. Thus we conclude that models,\nparticularly CNNs, ResNets and feature-selected RF, coupled with the Key Rank\nmetric, are an effective tool for side-channel key recovery, confirming the\npractical vulnerability of the cryptographic implementations."}
{"id": "2508.11907", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11907", "abs": "https://arxiv.org/abs/2508.11907", "authors": ["Xiaojin Zhang", "Mingcong Xu", "Yiming Li", "Wei Chen", "Qiang Yang"], "title": "Deciphering the Interplay between Attack and Protection Complexity in Privacy-Preserving Federated Learning", "comment": null, "summary": "Federated learning (FL) offers a promising paradigm for collaborative model\ntraining while preserving data privacy. However, its susceptibility to gradient\ninversion attacks poses a significant challenge, necessitating robust privacy\nprotection mechanisms. This paper introduces a novel theoretical framework to\ndecipher the intricate interplay between attack and protection complexities in\nprivacy-preserving FL. We formally define \"Attack Complexity\" as the minimum\ncomputational and data resources an adversary requires to reconstruct private\ndata below a given error threshold, and \"Protection Complexity\" as the expected\ndistortion introduced by privacy mechanisms. Leveraging Maximum Bayesian\nPrivacy (MBP), we derive tight theoretical bounds for protection complexity,\ndemonstrating its scaling with model dimensionality and privacy budget.\nFurthermore, we establish comprehensive bounds for attack complexity, revealing\nits dependence on privacy leakage, gradient distortion, model dimension, and\nthe chosen privacy level. Our findings quantitatively illuminate the\nfundamental trade-offs between privacy guarantees, system utility, and the\neffort required for both attacking and defending. This framework provides\ncritical insights for designing more secure and efficient federated learning\nsystems."}
{"id": "2508.11913", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.11913", "abs": "https://arxiv.org/abs/2508.11913", "authors": ["Huipeng Yang", "Li Yang", "Lichuan Ma", "Lu Zhou", "Junbo Jia", "Anyuan Sang", "Xinyue Wang"], "title": "WebGeoInfer: A Structure-Free and Multi-Stage Framework for Geolocation Inference of Devices Exposing Information", "comment": null, "summary": "Remote management devices facilitate critical infrastructure monitoring for\nadministrators but simultaneously increase asset exposure. Sensitive\ngeographical information overlooked in exposed device management pages poses\nsubstantial security risks. Therefore, identifying devices that reveal location\ninformation due to administrator negligence is crucial for cybersecurity\nregulation. Despite the rich information exposed by web interfaces of remote\nmanagement devices, automatically discovering geographical locations remains\nchallenging due to unstructured formats, varying styles, and incomplete\ngeographical details.\n  This study introduces WebGeoInfer, a structure-free geolocation inference\nframework utilizing multi-stage information enhancement. WebGeoInfer clusters\nsimilar device web pages and analyzes inter-cluster differences to extract\npotential geographical information, bypassing structural limitations. Through\nsearch engine enhancement and Large Language Models mining, the framework\nextracts geographical coordinates from identified information. WebGeoInfer\nsuccessfully inferred locations for 5,435 devices across 94 countries and 2,056\ncities, achieving accuracy rates of 96.96\\%, 88.05\\%, and 79.70\\% at country,\ncity, and street levels, respectively."}
{"id": "2508.11925", "categories": ["cs.CR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11925", "abs": "https://arxiv.org/abs/2508.11925", "authors": ["Zhimeng Guo", "Huaisheng Zhu", "Siyuan Xu", "Hangfan Zhang", "Teng Xiao", "Minhao Cheng"], "title": "Optimizing Token Choice for Code Watermarking: A RL Approach", "comment": "18 pages, 3 figures", "summary": "The need for detecting LLM-generated code necessitates watermarking systems\ncapable of operating within its highly structured and syntactically constrained\nenvironment. To address this, we introduce CodeTracer, an innovative adaptive\ncode watermarking framework underpinned by a novel reinforcement learning\ntraining paradigm. At its core, CodeTracer features a policy-driven approach\nthat utilizes a parameterized model to intelligently bias token choices during\nnext-token prediction. This strategy ensures that embedded watermarks maintain\ncode functionality while exhibiting subtle yet statistically detectable\ndeviations from typical token distributions. To facilitate policy learning, we\ndevise a comprehensive reward system that seamlessly integrates execution\nfeedback with watermark embedding signals, balancing process-level and\noutcome-level rewards. Additionally, we employ Gumbel Top-k reparameterization\nto enable gradient-based optimization of discrete watermarking decisions.\nExtensive comparative evaluations demonstrate CodeTracer's significant\nsuperiority over state-of-the-art baselines in both watermark detectability and\nthe preservation of generated code's functionality."}
{"id": "2508.11928", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.11928", "abs": "https://arxiv.org/abs/2508.11928", "authors": ["Lien Tran", "Boyuan Zhang", "Ratchanon Pawanja", "Rashid Hussain Khokhar"], "title": "The Passwordless Authentication with Passkey Technology from an Implementation Perspective", "comment": "6 pages, 3 figures", "summary": "With the rise of sophisticated authentication bypass techniques, passwords\nare no longer considered a reliable method for securing authentication systems.\nIn recent years, new authentication technologies have shifted from traditional\npassword-based logins to passwordless security. Among these, Time-Based\nOne-Time Passwords (TOTP) remain one of the most widely used mechanisms, while\nPasskeys are emerging as a promising alternative with growing adoption. This\npaper highlights the key techniques used during the implementation of the\nauthentication system with Passkey technology. It also suggests considerations\nfor integrating components during system development to ensure that users can\nsecurely access their accounts with minimal complexity, while still meeting the\nrequirements of a robust authentication system that balances security,\nusability, and performance. Additionally, by examining TOTP and Passkey\nmechanisms from an implementation perspective, this work not only addresses\nmajor security concerns such as password leaks, phishing attacks, and\nsusceptibility to brute-force attacks, but also evaluates the feasibility and\neffectiveness of these mechanisms in real-world implementations. This paper\ndemonstrates the superior security of Passkey technology and its potential for\nbroader adoption in secure authentication systems."}
{"id": "2508.11939", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.11939", "abs": "https://arxiv.org/abs/2508.11939", "authors": ["James Gu", "Ahmed Sartaj", "Mohammed Akram Taher Khan", "Rashid Hussain Khokhar"], "title": "Design and Implementation of a Controlled Ransomware Framework for Educational Purposes Using Flutter Cryptographic APIs on Desktop PCs and Android Devices", "comment": "6 pages, 1 figure, 1 table, 2 algorithms", "summary": "This study focuses on the creation and implementation of ransomware for\neducational purposes that leverages Python's native cryptographic APIs in a\ncontrolled environment. Additionally, an Android version of the framework is\nimplemented using Flutter and Dart. For both versions, open-source\ncryptographic libraries are utilized. With this framework, researchers can\nsystematically explore the functionalities of ransomware, including file\nencryption processes, cryptographic key management, and victim interaction\ndynamics. To ensure safe experimentation, multiple safeguards are incorporated,\nsuch as the ability to restrict the encryption process to a specific directory,\nproviding the RSA private key for immediate decryption, and narrowing the scope\nof targetable files to a carefully curated list (.txt, .jpg, .csv, .doc). This\npaper draws inspiration from the infamous WannaCry ransomware and aims to\nsimulate its behaviour on Android devices. By making the codebase open-source,\nit enables users to study, modify, and extend the program for pedagogical\npurposes and offers a hands-on tool that can be used to train the next\ngeneration of cybersecurity professionals."}
{"id": "2508.12035", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12035", "abs": "https://arxiv.org/abs/2508.12035", "authors": ["Fei Lin", "Tengchao Zhang", "Ziyang Gong", "Fei-Yue Wang"], "title": "ToxiEval-ZKP: A Structure-Private Verification Framework for Molecular Toxicity Repair Tasks", "comment": null, "summary": "In recent years, generative artificial intelligence (GenAI) has demonstrated\nremarkable capabilities in high-stakes domains such as molecular science.\nHowever, challenges related to the verifiability and structural privacy of its\noutputs remain largely unresolved. This paper focuses on the task of molecular\ntoxicity repair. It proposes a structure-private verification framework -\nToxiEval-ZKP - which, for the first time, introduces zero-knowledge proof (ZKP)\nmechanisms into the evaluation process of this task. The system enables model\ndevelopers to demonstrate to external verifiers that the generated molecules\nmeet multidimensional toxicity repair criteria, without revealing the molecular\nstructures themselves. To this end, we design a general-purpose circuit\ncompatible with both classification and regression tasks, incorporating\nevaluation logic, Poseidon-based commitment hashing, and a nullifier-based\nreplay prevention mechanism to build a complete end-to-end ZK verification\nsystem. Experimental results demonstrate that ToxiEval-ZKP facilitates adequate\nvalidation under complete structural invisibility, offering strong circuit\nefficiency, security, and adaptability, thereby opening up a novel paradigm for\ntrustworthy evaluation in generative scientific tasks."}
{"id": "2508.12072", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12072", "abs": "https://arxiv.org/abs/2508.12072", "authors": ["Wei Jie Yeo", "Ranjan Satapathy", "Erik Cambria"], "title": "Mitigating Jailbreaks with Intent-Aware LLMs", "comment": null, "summary": "Despite extensive safety-tuning, large language models (LLMs) remain\nvulnerable to jailbreak attacks via adversarially crafted instructions,\nreflecting a persistent trade-off between safety and task performance. In this\nwork, we propose Intent-FT, a simple and lightweight fine-tuning approach that\nexplicitly trains LLMs to infer the underlying intent of an instruction before\nresponding. By fine-tuning on a targeted set of adversarial instructions,\nIntent-FT enables LLMs to generalize intent deduction to unseen attacks,\nthereby substantially improving their robustness. We comprehensively evaluate\nboth parametric and non-parametric attacks across open-source and proprietary\nmodels, considering harmfulness from attacks, utility, over-refusal, and impact\nagainst white-box threats. Empirically, Intent-FT consistently mitigates all\nevaluated attack categories, with no single attack exceeding a 50\\% success\nrate -- whereas existing defenses remain only partially effective. Importantly,\nour method preserves the model's general capabilities and reduces excessive\nrefusals on benign instructions containing superficially harmful keywords.\nFurthermore, models trained with Intent-FT accurately identify hidden harmful\nintent in adversarial attacks, and these learned intentions can be effectively\ntransferred to enhance vanilla model defenses."}
{"id": "2508.12093", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12093", "abs": "https://arxiv.org/abs/2508.12093", "authors": ["Hyunmin Choi"], "title": "PP-STAT: An Efficient Privacy-Preserving Statistical Analysis Framework using Homomorphic Encryption", "comment": "Accepted to CIKM 2025 (Full Research Paper Track)", "summary": "With the widespread adoption of cloud computing, the need for outsourcing\nstatistical analysis to third-party platforms is growing rapidly. However,\nhandling sensitive data such as medical records and financial information in\ncloud environments raises serious privacy concerns. In this paper, we present\nPP-STAT, a novel and efficient Homomorphic Encryption (HE)-based framework for\nprivacy-preserving statistical analysis. HE enables computations to be\nperformed directly on encrypted data without revealing the underlying\nplaintext. PP-STAT supports advanced statistical measures, including Z-score\nnormalization, skewness, kurtosis, coefficient of variation, and Pearson\ncorrelation coefficient, all computed securely over encrypted data. To improve\nefficiency, PP-STAT introduces two key optimizations: (1) a Chebyshev-based\napproximation strategy for initializing inverse square root operations, and (2)\na pre-normalization scaling technique that reduces multiplicative depth by\nfolding constant scaling factors into mean and variance computations. These\ntechniques significantly lower computational overhead and minimize the number\nof expensive bootstrapping procedures. Our evaluation on real-world datasets\ndemonstrates that PP-STAT achieves high numerical accuracy, with mean relative\nerror (MRE) below 2.4x10-4. Notably, the encrypted Pearson correlation between\nthe smoker attribute and charges reaches 0.7873, with an MRE of 2.86x10-4.\nThese results confirm the practical utility of PP-STAT for secure and precise\nstatistical analysis in privacy-sensitive domains."}
{"id": "2508.12107", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12107", "abs": "https://arxiv.org/abs/2508.12107", "authors": ["Shixuan Guan", "Kai Li"], "title": "Ethereum Crypto Wallets under Address Poisoning: How Usable and Secure Are They?", "comment": "15 pages, 10 figures", "summary": "Blockchain address poisoning is an emerging phishing attack that crafts\n\"similar-looking\" transfer records in the victim's transaction history, which\naims to deceive victims and lure them into mistakenly transferring funds to the\nattacker. Recent works have shown that millions of Ethereum users were targeted\nand lost over 100 million US dollars.\n  Ethereum crypto wallets, serving users in browsing transaction history and\ninitiating transactions to transfer funds, play a central role in deploying\ncountermeasures to mitigate the address poisoning attack. However, whether they\nhave done so remains an open question. To fill the research void, in this\npaper, we design experiments to simulate address poisoning attacks and\nsystematically evaluate the usability and security of 53 popular Ethereum\ncrypto wallets. Our evaluation shows that there exist communication failures\nbetween 12 wallets and their transaction activity provider, which renders them\nunable to download the users' transaction history. Besides, our evaluation also\nshows that 16 wallets pose a high risk to their users due to displaying fake\ntoken phishing transfers. Moreover, our further analysis suggests that most\nwallets rely on transaction activity providers to filter out phishing\ntransfers. However, their phishing detection capability varies. Finally, we\nfound that only three wallets throw an explicit warning message when users\nattempt to transfer to the phishing address, implying a significant gap within\nthe broader Ethereum crypto wallet community in protecting users from address\npoisoning attacks.\n  Overall, our work shows that more efforts are needed by the Ethereum crypto\nwallet developer community to achieve the highest usability and security\nstandard. Our bug reports have been acknowledged by the developer community,\nwho are currently developing mitigation solutions."}
{"id": "2508.12138", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12138", "abs": "https://arxiv.org/abs/2508.12138", "authors": ["Mohammad Ishzaz Asif Rafid", "Morsalin Sakib"], "title": "Substituting Proof of Work in Blockchain with Training-Verified Collaborative Model Computation", "comment": null, "summary": "Bitcoin's Proof of Work (PoW) mechanism, while central to achieving\ndecentralized consensus, has long been criticized for excessive energy use and\nhardware inefficiencies \\cite{devries2018bitcoin, truby2018decarbonizing}. This\npaper introduces a hybrid architecture that replaces Bitcoin's traditional PoW\nwith a centralized, cloud-based collaborative training framework. In this\nmodel, miners contribute computing resources to train segments of horizontally\nscaled machine learning models on preprocessed datasets, ensuring privacy and\ngenerating meaningful outputs \\cite{li2017securing}. A central server evaluates\ncontributions using two metrics: number of parameters trained and reduction in\nmodel loss during each cycle. At the end of every cycle, a weighted lottery\nselects the winning miner, who receives a digitally signed certificate. This\ncertificate serves as a verifiable substitute for PoW and grants the right to\nappend a block to the blockchain \\cite{nakamoto2008bitcoin}. By integrating\ndigital signatures and SHA-256 hashing \\cite{nist2015sha}, the system preserves\nblockchain integrity while redirecting energy toward productive computation.\nThe proposed approach addresses the sustainability concerns of traditional\nmining by converting resource expenditure into socially valuable work, aligning\nsecurity incentives with real-world computational progress."}
{"id": "2508.12161", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.12161", "abs": "https://arxiv.org/abs/2508.12161", "authors": ["Ming Li", "John Hale"], "title": "Attack Graph Generation on HPC Clusters", "comment": null, "summary": "Attack graphs (AGs) are graphical tools to analyze the security of computer\nnetworks. By connecting the exploitation of individual vulnerabilities, AGs\nexpose possible multi-step attacks against target networks, allowing system\nadministrators to take preventive measures to enhance their network's security.\nAs powerful analytical tools, however, AGs are both time- and memory-consuming\nto be generated. As the numbers of network assets, interconnections between\ndevices, as well as vulnerabilities increase, the size and volume of the\nresulting AGs grow at a much higher rate, leading to the well-known state-space\nexplosion. In this paper, we propose the use of high performance computing\n(HPC) clusters to implement AG generators. We evaluate the performance through\nexperiments and provide insights into how cluster environments can help resolve\nthe issues of slow speed and high memory demands in AG generation in a balanced\nway."}
{"id": "2508.12175", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12175", "abs": "https://arxiv.org/abs/2508.12175", "authors": ["Ben Nassi", "Stav Cohen", "Or Yair"], "title": "Invitation Is All You Need! Promptware Attacks Against LLM-Powered Assistants in Production Are Practical and Dangerous", "comment": "https://sites.google.com/view/invitation-is-all-you-need/home", "summary": "The growing integration of LLMs into applications has introduced new security\nrisks, notably known as Promptware - maliciously engineered prompts designed to\nmanipulate LLMs to compromise the CIA triad of these applications. While prior\nresearch warned about a potential shift in the threat landscape for LLM-powered\napplications, the risk posed by Promptware is frequently perceived as low. In\nthis paper, we investigate the risk Promptware poses to users of Gemini-powered\nassistants (web application, mobile application, and Google Assistant). We\npropose a novel Threat Analysis and Risk Assessment (TARA) framework to assess\nPromptware risks for end users. Our analysis focuses on a new variant of\nPromptware called Targeted Promptware Attacks, which leverage indirect prompt\ninjection via common user interactions such as emails, calendar invitations,\nand shared documents. We demonstrate 14 attack scenarios applied against\nGemini-powered assistants across five identified threat classes: Short-term\nContext Poisoning, Permanent Memory Poisoning, Tool Misuse, Automatic Agent\nInvocation, and Automatic App Invocation. These attacks highlight both digital\nand physical consequences, including spamming, phishing, disinformation\ncampaigns, data exfiltration, unapproved user video streaming, and control of\nhome automation devices. We reveal Promptware's potential for on-device lateral\nmovement, escaping the boundaries of the LLM-powered application, to trigger\nmalicious actions using a device's applications. Our TARA reveals that 73% of\nthe analyzed threats pose High-Critical risk to end users. We discuss\nmitigations and reassess the risk (in response to deployed mitigations) and\nshow that the risk could be reduced significantly to Very Low-Medium. We\ndisclosed our findings to Google, which deployed dedicated mitigations."}
{"id": "2508.12181", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12181", "abs": "https://arxiv.org/abs/2508.12181", "authors": ["Ayman W. Baharia", "Khaled T. Naga", "Hesham S. Abdelfattah", "Shady A. Maged", "Sherif A. Hammad"], "title": "CAN Networks Security in Smart Grids Communication Technologies", "comment": "4 pages, 6 figures, International Conference on Energy Systems -\n  Smart and Sustainable Solutions -", "summary": "The rapid evolution of smart grids requires effective communication protocols\nto transfer data reliably and securely. Controller Area Network (CAN) is one of\nthe most recognized protocols that offer reliable data transmission in smart\ngrids due to its robustness, real-time capabilities, and relatively low initial\ncost of its required hardware. However, as a smart city becomes more\ninterconnected, it also becomes more vulnerable to cyber-attacks. As there are\nmany mechanisms to secure the CAN nodes from attacks, most of those mechanisms\nhave computational overhead, resulting in more delay in the network. We\nimplemented a solution that requires almost no overhead to any CAN node\nconnected to the network. It depends on a single node responsible for securing\nthe CAN network. This approach seeks to augment network security while reducing\nsecurity mechanisms overhead to all CAN network nodes. The methodology and\ncomprehensive test results will be presented in detail during a subsequent\ndiscussion. The used software for development is Code Composer Studio, and the\nused microcontroller evaluation boards (EVB) are TM4C 1294."}
{"id": "2508.12187", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.12187", "abs": "https://arxiv.org/abs/2508.12187", "authors": ["John Y. Kim", "Chaoshun Zuo", "Yanjie Zhao", "Zhiqiang Lin"], "title": "AUTOVR: Automated UI Exploration for Detecting Sensitive Data Flow Exposures in Virtual Reality Apps", "comment": "USENIX Security 2025, 19 Pages, 14 Figures, 7 Tables", "summary": "The rise of Virtual Reality (VR) has provided developers with an\nunprecedented platform for creating games and applications (apps) that require\ndistinct inputs, different from those of conventional devices like smartphones.\nThe Meta Quest VR platform, driven by Meta, has democratized VR app publishing\nand attracted millions of users worldwide. However, as the number of published\napps grows, there is a notable lack of robust headless tools for user interface\n(UI) exploration and user event testing. To address this need, we present\nAUTOVR, an automatic framework for dynamic UI and user event interaction in VR\napps built on the Unity Engine. Unlike conventional Android and GUI testers,\nAUTOVR analyzes the app's internal binary to reveal hidden events, resolves\ngenerative event dependencies, and utilizes them for comprehensive exploration\nof VR apps. Using sensitive data exposure as a performance metric, we compare\nAUTOVR with Android Monkey, a widely used headless Android GUI stress testing\ntool. Our empirical evaluation demonstrates AUTOVR's superior performance,\ntriggering an order of magnitude of more sensitive data exposures and\nsignificantly enhancing the privacy of VR apps."}
{"id": "2508.12259", "categories": ["cs.CR", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.12259", "abs": "https://arxiv.org/abs/2508.12259", "authors": ["Ken Huang", "Yasir Mehmood", "Hammad Atta", "Jerry Huang", "Muhammad Zeeshan Baig", "Sree Bhargavi Balija"], "title": "Fortifying the Agentic Web: A Unified Zero-Trust Architecture Against Logic-layer Threats", "comment": null, "summary": "This paper presents a Unified Security Architecture that fortifies the\nAgentic Web through a Zero-Trust IAM framework. This architecture is built on a\nfoundation of rich, verifiable agent identities using Decentralized Identifiers\n(DIDs) and Verifiable Credentials (VCs), with discovery managed by a\nprotocol-agnostic Agent Name Service (ANS). Security is operationalized through\na multi-layered Trust Fabric which introduces significant innovations,\nincluding Trust-Adaptive Runtime Environments (TARE), Causal Chain Auditing,\nand Dynamic Identity with Behavioral Attestation. By explicitly linking the\nLPCI threat to these enhanced architectural countermeasures within a formal\nsecurity model, we propose a comprehensive and forward-looking blueprint for a\nsecure, resilient, and trustworthy agentic ecosystem. Our formal analysis\ndemonstrates that the proposed architecture provides provable security\nguarantees against LPCI attacks with bounded probability of success."}
{"id": "2508.12264", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12264", "abs": "https://arxiv.org/abs/2508.12264", "authors": ["Saisai Xia", "Wenhao Wang", "Zihao Wang", "Yuhui Zhang", "Yier Jin", "Dan Meng", "Rui Hou"], "title": "CryptPEFT: Efficient and Private Neural Network Inference via Parameter-Efficient Fine-Tuning", "comment": "Preprint for the paper accepted for presentation at NDSS 2026", "summary": "Publicly available large pretrained models (i.e., backbones) and lightweight\nadapters for parameter-efficient fine-tuning (PEFT) have become standard\ncomponents in modern machine learning pipelines. However, preserving the\nprivacy of both user inputs and fine-tuned adapters -- often trained on\nsensitive data -- during inference remains a significant challenge. Applying\ncryptographic techniques, such as multi-party computation (MPC), to PEFT\nsettings still incurs substantial encrypted computation across both the\nbackbone and adapter, mainly due to the inherent two-way communication between\nthem. To address this limitation, we propose CryptPEFT, the first PEFT solution\nspecifically designed for private inference scenarios. CryptPEFT introduces a\nnovel one-way communication (OWC) architecture that confines encrypted\ncomputation solely to the adapter, significantly reducing both computational\nand communication overhead. To maintain strong model utility under this\nconstraint, we explore the design space of OWC-compatible adapters and employ\nan automated architecture search algorithm to optimize the trade-off between\nprivate inference efficiency and model utility. We evaluated CryptPEFT using\nVision Transformer backbones across widely used image classification datasets.\nOur results show that CryptPEFT significantly outperforms existing baselines,\ndelivering speedups ranging from $20.62\\times$ to $291.48\\times$ in simulated\nwide-area network (WAN) and local-area network (LAN) settings. On CIFAR-100,\nCryptPEFT attains 85.47% accuracy with just 2.26 seconds of inference latency.\nThese findings demonstrate that CryptPEFT offers an efficient and\nprivacy-preserving solution for modern PEFT-based inference."}
{"id": "2508.12304", "categories": ["cs.CR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12304", "abs": "https://arxiv.org/abs/2508.12304", "authors": ["Hao Li"], "title": "Adjustable AprilTags For Identity Secured Tasks", "comment": null, "summary": "Special tags such as AprilTags that facilitate image processing and pattern\nrecognition are useful in practical applications. In close and private\nenvironments, identity security is unlikely to be an issue because all involved\nAprilTags can be completely regulated. However, in open and public\nenvironments, identity security is no longer an issue that can be neglected. To\nhandle potential harm caused by adversarial attacks, this note advocates\nutilization of adjustable AprilTags instead of fixed ones."}
{"id": "2508.12398", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12398", "abs": "https://arxiv.org/abs/2508.12398", "authors": ["Zhixin Xie", "Xurui Song", "Jun Luo"], "title": "Where to Start Alignment? Diffusion Large Language Model May Demand a Distinct Position", "comment": null, "summary": "Diffusion Large Language Models (dLLMs) have recently emerged as a\ncompetitive non-autoregressive paradigm due to their unique training and\ninference approach. However, there is currently a lack of safety study on this\nnovel architecture. In this paper, we present the first analysis of dLLMs'\nsafety performance and propose a novel safety alignment method tailored to\ntheir unique generation characteristics. Specifically, we identify a critical\nasymmetry between the defender and attacker in terms of security. For the\ndefender, we reveal that the middle tokens of the response, rather than the\ninitial ones, are more critical to the overall safety of dLLM outputs; this\nseems to suggest that aligning middle tokens can be more beneficial to the\ndefender. The attacker, on the contrary, may have limited power to manipulate\nmiddle tokens, as we find dLLMs have a strong tendency towards a sequential\ngeneration order in practice, forcing the attack to meet this distribution and\ndiverting it from influencing the critical middle tokens. Building on this\nasymmetry, we introduce Middle-tOken Safety Alignment (MOSA), a novel method\nthat directly aligns the model's middle generation with safe refusals\nexploiting reinforcement learning. We implement MOSA and compare its security\nperformance against eight attack methods on two benchmarks. We also test the\nutility of MOSA-aligned dLLM on coding, math, and general reasoning. The\nresults strongly prove the superiority of MOSA."}
{"id": "2508.12412", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12412", "abs": "https://arxiv.org/abs/2508.12412", "authors": ["Ron Solomon", "Yarin Yerushalmi Levi", "Lior Vaknin", "Eran Aizikovich", "Amit Baras", "Etai Ohana", "Amit Giloni", "Shamik Bose", "Chiara Picardi", "Yuval Elovici", "Asaf Shabtai"], "title": "LumiMAS: A Comprehensive Framework for Real-Time Monitoring and Enhanced Observability in Multi-Agent Systems", "comment": null, "summary": "The incorporation of large language models in multi-agent systems (MASs) has\nthe potential to significantly improve our ability to autonomously solve\ncomplex problems. However, such systems introduce unique challenges in\nmonitoring, interpreting, and detecting system failures. Most existing MAS\nobservability frameworks focus on analyzing each individual agent separately,\noverlooking failures associated with the entire MAS. To bridge this gap, we\npropose LumiMAS, a novel MAS observability framework that incorporates advanced\nanalytics and monitoring techniques. The proposed framework consists of three\nkey components: a monitoring and logging layer, anomaly detection layer, and\nanomaly explanation layer. LumiMAS's first layer monitors MAS executions,\ncreating detailed logs of the agents' activity. These logs serve as input to\nthe anomaly detection layer, which detects anomalies across the MAS workflow in\nreal time. Then, the anomaly explanation layer performs classification and root\ncause analysis (RCA) of the detected anomalies. LumiMAS was evaluated on seven\ndifferent MAS applications, implemented using two popular MAS platforms, and a\ndiverse set of possible failures. The applications include two novel\nfailure-tailored applications that illustrate the effects of a hallucination or\nbias on the MAS. The evaluation results demonstrate LumiMAS's effectiveness in\nfailure detection, classification, and RCA."}
{"id": "2508.12470", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12470", "abs": "https://arxiv.org/abs/2508.12470", "authors": ["Afrah Gueriani", "Hamza Kheddar", "Ahmed Cherif Mazari", "Mohamed Chahine Ghanem"], "title": "A Robust Cross-Domain IDS using BiGRU-LSTM-Attention for Medical and Industrial IoT Security", "comment": "10 pages", "summary": "The increased Internet of Medical Things IoMT and the Industrial Internet of\nThings IIoT interconnectivity has introduced complex cybersecurity challenges,\nexposing sensitive data, patient safety, and industrial operations to advanced\ncyber threats. To mitigate these risks, this paper introduces a novel\ntransformer-based intrusion detection system IDS, termed BiGAT-ID a hybrid\nmodel that combines bidirectional gated recurrent units BiGRU, long short-term\nmemory LSTM networks, and multi-head attention MHA. The proposed architecture\nis designed to effectively capture bidirectional temporal dependencies, model\nsequential patterns, and enhance contextual feature representation. Extensive\nexperiments on two benchmark datasets, CICIoMT2024 medical IoT and EdgeIIoTset\nindustrial IoT demonstrate the model's cross-domain robustness, achieving\ndetection accuracies of 99.13 percent and 99.34 percent, respectively.\nAdditionally, the model exhibits exceptional runtime efficiency, with inference\ntimes as low as 0.0002 seconds per instance in IoMT and 0.0001 seconds in IIoT\nscenarios. Coupled with a low false positive rate, BiGAT-ID proves to be a\nreliable and efficient IDS for deployment in real-world heterogeneous IoT\nenvironments"}
{"id": "2508.12496", "categories": ["cs.CR", "cs.NI", "C.2.3"], "pdf": "https://arxiv.org/pdf/2508.12496", "abs": "https://arxiv.org/abs/2508.12496", "authors": ["Zhihao Wang", "Alessandro Cornacchia", "Andrea Bianco", "Idilio Drago", "Paolo Giaccone", "Dingde Jiang", "Marco Mellia"], "title": "ChamaleoNet: Programmable Passive Probe for Enhanced Visibility on Erroneous Traffic", "comment": "17 pages, 16 figures", "summary": "Traffic visibility remains a key component for management and security\noperations. Observing unsolicited and erroneous traffic, such as unanswered\ntraffic or errors, is fundamental to detect misconfiguration, temporary\nfailures or attacks. ChamaleoNet transforms any production network into a\ntransparent monitor to let administrators collect unsolicited and erroneous\ntraffic directed to hosts, whether offline or active, hosting a server or a\nclient, protected by a firewall, or unused addresses. ChamaleoNet is programmed\nto ignore well-formed traffic and collect only erroneous packets, including\nthose generated by misconfigured or infected internal hosts, and those sent by\nexternal actors which scan for services. Engineering such a system poses\nseveral challenges, from scalability to privacy. Leveraging the SDN paradigm,\nChamaleoNet processes the traffic flowing through a campus/corporate network\nand focuses on erroneous packets only, lowering the pressure on the collection\nsystem while respecting privacy regulations by design. ChamaleoNet enables the\nseamless integration with active deceptive systems like honeypots that can\nimpersonate unused hosts/ports/services and engage with senders. The SDN\nin-hardware filtering reduces the traffic to the controller by 96%, resulting\nin a scalable solution, which we offer as open source. Simple analytics unveil\ninternal misconfigured and infected hosts, identify temporary failures, and\nenhance visibility on external radiation produced by attackers looking for\nvulnerable services."}
{"id": "2508.12538", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.12538", "abs": "https://arxiv.org/abs/2508.12538", "authors": ["Yongjian Guo", "Puzhuo Liu", "Wanlun Ma", "Zehang Deng", "Xiaogang Zhu", "Peng Di", "Xi Xiao", "Sheng Wen"], "title": "Systematic Analysis of MCP Security", "comment": null, "summary": "The Model Context Protocol (MCP) has emerged as a universal standard that\nenables AI agents to seamlessly connect with external tools, significantly\nenhancing their functionality. However, while MCP brings notable benefits, it\nalso introduces significant vulnerabilities, such as Tool Poisoning Attacks\n(TPA), where hidden malicious instructions exploit the sycophancy of large\nlanguage models (LLMs) to manipulate agent behavior. Despite these risks,\ncurrent academic research on MCP security remains limited, with most studies\nfocusing on narrow or qualitative analyses that fail to capture the diversity\nof real-world threats. To address this gap, we present the MCP Attack Library\n(MCPLIB), which categorizes and implements 31 distinct attack methods under\nfour key classifications: direct tool injection, indirect tool injection,\nmalicious user attacks, and LLM inherent attack. We further conduct a\nquantitative analysis of the efficacy of each attack. Our experiments reveal\nkey insights into MCP vulnerabilities, including agents' blind reliance on tool\ndescriptions, sensitivity to file-based attacks, chain attacks exploiting\nshared context, and difficulty distinguishing external data from executable\ncommands. These insights, validated through attack experiments, underscore the\nurgency for robust defense strategies and informed MCP design. Our\ncontributions include 1) constructing a comprehensive MCP attack taxonomy, 2)\nintroducing a unified attack framework MCPLIB, and 3) conducting empirical\nvulnerability analysis to enhance MCP security mechanisms. This work provides a\nfoundational framework, supporting the secure evolution of MCP ecosystems."}
{"id": "2508.12539", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12539", "abs": "https://arxiv.org/abs/2508.12539", "authors": ["Sandaru Jayawardana", "Sennur Ulukus", "Ming Ding", "Kanchana Thilakarathna"], "title": "The Hidden Cost of Correlation: Rethinking Privacy Leakage in Local Differential Privacy", "comment": "19 pages with 8 figures", "summary": "Local differential privacy (LDP) has emerged as a promising paradigm for\nprivacy-preserving data collection in distributed systems, where users\ncontribute multi-dimensional records with potentially correlated attributes.\nRecent work has highlighted that correlation-induced privacy leakage (CPL)\nplays a critical role in shaping the privacy-utility trade-off under LDP,\nespecially when correlations exist among attributes. Nevertheless, it remains\nunclear to what extent the prevailing assumptions and proposed solutions are\nvalid and how significant CPL is in real-world data. To address this gap, we\nfirst perform a comprehensive statistical analysis of five widely used LDP\nmechanisms -- GRR, RAPPOR, OUE, OLH and Exponential mechanism -- to assess CPL\nacross four real-world datasets. We identify that many primary assumptions and\nmetrics in current approaches fall short of accurately characterising these\nleakages. Moreover, current studies have been limited to a set of pure LDP\n(i.e., {\\delta = 0}) mechanisms. In response, we develop the first algorithmic\nframework to theoretically quantify CPL for any general approximated LDP\n(({\\varepsilon},{\\delta})-LDP) mechanism. We validate our theoretical results\nagainst empirical statistical results and provide a theoretical explanation for\nthe observed statistical patterns. Finally, we propose two novel benchmarks to\nvalidate correlation analysis algorithms and evaluate the utility vs CPL of LDP\nmechanisms. Further, we demonstrate how these findings can be applied to\nachieve an efficient privacy-utility trade-off in real-world data governance."}
{"id": "2508.12553", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12553", "abs": "https://arxiv.org/abs/2508.12553", "authors": ["Peilun Wu", "Nan Sun", "Nour Moustafa", "Youyang Qu", "Ming Ding"], "title": "DEFENDCLI: {Command-Line} Driven Attack Provenance Examination", "comment": null, "summary": "Endpoint Detection and Response (EDR) solutions embrace the method of attack\nprovenance graph to discover unknown threats through system event correlation.\nHowever, this method still faces some unsolved problems in the fields of\ninteroperability, reliability, flexibility, and practicability to deliver\nactionable results. Our research highlights the limitations of current\nsolutions in detecting obfuscation, correlating attacks, identifying\nlow-frequency events, and ensuring robust context awareness in relation to\ncommand-line activities. To address these challenges, we introduce DEFENDCLI,\nan innovative system leveraging provenance graphs that, for the first time,\ndelves into command-line-level detection. By offering finer detection\ngranularity, it addresses a gap in modern EDR systems that has been overlooked\nin previous research. Our solution improves the precision of the information\nrepresentation by evaluating differentiation across three levels: unusual\nsystem process calls, suspicious command-line executions, and infrequent\nexternal network connections. This multi-level approach enables EDR systems to\nbe more reliable in complex and dynamic environments. Our evaluation\ndemonstrates that DEFENDCLI improves precision by approximately 1.6x compared\nto the state-of-the-art methods on the DARPA Engagement Series attack datasets.\nExtensive real-time industrial testing across various attack scenarios further\nvalidates its practical effectiveness. The results indicate that DEFENDCLI not\nonly detects previously unknown attack instances, which are missed by other\nmodern commercial solutions, but also achieves a 2.3x improvement in precision\nover the state-of-the-art research work."}
{"id": "2508.12560", "categories": ["cs.CR", "cs.DC", "cs.LG", "C.2; C.4; I.2"], "pdf": "https://arxiv.org/pdf/2508.12560", "abs": "https://arxiv.org/abs/2508.12560", "authors": ["Prabath Abeysekara", "Hai Dong"], "title": "Data-driven Trust Bootstrapping for Mobile Edge Computing-based Industrial IoT Services", "comment": "15 pages", "summary": "We propose a data-driven and context-aware approach to bootstrap\ntrustworthiness of homogeneous Internet of Things (IoT) services in Mobile Edge\nComputing (MEC) based industrial IoT (IIoT) systems. The proposed approach\naddresses key limitations in adapting existing trust bootstrapping approaches\ninto MEC-based IIoT systems. These key limitations include, the lack of\nopportunity for a service consumer to interact with a lesser-known service over\na prolonged period of time to get a robust measure of its trustworthiness,\ninability of service consumers to consistently interact with their peers to\nreceive reliable recommendations of the trustworthiness of a lesser-known\nservice as well as the impact of uneven context parameters in different MEC\nenvironments causing uneven trust environments for trust evaluation. In\naddition, the proposed approach also tackles the problem of data sparsity via\nenabling knowledge sharing among different MEC environments within a given MEC\ntopology. To verify the effectiveness of the proposed approach, we carried out\na comprehensive evaluation on two real-world datasets suitably adjusted to\nexhibit the context-dependent trust information accumulated in MEC environments\nwithin a given MEC topology. The experimental results affirmed the\neffectiveness of our approach and its suitability to bootstrap trustworthiness\nof services in MEC-based IIoT systems."}
{"id": "2508.12571", "categories": ["cs.CR", "cs.CY", "cs.ET", "cs.HC", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.12571", "abs": "https://arxiv.org/abs/2508.12571", "authors": ["Tyler Schroder", "Renee Sirbu", "Sohee Park", "Jessica Morley", "Sam Street", "Luciano Floridi"], "title": "Cyber Risks to Next-Gen Brain-Computer Interfaces: Analysis and Recommendations", "comment": null, "summary": "Brain-computer interfaces (BCIs) show enormous potential for advancing\npersonalized medicine. However, BCIs also introduce new avenues for\ncyber-attacks or security compromises. In this article, we analyze the problem\nand make recommendations for device manufacturers to better secure devices and\nto help regulators understand where more guidance is needed to protect patient\nsafety and data confidentiality. Device manufacturers should implement the\nprior suggestions in their BCI products. These recommendations help protect BCI\nusers from undue risks, including compromised personal health and genetic\ninformation, unintended BCI-mediated movement, and many other cybersecurity\nbreaches. Regulators should mandate non-surgical device update methods, strong\nauthentication and authorization schemes for BCI software modifications,\nencryption of data moving to and from the brain, and minimize network\nconnectivity where possible. We also design a hypothetical, average-case threat\nmodel that identifies possible cybersecurity threats to BCI patients and\npredicts the likeliness of risk for each category of threat. BCIs are at less\nrisk of physical compromise or attack, but are vulnerable to remote attack; we\nfocus on possible threats via network paths to BCIs and suggest technical\ncontrols to limit network connections."}
{"id": "2508.12584", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12584", "abs": "https://arxiv.org/abs/2508.12584", "authors": ["Dikshant", "Verma"], "title": "Reducing False Positives with Active Behavioral Analysis for Cloud Security", "comment": null, "summary": "Rule-based cloud security posture management (CSPM) solutions are known to\nproduce a lot of false positives based on the limited contextual understanding\nand dependence on static heuristics testing. This paper introduces a\nvalidation-driven methodology that integrates active behavioral testing in\ncloud security posture management solution(s) to evaluate the exploitability of\npolicy violations in real time. The proposed system employs lightweight and\nautomated probes, built from open-source tools, validation scripts, and\npenetration testing test cases, to simulate adversarial attacks on\nmisconfigured or vulnerable cloud assets without any impact to the cloud\nservices or environment. For instance, cloud services may be flagged as\npublicly exposed and vulnerable despite being protected by access control\nlayers, or secure policies, resulting in non-actionable alerts that consumes\nanalysts time during manual validation. Through controlled experimentation in a\nreproducible AWS setup, we evaluated the reduction in false positive rates\nacross various misconfiguration and vulnerable alerts. Our findings indicate an\naverage reduction of 93\\% in false positives. Furthermore, the framework\ndemonstrates low latency performance. These results demonstrate a scalable\nmethod to improve detection accuracy and analyst productivity in large cloud\nenvironments. While our evaluation focuses on AWS, the architecture is modular\nand extensible to multi-cloud setups."}
{"id": "2508.12597", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12597", "abs": "https://arxiv.org/abs/2508.12597", "authors": ["Haolin Zheng", "Ning Gao", "Donghong Cai", "Shi Jin", "Michail Matthaiou"], "title": "UAV Individual Identification via Distilled RF Fingerprints-Based LLM in ISAC Networks", "comment": null, "summary": "Unmanned aerial vehicle (UAV) individual (ID) identification is a critical\nsecurity surveillance strategy in low-altitude integrated sensing and\ncommunication (ISAC) networks. In this paper, we propose a novel dynamic\nknowledge distillation (KD)-enabled wireless radio frequency fingerprint large\nlanguage model (RFF-LLM) framework for UAV ID identification. First, we propose\nan RFF-LLM framework based on the modified GPT-2 model to improve the\nidentification accuracy in complex outdoor environments. Then, considering the\nparameter overhead of the RFF-LLM, we design a dynamic KD strategy to compress\nthe model. Specifically, the proximal policy optimization (PPO) algorithm is\nemployed to dynamically adjust the distillation temperature, overcoming the\nlocal optimum dilemma inherent in static KD. As a next step, the knowledge of\nthe RFF-LLM is adequately transferred to the lightweight Lite-HRNet model.\nFinally, our experiments are conducted based on the self-built drone RFF\ndataset of Release one, namely DRFF-R1, by collecting the I/Q signals of 20\ncommercial UAVs in channel 149. The experiment results show that the proposed\nframework achieves 98.38\\% ID identification accuracy with merely 0.15 million\nparameters and 2.74 ms response time, which outperforms the benchmarks."}
{"id": "2508.12622", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12622", "abs": "https://arxiv.org/abs/2508.12622", "authors": ["Zilong Lin", "Zichuan Li", "Xiaojing Liao", "XiaoFeng Wang"], "title": "Consiglieres in the Shadow: Understanding the Use of Uncensored Large Language Models in Cybercrimes", "comment": null, "summary": "The advancement of AI technologies, particularly Large Language Models\n(LLMs), has transformed computing while introducing new security and privacy\nrisks. Prior research shows that cybercriminals are increasingly leveraging\nuncensored LLMs (ULLMs) as backends for malicious services. Understanding these\nULLMs has been hindered by the challenge of identifying them among the vast\nnumber of open-source LLMs hosted on platforms like Hugging Face. In this\npaper, we present the first systematic study of ULLMs, overcoming this\nchallenge by modeling relationships among open-source LLMs and between them and\nrelated data, such as fine-tuning, merging, compressing models, and using or\ngenerating datasets with harmful content. Representing these connections as a\nknowledge graph, we applied graph-based deep learning to discover over 11,000\nULLMs from a small set of labeled examples and uncensored datasets.\n  A closer analysis of these ULLMs reveals their alarming scale and usage. Some\nhave been downloaded over a million times, with one over 19 million installs.\nThese models -- created through fine-tuning, merging, or compression of other\nmodels -- are capable of generating harmful content, including hate speech,\nviolence, erotic material, and malicious code. Evidence shows their integration\ninto hundreds of malicious applications offering services like erotic\nrole-play, child pornography, malicious code generation, and more. In addition,\nunderground forums reveal criminals sharing techniques and scripts to build\ncheap alternatives to commercial malicious LLMs. These findings highlight the\nwidespread abuse of LLM technology and the urgent need for effective\ncountermeasures against this growing threat."}
{"id": "2508.12641", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12641", "abs": "https://arxiv.org/abs/2508.12641", "authors": ["Yasaman Samadi", "Hai Dong", "Xiaoyu Xia"], "title": "MPOCryptoML: Multi-Pattern based Off-Chain Crypto Money Laundering Detection", "comment": null, "summary": "Recent advancements in money laundering detection have demonstrated the\npotential of using graph neural networks to capture laundering patterns\naccurately. However, existing models are not explicitly designed to detect the\ndiverse patterns of off-chain cryptocurrency money laundering. Neglecting any\nlaundering pattern introduces critical detection gaps, as each pattern reflects\nunique transactional structures that facilitate the obfuscation of illicit fund\norigins and movements. Failure to account for these patterns may result in\nunder-detection or omission of specific laundering activities, diminishing\nmodel accuracy and allowing schemes to bypass detection. To address this gap,\nwe propose the MPOCryptoML model to effectively detect multiple laundering\npatterns in cryptocurrency transactions. MPOCryptoML includes the development\nof a multi-source Personalized PageRank algorithm to identify random laundering\npatterns. Additionally, we introduce two novel algorithms by analyzing the\ntimestamp and weight of transactions in high-volume financial networks to\ndetect various money laundering structures, including fan-in, fan-out,\nbipartite, gather-scatter, and stack patterns. We further examine correlations\nbetween these patterns using a logistic regression model. An anomaly score\nfunction integrates results from each module to rank accounts by anomaly score,\nsystematically identifying high-risk accounts. Extensive experiments on public\ndatasets including Elliptic++, Ethereum fraud detection, and Wormhole\ntransaction datasets validate the efficacy and efficiency of MPOCryptoML.\nResults show consistent performance gains, with improvements up to 9.13% in\nprecision, up to 10.16% in recall, up to 7.63% in F1-score, and up to 10.19% in\naccuracy."}
{"id": "2508.12730", "categories": ["cs.CR", "cs.HC", "cs.LG", "H.5.2; I.3.6"], "pdf": "https://arxiv.org/pdf/2508.12730", "abs": "https://arxiv.org/abs/2508.12730", "authors": ["Jaeung Lee", "Suhyeon Yu", "Yurim Jang", "Simon S. Woo", "Jaemin Jo"], "title": "Unlearning Comparator: A Visual Analytics System for Comparative Evaluation of Machine Unlearning Methods", "comment": "Submitted to IEEE Transactions on Visualization and Computer Graphics\n  (TVCG), under review. 15 pages. This work has been submitted to the IEEE for\n  possible publication", "summary": "Machine Unlearning (MU) aims to remove target training data from a trained\nmodel so that the removed data no longer influences the model's behavior,\nfulfilling \"right to be forgotten\" obligations under data privacy laws. Yet, we\nobserve that researchers in this rapidly emerging field face challenges in\nanalyzing and understanding the behavior of different MU methods, especially in\nterms of three fundamental principles in MU: accuracy, efficiency, and privacy.\nConsequently, they often rely on aggregate metrics and ad-hoc evaluations,\nmaking it difficult to accurately assess the trade-offs between methods. To\nfill this gap, we introduce a visual analytics system, Unlearning Comparator,\ndesigned to facilitate the systematic evaluation of MU methods. Our system\nsupports two important tasks in the evaluation process: model comparison and\nattack simulation. First, it allows the user to compare the behaviors of two\nmodels, such as a model generated by a certain method and a retrained baseline,\nat class-, instance-, and layer-levels to better understand the changes made\nafter unlearning. Second, our system simulates membership inference attacks\n(MIAs) to evaluate the privacy of a method, where an attacker attempts to\ndetermine whether specific data samples were part of the original training set.\nWe evaluate our system through a case study visually analyzing prominent MU\nmethods and demonstrate that it helps the user not only understand model\nbehaviors but also gain insights that can inform the improvement of MU methods."}
{"id": "2508.12832", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12832", "abs": "https://arxiv.org/abs/2508.12832", "authors": ["Jinyu Lu", "Xinrong Sun", "Yunting Tao", "Tong Ji", "Fanyu Kong", "Guoqiang Yang"], "title": "Efficient and Verifiable Privacy-Preserving Convolutional Computation for CNN Inference with Untrusted Clouds", "comment": null, "summary": "The widespread adoption of convolutional neural networks (CNNs) in\nresource-constrained scenarios has driven the development of Machine Learning\nas a Service (MLaaS) system. However, this approach is susceptible to privacy\nleakage, as the data sent from the client to the untrusted cloud server often\ncontains sensitive information. Existing CNN privacy-preserving schemes, while\neffective in ensuring data confidentiality through homomorphic encryption and\nsecret sharing, face efficiency bottlenecks, particularly in convolution\noperations. In this paper, we propose a novel verifiable privacy-preserving\nscheme tailored for CNN convolutional layers. Our scheme enables efficient\nencryption and decryption, allowing resource-constrained clients to securely\noffload computations to the untrusted cloud server. Additionally, we present a\nverification mechanism capable of detecting the correctness of the results with\na success probability of at least $1-\\frac{1}{\\left|Z\\right|}$. Extensive\nexperiments conducted on 10 datasets and various CNN models demonstrate that\nour scheme achieves speedups ranging $26 \\times$ ~ $\\ 87\\times$ compared to the\noriginal plaintext model while maintaining accuracy."}
{"id": "2508.12859", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12859", "abs": "https://arxiv.org/abs/2508.12859", "authors": ["Xingxing Xu", "Minjia Shi", "Patrick Sole"], "title": "The covering radius of Butson Hadamard codes for the homogeneous metric", "comment": null, "summary": "Butson matrices are complex Hadamard matrices with entries in the complex\nroots of unity of given order. There is an interesting code in phase space\nrelated to this matrix (Armario et al. 2023). We study the covering radius of\nButson Hadamard codes for the homogeneous metric, a metric defined uniquely, up\nto scaling, for a commutative ring alphabet that is Quasi Frobenius. An upper\nbound is derived by an orthogonal array argument. A lower bound relies on the\nexistence of bent sequences in the sense of (Shi et al. 2022). This latter\nbound generalizes a bound of (Armario et al. 2025) for the Hamming metric."}
{"id": "2508.12870", "categories": ["cs.CR", "68M25"], "pdf": "https://arxiv.org/pdf/2508.12870", "abs": "https://arxiv.org/abs/2508.12870", "authors": ["Vinod Khandkar", "Kieron Ivy Turk", "Ehsan Toreini", "Nishanth Sastry"], "title": "Supporting Socially Constrained Private Communications with SecureWhispers", "comment": "14 pages, 13 figures, 3 tables", "summary": "Rapidly changing social norms and national, legal, and political conditions\nsocially constrain people from discussing sensitive topics such as sexuality or\nreligion. Such constrained, vulnerable minorities are often worried about\ninadvertent information disclosure and may be unsure about the extent to which\ntheir communications are being monitored in public or semi-public spaces like\nworkplaces or cafes. Personal devices extend trust to the digital domain,\nmaking it desirable to have strictly private communication between trusted\ndevices. Currently, messaging services like WhatsApp provide alternative means\nfor exchanging sensitive private information, while personal safety apps such\nas Noonlight enable private signaling. However, these rely on third-party\nmechanisms for secure and private communication, which may not be accessible\nfor justifiable reasons, such as insecure internet access or companion device\nconnections. In these cases, it is challenging to achieve communication that is\nstrictly private between two devices instead of user accounts without any\ndependency on third-party infrastructure. The goal of this paper is to support\nprivate communications by setting up a shared secret between two or more\ndevices without sending any data on the network. We develop a method to create\na shared secret between phones by shaking them together. Each device extracts\nthe shared randomness from the shake, then conditions the randomness to 7.798\nbits per byte of key material. This paper proposes three different applications\nof this generated shared secret: message obfuscation, trust delegation, and\nencrypted beacons. We have implemented the message obfuscation on Android as an\nindependent app that can be used for private communication with trusted\ncontacts. We also present research on the usability, design considerations, and\nfurther integration of these tools in mainstream services."}
{"id": "2508.12910", "categories": ["cs.CR", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.12910", "abs": "https://arxiv.org/abs/2508.12910", "authors": ["Ziteng Hu", "Yingjie Xia", "Xiyuan Chen", "Li Kuang"], "title": "SecFSM: Knowledge Graph-Guided Verilog Code Generation for Secure Finite State Machines in Systems-on-Chip", "comment": null, "summary": "Finite State Machines (FSMs) play a critical role in implementing control\nlogic for Systems-on-Chip (SoC). Traditionally, FSMs are implemented by\nhardware engineers through Verilog coding, which is often tedious and\ntime-consuming. Recently, with the remarkable progress of Large Language Models\n(LLMs) in code generation, LLMs have been increasingly explored for automating\nVerilog code generation. However, LLM-generated Verilog code often suffers from\nsecurity vulnerabilities, which is particularly concerning for\nsecurity-sensitive FSM implementations. To address this issue, we propose\nSecFSM, a novel method that leverages a security-oriented knowledge graph to\nguide LLMs in generating more secure Verilog code. Specifically, we first\nconstruct a FSM Security Knowledge Graph (FSKG) as an external aid to LLMs.\nSubsequently, we analyze users' requirements to identify vulnerabilities and\nget a list of vulnerabilities in the requirements. Then, we retrieve knowledge\nfrom FSKG based on the vulnerabilities list. Finally, we construct security\nprompts based on the security knowledge for Verilog code generation. To\nevaluate SecFSM, we build a dedicated dataset collected from academic datasets,\nartificial datasets, papers, and industrial cases. Extensive experiments\ndemonstrate that SecFSM outperforms state-of-the-art baselines. In particular,\non a benchmark of 25 security test cases evaluated by DeepSeek-R1, SecFSM\nachieves an outstanding pass rate of 21/25."}
{"id": "2508.12953", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12953", "abs": "https://arxiv.org/abs/2508.12953", "authors": ["Samuel Aiello"], "title": "Prescriptive Zero Trust- Assessing the impact of zero trust on cyber attack prevention", "comment": "232 pages in total, 21 figures, 32 tables", "summary": "Increasingly sophisticated and varied cyber threats necessitate ever\nimproving enterprise security postures. For many organizations today, those\npostures have a foundation in the Zero Trust Architecture. This strategy sees\ntrust as something an enterprise must not give lightly or assume too broadly.\nUnderstanding the ZTA and its numerous controls centered around the idea of not\ntrusting anything inside or outside the network without verification, will\nallow organizations to comprehend and leverage this increasingly common\nparadigm. The ZTA, unlike many other regulatory frameworks, is not tightly\ndefined. The research assesses the likelihood of quantifiable guidelines that\nmeasure cybersecurity maturity for an enterprise organization in relation to\nZTA implementation. This is a new, data driven methodology for quantifying\ncyber resilience enabled by the adoption of Zero Trust principles to\npragmatically address the critical need of organizations. It also looks at the\npractical aspects ZTA has on capabilities in deterring cyberattacks on a\nnetwork. The outcomes of this research define a prescriptive set of key\ntechnical controls across identity verification, microsegmentation, data\nencryption, analytics, and orchestration that characterize the comprehensive\nZTA deployment. By evaluating the depth of integration for each control\ncomponent and aligning to industry best practices, the study's results help\nassess an organization's ZTA maturity level on a scale from Initial to\nOptimized adoption. The research's resultant four tier model demarcates phases\nfor an organization on its security transformation journey, with each tier\nadding to the capability of the last."}
{"id": "2508.13033", "categories": ["cs.CR", "B.7.1; B.6"], "pdf": "https://arxiv.org/pdf/2508.13033", "abs": "https://arxiv.org/abs/2508.13033", "authors": ["Ishraq Tashdid", "Tasnuva Farheen", "Sazadur Rahman"], "title": "AuthenTree: A Scalable MPC-Based Distributed Trust Architecture for Chiplet-based Heterogeneous Systems", "comment": "Accepted to IEEE PAINE 2025", "summary": "The rapid adoption of chiplet-based heterogeneous integration is reshaping\nsemiconductor design by enabling modular, scalable, and faster time-to-market\nsolutions for AI and high-performance computing. However, multi-vendor assembly\nin post-fabrication environments fragments the supply chain and exposes SiP\nsystems to serious security threats, including cloning, overproduction, and\nchiplet substitution. Existing authentication solutions depend on trusted\nintegrators or centralized security anchors, which can expose sensitive data or\ncreate single points of failure. We introduce AuthenTree, a distributed\nauthentication framework that leverages multi-party computation (MPC) in a\nscalable tree-based architecture, removing the need for dedicated security\nhardware or centralized trust. AuthenTree enables secure chiplet validation\nwithout revealing raw signatures, distributing trust across multiple integrator\nchiplets. Our evaluation in five SiP benchmarks demonstrates that AuthenTree\nimposes minimal overhead, with an area as low as 0.48% (7,000 sq-micrometers),\nan overhead power under 0.5%, and an authentication latency below 1\nmicrosecond, surpassing previous work in some cases by 700 times. These results\nestablish AuthenTree as an efficient, robust, and scalable solution for\nnext-generation chiplet-based security in zero-trust SiP environments."}
{"id": "2508.13048", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.13048", "abs": "https://arxiv.org/abs/2508.13048", "authors": ["Weiwei Qi", "Shuo Shao", "Wei Gu", "Tianhang Zheng", "Puning Zhao", "Zhan Qin", "Kui Ren"], "title": "MAJIC: Markovian Adaptive Jailbreaking via Iterative Composition of Diverse Innovative Strategies", "comment": "7 pages, 3 figures", "summary": "Large Language Models (LLMs) have exhibited remarkable capabilities but\nremain vulnerable to jailbreaking attacks, which can elicit harmful content\nfrom the models by manipulating the input prompts. Existing black-box\njailbreaking techniques primarily rely on static prompts crafted with a single,\nnon-adaptive strategy, or employ rigid combinations of several underperforming\nattack methods, which limits their adaptability and generalization. To address\nthese limitations, we propose MAJIC, a Markovian adaptive jailbreaking\nframework that attacks black-box LLMs by iteratively combining diverse\ninnovative disguise strategies. MAJIC first establishes a ``Disguise Strategy\nPool'' by refining existing strategies and introducing several innovative\napproaches. To further improve the attack performance and efficiency, MAJIC\nformulate the sequential selection and fusion of strategies in the pool as a\nMarkov chain. Under this formulation, MAJIC initializes and employs a Markov\nmatrix to guide the strategy composition, where transition probabilities\nbetween strategies are dynamically adapted based on attack outcomes, thereby\nenabling MAJIC to learn and discover effective attack pathways tailored to the\ntarget model. Our empirical results demonstrate that MAJIC significantly\noutperforms existing jailbreak methods on prominent models such as GPT-4o and\nGemini-2.0-flash, achieving over 90\\% attack success rate with fewer than 15\nqueries per attempt on average."}
{"id": "2508.13092", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13092", "abs": "https://arxiv.org/abs/2508.13092", "authors": ["Xiang Long", "Yingjie Xia", "Xiyuan Chen", "Li Kuang"], "title": "VerilogLAVD: LLM-Aided Rule Generation for Vulnerability Detection in Verilog", "comment": null, "summary": "Timely detection of hardware vulnerabilities during the early design stage is\ncritical for reducing remediation costs. Existing early detection techniques\noften require specialized security expertise, limiting their usability. Recent\nefforts have explored the use of large language models (LLMs) for Verilog\nvulnerability detection. However, LLMs struggle to capture the structure in\nVerilog code, resulting in inconsistent detection results. To this end, we\npropose VerilogLAVD, the first LLM-aided graph traversal rule generation\napproach for Verilog vulnerability detection. Our approach introduces the\nVerilog Property Graph (VeriPG), a unified representation of Verilog code. It\ncombines syntactic features extracted from the abstract syntax tree (AST) with\nsemantic information derived from control flow and data dependency graphs. We\nleverage LLMs to generate VeriPG-based detection rules from Common Weakness\nEnumeration (CWE) descriptions. These rules guide the rule executor that\ntraversal VeriPG for potential vulnerabilities. To evaluate VerilogLAVD, we\nbuild a dataset collected from open-source repositories and synthesized data.\nIn our empirical evaluation on 77 Verilog designs encompassing 12 CWE types,\nVerilogLAVD achieves an F1-score of 0.54. Compared to the LLM-only and LLM with\nexternal knowledge baselines, VerilogLAVD improves F1-score by 0.31 and 0.27,\nrespectively."}
