{"id": "2509.14271", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14271", "abs": "https://arxiv.org/abs/2509.14271", "authors": ["Gustavo Sandoval", "Denys Fenchenko", "Junyao Chen"], "title": "Early Approaches to Adversarial Fine-Tuning for Prompt Injection Defense: A 2022 Study of GPT-3 and Contemporary Models", "comment": null, "summary": "This paper documents early research conducted in 2022 on defending against\nprompt injection attacks in large language models, providing historical context\nfor the evolution of this critical security domain. This research focuses on\ntwo adversarial attacks against Large Language Models (LLMs): prompt injection\nand goal hijacking. We examine how to construct these attacks, test them on\nvarious LLMs, and compare their effectiveness. We propose and evaluate a novel\ndefense technique called Adversarial Fine-Tuning. Our results show that,\nwithout this defense, the attacks succeeded 31\\% of the time on GPT-3 series\nmodels. When using our Adversarial Fine-Tuning approach, attack success rates\nwere reduced to near zero for smaller GPT-3 variants (Ada, Babbage, Curie),\nthough we note that subsequent research has revealed limitations of\nfine-tuning-based defenses. We also find that more flexible models exhibit\ngreater vulnerability to these attacks. Consequently, large models such as\nGPT-3 Davinci are more vulnerable than smaller models like GPT-2. While the\nspecific models tested are now superseded, the core methodology and empirical\nfindings contributed to the foundation of modern prompt injection defense\nresearch, including instruction hierarchy systems and constitutional AI\napproaches."}
{"id": "2509.14275", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14275", "abs": "https://arxiv.org/abs/2509.14275", "authors": ["Nobin Sarwar", "Shubhashis Roy Dipta"], "title": "FedMentor: Domain-Aware Differential Privacy for Heterogeneous Federated LLMs in Mental Health", "comment": "(e.g.: 18 pages, 6 figures, 6 tables)", "summary": "Privacy-preserving adaptation of Large Language Models (LLMs) in sensitive\ndomains (e.g., mental health) requires balancing strict confidentiality with\nmodel utility and safety. We propose FedMentor, a federated fine-tuning\nframework that integrates Low-Rank Adaptation (LoRA) and domain-aware\nDifferential Privacy (DP) to meet per-domain privacy budgets while maintaining\nperformance. Each client (domain) applies a custom DP noise scale proportional\nto its data sensitivity, and the server adaptively reduces noise when utility\nfalls below a threshold. In experiments on three mental health datasets, we\nshow that FedMentor improves safety over standard Federated Learning without\nprivacy, raising safe output rates by up to three points and lowering toxicity,\nwhile maintaining utility (BERTScore F1 and ROUGE-L) within 0.5% of the\nnon-private baseline and close to the centralized upper bound. The framework\nscales to backbones with up to 1.7B parameters on single-GPU clients, requiring\n< 173 MB of communication per round. FedMentor demonstrates a practical\napproach to privately fine-tune LLMs for safer deployments in healthcare and\nother sensitive fields."}
{"id": "2509.14278", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14278", "abs": "https://arxiv.org/abs/2509.14278", "authors": ["Yuntao Du", "Zitao Li", "Ninghui Li", "Bolin Ding"], "title": "Beyond Data Privacy: New Privacy Risks for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable progress in natural\nlanguage understanding, reasoning, and autonomous decision-making. However,\nthese advancements have also come with significant privacy concerns. While\nsignificant research has focused on mitigating the data privacy risks of LLMs\nduring various stages of model training, less attention has been paid to new\nthreats emerging from their deployment. The integration of LLMs into widely\nused applications and the weaponization of their autonomous abilities have\ncreated new privacy vulnerabilities. These vulnerabilities provide\nopportunities for both inadvertent data leakage and malicious exfiltration from\nLLM-powered systems. Additionally, adversaries can exploit these systems to\nlaunch sophisticated, large-scale privacy attacks, threatening not only\nindividual privacy but also financial security and societal trust. In this\npaper, we systematically examine these emerging privacy risks of LLMs. We also\ndiscuss potential mitigation strategies and call for the research community to\nbroaden its focus beyond data privacy risks, developing new defenses to address\nthe evolving threats posed by increasingly powerful LLMs and LLM-powered\nsystems."}
{"id": "2509.14282", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.14282", "abs": "https://arxiv.org/abs/2509.14282", "authors": ["Ali Al-kuwari", "Noureldin Mohamed", "Saif Al-kuwari", "Ahmed Farouk", "Bikash K. Behera"], "title": "Resisting Quantum Key Distribution Attacks Using Quantum Machine Learning", "comment": null, "summary": "The emergence of quantum computing poses significant risks to the security of\nmodern communication networks as it breaks today's public-key cryptographic\nalgorithms. Quantum Key Distribution (QKD) offers a promising solution by\nharnessing the principles of quantum mechanics to establish secure keys.\nHowever, practical QKD implementations remain vulnerable to hardware\nimperfections and advanced attacks such as Photon Number Splitting and\nTrojan-Horse attacks. In this work, we investigate the potential of using\nquantum machine learning (QML) to detect popular QKD attacks. In particular, we\npropose a Hybrid Quantum Long Short-Term Memory (QLSTM) model to improve the\ndetection of common QKD attacks. By combining quantum-enhanced learning with\nclassical deep learning, the model captures complex temporal patterns in QKD\ndata, improving detection accuracy. To evaluate the proposed model, we\nintroduce a realistic QKD dataset simulating normal QKD operations along with\nseven attack scenarios, Intercept-and-Resend, Photon-Number Splitting (PNS),\nTrojan-Horse attacks Random Number Generator (RNG), Detector Blinding,\nWavelength-dependent Trojan Horse, and Combined attacks. The dataset includes\nquantum security metrics such as Quantum Bit Error Rate (QBER), measurement\nentropy, signal and decoy loss rates, and time-based metrics, ensuring an\naccurate representation of real-world conditions. Our results demonstrate\npromising performance of the quantum machine learning approach compared to\ntraditional classical machine learning models, highlighting the potential of\nhybrid techniques to enhance the security of future quantum communication\nnetworks. The proposed Hybrid QLSTM model achieved an accuracy of 93.7.0\\%\nafter 50 training epochs, outperforming classical deep learning models such as\nLSTM, and CNN."}
{"id": "2509.14284", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14284", "abs": "https://arxiv.org/abs/2509.14284", "authors": ["Vaidehi Patil", "Elias Stengel-Eskin", "Mohit Bansal"], "title": "The Sum Leaks More Than Its Parts: Compositional Privacy Risks and Mitigations in Multi-Agent Collaboration", "comment": "Code: https://github.com/Vaidehi99/MultiAgentPrivacy", "summary": "As large language models (LLMs) become integral to multi-agent systems, new\nprivacy risks emerge that extend beyond memorization, direct inference, or\nsingle-turn evaluations. In particular, seemingly innocuous responses, when\ncomposed across interactions, can cumulatively enable adversaries to recover\nsensitive information, a phenomenon we term compositional privacy leakage. We\npresent the first systematic study of such compositional privacy leaks and\npossible mitigation methods in multi-agent LLM systems. First, we develop a\nframework that models how auxiliary knowledge and agent interactions jointly\namplify privacy risks, even when each response is benign in isolation. Next, to\nmitigate this, we propose and evaluate two defense strategies: (1)\nTheory-of-Mind defense (ToM), where defender agents infer a questioner's intent\nby anticipating how their outputs may be exploited by adversaries, and (2)\nCollaborative Consensus Defense (CoDef), where responder agents collaborate\nwith peers who vote based on a shared aggregated state to restrict sensitive\ninformation spread. Crucially, we balance our evaluation across compositions\nthat expose sensitive information and compositions that yield benign\ninferences. Our experiments quantify how these defense strategies differ in\nbalancing the privacy-utility trade-off. We find that while chain-of-thought\nalone offers limited protection to leakage (~39% sensitive blocking rate), our\nToM defense substantially improves sensitive query blocking (up to 97%) but can\nreduce benign task success. CoDef achieves the best balance, yielding the\nhighest Balanced Outcome (79.8%), highlighting the benefit of combining\nexplicit reasoning with defender collaboration. Together, our results expose a\nnew class of risks in collaborative LLM deployments and provide actionable\ninsights for designing safeguards against compositional, context-driven privacy\nleakage."}
{"id": "2509.14285", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14285", "abs": "https://arxiv.org/abs/2509.14285", "authors": ["S M Asif Hossain", "Ruksat Khan Shayoni", "Mohd Ruhul Ameen", "Akif Islam", "M. F. Mridha", "Jungpil Shin"], "title": "A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks", "comment": null, "summary": "Prompt injection attacks represent a major vulnerability in Large Language\nModel (LLM) deployments, where malicious instructions embedded in user inputs\ncan override system prompts and induce unintended behaviors. This paper\npresents a novel multi-agent defense framework that employs specialized LLM\nagents in coordinated pipelines to detect and neutralize prompt injection\nattacks in real-time. We evaluate our approach using two distinct\narchitectures: a sequential chain-of-agents pipeline and a hierarchical\ncoordinator-based system. Our comprehensive evaluation on 55 unique prompt\ninjection attacks, grouped into 8 categories and totaling 400 attack instances\nacross two LLM platforms (ChatGLM and Llama2), demonstrates significant\nsecurity improvements. Without defense mechanisms, baseline Attack Success\nRates (ASR) reached 30% for ChatGLM and 20% for Llama2. Our multi-agent\npipeline achieved 100% mitigation, reducing ASR to 0% across all tested\nscenarios. The framework demonstrates robustness across multiple attack\ncategories including direct overrides, code execution attempts, data\nexfiltration, and obfuscation techniques, while maintaining system\nfunctionality for legitimate queries."}
{"id": "2509.14297", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14297", "abs": "https://arxiv.org/abs/2509.14297", "authors": ["Xuan Luo", "Yue Wang", "Zefeng He", "Geng Tu", "Jing Li", "Ruifeng Xu"], "title": "A Simple and Efficient Jailbreak Method Exploiting LLMs' Helpfulness", "comment": null, "summary": "Safety alignment aims to prevent Large Language Models (LLMs) from responding\nto harmful queries. To strengthen safety protections, jailbreak methods are\ndeveloped to simulate malicious attacks and uncover vulnerabilities. In this\npaper, we introduce HILL (Hiding Intention by Learning from LLMs), a novel\njailbreak approach that systematically transforms imperative harmful requests\ninto learning-style questions with only straightforward hypotheticality\nindicators. Further, we introduce two new metrics to thoroughly evaluate the\nutility of jailbreak methods. Experiments on the AdvBench dataset across a wide\nrange of models demonstrate HILL's strong effectiveness, generalizability, and\nharmfulness. It achieves top attack success rates on the majority of models and\nacross malicious categories while maintaining high efficiency with concise\nprompts. Results of various defense methods show the robustness of HILL, with\nmost defenses having mediocre effects or even increasing the attack success\nrates. Moreover, the assessment on our constructed safe prompts reveals\ninherent limitations of LLMs' safety mechanisms and flaws in defense methods.\nThis work exposes significant vulnerabilities of safety measures against\nlearning-style elicitation, highlighting a critical challenge of balancing\nhelpfulness and safety alignments."}
{"id": "2509.14335", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.14335", "abs": "https://arxiv.org/abs/2509.14335", "authors": ["Xinran Zheng", "Xingzhi Qian", "Yiling He", "Shuo Yang", "Lorenzo Cavallaro"], "title": "Beyond Classification: Evaluating LLMs for Fine-Grained Automatic Malware Behavior Auditing", "comment": null, "summary": "Automated malware classification has achieved strong detection performance.\nYet, malware behavior auditing seeks causal and verifiable explanations of\nmalicious activities -- essential not only to reveal what malware does but also\nto substantiate such claims with evidence. This task is challenging, as\nadversarial intent is often hidden within complex, framework-heavy\napplications, making manual auditing slow and costly. Large Language Models\n(LLMs) could help address this gap, but their auditing potential remains\nlargely unexplored due to three limitations: (1) scarce fine-grained\nannotations for fair assessment; (2) abundant benign code obscuring malicious\nsignals; and (3) unverifiable, hallucination-prone outputs undermining\nattribution credibility. To close this gap, we introduce MalEval, a\ncomprehensive framework for fine-grained Android malware auditing, designed to\nevaluate how effectively LLMs support auditing under real-world constraints.\nMalEval provides expert-verified reports and an updated sensitive API list to\nmitigate ground truth scarcity and reduce noise via static reachability\nanalysis. Function-level structural representations serve as intermediate\nattribution units for verifiable evaluation. Building on this, we define four\nanalyst-aligned tasks -- function prioritization, evidence attribution,\nbehavior synthesis, and sample discrimination -- together with domain-specific\nmetrics and a unified workload-oriented score. We evaluate seven widely used\nLLMs on a curated dataset of recent malware and misclassified benign apps,\noffering the first systematic assessment of their auditing capabilities.\nMalEval reveals both promising potential and critical limitations across audit\nstages, providing a reproducible benchmark and foundation for future research\non LLM-enhanced malware behavior auditing. MalEval is publicly available at\nhttps://github.com/ZhengXR930/MalEval.git"}
{"id": "2509.14558", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14558", "abs": "https://arxiv.org/abs/2509.14558", "authors": ["Guorui Chen", "Yifan Xia", "Xiaojun Jia", "Zhijiang Li", "Philip Torr", "Jindong Gu"], "title": "LLM Jailbreak Detection for (Almost) Free!", "comment": null, "summary": "Large language models (LLMs) enhance security through alignment when widely\nused, but remain susceptible to jailbreak attacks capable of producing\ninappropriate content. Jailbreak detection methods show promise in mitigating\njailbreak attacks through the assistance of other models or multiple model\ninferences. However, existing methods entail significant computational costs.\nIn this paper, we first present a finding that the difference in output\ndistributions between jailbreak and benign prompts can be employed for\ndetecting jailbreak prompts. Based on this finding, we propose a Free Jailbreak\nDetection (FJD) which prepends an affirmative instruction to the input and\nscales the logits by temperature to further distinguish between jailbreak and\nbenign prompts through the confidence of the first token. Furthermore, we\nenhance the detection performance of FJD through the integration of virtual\ninstruction learning. Extensive experiments on aligned LLMs show that our FJD\ncan effectively detect jailbreak prompts with almost no additional\ncomputational costs during LLM inference."}
{"id": "2509.14583", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.14583", "abs": "https://arxiv.org/abs/2509.14583", "authors": ["Johnny So", "Michael Ferdman", "Nick Nikiforakis"], "title": "What Gets Measured Gets Managed: Mitigating Supply Chain Attacks with a Link Integrity Management System", "comment": "Extended version of the paper \"What Gets Measured Gets Managed:\n  Mitigating Supply Chain Attacks with a Link Integrity Management System\" that\n  will be published in ACM CCS 2025", "summary": "The web continues to grow, but dependency-monitoring tools and standards for\nresource integrity lag behind. Currently, there exists no robust method to\nverify the integrity of web resources, much less in a generalizable yet\nperformant manner, and supply chains remain one of the most targeted parts of\nthe attack surface of web applications.\n  In this paper, we present the design of LiMS, a transparent system to\nbootstrap link integrity guarantees in web browsing sessions with minimal\noverhead. At its core, LiMS uses a set of customizable integrity policies to\ndeclare the (un)expected properties of resources, verifies these policies, and\nenforces them for website visitors. We discuss how basic integrity policies can\nserve as building blocks for a comprehensive set of integrity policies, while\nproviding guarantees that would be sufficient to defend against recent supply\nchain attacks detailed by security industry reports. Finally, we evaluate our\nopen-sourced prototype by simulating deployments on a representative sample of\n450 domains that are diverse in ranking and category. We find that our proposal\noffers the ability to bootstrap marked security improvements with an overall\noverhead of hundreds of milliseconds on initial page loads, and negligible\noverhead on reloads, regardless of network speeds. In addition, from examining\narchived data for the sample sites, we find that several of the proposed policy\nbuilding blocks suit their dependency usage patterns, and would incur minimal\nadministrative overhead."}
{"id": "2509.14589", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14589", "abs": "https://arxiv.org/abs/2509.14589", "authors": ["Taesoo Kim", "HyungSeok Han", "Soyeon Park", "Dae R. Jeong", "Dohyeok Kim", "Dongkwan Kim", "Eunsoo Kim", "Jiho Kim", "Joshua Wang", "Kangsu Kim", "Sangwoo Ji", "Woosun Song", "Hanqing Zhao", "Andrew Chin", "Gyejin Lee", "Kevin Stevens", "Mansour Alharthi", "Yizhuo Zhai", "Cen Zhang", "Joonun Jang", "Yeongjin Jang", "Ammar Askar", "Dongju Kim", "Fabian Fleischer", "Jeongin Cho", "Junsik Kim", "Kyungjoon Ko", "Insu Yun", "Sangdon Park", "Dowoo Baik", "Haein Lee", "Hyeon Heo", "Minjae Gwon", "Minjae Lee", "Minwoo Baek", "Seunggi Min", "Wonyoung Kim", "Yonghwi Jin", "Younggi Park", "Yunjae Choi", "Jinho Jung", "Gwanhyun Lee", "Junyoung Jang", "Kyuheon Kim", "Yeonghyeon Cha", "Youngjoon Kim"], "title": "ATLANTIS: AI-driven Threat Localization, Analysis, and Triage Intelligence System", "comment": "Version 1.0 (September 17, 2025). Technical Report. Team Atlanta --\n  1st place in DARPA AIxCC Final Competition. Project page:\n  https://team-atlanta.github.io/", "summary": "We present ATLANTIS, the cyber reasoning system developed by Team Atlanta\nthat won 1st place in the Final Competition of DARPA's AI Cyber Challenge\n(AIxCC) at DEF CON 33 (August 2025). AIxCC (2023-2025) challenged teams to\nbuild autonomous cyber reasoning systems capable of discovering and patching\nvulnerabilities at the speed and scale of modern software. ATLANTIS integrates\nlarge language models (LLMs) with program analysis -- combining symbolic\nexecution, directed fuzzing, and static analysis -- to address limitations in\nautomated vulnerability discovery and program repair. Developed by researchers\nat Georgia Institute of Technology, Samsung Research, KAIST, and POSTECH, the\nsystem addresses core challenges: scaling across diverse codebases from C to\nJava, achieving high precision while maintaining broad coverage, and producing\nsemantically correct patches that preserve intended behavior. We detail the\ndesign philosophy, architectural decisions, and implementation strategies\nbehind ATLANTIS, share lessons learned from pushing the boundaries of automated\nsecurity when program analysis meets modern AI, and release artifacts to\nsupport reproducibility and future research."}
{"id": "2509.14604", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.14604", "abs": "https://arxiv.org/abs/2509.14604", "authors": ["Ramazan Yener", "Muhammad Hassan", "Masooda Bashir"], "title": "Threats and Security Strategies for IoMT Infusion Pumps", "comment": "9 pages. Published as a book chapter in Human Factors in\n  Cybersecurity (AHFE 2025)", "summary": "The integration of the Internet of Medical Things (IoMT) into healthcare\nsystems has transformed patient care by enabling real-time monitoring, enhanced\ndiagnostics, and enhanced operational efficiency. However, this increased\nconnectivity has also expanded the attack surface for cybercriminals, raising\nsignificant cybersecurity and privacy concerns. This study focuses on the\ncybersecurity vulnerabilities of IoMT infusion pumps, which are critical\ndevices in modern healthcare. Through a targeted literature review of the past\nfive years, we analyzed seven current studies from a pool of 132 papers to\nidentify security vulnerabilities. Our findings indicate that infusion pumps\nface vulnerabilities such as device-level flaws, authentication and access\ncontrol issues, network and communication weaknesses, data security and privacy\nrisks, and operational or organizational challenges that can expose them to\nlateral attacks within healthcare networks. Our analysis synthesizes findings\nfrom seven recent studies to clarify how and why infusion pumps remain\nvulnerable in each of these areas. By categorizing the security gaps, we\nhighlight critical risk patterns and their implications. This work underscores\nthe scope of the issue and provides a structured understanding that is valuable\nfor healthcare IT professionals and device manufacturers. Ultimately, the\nfindings can inform the development of targeted, proactive security strategies\nto better safeguard infusion pumps and protect patient well-being."}
{"id": "2509.14608", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14608", "abs": "https://arxiv.org/abs/2509.14608", "authors": ["Shashank Shreedhar Bhatt", "Tanmay Rajore", "Khushboo Aggarwal", "Ganesh Ananthanarayanan", "Ranveer Chandra", "Nishanth Chandran", "Suyash Choudhury", "Divya Gupta", "Emre Kiciman", "Sumit Kumar Pandey", "Srinath Setty", "Rahul Sharma", "Teijia Zhao"], "title": "Enterprise AI Must Enforce Participant-Aware Access Control", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in enterprise settings\nwhere they interact with multiple users and are trained or fine-tuned on\nsensitive internal data. While fine-tuning enhances performance by\ninternalizing domain knowledge, it also introduces a critical security risk:\nleakage of confidential training data to unauthorized users. These risks are\nexacerbated when LLMs are combined with Retrieval-Augmented Generation (RAG)\npipelines that dynamically fetch contextual documents at inference time.\n  We demonstrate data exfiltration attacks on AI assistants where adversaries\ncan exploit current fine-tuning and RAG architectures to leak sensitive\ninformation by leveraging the lack of access control enforcement. We show that\nexisting defenses, including prompt sanitization, output filtering, system\nisolation, and training-level privacy mechanisms, are fundamentally\nprobabilistic and fail to offer robust protection against such attacks.\n  We take the position that only a deterministic and rigorous enforcement of\nfine-grained access control during both fine-tuning and RAG-based inference can\nreliably prevent the leakage of sensitive data to unauthorized recipients.\n  We introduce a framework centered on the principle that any content used in\ntraining, retrieval, or generation by an LLM is explicitly authorized for\n\\emph{all users involved in the interaction}. Our approach offers a simple yet\npowerful paradigm shift for building secure multi-user LLM systems that are\ngrounded in classical access control but adapted to the unique challenges of\nmodern AI workflows. Our solution has been deployed in Microsoft Copilot\nTuning, a product offering that enables organizations to fine-tune models using\ntheir own enterprise-specific data."}
{"id": "2509.14622", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14622", "abs": "https://arxiv.org/abs/2509.14622", "authors": ["Yihao Guo", "Haocheng Bian", "Liutong Zhou", "Ze Wang", "Zhaoyi Zhang", "Francois Kawala", "Milan Dean", "Ian Fischer", "Yuantao Peng", "Noyan Tokgozoglu", "Ivan Barrientos", "Riyaaz Shaik", "Rachel Li", "Chandru Venkataraman", "Reza Shifteh Far", "Moses Pawar", "Venkat Sundaranatha", "Michael Xu", "Frank Chu"], "title": "Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection", "comment": null, "summary": "With the deployment of Large Language Models (LLMs) in interactive\napplications, online malicious intent detection has become increasingly\ncritical. However, existing approaches fall short of handling diverse and\ncomplex user queries in real time. To address these challenges, we introduce\nADRAG (Adversarial Distilled Retrieval-Augmented Guard), a two-stage framework\nfor robust and efficient online malicious intent detection. In the training\nstage, a high-capacity teacher model is trained on adversarially perturbed,\nretrieval-augmented inputs to learn robust decision boundaries over diverse and\ncomplex user queries. In the inference stage, a distillation scheduler\ntransfers the teacher's knowledge into a compact student model, with a\ncontinually updated knowledge base collected online. At deployment, the compact\nstudent model leverages top-K similar safety exemplars retrieved from the\nonline-updated knowledge base to enable both online and real-time malicious\nquery detection. Evaluations across ten safety benchmarks demonstrate that\nADRAG, with a 149M-parameter model, achieves 98.5% of WildGuard-7B's\nperformance, surpasses GPT-4 by 3.3% and Llama-Guard-3-8B by 9.5% on\nout-of-distribution detection, while simultaneously delivering up to 5.6x lower\nlatency at 300 queries per second (QPS) in real-time applications."}
{"id": "2509.14657", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14657", "abs": "https://arxiv.org/abs/2509.14657", "authors": ["Sergio Benlloch-Lopez", "Miquel Viel-Vazquez", "Javier Naranjo-Alcazar", "Jordi Grau-Haro", "Pedro Zuccarello"], "title": "Threat Modeling for Enhancing Security of IoT Audio Classification Devices under a Secure Protocols Framework", "comment": "Accepted at Computing Conference 2026, London, UK", "summary": "The rapid proliferation of IoT nodes equipped with microphones and capable of\nperforming on-device audio classification exposes highly sensitive data while\noperating under tight resource constraints. To protect against this, we present\na defence-in-depth architecture comprising a security protocol that treats the\nedge device, cellular network and cloud backend as three separate trust\ndomains, linked by TPM-based remote attestation and mutually authenticated TLS\n1.3. A STRIDE-driven threat model and attack-tree analysis guide the design. At\nstartup, each boot stage is measured into TPM PCRs. The node can only decrypt\nits LUKS-sealed partitions after the cloud has verified a TPM quote and\nreleased a one-time unlock key. This ensures that rogue or tampered devices\nremain inert. Data in transit is protected by TLS 1.3 and hybridised with Kyber\nand Dilithium to provide post-quantum resilience. Meanwhile, end-to-end\nencryption and integrity hashes safeguard extracted audio features. Signed,\nrollback-protected AI models and tamper-responsive sensors harden firmware and\nhardware. Data at rest follows a 3-2-1 strategy comprising a solid-state drive\nsealed with LUKS, an offline cold archive encrypted with a hybrid post-quantum\ncipher and an encrypted cloud replica. Finally, we set out a plan for\nevaluating the physical and logical security of the proposed protocol."}
{"id": "2509.14706", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.14706", "abs": "https://arxiv.org/abs/2509.14706", "authors": ["Yonghao Ni", "Zhongwen Li", "Xiaoqi Li"], "title": "Security Analysis of Web Applications Based on Gruyere", "comment": null, "summary": "With the rapid development of Internet technologies, web systems have become\nessential infrastructures for modern information exchange and business\noperations. However, alongside their expansion, numerous security\nvulnerabilities have emerged, making web security a critical research focus\nwithin the broader field of cybersecurity. These issues are closely related to\ndata protection, privacy preservation, and business continuity, and systematic\nresearch on web security is crucial for mitigating malicious attacks and\nenhancing the reliability and robustness of network systems. This paper first\nreviews the OWASP Top 10, summarizing the types, causes, and impacts of common\nweb vulnerabilities, and illustrates their exploitation mechanisms through\nrepresentative cases. Building upon this, the Gruyere platform is adopted as an\nexperimental subject for analyzing known vulnerabilities. The study presents\ndetailed reproduction steps for specific vulnerabilities, proposes\ncomprehensive remediation strategies, and further compares Gruyere's\nvulnerabilities with contemporary real-world cases. The findings suggest that,\nalthough Gruyere's vulnerabilities are relatively outdated, their underlying\nprinciples remain highly relevant for explaining a wide range of modern\nsecurity flaws. Overall, this research demonstrates that web system security\nanalysis based on Gruyere not only deepens the understanding of vulnerability\nmechanisms but also provides practical support for technological innovation and\nsecurity defense."}
{"id": "2509.14754", "categories": ["cs.CR", "G.2.0"], "pdf": "https://arxiv.org/pdf/2509.14754", "abs": "https://arxiv.org/abs/2509.14754", "authors": ["Minzhong Luo", "Yudong Sun", "Yin Long"], "title": "Variables Ordering Optimization in Boolean Characteristic Set Method Using Simulated Annealing and Machine Learning-based Time Prediction", "comment": null, "summary": "Solving systems of Boolean equations is a fundamental task in symbolic\ncomputation and algebraic cryptanalysis, with wide-ranging applications in\ncryptography, coding theory, and formal verification. Among existing\napproaches, the Boolean Characteristic Set (BCS) method[1] has emerged as one\nof the most efficient algorithms for tackling such problems. However, its\nperformance is highly sensitive to the ordering of variables, with solving\ntimes varying drastically under different orderings for fixed variable counts n\nand equations size m. To address this challenge, this paper introduces a novel\noptimization framework that synergistically integrates machine learning\n(ML)-based time prediction with simulated annealing (SA) to efficiently\nidentify high-performance variables orderings. Weconstruct a dataset comprising\nvariable frequency spectrum X and corresponding BCS solving time t for\nbenchmark systems(e.g., n = m = 28). Utilizing this data, we train an accurate\nML predictor ft(X) to estimate solving time for any given variables ordering.\nFor each target system, ft serves as the cost function within an SA algorithm,\nenabling rapid discovery of low-latency orderings that significantly expedite\nsubsequent BCS execution. Extensive experiments demonstrate that our method\nsubstantially outperforms the standard BCS algorithm[1], Gr\\\"obner basis method\n[2] and SAT solver[3], particularly for larger-scale systems(e.g., n = 32).\nFurthermore, we derive probabilistic time complexity bounds for the overall\nalgorithm using stochastic process theory, establishing a quantitative\nrelationship between predictor accuracy and expected solving complexity. This\nwork provides both a practical acceleration tool for algebraic cryptanalysis\nand a theoretical foundation for ML-enhanced combinatorial optimization in\nsymbolic computation."}
{"id": "2509.14987", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14987", "abs": "https://arxiv.org/abs/2509.14987", "authors": ["Md Talha Mohsin"], "title": "Blockchain-Enabled Explainable AI for Trusted Healthcare Systems", "comment": "6 Pages, 4 Figures", "summary": "This paper introduces a Blockchain-Integrated Explainable AI Framework (BXHF)\nfor healthcare systems to tackle two essential challenges confronting health\ninformation networks: safe data exchange and comprehensible AI-driven clinical\ndecision-making. Our architecture incorporates blockchain, ensuring patient\nrecords are immutable, auditable, and tamper-proof, alongside Explainable AI\n(XAI) methodologies that yield transparent and clinically relevant model\npredictions. By incorporating security assurances and interpretability\nrequirements into a unified optimization pipeline, BXHF ensures both data-level\ntrust (by verified and encrypted record sharing) and decision-level trust (with\nauditable and clinically aligned explanations). Its hybrid edge-cloud\narchitecture allows for federated computation across different institutions,\nenabling collaborative analytics while protecting patient privacy. We\ndemonstrate the framework's applicability through use cases such as\ncross-border clinical research networks, uncommon illness detection and\nhigh-risk intervention decision support. By ensuring transparency,\nauditability, and regulatory compliance, BXHF improves the credibility, uptake,\nand effectiveness of AI in healthcare, laying the groundwork for safer and more\nreliable clinical decision-making."}
{"id": "2509.15170", "categories": ["cs.CR", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.15170", "abs": "https://arxiv.org/abs/2509.15170", "authors": ["Aarushi Mahajan", "Wayne Burleson"], "title": "Watermarking and Anomaly Detection in Machine Learning Models for LORA RF Fingerprinting", "comment": "IEEE International Conference on Acoustics, Speech, and Signal\n  Processing (ICASSP)", "summary": "Radio frequency fingerprint identification (RFFI) distinguishes wireless\ndevices by the small variations in their analog circuits, avoiding heavy\ncryptographic authentication. While deep learning on spectrograms improves\naccuracy, models remain vulnerable to copying, tampering, and evasion. We\npresent a stronger RFFI system combining watermarking for ownership proof and\nanomaly detection for spotting suspicious inputs. Using a ResNet-34 on log-Mel\nspectrograms, we embed three watermarks: a simple trigger, an adversarially\ntrained trigger robust to noise and filtering, and a hidden gradient/weight\nsignature. A convolutional Variational Autoencoders (VAE) with Kullback-Leibler\n(KL) warm-up and free-bits flags off-distribution queries. On the LoRa dataset,\nour system achieves 94.6% accuracy, 98% watermark success, and 0.94 AUROC,\noffering verifiable, tamper-resistant authentication."}
{"id": "2509.15202", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.15202", "abs": "https://arxiv.org/abs/2509.15202", "authors": ["Yuanbo Xie", "Yingjie Zhang", "Tianyun Liu", "Duohe Ma", "Tingwen Liu"], "title": "Beyond Surface Alignment: Rebuilding LLMs Safety Mechanism via Probabilistically Ablating Refusal Direction", "comment": "Accepted by EMNLP2025 Finding", "summary": "Jailbreak attacks pose persistent threats to large language models (LLMs).\nCurrent safety alignment methods have attempted to address these issues, but\nthey experience two significant limitations: insufficient safety alignment\ndepth and unrobust internal defense mechanisms. These limitations make them\nvulnerable to adversarial attacks such as prefilling and refusal direction\nmanipulation. We introduce DeepRefusal, a robust safety alignment framework\nthat overcomes these issues. DeepRefusal forces the model to dynamically\nrebuild its refusal mechanisms from jailbreak states. This is achieved by\nprobabilistically ablating the refusal direction across layers and token depths\nduring fine-tuning. Our method not only defends against prefilling and refusal\ndirection attacks but also demonstrates strong resilience against other unseen\njailbreak strategies. Extensive evaluations on four open-source LLM families\nand six representative attacks show that DeepRefusal reduces attack success\nrates by approximately 95%, while maintaining model capabilities with minimal\nperformance degradation."}
{"id": "2509.15213", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.15213", "abs": "https://arxiv.org/abs/2509.15213", "authors": ["Yicheng Zhang", "Zijian Huang", "Sophie Chen", "Erfan Shayegani", "Jiasi Chen", "Nael Abu-Ghazaleh"], "title": "Evil Vizier: Vulnerabilities of LLM-Integrated XR Systems", "comment": null, "summary": "Extended reality (XR) applications increasingly integrate Large Language\nModels (LLMs) to enhance user experience, scene understanding, and even\ngenerate executable XR content, and are often called \"AI glasses\". Despite\nthese potential benefits, the integrated XR-LLM pipeline makes XR applications\nvulnerable to new forms of attacks. In this paper, we analyze LLM-Integated XR\nsystems in the literature and in practice and categorize them along different\ndimensions from a systems perspective. Building on this categorization, we\nidentify a common threat model and demonstrate a series of proof-of-concept\nattacks on multiple XR platforms that employ various LLM models (Meta Quest 3,\nMeta Ray-Ban, Android, and Microsoft HoloLens 2 running Llama and GPT models).\nAlthough these platforms each implement LLM integration differently, they share\nvulnerabilities where an attacker can modify the public context surrounding a\nlegitimate LLM query, resulting in erroneous visual or auditory feedback to\nusers, thus compromising their safety or privacy, sowing confusion, or other\nharmful effects. To defend against these threats, we discuss mitigation\nstrategies and best practices for developers, including an initial defense\nprototype, and call on the community to develop new protection mechanisms to\nmitigate these risks."}
