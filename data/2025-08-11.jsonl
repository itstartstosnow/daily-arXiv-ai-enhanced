{"id": "2508.05655", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.05655", "abs": "https://arxiv.org/abs/2508.05655", "authors": ["Guang Yang", "Peter Trinh", "Alma Nkemla", "Amuru Serikyaku", "Edward Tatchim", "Osman Sharaf"], "title": "Blockchain-Based Decentralized Domain Name System", "comment": null, "summary": "The current Domain Name System (DNS) infrastructure faces critical\nvulnerabilities including poisoning attacks, censorship mechanisms, and\ncentralized points of failure that compromise internet freedom and security.\nRecent incidents such as DNS poisoning attacks on ISP customers highlight the\nurgent need for resilient alternatives. This paper presents a novel\nblockchain-based Decentralized Domain Name System (DDNS). We designed a\nspecialized Proof-of-Work blockchain to maximize support for DNS-related\nprotocols and achieve node decentralization. The system integrates our\nblockchain with IPFS for distributed storage, implements cryptographic\nprimitives for end-to-end trust signatures, and achieves Never Trust, Always\nVerify zero-trust verification. Our implementation achieves 15-second domain\nrecord propagation times, supports 20 standard DNS record types, and provides\nperpetual free .ddns domains. The system has been deployed across distributed\ninfrastructure in San Jose, Los Angeles, and Orange County, demonstrating\npractical scalability and resistance to traditional DNS manipulation\ntechniques. Performance evaluation shows the system can handle up to Max Theor.\nTPS 1,111.1 tx/s (minimal transactions) and Max Theor. TPS 266.7 tx/s (regular\ntransactions) for domain operations while maintaining sub-second query\nresolution through intelligent caching mechanisms."}
{"id": "2508.05658", "categories": ["cs.CR", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.05658", "abs": "https://arxiv.org/abs/2508.05658", "authors": ["Song Yan", "Hui Wei", "Jinlong Fei", "Guoliang Yang", "Zhengyu Zhao", "Zheng Wamg"], "title": "Universally Unfiltered and Unseen:Input-Agnostic Multimodal Jailbreaks against Text-to-Image Model Safeguards", "comment": "ACM MM 2025", "summary": "Various (text) prompt filters and (image) safety checkers have been\nimplemented to mitigate the misuse of Text-to-Image (T2I) models in creating\nNot-Safe-For-Work (NSFW) content.In order to expose potential security\nvulnerabilities of such safeguards, multimodal jailbreaks have been\nstudied.However, existing jailbreaks are limited to prompt-specific and\nimage-specific perturbations, which suffer from poor scalability and\ntime-consuming optimization.To address these limitations, we propose\nUniversally Unfiltered and Unseen (U3)-Attack, a multimodal jailbreak attack\nmethod against T2I safeguards.Specifically, U3-Attack optimizes an adversarial\npatch on the image background to universally bypass safety checkers and\noptimizes a safe paraphrase set from a sensitive word to universally bypass\nprompt filters while eliminating redundant computations.Extensive experimental\nresults demonstrate the superiority of our U3-Attack on both open-source and\ncommercial T2I models.For example, on the commercial Runway-inpainting model\nwith both prompt filter and safety checker, our U3-Attack achieves $~4\\times$\nhigher success rates than the state-of-the-art multimodal jailbreak attack,\nMMA-Diffusion.Content Warning: This paper includes examples of NSFW content."}
{"id": "2508.05670", "categories": ["cs.CR", "cs.AI", "cs.CY", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.05670", "abs": "https://arxiv.org/abs/2508.05670", "authors": ["Daniele Proverbio", "Alessio Buscemi", "Alessandro Di Stefano", "The Anh Han", "German Castignani", "Pietro Li√≤"], "title": "Can LLMs effectively provide game-theoretic-based scenarios for cybersecurity?", "comment": null, "summary": "Game theory has long served as a foundational tool in cybersecurity to test,\npredict, and design strategic interactions between attackers and defenders. The\nrecent advent of Large Language Models (LLMs) offers new tools and challenges\nfor the security of computer systems; In this work, we investigate whether\nclassical game-theoretic frameworks can effectively capture the behaviours of\nLLM-driven actors and bots. Using a reproducible framework for game-theoretic\nLLM agents, we investigate two canonical scenarios -- the one-shot zero-sum\ngame and the dynamic Prisoner's Dilemma -- and we test whether LLMs converge to\nexpected outcomes or exhibit deviations due to embedded biases. Our experiments\ninvolve four state-of-the-art LLMs and span five natural languages, English,\nFrench, Arabic, Vietnamese, and Mandarin Chinese, to assess linguistic\nsensitivity. For both games, we observe that the final payoffs are influenced\nby agents characteristics such as personality traits or knowledge of repeated\nrounds. Moreover, we uncover an unexpected sensitivity of the final payoffs to\nthe choice of languages, which should warn against indiscriminate application\nof LLMs in cybersecurity applications and call for in-depth studies, as LLMs\nmay behave differently when deployed in different countries. We also employ\nquantitative metrics to evaluate the internal consistency and cross-language\nstability of LLM agents, to help guide the selection of the most stable LLMs\nand optimising models for secure applications."}
{"id": "2508.05671", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05671", "abs": "https://arxiv.org/abs/2508.05671", "authors": ["Ko-Wei Chuang", "Hen-Hsen Huang", "Tsai-Yen Li"], "title": "DINA: A Dual Defense Framework Against Internal Noise and External Attacks in Natural Language Processing", "comment": "7 pages", "summary": "As large language models (LLMs) and generative AI become increasingly\nintegrated into customer service and moderation applications, adversarial\nthreats emerge from both external manipulations and internal label corruption.\nIn this work, we identify and systematically address these dual adversarial\nthreats by introducing DINA (Dual Defense Against Internal Noise and\nAdversarial Attacks), a novel unified framework tailored specifically for NLP.\nOur approach adapts advanced noisy-label learning methods from computer vision\nand integrates them with adversarial training to simultaneously mitigate\ninternal label sabotage and external adversarial perturbations. Extensive\nexperiments conducted on a real-world dataset from an online gaming service\ndemonstrate that DINA significantly improves model robustness and accuracy\ncompared to baseline models. Our findings not only highlight the critical\nnecessity of dual-threat defenses but also offer practical strategies for\nsafeguarding NLP systems in realistic adversarial scenarios, underscoring\nbroader implications for fair and responsible AI deployment."}
{"id": "2508.05674", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05674", "abs": "https://arxiv.org/abs/2508.05674", "authors": ["Minghao Shao", "Nanda Rani", "Kimberly Milner", "Haoran Xi", "Meet Udeshi", "Saksham Aggarwal", "Venkata Sai Charan Putrevu", "Sandeep Kumar Shukla", "Prashanth Krishnamurthy", "Farshad Khorrami", "Ramesh Karri", "Muhammad Shafique"], "title": "Towards Effective Offensive Security LLM Agents: Hyperparameter Tuning, LLM as a Judge, and a Lightweight CTF Benchmark", "comment": null, "summary": "Recent advances in LLM agentic systems have improved the automation of\noffensive security tasks, particularly for Capture the Flag (CTF) challenges.\nWe systematically investigate the key factors that drive agent success and\nprovide a detailed recipe for building effective LLM-based offensive security\nagents. First, we present CTFJudge, a framework leveraging LLM as a judge to\nanalyze agent trajectories and provide granular evaluation across CTF solving\nsteps. Second, we propose a novel metric, CTF Competency Index (CCI) for\npartial correctness, revealing how closely agent solutions align with\nhuman-crafted gold standards. Third, we examine how LLM hyperparameters, namely\ntemperature, top-p, and maximum token length, influence agent performance and\nautomated cybersecurity task planning. For rapid evaluation, we present\nCTFTiny, a curated benchmark of 50 representative CTF challenges across binary\nexploitation, web, reverse engineering, forensics, and cryptography. Our\nfindings identify optimal multi-agent coordination settings and lay the\ngroundwork for future LLM agent research in cybersecurity. We make CTFTiny open\nsource to public https://github.com/NYU-LLM-CTF/CTFTiny along with CTFJudge on\nhttps://github.com/NYU-LLM-CTF/CTFJudge."}
{"id": "2508.05675", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05675", "abs": "https://arxiv.org/abs/2508.05675", "authors": ["Jing Wang", "Zheng Li", "Lei Li", "Fan He", "Liyu Lin", "Yao Lai", "Yan Li", "Xiaoyang Zeng", "Yufeng Guo"], "title": "Principle-Guided Verilog Optimization: IP-Safe Knowledge Transfer via Local-Cloud Collaboration", "comment": "Our code and dataset are available at\n  https://github.com/friyawang/VeriOptim", "summary": "Recent years have witnessed growing interest in adopting large language\nmodels (LLMs) for Register Transfer Level (RTL) code optimization. While\npowerful cloud-based LLMs offer superior optimization capabilities, they pose\nunacceptable intellectual property (IP) leakage risks when processing\nproprietary hardware designs. In this paper, we propose a new scenario where\nVerilog code must be optimized for specific attributes without leaking\nsensitive IP information. We introduce the first IP-preserving edge-cloud\ncollaborative framework that leverages the benefits of both paradigms. Our\napproach employs local small LLMs (e.g., Qwen-2.5-Coder-7B) to perform secure\ncomparative analysis between paired high-quality target designs and novice\ndraft codes, yielding general design principles that summarize key insights for\nimprovements. These principles are then used to query stronger cloud LLMs\n(e.g., Deepseek-V3) for targeted code improvement, ensuring that only\nabstracted and IP-safe guidance reaches external services. Our experimental\nresults demonstrate that the framework achieves significantly higher\noptimization success rates compared to baseline methods. For example, combining\nQwen-2.5-Coder-7B and Deepseek-V3 achieves a 66.67\\% optimization success rate\nfor power utilization, outperforming Deepseek-V3 alone (49.81\\%) and even\ncommercial models like GPT-4o (55.81\\%). Further investigation of local and\ncloud LLM combinations reveals that different model pairings exhibit varying\nstrengths for specific optimization objectives, with interesting trends\nemerging when varying the number of comparative code pairs. Our work\nestablishes a new paradigm for secure hardware design optimization that\nbalances performance gains with IP protection."}
{"id": "2508.05677", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05677", "abs": "https://arxiv.org/abs/2508.05677", "authors": ["Peizhuo Liu"], "title": "Adversarial Attacks on Reinforcement Learning-based Medical Questionnaire Systems: Input-level Perturbation Strategies and Medical Constraint Validation", "comment": "30 pages (21 pages main text, 3 pages references, 6 pages appendix),\n  4 figures", "summary": "RL-based medical questionnaire systems have shown great potential in medical\nscenarios. However, their safety and robustness remain unresolved. This study\nperforms a comprehensive evaluation on adversarial attack methods to identify\nand analyze their potential vulnerabilities. We formulate the diagnosis process\nas a Markov Decision Process (MDP), where the state is the patient responses\nand unasked questions, and the action is either to ask a question or to make a\ndiagnosis. We implemented six prevailing major attack methods, including the\nFast Gradient Signed Method (FGSM), Projected Gradient Descent (PGD), Carlini &\nWagner Attack (C&W) attack, Basic Iterative Method (BIM), DeepFool, and\nAutoAttack, with seven epsilon values each. To ensure the generated adversarial\nexamples remain clinically plausible, we developed a comprehensive medical\nvalidation framework consisting of 247 medical constraints, including\nphysiological bounds, symptom correlations, and conditional medical\nconstraints. We achieved a 97.6% success rate in generating clinically\nplausible adversarial samples. We performed our experiment on the National\nHealth Interview Survey (NHIS) dataset (https://www.cdc.gov/nchs/nhis/), which\nconsists of 182,630 samples, to predict the participant's 4-year mortality\nrate. We evaluated our attacks on the AdaptiveFS framework proposed in\narXiv:2004.00994. Our results show that adversarial attacks could significantly\nimpact the diagnostic accuracy, with attack success rates ranging from 33.08%\n(FGSM) to 64.70% (AutoAttack). Our work has demonstrated that even under strict\nmedical constraints on the input, such RL-based medical questionnaire systems\nstill show significant vulnerabilities."}
{"id": "2508.05681", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05681", "abs": "https://arxiv.org/abs/2508.05681", "authors": ["Yuhan Zhi", "Longtian Wang", "Xiaofei Xie", "Chao Shen", "Qiang Hu", "Xiaohong Guan"], "title": "Selection-Based Vulnerabilities: Clean-Label Backdoor Attacks in Active Learning", "comment": null, "summary": "Active learning(AL), which serves as the representative label-efficient\nlearning paradigm, has been widely applied in resource-constrained scenarios.\nThe achievement of AL is attributed to acquisition functions, which are\ndesigned for identifying the most important data to label. Despite this\nsuccess, one question remains unanswered: is AL safe? In this work, we\nintroduce ALA, a practical and the first framework to utilize the acquisition\nfunction as the poisoning attack surface to reveal the weakness of active\nlearning. Specifically, ALA optimizes imperceptibly poisoned inputs to exhibit\nhigh uncertainty scores, increasing their probability of being selected by\nacquisition functions. To evaluate ALA, we conduct extensive experiments across\nthree datasets, three acquisition functions, and two types of clean-label\nbackdoor triggers. Results show that our attack can achieve high success rates\n(up to 94%) even under low poisoning budgets (0.5%-1.0%) while preserving model\nutility and remaining undetectable to human annotators. Our findings remind\nactive learning users: acquisition functions can be easily exploited, and\nactive learning should be deployed with caution in trusted data scenarios."}
{"id": "2508.05684", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05684", "abs": "https://arxiv.org/abs/2508.05684", "authors": ["Junhao He", "Tianyu Liu", "Jingyuan Zhao", "Benjamin Turner"], "title": "MM-FusionNet: Context-Aware Dynamic Fusion for Multi-modal Fake News Detection with Large Vision-Language Models", "comment": null, "summary": "The proliferation of multi-modal fake news on social media poses a\nsignificant threat to public trust and social stability. Traditional detection\nmethods, primarily text-based, often fall short due to the deceptive interplay\nbetween misleading text and images. While Large Vision-Language Models (LVLMs)\noffer promising avenues for multi-modal understanding, effectively fusing\ndiverse modal information, especially when their importance is imbalanced or\ncontradictory, remains a critical challenge. This paper introduces\nMM-FusionNet, an innovative framework leveraging LVLMs for robust multi-modal\nfake news detection. Our core contribution is the Context-Aware Dynamic Fusion\nModule (CADFM), which employs bi-directional cross-modal attention and a novel\ndynamic modal gating network. This mechanism adaptively learns and assigns\nimportance weights to textual and visual features based on their contextual\nrelevance, enabling intelligent prioritization of information. Evaluated on the\nlarge-scale Multi-modal Fake News Dataset (LMFND) comprising 80,000 samples,\nMM-FusionNet achieves a state-of-the-art F1-score of 0.938, surpassing existing\nmulti-modal baselines by approximately 0.5% and significantly outperforming\nsingle-modal approaches. Further analysis demonstrates the model's dynamic\nweighting capabilities, its robustness to modality perturbations, and\nperformance remarkably close to human-level, underscoring its practical\nefficacy and interpretability for real-world fake news detection."}
{"id": "2508.05690", "categories": ["cs.CR", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05690", "abs": "https://arxiv.org/abs/2508.05690", "authors": ["Meital Shlezinger", "Shay Akirav", "Lei Zhou", "Liang Guo", "Avi Kessel", "Guoliang Li"], "title": "Leveraging large language models for SQL behavior-based database intrusion detection", "comment": null, "summary": "Database systems are extensively used to store critical data across various\ndomains. However, the frequency of abnormal database access behaviors, such as\ndatabase intrusion by internal and external attacks, continues to rise.\nInternal masqueraders often have greater organizational knowledge, making it\neasier to mimic employee behavior effectively. In contrast, external\nmasqueraders may behave differently due to their lack of familiarity with the\norganization. Current approaches lack the granularity needed to detect\nanomalies at the operational level, frequently misclassifying entire sequences\nof operations as anomalies, even though most operations are likely to represent\nnormal behavior. On the other hand, some anomalous behaviors often resemble\nnormal activities, making them difficult for existing detection methods to\nidentify. This paper introduces a two-tiered anomaly detection approach for\nStructured Query Language (SQL) using the Bidirectional Encoder Representations\nfrom Transformers (BERT) model, specifically DistilBERT, a more efficient,\npre-trained version. Our method combines both unsupervised and supervised\nmachine learning techniques to accurately identify anomalous activities while\nminimizing the need for data labeling. First, the unsupervised method uses\nensemble anomaly detectors that flag embedding vectors distant from learned\nnormal patterns of typical user behavior across the database (out-of-scope\nqueries). Second, the supervised method uses fine-tuned transformer-based\nmodels to detect internal attacks with high precision (in-scope queries), using\nrole-labeled classification, even on limited labeled SQL data. Our findings\nmake a significant contribution by providing an effective solution for\nsafeguarding critical database systems from sophisticated threats."}
{"id": "2508.05691", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.05691", "abs": "https://arxiv.org/abs/2508.05691", "authors": ["Kai Yao", "Marc Juarez"], "title": "AuthPrint: Fingerprinting Generative Models Against Malicious Model Providers", "comment": null, "summary": "Generative models are increasingly adopted in high-stakes domains, yet\ncurrent deployments offer no mechanisms to verify the origin of model outputs.\nWe address this gap by extending model fingerprinting techniques beyond the\ntraditional collaborative setting to one where the model provider may act\nadversarially. To our knowledge, this is the first work to evaluate\nfingerprinting for provenance attribution under such a threat model. The\nmethods rely on a trusted verifier that extracts secret fingerprints from the\nmodel's output space, unknown to the provider, and trains a model to predict\nand verify them. Our empirical evaluation shows that our methods achieve\nnear-zero FPR@95%TPR for instances of GAN and diffusion models, even when\ntested on small modifications to the original architecture and training data.\nMoreover, the methods remain robust against adversarial attacks that actively\nmodify the outputs to bypass detection. Source codes are available at\nhttps://github.com/PSMLab/authprint."}
{"id": "2508.05694", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05694", "abs": "https://arxiv.org/abs/2508.05694", "authors": ["Kaichuan Kong", "Dongjie Liu", "Xiaobo Jin", "Guanggang Geng", "Zhiying Li", "Jian Weng"], "title": "DMFI: Dual-Modality Fine-Tuning and Inference Framework for LLM-Based Insider Threat Detection", "comment": "Submitted to the 2025 IEEE International Conference on Data Mining\n  (ICDM)", "summary": "Insider threat detection (ITD) poses a persistent and high-impact challenge\nin cybersecurity due to the subtle, long-term, and context-dependent nature of\nmalicious insider behaviors. Traditional models often struggle to capture\nsemantic intent and complex behavior dynamics, while existing LLM-based\nsolutions face limitations in prompt adaptability and modality coverage. To\nbridge this gap, we propose DMFI, a dual-modality framework that integrates\nsemantic inference with behavior-aware fine-tuning. DMFI converts raw logs into\ntwo structured views: (1) a semantic view that processes content-rich artifacts\n(e.g., emails, https) using instruction-formatted prompts; and (2) a behavioral\nabstraction, constructed via a 4W-guided (When-Where-What-Which) transformation\nto encode contextual action sequences. Two LoRA-enhanced LLMs are fine-tuned\nindependently, and their outputs are fused via a lightweight MLP-based decision\nmodule. We further introduce DMFI-B, a discriminative adaptation strategy that\nseparates normal and abnormal behavior representations, improving robustness\nunder severe class imbalance. Experiments on CERT r4.2 and r5.2 datasets\ndemonstrate that DMFI outperforms state-of-the-art methods in detection\naccuracy. Our approach combines the semantic reasoning power of LLMs with\nstructured behavior modeling, offering a scalable and effective solution for\nreal-world insider threat detection. Our work demonstrates the effectiveness of\ncombining LLM reasoning with structured behavioral modeling, offering a\nscalable and deployable solution for modern insider threat detection."}
{"id": "2508.05695", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05695", "abs": "https://arxiv.org/abs/2508.05695", "authors": ["Kaichuan Kong", "Dongjie Liu", "Xiaobo Jin", "Zhiying Li", "Guanggang Geng", "Jian Weng"], "title": "MambaITD: An Efficient Cross-Modal Mamba Network for Insider Threat Detection", "comment": "Submitted to the 2025 IEEE International Conference on Data Mining\n  (ICDM)", "summary": "Enterprises are facing increasing risks of insider threats, while existing\ndetection methods are unable to effectively address these challenges due to\nreasons such as insufficient temporal dynamic feature modeling, computational\nefficiency and real-time bottlenecks and cross-modal information island\nproblem. This paper proposes a new insider threat detection framework MambaITD\nbased on the Mamba state space model and cross-modal adaptive fusion. First,\nthe multi-source log preprocessing module aligns heterogeneous data through\nbehavioral sequence encoding, interval smoothing, and statistical feature\nextraction. Second, the Mamba encoder models long-range dependencies in\nbehavioral and interval sequences, and combines the sequence and statistical\ninformation dynamically in combination with the gated feature fusion mechanism.\nFinally, we propose an adaptive threshold optimization method based on\nmaximizing inter-class variance, which dynamically adjusts the decision\nthreshold by analyzing the probability distribution, effectively identifies\nanomalies, and alleviates class imbalance and concept drift. Compared with\ntraditional methods, MambaITD shows significant advantages in modeling\nefficiency and feature fusion capabilities, outperforming Transformer-based\nmethods, and provides a more effective solution for insider threat detection."}
{"id": "2508.05696", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05696", "abs": "https://arxiv.org/abs/2508.05696", "authors": ["Kaichuan Kong", "Dongjie Liu", "Xiaobo Jin", "Zhiying Li", "Guanggang Geng"], "title": "Log2Sig: Frequency-Aware Insider Threat Detection via Multivariate Behavioral Signal Decomposition", "comment": "Submitted to the 2025 IEEE International Conference on Trust,\n  Security and Privacy in Computing and Communications (TrustCom)", "summary": "Insider threat detection presents a significant challenge due to the\ndeceptive nature of malicious behaviors, which often resemble legitimate user\noperations. However, existing approaches typically model system logs as flat\nevent sequences, thereby failing to capture the inherent frequency dynamics and\nmultiscale disturbance patterns embedded in user behavior. To address these\nlimitations, we propose Log2Sig, a robust anomaly detection framework that\ntransforms user logs into multivariate behavioral frequency signals,\nintroducing a novel representation of user behavior. Log2Sig employs\nMultivariate Variational Mode Decomposition (MVMD) to extract Intrinsic Mode\nFunctions (IMFs), which reveal behavioral fluctuations across multiple temporal\nscales. Based on this, the model further performs joint modeling of behavioral\nsequences and frequency-decomposed signals: the daily behavior sequences are\nencoded using a Mamba-based temporal encoder to capture long-term dependencies,\nwhile the corresponding frequency components are linearly projected to match\nthe encoder's output dimension. These dual-view representations are then fused\nto construct a comprehensive user behavior profile, which is fed into a\nmultilayer perceptron for precise anomaly detection. Experimental results on\nthe CERT r4.2 and r5.2 datasets demonstrate that Log2Sig significantly\noutperforms state-of-the-art baselines in both accuracy and F1 score."}
{"id": "2508.05707", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.05707", "abs": "https://arxiv.org/abs/2508.05707", "authors": ["Sasa Maric", "Rasil Baidar", "Robert Abbas", "Sam Reisenfeld"], "title": "System Security Framework for 5G Advanced /6G IoT Integrated Terrestrial Network-Non-Terrestrial Network (TN-NTN) with AI-Enabled Cloud Security", "comment": null, "summary": "The integration of Terrestrial Networks (TN) and Non-Terrestrial Networks\n(NTN), including 5G Advanced/6G and the Internet of Things (IoT) technologies,\nusing Low Earth Orbit (LEO) satellites, high-altitude platforms (HAPS), and\nUnmanned Aerial Vehicles (UAVs), is redefining the landscape of global\nconnectivity. This paper introduces a new system-level security framework for\n5G Advanced/6G IoT-integrated TN-NTN architectures with AI-native-enabled cloud\nsecurity. Due to the heterogeneity, scale, and distributed nature of these\nnetworks, new security challenges have emerged. Leveraging AI-native cloud\nplatforms offers powerful capabilities for real-time threat detection, security\nautomation, and intelligent policy enforcement. The NTN satellite access\nfunction enhances security for discontinuous coverage via satellite\nconnections. In addition, this paper explores the security risks associated\nwith integrated 5G Advanced/6G IoT TN-NTN systems, including full network\nsegmentation, network slicing, and the cloudification of the RAN and core. We\npresent a comprehensive AI-enabled cloud security framework and conclude with\nproposals for implementing AI-powered, satellite-based NTN within future 5G\nAdvanced/6G IoT networks. Our approach emphasizes zero-trust principles,\nfederated learning, secure orchestration, a layered security framework, and\nresilience against adversarial threats."}
{"id": "2508.05717", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.05717", "abs": "https://arxiv.org/abs/2508.05717", "authors": ["Marco Giberna", "Holger Voos", "Paulo Tavares", "Jo√£o Nunes", "Tobias Sorg", "Andrea Masini", "Jose Luis Sanchez-Lopez"], "title": "On Digital Twins in Defence: Overview and Applications", "comment": "27 pages, 10 figures, 2 tables", "summary": "Digital twin technology has gained increasing attention across various\nsectors due to its ability to create virtual replicas of physical systems,\nenabling real-time monitoring, optimization, and simulation. This paper\nexplores the integration of digital twins within defence applications, focusing\non key use cases ranging from system design and development, operational\nplanning and training, to mission execution and debriefing. By examining the\napplication of digital twin technologies across defense platforms, we highlight\ntheir key advantages such as enhanced operational performance, predictive\ncapabilities, and increased system uptime. Additionally, we introduce a novel\ncharacterization framework for digital twins that aims to standardize and unify\ntheir application across different defence domains to facilitate\ninteroperability. Thereafter, we discuss the main challenges, gaps and\nlimitations in implementing and adopting digital twins within defence\norganizations by analyzing a combination of scientific literature, current\nindustry practices, governmental strategies, and the findings from a\ncomprehensive survey of industrial stakeholders and ministries of defense.\nFinally, we outline future research directions and development opportunities,\nemphasizing the need for robust frameworks and interdisciplinary collaborations\nto fully realize the potential of digital twins in the defence sector."}
{"id": "2508.05865", "categories": ["cs.CR", "cs.SE", "C.2; D.2; D.4.1; D.4.4; D.4.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2508.05865", "abs": "https://arxiv.org/abs/2508.05865", "authors": ["Kiana Kiashemshaki", "Elvis Nnaemeka Chukwuani", "Mohammad Jalili Torkamani", "Negin Mahmoudi"], "title": "Secure and Scalable Blockchain Voting: A Comparative Framework and the Role of Large Language Models", "comment": "9 pages, 8 figures, 1 table", "summary": "Blockchain technology offers a promising foundation for modernizing E-Voting\nsystems by enhancing transparency, decentralization, and security. Yet,\nreal-world adoption remains limited due to persistent challenges such as\nscalability constraints, high computational demands, and complex privacy\nrequirements. This paper presents a comparative framework for analyzing\nblockchain-based E-Voting architectures, consensus mechanisms, and\ncryptographic protocols. We examine the limitations of prevalent models like\nProof of Work, Proof of Stake, and Delegated Proof of Stake, and propose\noptimization strategies that include hybrid consensus, lightweight\ncryptography, and decentralized identity management. Additionally, we explore\nthe novel role of Large Language Models (LLMs) in smart contract generation,\nanomaly detection, and user interaction. Our findings offer a foundation for\ndesigning secure, scalable, and intelligent blockchain-based E-Voting systems\nsuitable for national-scale deployment. This work lays the groundwork for\nbuilding an end-to-end blockchain E-Voting prototype enhanced by LLM-guided\nsmart contract generation and validation, supported by a systematic framework\nand simulation-based analysis."}
{"id": "2508.06059", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06059", "abs": "https://arxiv.org/abs/2508.06059", "authors": ["Haorui He", "Yupeng Li", "Bin Benjamin Zhu", "Dacheng Wen", "Reynold Cheng", "Francis C. M. Lau"], "title": "Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System", "comment": null, "summary": "State-of-the-art fact-checking systems combat misinformation at scale by\nemploying autonomous LLM-based agents to decompose complex claims into smaller\nsub-claims, verify each sub-claim individually, and aggregate the partial\nresults to produce verdicts with justifications (explanatory rationales for the\nverdicts). The security of these systems is crucial, as compromised\nfact-checkers, which tend to be easily underexplored, can amplify\nmisinformation. This work introduces Fact2Fiction, the first poisoning attack\nframework targeting such agentic fact-checking systems. Fact2Fiction mirrors\nthe decomposition strategy and exploits system-generated justifications to\ncraft tailored malicious evidences that compromise sub-claim verification.\nExtensive experiments demonstrate that Fact2Fiction achieves 8.9\\%--21.2\\%\nhigher attack success rates than state-of-the-art attacks across various\npoisoning budgets. Fact2Fiction exposes security weaknesses in current\nfact-checking systems and highlights the need for defensive countermeasures."}
{"id": "2508.06071", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.06071", "abs": "https://arxiv.org/abs/2508.06071", "authors": ["Liang Chen"], "title": "A Game-Theoretic Foundation for Bitcoin's Price: A Security-Utility Equilibrium", "comment": null, "summary": "This paper introduces a structural game-theoretic model to value\ndecentralized digital assets like Bitcoin. Instead of relying on speculative\nbeliefs, it frames the asset's price within a Rational-Expectations\nSecurity-Utility Nash Equilibrium (RESUNE). This equilibrium is a fixed point\nwhere the market-clearing price dictates the hash rate through a free-entry\nmining model, which in turn endogenously sets the network's security. The\nsecurity, defined as one minus the probability of a 51% attack, is determined\nvia a global games model of attacker coordination, providing a unique and\ncontinuous security function. We prove the existence of a RESUNE and offer\nconditions for its uniqueness and stability. The model predicts that the\nstabilizing direct effect of price on demand must outweigh the potentially\ndestabilizing feedback from price to security. The framework generates testable\npredictions, such as a protocol halving causing a contraction in both hash rate\nand price. A structural Vector Autoregression (VAR) model is proposed to test\nthis mechanism. The model decomposes Bitcoin's value into transactional\nutility, security, and speculative components and explains the observed\nunidirectional causality from price to hash rate."}
{"id": "2508.06073", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.06073", "abs": "https://arxiv.org/abs/2508.06073", "authors": ["Weiheng Wu", "Wei Qiao", "Teng Li", "Yebo Feng", "Zhuo Ma", "Jianfeng Ma", "Yang Liu"], "title": "ProvX: Generating Counterfactual-Driven Attack Explanations for Provenance-Based Detection", "comment": null, "summary": "Provenance graph-based intrusion detection systems are deployed on hosts to\ndefend against increasingly severe Advanced Persistent Threat. Using Graph\nNeural Networks to detect these threats has become a research focus and has\ndemonstrated exceptional performance. However, the widespread adoption of\nGNN-based security models is limited by their inherent black-box nature, as\nthey fail to provide security analysts with any verifiable explanations for\nmodel predictions or any evidence regarding the model's judgment in relation to\nreal-world attacks. To address this challenge, we propose ProvX, an effective\nexplanation framework for exlaining GNN-based security models on provenance\ngraphs. ProvX introduces counterfactual explanation logic, seeking the minimal\nstructural subset within a graph predicted as malicious that, when perturbed,\ncan subvert the model's original prediction. We innovatively transform the\ndiscrete search problem of finding this critical subgraph into a continuous\noptimization task guided by a dual objective of prediction flipping and\ndistance minimization. Furthermore, a Staged Solidification strategy is\nincorporated to enhance the precision and stability of the explanations. We\nconducted extensive evaluations of ProvX on authoritative datasets. The\nexperimental results demonstrate that ProvX can locate critical graph\nstructures that are highly relevant to real-world attacks and achieves an\naverage explanation necessity of 51.59\\%, with these metrics outperforming\ncurrent SOTA explainers. Furthermore, we explore and provide a preliminary\nvalidation of a closed-loop Detection-Explanation-Feedback enhancement\nframework, demonstrating through experiments that the explanation results from\nProvX can guide model optimization, effectively enhancing its robustness\nagainst adversarial attacks."}
{"id": "2508.06087", "categories": ["cs.CR", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.06087", "abs": "https://arxiv.org/abs/2508.06087", "authors": ["Zhihao Yao", "Yuxuan Gu", "Xiachong Feng", "Weitao Ma", "Bo Li", "Xiaocheng Feng"], "title": "Adaptive Backtracking for Privacy Protection in Large Language Models", "comment": null, "summary": "The preservation of privacy has emerged as a critical topic in the era of\nartificial intelligence. However, current work focuses on user-oriented\nprivacy, overlooking severe enterprise data leakage risks exacerbated by the\nRetrieval-Augmented Generation paradigm. To address this gap, our paper\nintroduces a novel objective: enterprise-oriented privacy concerns. Achieving\nthis objective requires overcoming two fundamental challenges: existing methods\nsuch as data sanitization severely degrade model performance, and the field\nlacks public datasets for evaluation. We address these challenges with several\nsolutions. (1) To prevent performance degradation, we propose ABack, a\ntraining-free mechanism that leverages a Hidden State Model to pinpoint the\norigin of a leakage intention and rewrite the output safely. (2) To solve the\nlack of datasets, we construct PriGenQA, a new benchmark for enterprise privacy\nscenarios in healthcare and finance. To ensure a rigorous evaluation, we move\nbeyond simple static attacks by developing a powerful adaptive attacker with\nGroup Relative Policy Optimization. Experiments show that against this superior\nadversary, ABack improves the overall privacy utility score by up to 15\\% over\nstrong baselines, avoiding the performance trade-offs of prior methods."}
{"id": "2508.06106", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.06106", "abs": "https://arxiv.org/abs/2508.06106", "authors": ["Luca Serena", "Gabriele D'Angelo", "Stefano Ferretti", "Moreno Marzolla"], "title": "Simulation in Cybersecurity: Understanding Techniques, Applications, and Goals", "comment": "Working paper", "summary": "Modeling and simulation are widely used in cybersecurity research to assess\ncyber threats, evaluate defense mechanisms, and analyze vulnerabilities.\nHowever, the diversity of application areas, the variety of cyberattacks\nscenarios, and the differing objectives of these simulations makes it difficult\nto identify methodological trends. Existing reviews often focus on specific\nmodeling techniques or application domains, making it challenging to analyze\nthe field as a whole. To address these limitations, we present a comprehensive\nreview of the current state of the art, classifying the selected papers based\non four dimensions: the application domain, the types of cyber threats\nrepresented, the simulation techniques employed, and the primary goals of the\nsimulation. The review discusses the strengths and limitations of different\napproaches, identifies which cyber threats are the most suited for\nsimulation-based investigations, and analyzes which modeling paradigms are most\nappropriate for specific cybersecurity challenges."}
{"id": "2508.06153", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.06153", "abs": "https://arxiv.org/abs/2508.06153", "authors": ["Zhengxian Wu", "Juan Wen", "Wanli Peng", "Haowei Chang", "Yinghan Zhou", "Yiming Xue"], "title": "SLIP: Soft Label Mechanism and Key-Extraction-Guided CoT-based Defense Against Instruction Backdoor in APIs", "comment": null, "summary": "With the development of customized large language model (LLM) agents, a new\nthreat of black-box backdoor attacks has emerged, where malicious instructions\nare injected into hidden system prompts. These attacks easily bypass existing\ndefenses that rely on white-box access, posing a serious security challenge. To\naddress this, we propose SLIP, a Soft Label mechanism and key-extraction-guided\nCoT-based defense against Instruction backdoors in APIs. SLIP is designed based\non two key insights. First, to counteract the model's oversensitivity to\ntriggers, we propose a Key-extraction-guided Chain-of-Thought (KCoT). Instead\nof only considering the single trigger or the input sentence, KCoT prompts the\nagent to extract task-relevant key phrases. Second, to guide the LLM toward\ncorrect answers, our proposed Soft Label Mechanism (SLM) prompts the agent to\nquantify the semantic correlation between key phrases and candidate answers.\nCrucially, to mitigate the influence of residual triggers or misleading content\nin phrases extracted by KCoT, which typically causes anomalous scores, SLM\nexcludes anomalous scores deviating significantly from the mean and\nsubsequently averages the remaining scores to derive a more reliable semantic\nrepresentation. Extensive experiments on classification and question-answer\n(QA) tasks demonstrate that SLIP is highly effective, reducing the average\nattack success rate (ASR) from 90.2% to 25.13% while maintaining high accuracy\non clean data and outperforming state-of-the-art defenses. Our code are\navailable in\nhttps://github.com/CAU-ISS-Lab/Backdoor-Attack-Defense-LLMs/tree/main/SLIP."}
{"id": "2508.06325", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06325", "abs": "https://arxiv.org/abs/2508.06325", "authors": ["Zelin Li", "Ruohan Zong", "Yifan Liu", "Ruichen Yao", "Yaokun Liu", "Yang Zhang", "Dong Wang"], "title": "Anti-Tamper Protection for Unauthorized Individual Image Generation", "comment": "22 pages ,22 figures, Paper has been accepted by ICCV'2025", "summary": "With the advancement of personalized image generation technologies, concerns\nabout forgery attacks that infringe on portrait rights and privacy are growing.\nTo address these concerns, protection perturbation algorithms have been\ndeveloped to disrupt forgery generation. However, the protection algorithms\nwould become ineffective when forgery attackers apply purification techniques\nto bypass the protection. To address this issue, we present a novel approach,\nAnti-Tamper Perturbation (ATP). ATP introduces a tamper-proof mechanism within\nthe perturbation. It consists of protection and authorization perturbations,\nwhere the protection perturbation defends against forgery attacks, while the\nauthorization perturbation detects purification-based tampering. Both\nprotection and authorization perturbations are applied in the frequency domain\nunder the guidance of a mask, ensuring that the protection perturbation does\nnot disrupt the authorization perturbation. This design also enables the\nauthorization perturbation to be distributed across all image pixels,\npreserving its sensitivity to purification-based tampering. ATP demonstrates\nits effectiveness in defending forgery attacks across various attack settings\nthrough extensive experiments, providing a robust solution for protecting\nindividuals' portrait rights and privacy. Our code is available at:\nhttps://github.com/Seeyn/Anti-Tamper-Perturbation ."}
{"id": "2508.06394", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.06394", "abs": "https://arxiv.org/abs/2508.06394", "authors": ["Dario Pasquini", "Evgenios M. Kornaropoulos", "Giuseppe Ateniese", "Omer Akgul", "Athanasios Theocharis", "Petros Efstathopoulos"], "title": "When AIOps Become \"AI Oops\": Subverting LLM-driven IT Operations via Telemetry Manipulation", "comment": "v0.1", "summary": "AI for IT Operations (AIOps) is transforming how organizations manage complex\nsoftware systems by automating anomaly detection, incident diagnosis, and\nremediation. Modern AIOps solutions increasingly rely on autonomous LLM-based\nagents to interpret telemetry data and take corrective actions with minimal\nhuman intervention, promising faster response times and operational cost\nsavings.\n  In this work, we perform the first security analysis of AIOps solutions,\nshowing that, once again, AI-driven automation comes with a profound security\ncost. We demonstrate that adversaries can manipulate system telemetry to\nmislead AIOps agents into taking actions that compromise the integrity of the\ninfrastructure they manage. We introduce techniques to reliably inject\ntelemetry data using error-inducing requests that influence agent behavior\nthrough a form of adversarial reward-hacking; plausible but incorrect system\nerror interpretations that steer the agent's decision-making. Our attack\nmethodology, AIOpsDoom, is fully automated--combining reconnaissance, fuzzing,\nand LLM-driven adversarial input generation--and operates without any prior\nknowledge of the target system.\n  To counter this threat, we propose AIOpsShield, a defense mechanism that\nsanitizes telemetry data by exploiting its structured nature and the minimal\nrole of user-generated content. Our experiments show that AIOpsShield reliably\nblocks telemetry-based attacks without affecting normal agent performance.\n  Ultimately, this work exposes AIOps as an emerging attack vector for system\ncompromise and underscores the urgent need for security-aware AIOps design."}
{"id": "2508.06457", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06457", "abs": "https://arxiv.org/abs/2508.06457", "authors": ["Sanket Badhe"], "title": "ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls", "comment": "Accepted at CAMLIS 25: Conference on Applied Machine Learning for\n  Information Security. 10 pages, 3 figures", "summary": "Large Language Models (LLMs) have demonstrated impressive fluency and\nreasoning capabilities, but their potential for misuse has raised growing\nconcern. In this paper, we present ScamAgent, an autonomous multi-turn agent\nbuilt on top of LLMs, capable of generating highly realistic scam call scripts\nthat simulate real-world fraud scenarios. Unlike prior work focused on\nsingle-shot prompt misuse, ScamAgent maintains dialogue memory, adapts\ndynamically to simulated user responses, and employs deceptive persuasion\nstrategies across conversational turns. We show that current LLM safety\nguardrails, including refusal mechanisms and content filters, are ineffective\nagainst such agent-based threats. Even models with strong prompt-level\nsafeguards can be bypassed when prompts are decomposed, disguised, or delivered\nincrementally within an agent framework. We further demonstrate the\ntransformation of scam scripts into lifelike voice calls using modern\ntext-to-speech systems, completing a fully automated scam pipeline. Our\nfindings highlight an urgent need for multi-turn safety auditing, agent-level\ncontrol frameworks, and new methods to detect and disrupt conversational\ndeception powered by generative AI."}
{"id": "2508.06489", "categories": ["cs.CR", "cs.DC", "cs.DM", "cs.IT", "math.IT", "math.PR"], "pdf": "https://arxiv.org/pdf/2508.06489", "abs": "https://arxiv.org/abs/2508.06489", "authors": ["Mustafa Doger", "Sennur Ulukus"], "title": "Voting-Based Semi-Parallel Proof-of-Work Protocol", "comment": null, "summary": "Parallel Proof-of-Work (PoW) protocols are suggested to improve the safety\nguarantees, transaction throughput and confirmation latencies of Nakamoto\nconsensus. In this work, we first consider the existing parallel PoW protocols\nand develop hard-coded incentive attack structures. Our theoretical results and\nsimulations show that the existing parallel PoW protocols are more vulnerable\nto incentive attacks than the Nakamoto consensus, e.g., attacks have smaller\nprofitability threshold and they result in higher relative rewards. Next, we\nintroduce a voting-based semi-parallel PoW protocol that outperforms both\nNakamoto consensus and the existing parallel PoW protocols from most practical\nperspectives such as communication overheads, throughput, transaction\nconflicts, incentive compatibility of the protocol as well as a fair\ndistribution of transaction fees among the voters and the leaders. We use\nstate-of-the-art analysis to evaluate the consistency of the protocol and\nconsider Markov decision process (MDP) models to substantiate our claims about\nthe resilience of our protocol against incentive attacks."}
