{"id": "2508.21219", "categories": ["cs.CR", "cs.ET", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.21219", "abs": "https://arxiv.org/abs/2508.21219", "authors": ["A H M Nazmus Sakib", "Mahsin Bin Akram", "Joseph Spracklen", "Sahan Kalutarage", "Raveen Wijewickrama", "Igor Bilogrevic", "Murtuza Jadliwala"], "title": "The WASM Cloak: Evaluating Browser Fingerprinting Defenses Under WebAssembly based Obfuscation", "comment": null, "summary": "Browser fingerprinting defenses have historically focused on detecting\nJavaScript(JS)-based tracking techniques. However, the widespread adoption of\nWebAssembly (WASM) introduces a potential blind spot, as adversaries can\nconvert JS to WASM's low-level binary format to obfuscate malicious logic. This\npaper presents the first systematic evaluation of how such WASM-based\nobfuscation impacts the robustness of modern fingerprinting defenses. We\ndevelop an automated pipeline that translates real-world JS fingerprinting\nscripts into functional WASM-obfuscated variants and test them against two\nclasses of defenses: state-of-the-art detectors in research literature and\ncommercial, in-browser tools. Our findings reveal a notable divergence:\ndetectors proposed in the research literature that rely on feature-based\nanalysis of source code show moderate vulnerability, stemming from outdated\ndatasets or a lack of WASM compatibility. In contrast, defenses such as browser\nextensions and native browser features remained completely effective, as their\nAPI-level interception is agnostic to the script's underlying implementation.\nThese results highlight a gap between academic and practical defense strategies\nand offer insights into strengthening detection approaches against WASM-based\nobfuscation, while also revealing opportunities for more evasive techniques in\nfuture attacks."}
{"id": "2508.21302", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.21302", "abs": "https://arxiv.org/abs/2508.21302", "authors": ["Jie Zhu", "Chihao Shen", "Ziyang Li", "Jiahao Yu", "Yizheng Chen", "Kexin Pei"], "title": "Locus: Agentic Predicate Synthesis for Directed Fuzzing", "comment": null, "summary": "Directed fuzzing aims to find program inputs that lead to specified target\nprogram states. It has broad applications, such as debugging system crashes,\nconfirming reported bugs, and generating exploits for potential\nvulnerabilities. This task is inherently challenging because target states are\noften deeply nested in the program, while the search space manifested by\nnumerous possible program inputs is prohibitively large. Existing approaches\nrely on branch distances or manually-specified constraints to guide the search;\nhowever, the branches alone are often insufficient to precisely characterize\nprogress toward reaching the target states, while the manually specified\nconstraints are often tailored for specific bug types and thus difficult to\ngeneralize to diverse target states and programs.\n  We present Locus, a novel framework to improve the efficiency of directed\nfuzzing. Our key insight is to synthesize predicates to capture fuzzing\nprogress as semantically meaningful intermediate states, serving as milestones\ntowards reaching the target states. When used to instrument the program under\nfuzzing, they can reject executions unlikely to reach the target states, while\nproviding additional coverage guidance. To automate this task and generalize to\ndiverse programs, Locus features an agentic framework with program analysis\ntools to synthesize and iteratively refine the candidate predicates, while\nensuring the predicates strictly relax the target states to prevent false\nrejections via symbolic execution. Our evaluation shows that Locus\nsubstantially improves the efficiency of eight state-of-the-art fuzzers in\ndiscovering real-world vulnerabilities, achieving an average speedup of 41.6x.\nSo far, Locus has found eight previously unpatched bugs, with one already\nacknowledged with a draft patch."}
{"id": "2508.21323", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.21323", "abs": "https://arxiv.org/abs/2508.21323", "authors": ["Kunal Mukherjee", "Murat Kantarcioglu"], "title": "LLM-driven Provenance Forensics for Threat Investigation and Detection", "comment": null, "summary": "We introduce PROVSEEK, an LLM-powered agentic framework for automated\nprovenance-driven forensic analysis and threat intelligence extraction.\nPROVSEEK employs specialized toolchains to dynamically retrieve relevant\ncontext by generating precise, context-aware queries that fuse a vectorized\nthreat report knowledge base with data from system provenance databases. The\nframework resolves provenance queries, orchestrates multiple role-specific\nagents to mitigate hallucinations, and synthesizes structured, ground-truth\nverifiable forensic summaries. By combining agent orchestration with\nRetrieval-Augmented Generation (RAG) and chain-of-thought (CoT) reasoning,\nPROVSEEK enables adaptive multi-step analysis that iteratively refines\nhypotheses, verifies supporting evidence, and produces scalable, interpretable\nforensic explanations of attack behaviors. By combining provenance data with\nagentic reasoning, PROVSEEK establishes a new paradigm for grounded agentic\nforecics to investigate APTs. We conduct a comprehensive evaluation on publicly\navailable DARPA datasets, demonstrating that PROVSEEK outperforms\nretrieval-based methods for intelligence extraction task, achieving a 34%\nimprovement in contextual precision/recall; and for threat detection task,\nPROVSEEK achieves 22%/29% higher precision/recall compared to both a baseline\nagentic AI approach and State-Of-The-Art (SOTA) Provenance-based Intrusion\nDetection System (PIDS)."}
{"id": "2508.21386", "categories": ["cs.CR", "cs.CY", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.21386", "abs": "https://arxiv.org/abs/2508.21386", "authors": ["Jukka Ruohonen", "Jesper Løffler Nielsen", "Jakub Skórczynski"], "title": "Risks and Compliance with the EU's Core Cyber Security Legislation", "comment": "Submitted to IST (VSI:RegCompliance in SE)", "summary": "The European Union (EU) has long favored a risk-based approach to regulation.\nSuch an approach is also used in recent cyber security legislation enacted in\nthe EU. Risks are also inherently related to compliance with the new\nlegislation. Objective: The paper investigates how risks are framed in the EU's\nfive core cyber security legislative acts, whether the framings indicate\nconvergence or divergence between the acts and their risk concepts, and what\nqualifying words and terms are used when describing the legal notions of risks.\nMethod : The paper's methodology is based on qualitative legal interpretation\nand taxonomy-building. Results: The five acts have an encompassing coverage of\ndifferent cyber security risks, including but not limited to risks related to\ntechnical, organizational, and human security as well as those not originating\nfrom man-made actions. Both technical aspects and assets are used to frame the\nlegal risk notions in many of the legislative acts. A threat-centric viewpoint\nis also present in one of the acts. Notable gaps are related to acceptable\nrisks, non-probabilistic risks, and residual risks. Conclusion: The EU's new\ncyber security legislation has significantly extended the risk-based approach\nto regulations. At the same time, complexity and compliance burden have\nincreased. With this point in mind, the paper concludes with a few practical\ntakeaways about means to deal with compliance and research it."}
{"id": "2508.21393", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21393", "abs": "https://arxiv.org/abs/2508.21393", "authors": ["Guofu Liao", "Taotao Wang", "Shengli Zhang", "Jiqun Zhang", "Shi Long", "Dacheng Tao"], "title": "zkLoRA: Fine-Tuning Large Language Models with Verifiable Security via Zero-Knowledge Proofs", "comment": null, "summary": "Fine-tuning large language models (LLMs) is crucial for adapting them to\nspecific tasks, yet it remains computationally demanding and raises concerns\nabout correctness and privacy, particularly in untrusted environments. Although\nparameter-efficient methods like Low-Rank Adaptation (LoRA) significantly\nreduce resource requirements, ensuring the security and verifiability of\nfine-tuning under zero-knowledge constraints remains an unresolved challenge.\nTo address this, we introduce zkLoRA, the first framework to integrate LoRA\nfine-tuning with zero-knowledge proofs (ZKPs), achieving provable security and\ncorrectness. zkLoRA employs advanced cryptographic techniques -- such as lookup\narguments, sumcheck protocols, and polynomial commitments -- to verify both\narithmetic and non-arithmetic operations in Transformer-based architectures.\nThe framework provides end-to-end verifiability for forward propagation,\nbackward propagation, and parameter updates during LoRA fine-tuning, while\nsafeguarding the privacy of model parameters and training data. Leveraging\nGPU-based implementations, zkLoRA demonstrates practicality and efficiency\nthrough experimental validation on open-source LLMs like LLaMA, scaling up to\n13 billion parameters. By combining parameter-efficient fine-tuning with ZKPs,\nzkLoRA bridges a critical gap, enabling secure and trustworthy deployment of\nLLMs in sensitive or untrusted environments."}
{"id": "2508.21417", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.21417", "abs": "https://arxiv.org/abs/2508.21417", "authors": ["Shuhan Liu", "Xing Hu", "Xin Xia", "David Lo", "Xiaohu Yang"], "title": "An Empirical Study of Vulnerable Package Dependencies in LLM Repositories", "comment": null, "summary": "Large language models (LLMs) have developed rapidly in recent years,\nrevolutionizing various fields. Despite their widespread success, LLMs heavily\nrely on external code dependencies from package management systems, creating a\ncomplex and interconnected LLM dependency supply chain. Vulnerabilities in\ndependencies can expose LLMs to security risks. While existing research\npredominantly focuses on model-level security threats, vulnerabilities within\nthe LLM dependency supply chain have been overlooked. To fill this gap, we\nconducted an empirical analysis of 52 open-source LLMs, examining their\nthird-party dependencies and associated vulnerabilities. We then explored\nactivities within the LLM repositories to understand how maintainers manage\nthird-party vulnerabilities in practice. Finally, we compared third-party\ndependency vulnerabilities in the LLM ecosystem to those in the Python\necosystem. Our results show that half of the vulnerabilities in the LLM\necosystem remain undisclosed for more than 56.2 months, significantly longer\nthan those in the Python ecosystem. Additionally, 75.8% of LLMs include\nvulnerable dependencies in their configuration files. This study advances the\nunderstanding of LLM supply chain risks, provides insights for practitioners,\nand highlights potential directions for improving the security of the LLM\nsupply chain."}
{"id": "2508.21432", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.21432", "abs": "https://arxiv.org/abs/2508.21432", "authors": ["Wenjie Qu", "Yuguang Zhou", "Bo Wang", "Wengrui Zheng", "Yuexin Li", "Jinyuan Jia", "Jiaheng Zhang"], "title": "RepoMark: A Code Usage Auditing Framework for Code Large Language Models", "comment": null, "summary": "The rapid development of Large Language Models (LLMs) for code generation has\ntransformed software development by automating coding tasks with unprecedented\nefficiency.\n  However, the training of these models on open-source code repositories (e.g.,\nfrom GitHub) raises critical ethical and legal concerns, particularly regarding\ndata authorization and open-source license compliance. Developers are\nincreasingly questioning whether model trainers have obtained proper\nauthorization before using repositories for training, especially given the lack\nof transparency in data collection.\n  To address these concerns, we propose a novel data marking framework RepoMark\nto audit the data usage of code LLMs. Our method enables repository owners to\nverify whether their code has been used in training, while ensuring semantic\npreservation, imperceptibility, and theoretical false detection rate (FDR)\nguarantees. By generating multiple semantically equivalent code variants,\nRepoMark introduces data marks into the code files, and during detection,\nRepoMark leverages a novel ranking-based hypothesis test to detect memorization\nwithin the model. Compared to prior data auditing approaches, RepoMark\nsignificantly enhances sample efficiency, allowing effective auditing even when\nthe user's repository possesses only a small number of code files.\n  Experiments demonstrate that RepoMark achieves a detection success rate over\n90\\% on small code repositories under a strict FDR guarantee of 5\\%. This\nrepresents a significant advancement over existing data marking techniques, all\nof which only achieve accuracy below 55\\% under identical settings. This\nfurther validates RepoMark as a robust, theoretically sound, and promising\nsolution for enhancing transparency in code LLM training, which can safeguard\nthe rights of repository owners."}
{"id": "2508.21440", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.21440", "abs": "https://arxiv.org/abs/2508.21440", "authors": ["Shan Wang", "Ming Yang", "Yu Liu", "Yue Zhang", "Shuaiqing Zhang", "Zhen Ling", "Jiannong Cao", "Xinwen Fu"], "title": "Time Tells All: Deanonymization of Blockchain RPC Users with Zero Transaction Fee (Extended Version)", "comment": null, "summary": "Remote Procedure Call (RPC) services have become a primary gateway for users\nto access public blockchains. While they offer significant convenience, RPC\nservices also introduce critical privacy challenges that remain insufficiently\nexamined. Existing deanonymization attacks either do not apply to blockchain\nRPC users or incur costs like transaction fees assuming an active network\neavesdropper. In this paper, we propose a novel deanonymization attack that can\nlink an IP address of a RPC user to this user's blockchain pseudonym. Our\nanalysis reveals a temporal correlation between the timestamps of transaction\nconfirmations recorded on the public ledger and those of TCP packets sent by\nthe victim when querying transaction status. We assume a strong passive\nadversary with access to network infrastructure, capable of monitoring traffic\nat network border routers or Internet exchange points. By monitoring network\ntraffic and analyzing public ledgers, the attacker can link the IP address of\nthe TCP packet to the pseudonym of the transaction initiator by exploiting the\ntemporal correlation. This deanonymization attack incurs zero transaction fee.\nWe mathematically model and analyze the attack method, perform large-scale\nmeasurements of blockchain ledgers, and conduct real-world attacks to validate\nthe attack. Our attack achieves a high success rate of over 95% against normal\nRPC users on various blockchain networks, including Ethereum, Bitcoin and\nSolana."}
{"id": "2508.21457", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.21457", "abs": "https://arxiv.org/abs/2508.21457", "authors": ["Fengchao Chen", "Tingmin Wu", "Van Nguyen", "Carsten Rudolph"], "title": "SoK: Large Language Model-Generated Textual Phishing Campaigns End-to-End Analysis of Generation, Characteristics, and Detection", "comment": "13 pages, 3 tables, 4 figures", "summary": "Phishing is a pervasive form of social engineering in which attackers\nimpersonate trusted entities to steal information or induce harmful actions.\nText-based phishing dominates for its low cost, scalability, and\nconcealability, advantages recently amplified by large language models (LLMs)\nthat enable ``Phishing-as-a-Service'' attacks at scale within minutes. Despite\nthe growing research into LLM-facilitated phishing attacks, consolidated\nsystematic research on the phishing attack life cycle remains scarce. In this\nwork, we present the first systematization of knowledge (SoK) on LLM-generated\nphishing, offering an end-to-end analysis that spans generation techniques,\nattack features, and mitigation strategies. We introduce\nGeneration-Characterization-Defense (GenCharDef), which systematizes the ways\nin which LLM-generated phishing differs from traditional phishing across\nmethodologies, security perspectives, data dependencies, and evaluation\npractices. This framework highlights unique challenges of LLM-driven phishing,\nproviding a coherent foundation for understanding the evolving threat landscape\nand guiding the design of more resilient defenses."}
{"id": "2508.21480", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.21480", "abs": "https://arxiv.org/abs/2508.21480", "authors": ["Narges Dadkhah", "Khan Reaz", "Gerhard Wunder"], "title": "Towards a Decentralized IoT Onboarding for Smart Homes Using Consortium Blockchain", "comment": null, "summary": "The increasing adoption of smart home devices and IoT-based security systems\npresents significant opportunities to enhance convenience, safety, and risk\nmanagement for homeowners and service providers. However, secure\nonboarding-provisioning credentials and establishing trust with cloud\nplatforms-remains a considerable challenge. Traditional onboarding methods\noften rely on centralized Public Key Infrastructure (PKI) models and\nmanufacturer-controlled keys, which introduce security risks and limit the\nuser's digital sovereignty. These limitations hinder the widespread deployment\nof scalable IoT solutions. This paper presents a novel onboarding framework\nthat builds upon existing network-layer onboarding techniques and extends them\nto the application layer to address these challenges. By integrating consortium\nblockchain technology, we propose a decentralized onboarding mechanism that\nenhances transparency, security, and monitoring for smart home architectures.\nThe architecture supports device registration, key revocation, access control\nmanagement, and risk detection through event-driven alerts across dedicated\nblockchain channels and smart contracts. To evaluate the framework, we formally\nmodel the protocol using the Tamarin Prover under the Dolev-Yao adversary\nmodel. The analysis focuses on authentication, token integrity, key\nconfidentiality, and resilience over public channels. A prototype\nimplementation demonstrates the system's viability in smart home settings, with\nverification completing in 0.34 seconds, highlighting its scalability and\nsuitability for constrained devices and diverse stakeholders. Additionally,\nperformance evaluation shows that the blockchain-based approach effectively\nhandles varying workloads, maintains high throughput and low latency, and\nsupports near real-time IoT data processing."}
{"id": "2508.21558", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.21558", "abs": "https://arxiv.org/abs/2508.21558", "authors": ["Federica Bianchi", "Edoardo Di Paolo", "Angelo Spognardi"], "title": "Generalized Encrypted Traffic Classification Using Inter-Flow Signals", "comment": "Accepted manuscript at Availability, Reliability and Security (ARES\n  2025), published in Lecture Notes in Computer Science, vol. 15992, Springer,\n  Cham. DOI: https://doi.org/10.1007/978-3-032-00624-0_11", "summary": "In this paper, we present a novel encrypted traffic classification model that\noperates directly on raw PCAP data without requiring prior assumptions about\ntraffic type. Unlike existing methods, it is generalizable across multiple\nclassification tasks and leverages inter-flow signals - an innovative\nrepresentation that captures temporal correlations and packet volume\ndistributions across flows. Experimental results show that our model\noutperforms well-established methods in nearly every classification task and\nacross most datasets, achieving up to 99% accuracy in some cases, demonstrating\nits robustness and adaptability."}
{"id": "2508.21579", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.21579", "abs": "https://arxiv.org/abs/2508.21579", "authors": ["Ziyue Wang", "Liyi Zhou"], "title": "Agentic Discovery and Validation of Android App Vulnerabilities", "comment": null, "summary": "Existing Android vulnerability detection tools overwhelm teams with thousands\nof low-signal warnings yet uncover few true positives. Analysts spend days\ntriaging these results, creating a bottleneck in the security pipeline.\nMeanwhile, genuinely exploitable vulnerabilities often slip through, leaving\nopportunities open to malicious counterparts.\n  We introduce A2, a system that mirrors how security experts analyze and\nvalidate Android vulnerabilities through two complementary phases: (i) Agentic\nVulnerability Discovery, which reasons about application security by combining\nsemantic understanding with traditional security tools; and (ii) Agentic\nVulnerability Validation, which systematically validates vulnerabilities across\nAndroid's multi-modal attack surface-UI interactions, inter-component\ncommunication, file system operations, and cryptographic computations.\n  On the Ghera benchmark (n=60), A2 achieves 78.3% coverage, surpassing\nstate-of-the-art analyzers (e.g., APKHunt 30.0%). Rather than overwhelming\nanalysts with thousands of warnings, A2 distills results into 82 speculative\nvulnerability findings, including 47 Ghera cases and 28 additional true\npositives. Crucially, A2 then generates working Proof-of-Concepts (PoCs) for 51\nof these speculative findings, transforming them into validated vulnerability\nfindings that provide direct, self-confirming evidence of exploitability.\n  In real-world evaluation on 169 production APKs, A2 uncovers 104\ntrue-positive zero-day vulnerabilities. Among these, 57 (54.8%) are\nself-validated with automatically generated PoCs, including a medium-severity\nvulnerability in a widely used application with over 10 million installs."}
{"id": "2508.21602", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.21602", "abs": "https://arxiv.org/abs/2508.21602", "authors": ["Tomasz Kazana"], "title": "Condense to Conduct and Conduct to Condense", "comment": null, "summary": "In this paper we give the first examples of low-conductance permutations. The\nnotion of conductance of permutations was introduced in the paper\n\"Indifferentiability of Confusion-Diffusion Networks\" by Dodis et al., where\nthe search for low-conductance permutations was initiated and motivated. In\nthis paper we not only give the desired examples, but also make a general\ncharacterization of the problem -- i.e. we show that low-conductance\npermutations are equivalent to permutations that have the information-theoretic\nproperties of the so-called Multi-Source-Somewhere-Condensers."}
{"id": "2508.21606", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.21606", "abs": "https://arxiv.org/abs/2508.21606", "authors": ["Nishant Chinnasami", "Rasha Karakchi"], "title": "Hybrid Cryptographic Monitoring System for Side-Channel Attack Detection on PYNQ SoCs", "comment": "This paper is submitted at Supercomputing (SC'25)", "summary": "AES-128 encryption is theoretically secure but vulnerable in practical\ndeployments due to timing and fault injection attacks on embedded systems. This\nwork presents a lightweight dual-detection framework combining statistical\nthresholding and machine learning (ML) for real-time anomaly detection. By\nsimulating anomalies via delays and ciphertext corruption, we collect timing\nand data features to evaluate two strategies: (1) a statistical threshold\nmethod based on execution time and (2) a Random Forest classifier trained on\nblock-level anomalies. Implemented on CPU and FPGA (PYNQ-Z1), our results show\nthat the ML approach outperforms static thresholds in accuracy, while\nmaintaining real-time feasibility on embedded platforms. The framework operates\nwithout modifying AES internals or relying on hardware performance counters.\nThis makes it especially suitable for low-power, resource-constrained systems\nwhere detection accuracy and computational efficiency must be balanced."}
{"id": "2508.21636", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.21636", "abs": "https://arxiv.org/abs/2508.21636", "authors": ["Cristina Improta"], "title": "Detecting Stealthy Data Poisoning Attacks in AI Code Generators", "comment": "Accepted to the 3rd IEEE International Workshop on Reliable and\n  Secure AI for Software Engineering (ReSAISE, 2025), co-located with ISSRE\n  2025", "summary": "Deep learning (DL) models for natural language-to-code generation have become\nintegral to modern software development pipelines. However, their heavy\nreliance on large amounts of data, often collected from unsanitized online\nsources, exposes them to data poisoning attacks, where adversaries inject\nmalicious samples to subtly bias model behavior. Recent targeted attacks\nsilently replace secure code with semantically equivalent but vulnerable\nimplementations without relying on explicit triggers to launch the attack,\nmaking it especially hard for detection methods to distinguish clean from\npoisoned samples. We present a systematic study on the effectiveness of\nexisting poisoning detection methods under this stealthy threat model.\nSpecifically, we perform targeted poisoning on three DL models (CodeBERT,\nCodeT5+, AST-T5), and evaluate spectral signatures analysis, activation\nclustering, and static analysis as defenses. Our results show that all methods\nstruggle to detect triggerless poisoning, with representation-based approaches\nfailing to isolate poisoned samples and static analysis suffering false\npositives and false negatives, highlighting the need for more robust,\ntrigger-independent defenses for AI-assisted code generation."}
{"id": "2508.21654", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.21654", "abs": "https://arxiv.org/abs/2508.21654", "authors": ["Daryna Oliynyk", "Rudolf Mayer", "Kathrin Grosse", "Andreas Rauber"], "title": "I Stolenly Swear That I Am Up to (No) Good: Design and Evaluation of Model Stealing Attacks", "comment": "Under review", "summary": "Model stealing attacks endanger the confidentiality of machine learning\nmodels offered as a service. Although these models are kept secret, a malicious\nparty can query a model to label data samples and train their own substitute\nmodel, violating intellectual property. While novel attacks in the field are\ncontinually being published, their design and evaluations are not standardised,\nmaking it challenging to compare prior works and assess progress in the field.\nThis paper is the first to address this gap by providing recommendations for\ndesigning and evaluating model stealing attacks. To this end, we study the\nlargest group of attacks that rely on training a substitute model -- those\nattacking image classification models. We propose the first comprehensive\nthreat model and develop a framework for attack comparison. Further, we analyse\nattack setups from related works to understand which tasks and models have been\nstudied the most. Based on our findings, we present best practices for attack\ndevelopment before, during, and beyond experiments and derive an extensive list\nof open research questions regarding the evaluation of model stealing attacks.\nOur findings and recommendations also transfer to other problem domains, hence\nestablishing the first generic evaluation methodology for model stealing\nattacks."}
{"id": "2508.21669", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.21669", "abs": "https://arxiv.org/abs/2508.21669", "authors": ["Víctor Mayoral-Vilches", "Per Mannermaa Rynning"], "title": "Cybersecurity AI: Hacking the AI Hackers via Prompt Injection", "comment": null, "summary": "We demonstrate how AI-powered cybersecurity tools can be turned against\nthemselves through prompt injection attacks. Prompt injection is reminiscent of\ncross-site scripting (XSS): malicious text is hidden within seemingly trusted\ncontent, and when the system processes it, that text is transformed into\nunintended instructions. When AI agents designed to find and exploit\nvulnerabilities interact with malicious web servers, carefully crafted reponses\ncan hijack their execution flow, potentially granting attackers system access.\nWe present proof-of-concept exploits against the Cybersecurity AI (CAI)\nframework and its CLI tool, and detail our mitigations against such attacks in\na multi-layered defense implementation. Our findings indicate that prompt\ninjection is a recurring and systemic issue in LLM-based architectures, one\nthat will require dedicated work to address, much as the security community has\nhad to do with XSS in traditional web applications."}
{"id": "2508.21727", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21727", "abs": "https://arxiv.org/abs/2508.21727", "authors": ["Jiazheng Xing", "Hai Ci", "Hongbin Xu", "Hangjie Yuan", "Yong Liu", "Mike Zheng Shou"], "title": "OptMark: Robust Multi-bit Diffusion Watermarking via Inference Time Optimization", "comment": null, "summary": "Watermarking diffusion-generated images is crucial for copyright protection\nand user tracking. However, current diffusion watermarking methods face\nsignificant limitations: zero-bit watermarking systems lack the capacity for\nlarge-scale user tracking, while multi-bit methods are highly sensitive to\ncertain image transformations or generative attacks, resulting in a lack of\ncomprehensive robustness. In this paper, we propose OptMark, an\noptimization-based approach that embeds a robust multi-bit watermark into the\nintermediate latents of the diffusion denoising process. OptMark strategically\ninserts a structural watermark early to resist generative attacks and a detail\nwatermark late to withstand image transformations, with tailored regularization\nterms to preserve image quality and ensure imperceptibility. To address the\nchallenge of memory consumption growing linearly with the number of denoising\nsteps during optimization, OptMark incorporates adjoint gradient methods,\nreducing memory usage from O(N) to O(1). Experimental results demonstrate that\nOptMark achieves invisible multi-bit watermarking while ensuring robust\nresilience against valuemetric transformations, geometric transformations,\nediting, and regeneration attacks."}
