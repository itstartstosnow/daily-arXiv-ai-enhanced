<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 9]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Beyond Detection: A Comprehensive Benchmark and Study on Representation Learning for Fine-Grained Webshell Family Classification](https://arxiv.org/abs/2512.05288)
*Feijiang Han*

Main category: cs.CR

TL;DR: 本文首次系统研究WebShell家族分类自动化，通过提取动态函数调用轨迹、使用LLM生成变种增强数据集，并比较多种表示方法，为WebShell家族分类建立了基准。


<details>
  <summary>Details</summary>
Motivation: WebShell对关键数字基础设施构成严重威胁，现有研究主要关注检测而非深入分析。从被动检测转向主动防御需要自动化家族分类来理解攻击者战术并实现精准快速响应，但目前该任务仍依赖缓慢的手动专家分析。

Method: 1) 提取动态函数调用轨迹捕获抗加密和混淆的内在行为；2) 使用大语言模型合成新变种增强数据集规模和多样性；3) 将轨迹抽象为序列、图和树；4) 系统评估经典序列嵌入、transformer模型和结构感知算法等多种表示方法。

Result: 在四个真实世界家族标注数据集上进行监督和无监督实验，建立了稳健的基准，并提供了数据抽象、表示模型和学习范式的最有效组合的实用见解。

Conclusion: 这是WebShell家族分类自动化的首次系统研究，为从被动检测转向主动防御提供了重要基础，通过全面评估多种方法为后续研究建立了基准。

Abstract: Malicious WebShells pose a significant and evolving threat by compromising critical digital infrastructures and endangering public services in sectors such as healthcare and finance. While the research community has made significant progress in WebShell detection (i.e., distinguishing malicious samples from benign ones), we argue that it is time to transition from passive detection to in-depth analysis and proactive defense. One promising direction is the automation of WebShell family classification, which involves identifying the specific malware lineage in order to understand an adversary's tactics and enable a precise, rapid response. This crucial task, however, remains a largely unexplored area that currently relies on slow, manual expert analysis. To address this gap, we present the first systematic study to automate WebShell family classification. Our method begins with extracting dynamic function call traces to capture inherent behaviors that are resistant to common encryption and obfuscation. To enhance the scale and diversity of our dataset for a more stable evaluation, we augment these real-world traces with new variants synthesized by Large Language Models. These augmented traces are then abstracted into sequences, graphs, and trees, providing a foundation to benchmark a comprehensive suite of representation methods. Our evaluation spans classic sequence-based embeddings (CBOW, GloVe), transformers (BERT, SimCSE), and a range of structure-aware algorithms, including Graph Kernels, Graph Edit Distance, Graph2Vec, and various Graph Neural Networks. Through extensive experiments on four real-world, family-annotated datasets under both supervised and unsupervised settings, we establish a robust baseline and provide practical insights into the most effective combinations of data abstractions, representation models, and learning paradigms for this challenge.

</details>


### [2] [A Practical Honeypot-Based Threat Intelligence Framework for Cyber Defence in the Cloud](https://arxiv.org/abs/2512.05321)
*Darren Malvern Chin,Bilal Isfaq,Simon Yusuf Enoch*

Main category: cs.CR

TL;DR: 提出一个基于蜜罐遥测的自动化云防火墙防御框架，利用Azure原生工具实现实时威胁检测与阻断，平均阻断时间仅0.86秒。


<details>
  <summary>Details</summary>
Motivation: 传统防火墙依赖预定义规则和手动配置，无法有效应对云环境中不断演变的零日威胁、僵尸网络和高级持续性威胁，特别是在Azure等云平台中静态防御模型存在安全漏洞。

Method: 开发自动化防御框架，整合中高交互蜜罐(Cowrie)、Azure原生自动化工具(Monitor、Sentinel、Logic Apps)和MITRE ATT&CK对齐检测，通过闭环反馈机制实时更新防火墙规则。建立测试平台自动观察攻击者战术，使用MITRE ATT&CK框架进行分类，并以最小人工干预自动缓解网络级威胁。

Result: 平均阻断时间仅0.86秒，显著快于基准系统；准确分类超过12,000次SSH尝试，覆盖多个MITRE ATT&CK战术；减少攻击者驻留时间，增强SOC可见性。

Conclusion: 将蜜罐遥测与Azure原生自动化集成，为现代云基础设施提供了可扩展、可操作的防御模型，显著提升了云环境的安全防护能力。

Abstract: In cloud environments, conventional firewalls rely on predefined rules and manual configurations, limiting their ability to respond effectively to evolving or zero-day threats. As organizations increasingly adopt platforms such as Microsoft Azure, this static defense model exposes cloud assets to zero-day exploits, botnets, and advanced persistent threats. In this paper, we introduce an automated defense framework that leverages medium- to high-interaction honeypot telemetry to dynamically update firewall rules in real time. The framework integrates deception sensors (Cowrie), Azure-native automation tools (Monitor, Sentinel, Logic Apps), and MITRE ATT&CK-aligned detection within a closed-loop feedback mechanism. We developed a testbed to automatically observe adversary tactics, classify them using the MITRE ATT&CK framework, and mitigate network-level threats automatically with minimal human intervention.
  To assess the framework's effectiveness, we defined and applied a set of attack- and defense-oriented security metrics. Building on existing adaptive defense strategies, our solution extends automated capabilities into cloud-native environments. The experimental results show an average Mean Time to Block of 0.86 seconds - significantly faster than benchmark systems - while accurately classifying over 12,000 SSH attempts across multiple MITRE ATT&CK tactics. These findings demonstrate that integrating deception telemetry with Azure-native automation reduces attacker dwell time, enhances SOC visibility, and provides a scalable, actionable defense model for modern cloud infrastructures.

</details>


### [3] [Please Don't Kill My Vibe: Empowering Agents with Data Flow Control](https://arxiv.org/abs/2512.05374)
*Charlie Summers,Haneen Mohammed,Eugene Wu*

Main category: cs.CR

TL;DR: 论文提出为LLM智能体系统引入原生数据流控制机制，以解决当前智能体工作流中存在的策略违规、流程腐败和安全漏洞等风险。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在执行复杂任务时存在重大风险，包括策略违规、流程腐败和安全漏洞，这些问题源于缺乏对智能体行为产生的不良数据流的可见性和管理机制。目前智能体工作流只能以临时方式执行策略控制。

Method: 借鉴数据库管理系统将数据验证和访问控制从应用层转移到DBMS层的思路，提出系统应原生支持数据流控制并强制执行DFC策略。论文描述了为DBMS开发可移植DFC实例的早期工作。

Result: 论文提出了DFC框架的初步实现，并概述了为智能体生态系统开发DFC的更广泛研究议程。

Conclusion: 需要为LLM智能体系统建立原生数据流控制机制，就像数据库管理系统将数据控制从应用层转移到系统层一样，这将使智能体开发者从策略执行负担中解放出来，提高系统安全性和可靠性。

Abstract: The promise of Large Language Model (LLM) agents is to perform complex, stateful tasks. This promise is stunted by significant risks - policy violations, process corruption, and security flaws - that stem from the lack of visibility and mechanisms to manage undesirable data flows produced by agent actions. Today, agent workflows are responsible for enforcing these policies in ad hoc ways. Just as data validation and access controls shifted from the application to the DBMS, freeing application developers from these concerns, we argue that systems should support Data Flow Controls (DFCs) and enforce DFC policies natively. This paper describes early work developing a portable instance of DFC for DBMSes and outlines a broader research agenda toward DFC for agent ecosystems.

</details>


### [4] [PrivCode: When Code Generation Meets Differential Privacy](https://arxiv.org/abs/2512.05459)
*Zheng Liu,Chen Gong,Terry Yue Zhuo,Kecen Li,Weichen Yu,Matt Fredrikson,Tianhao Wang*

Main category: cs.CR

TL;DR: PrivCode：首个专为代码数据集设计的差分隐私合成器，采用两阶段框架提升隐私保护和代码效用


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成方面表现出色，但在私有数据集上微调会引发隐私和专有信息泄露问题。差分隐私代码生成虽然提供理论保护，但面临严格的语法依赖性和隐私-效用权衡的挑战

Method: 提出PrivCode两阶段框架：第一阶段"隐私净化"使用DP-SGD训练模型生成差分隐私合规的合成代码，同时引入语法信息保持代码结构；第二阶段"效用提升"在合成隐私代码上微调大型预训练LLM，减轻DP带来的效用损失

Result: 在四个LLM上的广泛实验表明，PrivCode在四个基准测试的各种任务中生成更高效用的代码。实验还证实了在不同隐私预算下保护敏感数据的能力

Conclusion: PrivCode是首个专为代码数据集设计的差分隐私合成器，成功解决了隐私保护和代码效用之间的权衡问题，为保护敏感代码数据提供了有效解决方案

Abstract: Large language models (LLMs) have presented outstanding performance in code generation and completion. However, fine-tuning these models on private datasets can raise privacy and proprietary concerns, such as the leakage of sensitive personal information. Differentially private (DP) code generation provides theoretical guarantees for protecting sensitive code by generating synthetic datasets that preserve statistical properties while reducing privacy leakage concerns. However, DP code generation faces significant challenges due to the strict syntactic dependencies and the privacy-utility trade-off.
  We propose PrivCode, the first DP synthesizer specifically designed for code datasets. It incorporates a two-stage framework to improve both privacy and utility. In the first stage, termed "privacy-sanitizing", PrivCode generates DP-compliant synthetic code by training models using DP-SGD while introducing syntactic information to preserve code structure. The second stage, termed "utility-boosting", fine-tunes a larger pre-trained LLM on the synthetic privacy-free code to mitigate the utility loss caused by DP, enhancing the utility of the generated code. Extensive experiments on four LLMs show that PrivCode generates higher-utility code across various testing tasks under four benchmarks. The experiments also confirm its ability to protect sensitive data under varying privacy budgets. We provide the replication package at the anonymous link.

</details>


### [5] [TeleAI-Safety: A comprehensive LLM jailbreaking benchmark towards attacks, defenses, and evaluations](https://arxiv.org/abs/2512.05485)
*Xiuyuan Chen,Jian Zhao,Yuxiang He,Yuan Xun,Xinwei Liu,Yanshu Li,Huilin Zhou,Wei Cai,Ziyan Shi,Yuchen Yuan,Tianle Zhang,Chi Zhang,Xuelong Li*

Main category: cs.CR

TL;DR: TeleAI-Safety是一个模块化、可复现的LLM安全评估框架与基准，整合了19种攻击方法、29种防御方法和19种评估方法，在12个风险类别上评估14个目标模型，揭示了系统漏洞和安全与效用的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全评估存在不足：现有基准和框架在核心组件（攻击、防御、评估方法）整合不平衡，且灵活评估框架与标准化基准能力之间存在隔离，这阻碍了可靠的跨研究比较，并给全面风险评估带来不必要的负担。

Method: 提出了TeleAI-Safety框架，包含模块化、可复现的设计，整合了19种攻击方法（含1种自研）、29种防御方法和19种评估方法（含1种自研）。使用包含12个风险类别342个样本的攻击语料库，对14个目标模型进行广泛评估。

Result: 评估结果揭示了系统漏洞和模型特定的失败案例，突出了安全与效用之间的关键权衡，并识别了未来优化的潜在防御模式。框架可根据实际需求灵活调整攻击、防御和评估组合。

Conclusion: TeleAI-Safety为LLM安全评估提供了统一、可复现的框架和基准，有助于建立统一的安全基线，促进可复现研究，并支持根据特定需求进行灵活调整。

Abstract: While the deployment of large language models (LLMs) in high-value industries continues to expand, the systematic assessment of their safety against jailbreak and prompt-based attacks remains insufficient. Existing safety evaluation benchmarks and frameworks are often limited by an imbalanced integration of core components (attack, defense, and evaluation methods) and an isolation between flexible evaluation frameworks and standardized benchmarking capabilities. These limitations hinder reliable cross-study comparisons and create unnecessary overhead for comprehensive risk assessment. To address these gaps, we present TeleAI-Safety, a modular and reproducible framework coupled with a systematic benchmark for rigorous LLM safety evaluation. Our framework integrates a broad collection of 19 attack methods (including one self-developed method), 29 defense methods, and 19 evaluation methods (including one self-developed method). With a curated attack corpus of 342 samples spanning 12 distinct risk categories, the TeleAI-Safety benchmark conducts extensive evaluations across 14 target models. The results reveal systematic vulnerabilities and model-specific failure cases, highlighting critical trade-offs between safety and utility, and identifying potential defense patterns for future optimization. In practical scenarios, TeleAI-Safety can be flexibly adjusted with customized attack, defense, and evaluation combinations to meet specific demands. We release our complete code and evaluation results to facilitate reproducible research and establish unified safety baselines.

</details>


### [6] [Matching Ranks Over Probability Yields Truly Deep Safety Alignment](https://arxiv.org/abs/2512.05518)
*Jason Vega,Gagandeep Singh*

Main category: cs.CR

TL;DR: 论文提出RAP攻击可绕过基于数据增强的SFT安全对齐防御，并提出PRESTO方法通过正则化有害预填充token的注意力来提升安全性。


<details>
  <summary>Details</summary>
Motivation: 现有基于数据增强的SFT防御方法虽然能让模型在有害预填充后生成拒绝响应，但实际安全对齐不够"深入"，仍存在漏洞。

Method: 提出RAP攻击（Rank-Assisted Prefilling），通过从top-20预测token中选择低概率的有害token来绕过防御；然后提出PRESTO防御方法，通过正则化有害预填充token的注意力来匹配目标分布的token排序而非概率。

Result: PRESTO方法在三个开源LLM上使RAP攻击下的StrongREJECT分数平均提升达4.7倍，且对模型实用性影响较小。

Conclusion: 基于概率匹配的SFT安全对齐存在根本缺陷，应转向基于token排序匹配的"深度"安全对齐，PRESTO提供了一种简单有效的实现方法。

Abstract: A frustratingly easy technique known as the prefilling attack has been shown to effectively circumvent the safety alignment of frontier LLMs by simply prefilling the assistant response with an affirmative prefix before decoding. In response, recent work proposed a supervised fine-tuning (SFT) defense using data augmentation to achieve a \enquote{deep} safety alignment, allowing the model to generate natural language refusals immediately following harmful prefills. Unfortunately, we show in this work that the "deep" safety alignment produced by such an approach is in fact not very deep. A generalization of the prefilling attack, which we refer to as the Rank-Assisted Prefilling (RAP) attack, can effectively extract harmful content from models fine-tuned with the data augmentation defense by selecting low-probability "harmful" tokens from the top 20 predicted next tokens at each step (thus ignoring high-probability "refusal" tokens). We argue that this vulnerability is enabled due to the "gaming" of the SFT objective when the target distribution entropies are low, where low fine-tuning loss is achieved by shifting large probability mass to a small number of refusal tokens while neglecting the high ranks of harmful tokens. We then propose a new perspective on achieving deep safety alignment by matching the token ranks of the target distribution, rather than their probabilities. This perspective yields a surprisingly simple fix to the data augmentation defense based on regularizing the attention placed on harmful prefill tokens, an approach we call PRefill attEntion STOpping (PRESTO). Adding PRESTO yields up to a 4.7x improvement in the mean StrongREJECT score under RAP attacks across three popular open-source LLMs, with low impact to model utility.

</details>


### [7] [Evaluating Concept Filtering Defenses against Child Sexual Abuse Material Generation by Text-to-Image Models](https://arxiv.org/abs/2512.05707)
*Ana-Maria Cretu,Klim Kireev,Amro Abdalla,Wisdom Obinna,Raphael Meier,Sarah Adel Bargal,Elissa M. Redmiles,Carmela Troncoso*

Main category: cs.CR

TL;DR: 当前儿童过滤方法对防止T2I模型生成儿童性虐待材料效果有限，即使过滤后仍有方法通过少量额外查询生成儿童内容，且微调可完全恢复概念


<details>
  <summary>Details</summary>
Motivation: 评估儿童过滤在防止文本到图像模型被滥用于生成儿童性虐待材料方面的有效性，揭示现有安全缓解措施的局限性

Method: 1) 使用基于游戏的安全定义分析CSAM预防复杂性；2) 证明当前检测方法无法完全移除数据集中的儿童图像；3) 使用戴眼镜儿童作为CSAM的伦理代理，测试过滤后模型的脆弱性；4) 通过微调实验验证概念恢复的可能性

Result: 即使训练数据集中只残留少量儿童图像，攻击者仍可通过少量额外查询从过滤后的模型生成儿童内容。微调可进一步降低查询开销，甚至完美过滤后也能通过微调恢复概念。当前过滤方法对闭源模型保护有限，对开源模型无保护，同时损害模型生成儿童相关概念的能力

Conclusion: 现有儿童过滤方法提供的保护有限，需要更严格的评估方法来建立AI安全缓解措施对防止CSAM生成影响的可靠证据

Abstract: We evaluate the effectiveness of child filtering to prevent the misuse of text-to-image (T2I) models to create child sexual abuse material (CSAM). First, we capture the complexity of preventing CSAM generation using a game-based security definition. Second, we show that current detection methods cannot remove all children from a dataset. Third, using an ethical proxy for CSAM (a child wearing glasses, hereafter, CWG), we show that even when only a small percentage of child images are left in the training dataset, there exist prompting strategies that generate CWG from a child-filtered T2I model using only a few more queries than when the model is trained on the unfiltered data. Fine-tuning the filtered model on child images further reduces the additional query overhead. We also show that reintroducing a concept is possible via fine-tuning even if filtering is perfect. Our results demonstrate that current filtering methods offer limited protection to closed-weight models and no protection to open-weight models, while reducing the generality of the model by hindering the generation of child-related concepts or changing their representation. We conclude by outlining challenges in conducting evaluations that establish robust evidence on the impact of AI safety mitigations for CSAM.

</details>


### [8] [ARGUS: Defending Against Multimodal Indirect Prompt Injection via Steering Instruction-Following Behavior](https://arxiv.org/abs/2512.05745)
*Weikai Lu,Ziqian Zeng,Kehua Zhang,Haoran Li,Huiping Zhuang,Ruidong Wang,Cen Chen,Hao Peng*

Main category: cs.CR

TL;DR: ARGUS提出了一种针对多模态间接提示注入攻击的防御方法，通过表示空间中的行为引导实现模态无关的鲁棒防御，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型面临来自图像、视频、音频等模态的间接提示注入攻击威胁，现有针对纯文本LLM的防御方法无法有效应对多模态攻击，存在易被绕过、模态依赖或泛化能力差的问题。

Method: ARGUS方法包含三个核心阶段：1）在安全子空间中搜索最优防御方向，使其与性能退化方向解耦；2）引入轻量级注入检测阶段，按需激活防御；3）后过滤阶段验证防御成功。通过自适应强度引导实现安全与性能的最佳权衡。

Result: 实验结果表明，ARGUS能够实现对多模态IPI攻击的鲁棒防御，同时最大限度地保持MLLM的实用性，在安全性和性能之间取得了良好平衡。

Conclusion: 通过表示空间中的行为引导，可以构建独立于模态的通用防御机制，ARGUS通过解耦防御方向与性能退化方向、自适应强度引导和多阶段防御架构，有效应对多模态间接提示注入攻击。

Abstract: Multimodal Large Language Models (MLLMs) are increasingly vulnerable to multimodal Indirect Prompt Injection (IPI) attacks, which embed malicious instructions in images, videos, or audio to hijack model behavior. Existing defenses, designed primarily for text-only LLMs, are unsuitable for countering these multimodal threats, as they are easily bypassed, modality-dependent, or generalize poorly. Inspired by activation steering researches, we hypothesize that a robust, general defense independent of modality can be achieved by steering the model's behavior in the representation space. Through extensive experiments, we discover that the instruction-following behavior of MLLMs is encoded in a subspace. Steering along directions within this subspace can enforce adherence to user instructions, forming the basis of a defense. However, we also found that a naive defense direction could be coupled with a utility-degrading direction, and excessive intervention strength harms model performance. To address this, we propose ARGUS, which searches for an optimal defense direction within the safety subspace that decouples from the utility degradation direction, further combining adaptive strength steering to achieve a better safety-utility trade-off. ARGUS also introduces lightweight injection detection stage to activate the defense on-demand, and a post-filtering stage to verify defense success. Experimental results show that ARGUS can achieve robust defense against multimodal IPI while maximally preserving the MLLM's utility.

</details>


### [9] [Trusted AI Agents in the Cloud](https://arxiv.org/abs/2512.05951)
*Teofil Bodea,Masanori Misono,Julian Pritzi,Patrick Sabanic,Thore Sommer,Harshavardhan Unnibhavi,David Schall,Nuno Santos,Dimitrios Stavrakakis,Pramod Bhatotia*

Main category: cs.CR

TL;DR: Omega是一个可信AI代理系统，通过端到端隔离、跨主体可验证信任建立和监督外部交互来保护AI代理，支持云规模的高密度多代理部署。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的AI代理作为云服务部署时面临多主体环境中的安全风险，现有机密虚拟机仅提供二进制级别保护，无法保证跨主体信任、加速器级隔离或代理行为监督。

Method: 基于AMD SEV-SNP和NVIDIA H100的机密虚拟机和机密GPU构建可信代理平台，使用嵌套隔离在单个CVM内托管多个代理，通过差异认证建立跨主体信任，并提供策略规范和执行框架。

Result: Omega完全保护了CVM-GPU间的代理状态，在实现高性能的同时支持云规模的高密度、策略合规的多代理部署。

Conclusion: Omega系统通过端到端隔离、可验证信任建立和监督机制，为AI代理提供了全面的安全保障，解决了现有机密虚拟机在多主体环境中的局限性。

Abstract: AI agents powered by large language models are increasingly deployed as cloud services that autonomously access sensitive data, invoke external tools, and interact with other agents. However, these agents run within a complex multi-party ecosystem, where untrusted components can lead to data leakage, tampering, or unintended behavior. Existing Confidential Virtual Machines (CVMs) provide only per binary protection and offer no guarantees for cross-principal trust, accelerator-level isolation, or supervised agent behavior. We present Omega, a system that enables trusted AI agents by enforcing end-to-end isolation, establishing verifiable trust across all contributing principals, and supervising every external interaction with accountable provenance. Omega builds on Confidential VMs and Confidential GPUs to create a Trusted Agent Platform that hosts many agents within a single CVM using nested isolation. It also provides efficient multi-agent orchestration with cross-principal trust establishment via differential attestation, and a policy specification and enforcement framework that governs data access, tool usage, and inter-agent communication for data protection and regulatory compliance. Implemented on AMD SEV-SNP and NVIDIA H100, Omega fully secures agent state across CVM-GPU, and achieves high performance while enabling high-density, policy-compliant multi-agent deployments at cloud scale.

</details>
