<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 15]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [A Survey of Agentic AI and Cybersecurity: Challenges, Opportunities and Use-case Prototypes](https://arxiv.org/abs/2601.05293)
*Sahaya Jestus Lazer,Kshitiz Aryal,Maanak Gupta,Elisa Bertino*

Main category: cs.CR

TL;DR: 该调查分析了智能体AI对网络安全的影响，探讨了其在防御和攻击方面的双重用途，以及由此产生的治理挑战和系统性风险。


<details>
  <summary>Details</summary>
Motivation: 智能体AI从单步生成模型转向能够推理、规划、行动和适应的系统，这种转变对网络安全产生了深远影响。需要研究智能体AI在网络安全中的双重用途特性，以及现有治理机制如何应对这些新型自主系统的挑战。

Method: 通过调查新兴威胁模型、安全框架和评估流程，分析智能体系统特有的系统性风险，并展示三个代表性用例实现，说明智能体AI在实际网络安全工作流程中的行为。

Result: 智能体AI在防御方面能够实现持续监控、自主事件响应、自适应威胁狩猎和大规模欺诈检测；在攻击方面则增强了侦察、利用、协调和社会工程攻击能力。现有治理、保证和问责机制存在根本性差距，无法有效应对自主系统的挑战。

Conclusion: 智能体AI在网络安全领域具有双重用途特性，需要专门的安全框架和评估流程来应对系统性风险。设计选择直接影响智能体AI的可靠性、安全性和操作有效性，必须建立适应自主系统特性的治理机制。

Abstract: Agentic AI marks an important transition from single-step generative models to systems capable of reasoning, planning, acting, and adapting over long-lasting tasks. By integrating memory, tool use, and iterative decision cycles, these systems enable continuous, autonomous workflows in real-world environments. This survey examines the implications of agentic AI for cybersecurity. On the defensive side, agentic capabilities enable continuous monitoring, autonomous incident response, adaptive threat hunting, and fraud detection at scale. Conversely, the same properties amplify adversarial power by accelerating reconnaissance, exploitation, coordination, and social-engineering attacks. These dual-use dynamics expose fundamental gaps in existing governance, assurance, and accountability mechanisms, which were largely designed for non-autonomous and short-lived AI systems. To address these challenges, we survey emerging threat models, security frameworks, and evaluation pipelines tailored to agentic systems, and analyze systemic risks including agent collusion, cascading failures, oversight evasion, and memory poisoning. Finally, we present three representative use-case implementations that illustrate how agentic AI behaves in practical cybersecurity workflows, and how design choices shape reliability, safety, and operational effectiveness.

</details>


### [2] [Multi-turn Jailbreaking Attack in Multi-Modal Large Language Models](https://arxiv.org/abs/2601.05339)
*Badhan Chandra Das,Md Tasnim Jawad,Joaquin Molto,M. Hadi Amini,Yanzhao Wu*

Main category: cs.CR

TL;DR: MJAD-MLLMs框架系统分析多模态大语言模型的多轮越狱攻击与防御技术，提出新型多轮攻击方法和基于多LLM的防御机制FragGuard。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型虽然智能且准确，但存在严重安全漏洞，特别是容易受到精心设计的越狱攻击，这些攻击能操纵模型行为并绕过安全约束，需要系统性的安全分析框架。

Method: 提出MJAD-MLLMs整体框架，包含三个主要贡献：1) 新型多轮越狱攻击方法，利用多轮提示利用MLLM漏洞；2) 基于多LLM的防御机制FragGuard，采用片段优化策略；3) 在多个SOTA开源和闭源MLLM及基准数据集上进行广泛实验评估。

Result: 通过实验评估验证了所提攻击和防御方法的有效性，并与现有技术进行了性能比较，展示了MJAD-MLLMs框架在分析MLLM安全漏洞和防御越狱攻击方面的能力。

Conclusion: MJAD-MLLMs为MLLM安全研究提供了系统性分析框架，提出的多轮越狱攻击和FragGuard防御机制为解决MLLM安全漏洞问题提供了有效方案，对提升生成式AI安全性具有重要意义。

Abstract: In recent years, the security vulnerabilities of Multi-modal Large Language Models (MLLMs) have become a serious concern in the Generative Artificial Intelligence (GenAI) research. These highly intelligent models, capable of performing multi-modal tasks with high accuracy, are also severely susceptible to carefully launched security attacks, such as jailbreaking attacks, which can manipulate model behavior and bypass safety constraints. This paper introduces MJAD-MLLMs, a holistic framework that systematically analyzes the proposed Multi-turn Jailbreaking Attacks and multi-LLM-based defense techniques for MLLMs. In this paper, we make three original contributions. First, we introduce a novel multi-turn jailbreaking attack to exploit the vulnerabilities of the MLLMs under multi-turn prompting. Second, we propose a novel fragment-optimized and multi-LLM defense mechanism, called FragGuard, to effectively mitigate jailbreaking attacks in the MLLMs. Third, we evaluate the efficacy of the proposed attacks and defenses through extensive experiments on several state-of-the-art (SOTA) open-source and closed-source MLLMs and benchmark datasets, and compare their performance with the existing techniques.

</details>


### [3] [Knowledge-Driven Multi-Turn Jailbreaking on Large Language Models](https://arxiv.org/abs/2601.05445)
*Songze Li,Ruishi He,Xiaojun Jia,Jun Wang,Zhihui Fu*

Main category: cs.CR

TL;DR: Mastermind是一个多轮越狱攻击框架，采用动态自改进方法，通过规划-执行-反思的闭环系统，显著提升了对大型语言模型的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有多轮越狱攻击存在关键局限性：难以在长对话中保持连贯进展，依赖僵化预定义模式，无法适应LLM动态不可预测的对话状态。

Method: 采用动态自改进的闭环框架，包含规划、执行、反思三个环节。使用分层规划架构将高层攻击目标与低层战术执行解耦，通过知识库自主发现和优化攻击模式，动态重组和适应攻击向量。

Result: 在GPT-5和Claude 3.7 Sonnet等最先进模型上的实验表明，Mastermind显著优于现有基线，实现了更高的攻击成功率和危害性评分，并对多种高级防御机制表现出显著韧性。

Conclusion: Mastermind通过动态自改进的闭环框架有效解决了现有多轮越狱攻击的局限性，显著提升了攻击效果和韧性，为LLM安全研究提供了重要参考。

Abstract: Large Language Models (LLMs) face a significant threat from multi-turn jailbreak attacks, where adversaries progressively steer conversations to elicit harmful outputs. However, the practical effectiveness of existing attacks is undermined by several critical limitations: they struggle to maintain a coherent progression over long interactions, often losing track of what has been accomplished and what remains to be done; they rely on rigid or pre-defined patterns, and fail to adapt to the LLM's dynamic and unpredictable conversational state. To address these shortcomings, we introduce Mastermind, a multi-turn jailbreak framework that adopts a dynamic and self-improving approach. Mastermind operates in a closed loop of planning, execution, and reflection, enabling it to autonomously build and refine its knowledge of model vulnerabilities through interaction. It employs a hierarchical planning architecture that decouples high-level attack objectives from low-level tactical execution, ensuring long-term focus and coherence. This planning is guided by a knowledge repository that autonomously discovers and refines effective attack patterns by reflecting on interactive experiences. Mastermind leverages this accumulated knowledge to dynamically recombine and adapt attack vectors, dramatically improving both effectiveness and resilience. We conduct comprehensive experiments against state-of-the-art models, including GPT-5 and Claude 3.7 Sonnet. The results demonstrate that Mastermind significantly outperforms existing baselines, achieving substantially higher attack success rates and harmfulness ratings. Moreover, our framework exhibits notable resilience against multiple advanced defense mechanisms.

</details>


### [4] [Jailbreaking Large Language Models through Iterative Tool-Disguised Attacks via Reinforcement Learning](https://arxiv.org/abs/2601.05466)
*Zhaoqi Wang,Zijian Zhang,Daqing He,Pengtao Kou,Xin Li,Jiamou Liu,Jincheng An,Yong Liu*

Main category: cs.CR

TL;DR: iMIST是一种新型自适应越狱攻击方法，通过将恶意查询伪装成正常工具调用来绕过内容过滤器，同时采用交互式渐进优化算法动态提升响应危害性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在各种应用中表现出色，但它们仍然极易受到越狱攻击，这些攻击会引发违反人类价值观和安全准则的有害响应。现有防御机制对复杂对抗策略的防护不足。

Method: iMIST将恶意查询伪装成正常工具调用来绕过内容过滤器，同时引入交互式渐进优化算法，通过基于实时危害性评估的多轮对话动态提升响应危害性。

Result: 实验表明iMIST在广泛使用的模型上实现了更高的攻击有效性，同时保持较低的拒绝率。

Conclusion: 这些结果揭示了当前LLM安全机制的关键漏洞，并强调了开发更强大防御策略的紧迫性。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across diverse applications, however, they remain critically vulnerable to jailbreak attacks that elicit harmful responses violating human values and safety guidelines. Despite extensive research on defense mechanisms, existing safeguards prove insufficient against sophisticated adversarial strategies. In this work, we propose iMIST (\underline{i}nteractive \underline{M}ulti-step \underline{P}rogre\underline{s}sive \underline{T}ool-disguised Jailbreak Attack), a novel adaptive jailbreak method that synergistically exploits vulnerabilities in current defense mechanisms. iMIST disguises malicious queries as normal tool invocations to bypass content filters, while simultaneously introducing an interactive progressive optimization algorithm that dynamically escalates response harmfulness through multi-turn dialogues guided by real-time harmfulness assessment. Our experiments on widely-used models demonstrate that iMIST achieves higher attack effectiveness, while maintaining low rejection rates. These results reveal critical vulnerabilities in current LLM safety mechanisms and underscore the urgent need for more robust defense strategies.

</details>


### [5] [Memory Poisoning Attack and Defense on Memory Based LLM-Agents](https://arxiv.org/abs/2601.05504)
*Balachandra Devarangadi Sunil,Isheeta Sinha,Piyush Maheshwari,Shantanu Todmal,Shreyan Malik,Shuchi Mishra*

Main category: cs.CR

TL;DR: 该论文系统评估了LLM智能体在电子健康记录场景中的记忆中毒攻击与防御机制，发现现实条件下攻击效果显著降低，并提出了基于信任评分的输入输出审核与记忆净化两种防御方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明记忆中毒攻击在理想条件下成功率很高，但缺乏对现实部署中攻击鲁棒性的研究，以及在电子健康记录等敏感应用场景中的有效防御机制。

Method: 通过系统实证评估，研究攻击鲁棒性的三个关键维度：初始记忆状态、提示数量和检索参数。在GPT-4o-mini、Gemini-2.0-Flash和Llama-3.1-8B-Instruct模型上使用MIMIC-III临床数据进行实验，并提出了两种防御机制：基于多正交信号的输入输出审核和基于时间衰减与模式过滤的信任感知记忆净化。

Result: 现实条件下（存在预先合法记忆）攻击效果显著降低。防御评估显示有效的记忆净化需要精细的信任阈值校准，以避免过度保守（拒绝所有条目）或过滤不足（遗漏微妙攻击）。

Conclusion: 研究为生产环境中记忆增强型LLM智能体的安全防护提供了重要基准和见解，强调了在现实条件下攻击鲁棒性的局限性以及防御机制需要精细调校的重要性。

Abstract: Large language model agents equipped with persistent memory are vulnerable to memory poisoning attacks, where adversaries inject malicious instructions through query only interactions that corrupt the agents long term memory and influence future responses. Recent work demonstrated that the MINJA (Memory Injection Attack) achieves over 95 % injection success rate and 70 % attack success rate under idealized conditions. However, the robustness of these attacks in realistic deployments and effective defensive mechanisms remain understudied. This work addresses these gaps through systematic empirical evaluation of memory poisoning attacks and defenses in Electronic Health Record (EHR) agents. We investigate attack robustness by varying three critical dimensions: initial memory state, number of indication prompts, and retrieval parameters. Our experiments on GPT-4o-mini, Gemini-2.0-Flash and Llama-3.1-8B-Instruct models using MIMIC-III clinical data reveal that realistic conditions with pre-existing legitimate memories dramatically reduce attack effectiveness. We then propose and evaluate two novel defense mechanisms: (1) Input/Output Moderation using composite trust scoring across multiple orthogonal signals, and (2) Memory Sanitization with trust-aware retrieval employing temporal decay and pattern-based filtering. Our defense evaluation reveals that effective memory sanitization requires careful trust threshold calibration to prevent both overly conservative rejection (blocking all entries) and insufficient filtering (missing subtle attacks), establishing important baselines for future adaptive defense mechanisms. These findings provide crucial insights for securing memory-augmented LLM agents in production environments.

</details>


### [6] [Blockchain Verifiable Proof of Quantum Supremacy as a Trigger for Quantum-Secure Signatures](https://arxiv.org/abs/2601.05534)
*Nicholas Papadopoulos*

Main category: cs.CR

TL;DR: 论文提出并实现了一个部署在以太坊上的智能合约，通过生成难以分解的大数难题来检测量子计算优势，并触发量子安全回退协议以保护区块链资产。


<details>
  <summary>Details</summary>
Motivation: 量子计算的进步威胁到经典密码学方案的安全性，一旦实现密码学量子优势，现有区块链安全标准将变得脆弱，可能导致资产被盗或欺诈。需要一种机制来检测量子优势并平滑过渡到量子安全标准。

Method: 设计并实现了一个以太坊智能合约，该合约能够概率性地生成难以分解的大数（无需秘密信息），创建经典计算难以解决的难题。该合约有两个功能：(1) 通过验证这些难题的解决方案来建立无信任、无偏见的密码学量子优势证明机制；(2) 在检测到密码学量子优势时触发量子安全回退协议来保护用户资金。

Result: 成功实现了能够生成经典计算难以分解的大数难题的智能合约，建立了量子优势检测机制，并设计了量子安全回退协议触发系统，为区块链向量子安全标准过渡提供了可行方案。

Conclusion: 该智能合约系统能够识别密码学漏洞，确保区块链资产在后量子时代的安全，为区块链向量子安全标准的平滑过渡提供了实用解决方案，同时避免了过早采用量子安全方案带来的额外成本和复杂性。

Abstract: Blockchain is a decentralized, distributed ledger technology that ensures transparency, security, and immutability through cryptographic techniques. However, advancements in quantum computing threaten the security of classical cryptographic schemes, jeopardizing blockchain integrity once cryptographic quantum supremacy is achieved. This milestone, defined here as the realization of quantum computers to solve practical cryptographic problems, would render existing security standards vulnerable, exposing blockchain assets (currency, data, etc.) to fraud and theft. To address this risk, we propose and implement a smart contract deployable on the Ethereum blockchain, having the ability to run applications on its blockchain, that generates classically intractable puzzles by probabilistically generating large, hard-to-factor numbers without requiring secret information. This contract then serves two purposes: to establish a mechanism (1) for a trustless, unbiased proof of cryptographic quantum supremacy by verifying solutions to these puzzles, and (2) to protect user funds on Ethereum by triggering quantum-secure fallback protocols upon detecting cryptographic quantum supremacy, since it is desirable to wait as long as possible to fall back to a quantum-secure scheme because of its inherent additional cost and complexity. These mechanisms demonstrate the ability to identify cryptographic vulnerabilities and ensure a smooth transition to quantum-secure standards, safeguarding blockchain assets in a post-quantum era.

</details>


### [7] [HogVul: Black-box Adversarial Code Generation Framework Against LM-based Vulnerability Detectors](https://arxiv.org/abs/2601.05587)
*Jingxiao Yang,Ping He,Tianyu Du,Sun Bing,Xuhong Zhang*

Main category: cs.CR

TL;DR: HogVul是一个基于粒子群优化的黑盒对抗代码生成框架，通过统一的双通道优化策略整合词汇和语法扰动，显著提高了对基于语言模型的漏洞检测器的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 当前基于语言模型的漏洞检测器容易受到利用词汇和语法扰动的对抗攻击，现有黑盒攻击方法主要依赖孤立的扰动策略，限制了在对抗代码空间中高效探索最优扰动的能力。

Method: 提出HogVul框架，采用基于粒子群优化的统一双通道优化策略，系统协调词汇和语法两个层面的扰动，有效扩展对抗样本的搜索空间。

Result: 在四个基准数据集上的实验表明，HogVul相比最先进的基线方法平均攻击成功率提高了26.05%。

Conclusion: 混合优化策略在暴露模型漏洞方面具有巨大潜力，HogVul框架为评估和改进基于语言模型的漏洞检测器的鲁棒性提供了有效工具。

Abstract: Recent advances in software vulnerability detection have been driven by Language Model (LM)-based approaches. However, these models remain vulnerable to adversarial attacks that exploit lexical and syntax perturbations, allowing critical flaws to evade detection. Existing black-box attacks on LM-based vulnerability detectors primarily rely on isolated perturbation strategies, limiting their ability to efficiently explore the adversarial code space for optimal perturbations. To bridge this gap, we propose HogVul, a black-box adversarial code generation framework that integrates both lexical and syntax perturbations under a unified dual-channel optimization strategy driven by Particle Swarm Optimization (PSO). By systematically coordinating two-level perturbations, HogVul effectively expands the search space for adversarial examples, enhancing the attack efficacy. Extensive experiments on four benchmark datasets demonstrate that HogVul achieves an average attack success rate improvement of 26.05\% over state-of-the-art baseline methods. These findings highlight the potential of hybrid optimization strategies in exposing model vulnerabilities.

</details>


### [8] [Continual Pretraining on Encrypted Synthetic Data for Privacy-Preserving LLMs](https://arxiv.org/abs/2601.05635)
*Honghao Liu,Xuhui Jiang,Chengjin Xu,Cehao Yang,Yiran Cheng,Lionel Ni,Jian Guo*

Main category: cs.CR

TL;DR: 提出基於實體的加密數據合成框架，用於保護隱私的持續預訓練LLM，通過確定性加密保護PII，在保持模型性能的同時確保敏感數據安全。


<details>
  <summary>Details</summary>
Motivation: 在小型領域特定語料庫上預訓練大型語言模型時，保護敏感數據中的隱私是一大挑戰。需要探索如何在持續預訓練中保護個人可識別信息(PII)。

Method: 提出基於實體的框架：1)構建加權實體圖指導數據合成；2)對PII實體應用確定性加密；3)通過解密密鑰授權訪問敏感數據，使LLM能夠通過持續預訓練編碼新知識。

Result: 在有限規模數據集上，預訓練模型優於基礎模型並確保PII安全，與未加密合成數據訓練的模型相比存在適度性能差距。增加實體數量和基於圖的合成可提升性能，加密模型保持指令跟隨能力。

Conclusion: 這項工作作為加密數據預訓練設計空間的初步探索，討論了確定性加密的安全影響和限制，為隱私保護LLM提供了有前景的方向。

Abstract: Preserving privacy in sensitive data while pretraining large language models on small, domain-specific corpora presents a significant challenge. In this work, we take an exploratory step toward privacy-preserving continual pretraining by proposing an entity-based framework that synthesizes encrypted training data to protect personally identifiable information (PII). Our approach constructs a weighted entity graph to guide data synthesis and applies deterministic encryption to PII entities, enabling LLMs to encode new knowledge through continual pretraining while granting authorized access to sensitive data through decryption keys. Our results on limited-scale datasets demonstrate that our pretrained models outperform base models and ensure PII security, while exhibiting a modest performance gap compared to models trained on unencrypted synthetic data. We further show that increasing the number of entities and leveraging graph-based synthesis improves model performance, and that encrypted models retain instruction-following capabilities with long retrieved contexts. We discuss the security implications and limitations of deterministic encryption, positioning this work as an initial investigation into the design space of encrypted data pretraining for privacy-preserving LLMs. Our code is available at https://github.com/DataArcTech/SoE.

</details>


### [9] [The Echo Chamber Multi-Turn LLM Jailbreak](https://arxiv.org/abs/2601.05742)
*Ahmad Alobaid,Martí Jordà Roca,Carlos Castillo,Joan Vendrell*

Main category: cs.CR

TL;DR: Echo Chamber是一种新型的多轮越狱攻击方法，通过逐步升级的方式绕过聊天机器人的安全防护机制


<details>
  <summary>Details</summary>
Motivation: 随着LLM聊天机器人的普及，安全挑战日益突出，特别是越狱攻击可能导致财务损失和声誉损害。多轮攻击作为一种新型越狱方式需要被深入研究。

Method: 提出Echo Chamber攻击方法，采用逐步升级策略，通过精心设计的交互链来绕过聊天机器人的安全防护机制。

Result: 通过广泛评估，Echo Chamber攻击在多个最先进模型上表现出色，证明了其有效性。

Conclusion: Echo Chamber作为一种新型多轮越狱攻击方法，突显了当前聊天机器人安全防护的脆弱性，需要加强防御措施。

Abstract: The availability of Large Language Models (LLMs) has led to a new generation of powerful chatbots that can be developed at relatively low cost. As companies deploy these tools, security challenges need to be addressed to prevent financial loss and reputational damage. A key security challenge is jailbreaking, the malicious manipulation of prompts and inputs to bypass a chatbot's safety guardrails. Multi-turn attacks are a relatively new form of jailbreaking involving a carefully crafted chain of interactions with a chatbot. We introduce Echo Chamber, a new multi-turn attack using a gradual escalation method. We describe this attack in detail, compare it to other multi-turn attacks, and demonstrate its performance against multiple state-of-the-art models through extensive evaluation.

</details>


### [10] [VIGIL: Defending LLM Agents Against Tool Stream Injection via Verify-Before-Commit](https://arxiv.org/abs/2601.05755)
*Junda Lin,Zhaomeng Zhou,Zhi Zheng,Shuochen Liu,Tong Xu,Yong Chen,Enhong Chen*

Main category: cs.CR

TL;DR: VIGIL框架通过验证前提交协议解决LLM代理在开放环境中的间接提示注入攻击，在保持推理灵活性的同时确保安全控制，显著降低攻击成功率并提高实用性。


<details>
  <summary>Details</summary>
Motivation: LLM代理在开放环境中面临间接提示注入攻击的升级风险，特别是在工具流中，被操纵的元数据和运行时反馈会劫持执行流程。现有防御方法面临关键困境：高级模型因严格对齐而优先执行注入规则，而静态保护机制则切断了自适应推理所需的反馈循环。

Method: 提出VIGIL框架，将范式从限制性隔离转向验证前提交协议。通过促进推测性假设生成和执行基于意图的验证来确保安全，在保持推理灵活性的同时实现稳健控制。还引入了SIREN基准测试，包含959个工具流注入案例，用于模拟具有动态依赖性的普遍威胁。

Result: 广泛实验表明，VIGIL优于最先进的动态防御方法，将攻击成功率降低了超过22%，同时与静态基线相比，在攻击下的实用性提高了一倍以上，实现了安全性和实用性的最佳平衡。

Conclusion: VIGIL框架成功解决了LLM代理在开放环境中面临的安全与实用性之间的权衡问题，通过验证前提交协议在保持推理灵活性的同时确保安全控制，为间接提示注入攻击提供了有效的防御解决方案。

Abstract: LLM agents operating in open environments face escalating risks from indirect prompt injection, particularly within the tool stream where manipulated metadata and runtime feedback hijack execution flow. Existing defenses encounter a critical dilemma as advanced models prioritize injected rules due to strict alignment while static protection mechanisms sever the feedback loop required for adaptive reasoning. To reconcile this conflict, we propose \textbf{VIGIL}, a framework that shifts the paradigm from restrictive isolation to a verify-before-commit protocol. By facilitating speculative hypothesis generation and enforcing safety through intent-grounded verification, \textbf{VIGIL} preserves reasoning flexibility while ensuring robust control. We further introduce \textbf{SIREN}, a benchmark comprising 959 tool stream injection cases designed to simulate pervasive threats characterized by dynamic dependencies. Extensive experiments demonstrate that \textbf{VIGIL} outperforms state-of-the-art dynamic defenses by reducing the attack success rate by over 22\% while more than doubling the utility under attack compared to static baselines, thereby achieving an optimal balance between security and utility. Code is available at https://anonymous.4open.science/r/VIGIL-378B/.

</details>


### [11] [Influence of Parallelism in Vector-Multiplication Units on Correlation Power Analysis](https://arxiv.org/abs/2601.05828)
*Manuel Brosch,Matthias Probst,Stefan Kögler,Georg Sigl*

Main category: cs.CR

TL;DR: 研究神经网络硬件加速器中并行处理对基于相关性的功耗侧信道攻击的影响，发现并行度增加会降低攻击成功率


<details>
  <summary>Details</summary>
Motivation: 随着神经网络在边缘设备中应用增加，设备物理可访问性带来了侧信道攻击的安全挑战。硬件加速器虽然提升推理性能，但其并行处理特性可能影响功耗侧信道攻击的效果，需要研究这种影响

Method: 研究全连接层中并行运行的神经元对功耗侧信道攻击的影响，分析并发乘加运算对总体功耗的理论影响和相关功耗分析的成功率。推导描述相关性随并行度增加而降低的方程，并在FPGA上实现的向量乘法单元进行验证

Result: 建立了描述相关性随并行度增加而降低的数学方程，通过FPGA实验验证了这些方程的适用性，证明了并行处理确实会降低基于相关性的功耗侧信道攻击的成功率

Conclusion: 神经网络硬件加速器的并行处理特性会降低基于相关性的功耗侧信道攻击的有效性，这为设计更安全的神经网络硬件实现提供了理论依据，有助于在性能和安全性之间取得平衡

Abstract: The use of neural networks in edge devices is increasing, which introduces new security challenges related to the neural networks' confidentiality. As edge devices often offer physical access, attacks targeting the hardware, such as side-channel analysis, must be considered. To enhance the performance of neural network inference, hardware accelerators are commonly employed. This work investigates the influence of parallel processing within such accelerators on correlation-based side-channel attacks that exploit power consumption. The focus is on neurons that are part of the same fully-connected layer, which run parallel and simultaneously process the same input value. The theoretical impact of concurrent multiply-and-accumulate operations on overall power consumption is evaluated, as well as the success rate of correlation power analysis. Based on the observed behavior, equations are derived that describe how the correlation decreases with increasing levels of parallelism. The applicability of these equations is validated using a vector-multiplication unit implemented on an FPGA.

</details>


### [12] [Secure Change-Point Detection for Time Series under Homomorphic Encryption](https://arxiv.org/abs/2601.05865)
*Federico Mazzone,Giorgio Micali,Massimiliano Pronesti*

Main category: cs.CR

TL;DR: 首个在加密时间序列上进行变点检测的方法，使用CKKS同态加密方案，无需解密数据即可检测统计特性变化，保持与明文相当的准确性，处理百万点数据仅需3分钟。


<details>
  <summary>Details</summary>
Motivation: 现有基于差分隐私的解决方案通过噪声注入会降低准确性，而加密时间序列上的变点检测需求日益增长，特别是在医疗和网络监控等隐私敏感领域。

Method: 采用CKKS同态加密方案，在加密状态下检测统计特性（如均值、方差、频率）的变化点，无需解密原始数据。

Result: 在合成数据集和真实世界时间序列（医疗和网络监控）上评估性能，保持与明文基准相当的实用性，处理百万点数据仅需3分钟。

Conclusion: 该方法首次实现了加密时间序列上的高效变点检测，在保护隐私的同时保持了实用性，为隐私敏感应用提供了可行的解决方案。

Abstract: We introduce the first method for change-point detection on encrypted time series. Our approach employs the CKKS homomorphic encryption scheme to detect shifts in statistical properties (e.g., mean, variance, frequency) without ever decrypting the data. Unlike solutions based on differential privacy, which degrade accuracy through noise injection, our solution preserves utility comparable to plaintext baselines. We assess its performance through experiments on both synthetic datasets and real-world time series from healthcare and network monitoring. Notably, our approach can process one million points within 3 minutes.

</details>


### [13] [Cybersecurity AI: A Game-Theoretic AI for Guiding Attack and Defense](https://arxiv.org/abs/2601.05887)
*Víctor Mayoral-Vilches,María Sanz-Gómez,Francesco Balassone,Stefan Rass,Lidia Salas-Espejo,Benjamin Jablonski,Luis Javier Navarrete-Lozano,Maite del Mundo de Torres,Cristóbal R. J. Veas Chavez*

Main category: cs.CR

TL;DR: G-CTR是一个游戏论指导层，通过提取攻击图、计算纳什均衡并提供简明摘要来指导LLM代理，显著提升AI渗透测试的成功率、一致性和成本效益。


<details>
  <summary>Details</summary>
Motivation: 当前AI驱动的渗透测试虽然能执行大量操作，但缺乏人类在竞争性安全中的战略直觉。为了构建超越人类最佳能力的网络安全超级智能，需要将这种战略直觉嵌入到代理推理过程中。

Method: 提出Generative Cut-the-Rope (G-CTR)方法：从代理上下文中提取攻击图，使用努力感知评分计算纳什均衡，并将简明摘要反馈到LLM循环中指导代理行动。

Result: 在五个真实世界练习中，G-CTR匹配了70-90%的专家图结构，运行速度比手动分析快60-245倍，成本降低140倍以上。在44次网络靶场测试中，成功率从20.0%提升到42.9%，每次成功成本降低2.7倍，行为方差减少5.2倍。在攻防演练中，共享摘要的紫色代理以约2:1战胜仅使用LLM的基线，以3.7:1战胜独立指导的团队。

Conclusion: 闭环指导通过减少歧义、压缩LLM搜索空间、抑制幻觉并将模型锚定到问题最相关部分，实现了在成功率、一致性和可靠性方面的重大突破。

Abstract: AI-driven penetration testing now executes thousands of actions per hour but still lacks the strategic intuition humans apply in competitive security. To build cybersecurity superintelligence --Cybersecurity AI exceeding best human capability-such strategic intuition must be embedded into agentic reasoning processes. We present Generative Cut-the-Rope (G-CTR), a game-theoretic guidance layer that extracts attack graphs from agent's context, computes Nash equilibria with effort-aware scoring, and feeds a concise digest back into the LLM loop \emph{guiding} the agent's actions. Across five real-world exercises, G-CTR matches 70--90% of expert graph structure while running 60--245x faster and over 140x cheaper than manual analysis. In a 44-run cyber-range, adding the digest lifts success from 20.0% to 42.9%, cuts cost-per-success by 2.7x, and reduces behavioral variance by 5.2x. In Attack-and-Defense exercises, a shared digest produces the Purple agent, winning roughly 2:1 over the LLM-only baseline and 3.7:1 over independently guided teams. This closed-loop guidance is what produces the breakthrough: it reduces ambiguity, collapses the LLM's search space, suppresses hallucinations, and keeps the model anchored to the most relevant parts of the problem, yielding large gains in success rate, consistency, and reliability.

</details>


### [14] [Agentic LLMs as Powerful Deanonymizers: Re-identification of Participants in the Anthropic Interviewer Dataset](https://arxiv.org/abs/2601.05918)
*Tianshi Li*

Main category: cs.CR

TL;DR: LLM智能体通过网页搜索和交叉引用，能够从Anthropic发布的科学家访谈数据集中重新识别出受访者身份，暴露出当前定性数据发布的安全风险。


<details>
  <summary>Details</summary>
Motivation: 随着Anthropic发布包含125名科学家访谈的公开数据集，研究旨在评估现代LLM智能体是否能够通过这些访谈内容重新识别出科学家的具体身份，揭示当前数据发布在AI时代的安全隐患。

Method: 使用现成的LLM工具（具备网页搜索和智能体能力），通过自然语言提示将重新识别攻击分解为多个良性任务，包括搜索网络、交叉引用细节和提出可能的匹配项。

Result: 在24个科学家访谈中，成功重新识别出6个访谈对应的具体科学工作、相关作者，在某些情况下甚至能唯一确定受访者身份，证明现代LLM智能体使此类重新识别攻击变得容易且低门槛。

Conclusion: LLM智能体时代下，发布丰富的定性数据存在严重的隐私风险，现有安全措施可能被绕过，需要制定新的缓解建议并解决相关开放性问题。

Abstract: On December 4, 2025, Anthropic released Anthropic Interviewer, an AI tool for running qualitative interviews at scale, along with a public dataset of 1,250 interviews with professionals, including 125 scientists, about their use of AI for research. Focusing on the scientist subset, I show that widely available LLMs with web search and agentic capabilities can link six out of twenty-four interviews to specific scientific works, recovering associated authors and, in some cases, uniquely identifying the interviewees. My contribution is to show that modern LLM-based agents make such re-identification attacks easy and low-effort: off-the-shelf tools can, with a few natural-language prompts, search the web, cross-reference details, and propose likely matches, effectively lowering the technical barrier. Existing safeguards can be bypassed by breaking down the re-identification into benign tasks. I outline the attack at a high level, discuss implications for releasing rich qualitative data in the age of LLM agents, and propose mitigation recommendations and open problems. I have notified Anthropic of my findings.

</details>


### [15] [CyberGFM: Graph Foundation Models for Lateral Movement Detection in Enterprise Networks](https://arxiv.org/abs/2601.05988)
*Isaiah J. King,Bernardo Trindade,Benjamin Bowman,H. Howie Huang*

Main category: cs.CR

TL;DR: 提出CyberGFM系统，将随机游走与Transformer基础模型结合，用于网络异常检测，在三个数据集上达到SOTA，AP提升最高达2倍。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：基于随机游走的方法无法利用丰富的边数据，而基于GNN的方法需要大量内存训练。需要结合随机游走的高效性和深度学习的丰富语义表示能力。

Method: 将随机游走类比为语料库中的句子，使用GPU优化的语言模型训练图基础模型来预测随机游走中的缺失标记，然后微调用于链路预测，作为网络异常检测器。

Result: 在三个广泛使用的网络异常检测数据集上达到最先进结果，平均精度最高提升2倍，在无监督链路预测中优于所有先前工作，参数数量相同，效率相当或更好。

Conclusion: CyberGFM成功结合了随机游走方法的效率和深度学习方法的丰富语义表示能力，为网络异常检测提供了高效有效的解决方案。

Abstract: Representing networks as a graph and training a link prediction model using benign connections is an effective method of anomaly-based intrusion detection. Existing works using this technique have shown great success using temporal graph neural networks and skip-gram-based approaches on random walks. However, random walk-based approaches are unable to incorporate rich edge data, while the GNN-based approaches require large amounts of memory to train. In this work, we propose extending the original insight from random walk-based skip-grams--that random walks through a graph are analogous to sentences in a corpus--to the more modern transformer-based foundation models. Using language models that take advantage of GPU optimizations, we can quickly train a graph foundation model to predict missing tokens in random walks through a network of computers. The graph foundation model is then finetuned for link prediction and used as a network anomaly detector. This new approach allows us to combine the efficiency of random walk-based methods and the rich semantic representation of deep learning methods. This system, which we call CyberGFM, achieved state-of-the-art results on three widely used network anomaly detection datasets, delivering a up to 2$\times$ improvement in average precision. We found that CyberGFM outperforms all prior works in unsupervised link prediction for network anomaly detection, using the same number of parameters, and with equal or better efficiency than the previous best approaches.

</details>
