{"id": "2509.10482", "categories": ["cs.CR", "cs.AI", "D.4.6; I.2.7; K.6.5"], "pdf": "https://arxiv.org/pdf/2509.10482", "abs": "https://arxiv.org/abs/2509.10482", "authors": ["Matthew Grofsky"], "title": "AegisShield: Democratizing Cyber Threat Modeling with Generative AI", "comment": "Master's thesis", "summary": "The increasing sophistication of technology systems makes traditional threat\nmodeling hard to scale, especially for small organizations with limited\nresources. This paper develops and evaluates AegisShield, a generative AI\nenhanced threat modeling tool that implements STRIDE and MITRE ATT&CK to\nautomate threat generation and provide systematic assessments. By integrating\nreal time threat intelligence from the National Vulnerability Database and\nAlienVault Open Threat Exchange, AegisShield produces streamlined and\naccessible threat descriptions. Our assessment of 243 threats from 15 case\nstudies and over 8000 AI generated threats shows that AegisShield reduces\ncomplexity (p less than 0.001), yields outputs semantically aligned with expert\ndeveloped threats (p less than 0.05), and achieves an 85.4 percent success rate\nin mapping threats to MITRE ATT&CK techniques (p less than 0.001). Automating\nand standardizing threat modeling helps under resourced organizations address\nrisk earlier and supports wider adoption of secure by design practices."}
{"id": "2509.10488", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.10488", "abs": "https://arxiv.org/abs/2509.10488", "authors": ["Trueye Tafese"], "title": "Turning CVEs into Educational Labs:Insights and Challenges", "comment": "20 pages, 5 figures, git hub files to run on your enivronment, step\n  by step tutorial, survey results", "summary": "This research focuses on transforming CVEs to hands-on educational lab for\ncybersecurity training. The study shows the practical application of CVEs by\ndeveloping containerized lab environments- Docker to simulate real-world\nvulnerabilities like SQL Injection, arbitrary code execution, and improper SSL\ncertificate validation. These labs has structured tutorials, pre- and\npost-surveys to evaluate learning outcomes, and remediation steps.Key\nchallenges included interpreting limited CVE data, resolving technical\ncomplexities in lab design, and ensuring accessibility for diverse learners.\nDespite these difficulties, the findings highlight the use of educational\nbenefits of vulnerability analysis, bridging theoretical concepts with hands-on\nexperience. The results indicate that students improved comprehension of\ncybersecurity principles, threat mitigation techniques, and secure coding\npractices. This innovative approach provides a scalable and reproducible model\nfor integrating CVEs into cybersecurity education, fostering a deeper\nunderstanding of real-world security challenges in a controlled and safe\nenvironment."}
{"id": "2509.10492", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.10492", "abs": "https://arxiv.org/abs/2509.10492", "authors": ["Philip Laryea Doku"], "title": "Investigation Of The Distinguishability Of Giraud-Verneuil Atomic Blocks", "comment": "Master Thesis", "summary": "In this work, we investigate the security of Elliptic Curve Cryptosystem\n(ECC) implementations against Side-Channel Analysis (SCA). ECC is well known\nfor its efficiency and strong security, yet vulnerable to SCA which exploits\nphysical information leaked during scalar multiplication (kP). Countermeasures\nsuch as regularity and atomicity exist; this thesis focuses on atomicity. In\nthis work, we study the Giraud and Verneuil atomic pattern for kP, implementing\nit using the right-to-left kP algorithm on the NIST EC P-256 curve. We use the\nFLECC library with constant-time operations and execute on the Texas\nInstruments LAUNCHXLF28379D MCU. We measure Electromagnetic (EM) emissions\nduring kP using a Lecroy WavePro 604HD Oscilloscope, a Langer ICS 105\nIntegrated Circuit Scanner, and a Langer MFA-R 0.2-75 Near Field Probe. We\ninvestigate whether the Giraud and Verneuil atomic blocks are distinguishable\nin EM traces. Our findings show that, when additional clock cycle processes are\npresent, the atomic blocks can be visually distinguished; after removing these\nprocesses, they become more synchronised and harder to distinguish, reducing\nthe risk of a successful SCA attack. These results show that, although the\natomic pattern is correctly implemented with dummy operations, resistance to\nSCA can still be affected by additional processes inserted at hardware or\nsoftware level.This means atomicity alone may not fully protect ECC from SCA.\nMore research is needed to investigate the causes of the additional clock cycle\nprocesses and how intermediate operations are addressed in memory registers.\nThis will help to understand the processes that lead to the insertion of these\nadditional clock cycles. This thesis is the first to experimentally implement\nand investigate Giraud and Verneuil's atomic pattern on hardware, and it offers\nuseful results to improve countermeasures against SCA."}
{"id": "2509.10540", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10540", "abs": "https://arxiv.org/abs/2509.10540", "authors": ["Pavan Reddy", "Aditya Sanjay Gujral"], "title": "EchoLeak: The First Real-World Zero-Click Prompt Injection Exploit in a Production LLM System", "comment": "8 pages content, 1 page references, 2 figures, Published at AAAI Fall\n  Symposium Series 2025", "summary": "Large language model (LLM) assistants are increasingly integrated into\nenterprise workflows, raising new security concerns as they bridge internal and\nexternal data sources. This paper presents an in-depth case study of EchoLeak\n(CVE-2025-32711), a zero-click prompt injection vulnerability in Microsoft 365\nCopilot that enabled remote, unauthenticated data exfiltration via a single\ncrafted email. By chaining multiple bypasses-evading Microsofts XPIA (Cross\nPrompt Injection Attempt) classifier, circumventing link redaction with\nreference-style Markdown, exploiting auto-fetched images, and abusing a\nMicrosoft Teams proxy allowed by the content security policy-EchoLeak achieved\nfull privilege escalation across LLM trust boundaries without user interaction.\nWe analyze why existing defenses failed, and outline a set of engineering\nmitigations including prompt partitioning, enhanced input/output filtering,\nprovenance-based access control, and strict content security policies. Beyond\nthe specific exploit, we derive generalizable lessons for building secure AI\ncopilots, emphasizing the principle of least privilege, defense-in-depth\narchitectures, and continuous adversarial testing. Our findings establish\nprompt injection as a practical, high-severity vulnerability class in\nproduction AI systems and provide a blueprint for defending against future\nAI-native threats."}
{"id": "2509.10543", "categories": ["cs.CR", "cs.AI", "cs.LG", "68M12, 68T07", "C.2.0; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.10543", "abs": "https://arxiv.org/abs/2509.10543", "authors": ["Landon Bragg", "Nathan Dorsey", "Josh Prior", "John Ajit", "Ben Kim", "Nate Willis", "Pablo Rivas"], "title": "Robust DDoS-Attack Classification with 3D CNNs Against Adversarial Methods", "comment": "The 27th International Conference on Artificial Intelligence\n  (ICAI'25)", "summary": "Distributed Denial-of-Service (DDoS) attacks remain a serious threat to\nonline infrastructure, often bypassing detection by altering traffic in subtle\nways. We present a method using hive-plot sequences of network data and a 3D\nconvolutional neural network (3D CNN) to classify DDoS traffic with high\naccuracy. Our system relies on three main ideas: (1) using spatio-temporal\nhive-plot encodings to set a pattern-recognition baseline, (2) applying\nadversarial training with FGSM and PGD alongside spatial noise and image\nshifts, and (3) analyzing frame-wise predictions to find early signals. On a\nbenchmark dataset, our method lifts adversarial accuracy from 50-55% to over\n93% while maintaining clean-sample performance. Frames 3-4 offer strong\npredictive signals, showing early-stage classification is possible."}
{"id": "2509.10545", "categories": ["cs.CR", "cs.IR", "E.1, H.3.3, E.3", "E.1; H.3.3; E.3"], "pdf": "https://arxiv.org/pdf/2509.10545", "abs": "https://arxiv.org/abs/2509.10545", "authors": ["Ruwanga Konara", "Kasun De Zoysa", "Asanka Sayakkara"], "title": "Decentralized Identity Management on Ripple: A Conceptual Framework for High-Speed, Low-Cost Identity Transactions in Attestation-Based Attribute-Based Identity", "comment": null, "summary": "Recent years have seen many industrial implementations and much scholastic\nresearch, i.e., prototypes and theoretical frameworks, in Decentralized\nIdentity Management Systems (DIDMS). It is safe to say that Attestation-Based\nAttribute-Based Decentralized IDM (ABABDIDM) has not received anywhere near the\nsame level of attention in the literature as general Attribute-Based DIDMs\n(ABDIDM), i.e, decentralized Attribute-Based Access Control (ABAC). The use of\ndecentralization, i.e., DIDM, is to improve upon the security and\nprivacy-related issues of centralized Identity Management Systems (IDM) and\nAttribute-Based IDMs (ABIDM). And blockchain is the framework used for\ndecentralization in all these schemes. Many DIDMs - even ABDIDMs - have been\ndefined on popular blockchains such as Hyperledger, Ethereum, and Bitcoin.\nHowever, despite the characteristics of Ripple that makes it appealing for an\nABIDM, there is a lack of research to develop an Identity Management System\n(IDMS) on Ripple in literature. We have attempted to conceptualize an ABABDIDM\non Ripple."}
{"id": "2509.10550", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10550", "abs": "https://arxiv.org/abs/2509.10550", "authors": ["Shivam Akhauri"], "title": "Auditable Early Stopping for Agentic Routing: Ledger-Verified Run-Wise Certificates under Local DP", "comment": null, "summary": "In production tool-use agents (e.g., retrieval $\\to$ summarization $\\to$\ncalculator), routers must know when to stop exploring while preserving local DP\nand leaving an auditable trail. We present run-wise early-stopping certificates\nfor perturb-and-MAP (PaM) best-first search on context-indexed prefix DAGs\nwhose children partition the leaves. We couple realized path scores and pruning\nkeys to a single exponential race realized lazily via offset propagation. With\nexact leaf counts $N(v)$, lazy reuse at winners and independent residuals yield\nan Exact mode with a sound halting rule based on Key$(v) = M_tau(v) - \\log\nt(v)$, where $t(v)$ is the minimum arrival time among leaves under $v$. With\nonly upper bounds $N_{ub} \\ge N$, a Surrogate mode uses a parent-anchored\nsurrogate race without winner reuse; because $-\\log \\hat t \\ge -\\log t$, the\nfrontier invariant holds and stopping remains sound. We add a compiler from\nshared-node DAGs to prefix DAGs, local finiteness checks, a SuffixCountDP\nroutine for exact counts with safe downgrades, a validator-side tightening term\n$\\kappa = \\log(N/N_{ub})$, and an auditable ledger/validator that replays runs\ndeterministically. We also give an absolute LogSumExp tail bound, an acyclicity\ncertificate, and a fallback PRF-per-leaf scheme (NoCert) whose work matches a\nrealized-score best-first baseline up to a small per-node overhead. Finally, we\nintegrate a price/latency/$(\\epsilon, \\delta)$-aware multi-LLM controller and\nDP-trained LoRA adapters chosen at runtime; these choices do not affect the\ntwo-mode frontier invariants. We report Mac/commodity-hardware reproducible\nresults, a small real tool-use pipeline, and validator-checked audit trails,\nwith code and ledgers provided."}
{"id": "2509.10551", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.10551", "abs": "https://arxiv.org/abs/2509.10551", "authors": ["Amal Raj", "Vivek Balachandran"], "title": "A Hybrid Encryption Framework Combining Classical, Post-Quantum, and QKD Methods", "comment": "This version corrects an error in a table entry compared to the\n  accepted Springer version", "summary": "This paper introduces a hybrid encryption framework combining classical\ncryptography (EdDSA, ECDH), post-quantum cryptography (ML-DSA-6x5, ML-KEM-768),\nand Quantum Key Distribution (QKD) via Guardian to counter quantum computing\nthreats. Our prototype implements this integration, using a key derivation\nfunction to generate secure symmetric and HMAC keys, and evaluates its\nperformance across execution time and network metrics. The approach improves\ndata protection by merging classical efficiency with PQC's quantum resilience\nand QKD's key security, offering a practical transition path for cryptographic\nsystems. This research lays the foundation for future adoption of PQC in\nsecuring digital communication."}
{"id": "2509.10561", "categories": ["cs.CR", "cs.AI", "68Q25, 68T50, 68P27", "F.2.2; I.2.7; K.4.1"], "pdf": "https://arxiv.org/pdf/2509.10561", "abs": "https://arxiv.org/abs/2509.10561", "authors": ["Madhava Gaikwad"], "title": "AVEC: Bootstrapping Privacy for Local LLMs", "comment": "12 pages", "summary": "This position paper presents AVEC (Adaptive Verifiable Edge Control), a\nframework for bootstrapping privacy for local language models by enforcing\nprivacy at the edge with explicit verifiability for delegated queries. AVEC\nintroduces an adaptive budgeting algorithm that allocates per-query\ndifferential privacy parameters based on sensitivity, local confidence, and\nhistorical usage, and uses verifiable transformation with on-device integrity\nchecks. We formalize guarantees using R\\'enyi differential privacy with\nodometer-based accounting, and establish utility ceilings, delegation-leakage\nbounds, and impossibility results for deterministic gating and hash-only\ncertification. Our evaluation is simulation-based by design to study mechanism\nbehavior and accounting; we do not claim deployment readiness or task-level\nutility with live LLMs. The contribution is a conceptual architecture and\ntheoretical foundation that chart a pathway for empirical follow-up on\nprivately bootstrapping local LLMs."}
{"id": "2509.10563", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.10563", "abs": "https://arxiv.org/abs/2509.10563", "authors": ["Mohammed Yacoubi", "Omar Moussaoui", "C. Drocourt"], "title": "Enhancing IoMT Security with Explainable Machine Learning: A Case Study on the CICIOMT2024 Dataset", "comment": null, "summary": "Explainable Artificial Intelligence (XAI) enhances the transparency and\ninterpretability of AI models, addressing their inherent opacity. In\ncybersecurity, particularly within the Internet of Medical Things (IoMT), the\nblack-box nature of AI-driven threat detection poses a significant challenge.\nCybersecurity professionals must not only detect attacks but also understand\nthe reasoning behind AI decisions to ensure trust and accountability. The rapid\nincrease in cyberattacks targeting connected medical devices threatens patient\nsafety and data privacy, necessitating advanced AI-driven solutions. This study\ncompares two ensemble learning techniques, bagging and boosting, for\ncyber-attack classification in IoMT environments. We selected Random Forest for\nbagging and CatBoost for boosting. Random Forest helps reduce variance, while\nCatBoost improves bias by combining weak classifiers into a strong ensemble\nmodel, making them effective for detecting sophisticated attacks. However,\ntheir complexity often reduces transparency, making it difficult for\ncybersecurity professionals to interpret and trust their decisions. To address\nthis issue, we apply XAI models to generate local and global explanations,\nproviding insights into AI decision-making. Using techniques like SHAP (Shapley\nAdditive Explanations) and LIME (Local Interpretable Model-agnostic\nExplanations), we highlight feature importance to help stakeholders understand\nthe key factors driving cyber threat detection."}
{"id": "2509.10568", "categories": ["cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.10568", "abs": "https://arxiv.org/abs/2509.10568", "authors": ["Muhammad M. Roomi", "Suhail S. M. Hussain", "Daisuke Mashima"], "title": "SG-ML: Smart Grid Cyber Range Modelling Language", "comment": "28 pages, 38 figures, 3 tables", "summary": "This work provides a detailed specification of the Smart Grid Modelling\nLanguage (SG-ML), which is designed for the automated generation of smart grid\ncyber ranges. SG-ML is defined as a set of XML schemas that describe a smart\ngrid's configuration in both machine-readable and human-friendly ways, thereby\nbridging the gap between system modelling and automated deployment. Unlike\nprior ad-hoc approaches to cyber range design, SG-ML provides a unified\nmethodology that integrates both power system and cyber network\nrepresentations. The SG-ML model can be customized by users to meet specific\nrequirements, such as emulating physical or cyber topologies and configuring\nnetwork devices. An SG-ML Processor then parses this configured model to\ninstantiate the cyber range environment. The modelling language leverages\nestablished standards like the IEC 61850 Substation Configuration Language\n(SCL) and IEC 61131 PLCopen XML to define power system topology, cyber network\ntopology, and device configurations. This approach allows for the reuse of\nexisting assets, reducing the effort needed to create the SG-ML model. To\naddress gaps not covered by these standards such as attack injection\nparameters, scenario-specific metadata, and additional network constraints,\nSG-ML introduces proprietary schemas that complement standard models. Overall,\nSG-ML enables reproducible, scalable, and automated generation of realistic\nsmart grid cyber ranges for research, training, and security assessment."}
{"id": "2509.10569", "categories": ["cs.CR", "cs.AI", "cs.MM", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.10569", "abs": "https://arxiv.org/abs/2509.10569", "authors": ["Leyi Pan", "Sheng Guan", "Zheyu Fu", "Luyang Si", "Zian Wang", "Xuming Hu", "Irwin King", "Philip S. Yu", "Aiwei Liu", "Lijie Wen"], "title": "MarkDiffusion: An Open-Source Toolkit for Generative Watermarking of Latent Diffusion Models", "comment": "23 pages, 13 figures, 5 tables", "summary": "We introduce MarkDiffusion, an open-source Python toolkit for generative\nwatermarking of latent diffusion models. It comprises three key components: a\nunified implementation framework for streamlined watermarking algorithm\nintegrations and user-friendly interfaces; a mechanism visualization suite that\nintuitively showcases added and extracted watermark patterns to aid public\nunderstanding; and a comprehensive evaluation module offering standard\nimplementations of 24 tools across three essential aspects - detectability,\nrobustness, and output quality - plus 8 automated evaluation pipelines. Through\nMarkDiffusion, we seek to assist researchers, enhance public awareness and\nengagement in generative watermarking, and promote consensus while advancing\nresearch and applications."}
{"id": "2509.10573", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.10573", "abs": "https://arxiv.org/abs/2509.10573", "authors": ["Christophe Parisel"], "title": "Directionality of the Voynich Script", "comment": null, "summary": "While the Voynich Manuscript was almost certainly written left-to-right\n(LTR), the question whether the underlying script or cipher reads LTR or\nright-to-left (RTL) has received little quantitative attention. We introduce a\nstatistical method that leverages n-gram perplexity asymmetry to determine\ndirectional bias in character sequences."}
{"id": "2509.10577", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10577", "abs": "https://arxiv.org/abs/2509.10577", "authors": ["Danilo Francati", "Yevin Nikhel Goonatilake", "Shubham Pawar", "Daniele Venturi", "Giuseppe Ateniese"], "title": "The Coding Limits of Robust Watermarking for Generative Models", "comment": null, "summary": "We prove a sharp threshold for the robustness of cryptographic watermarking\nfor generative models. This is achieved by introducing a coding abstraction,\nwhich we call messageless secret-key codes, that formalizes sufficient and\nnecessary requirements of robust watermarking: soundness, tamper detection, and\npseudorandomness. Thus, we establish that robustness has a precise limit: For\nbinary outputs no scheme can survive if more than half of the encoded bits are\nmodified, and for an alphabet of size q the corresponding threshold is\n$(1-1/q)$ of the symbols.\n  Complementing this impossibility, we give explicit constructions that meet\nthe bound up to a constant slack. For every ${\\delta} > 0$, assuming\npseudorandom functions and access to a public counter, we build linear-time\ncodes that tolerate up to $(1/2)(1-{\\delta})$ errors in the binary case and\n$(1-1/q)(1-{\\delta})$ errors in the $q$-ary case. Together with the lower\nbound, these yield the maximum robustness achievable under standard\ncryptographic assumptions.\n  We then test experimentally whether this limit appears in practice by looking\nat the recent watermarking for images of Gunn, Zhao, and Song (ICLR 2025). We\nshow that a simple crop and resize operation reliably flipped about half of the\nlatent signs and consistently prevented belief-propagation decoding from\nrecovering the codeword, erasing the watermark while leaving the image visually\nintact.\n  These results provide a complete characterization of robust watermarking,\nidentifying the threshold at which robustness fails, constructions that achieve\nit, and an experimental confirmation that the threshold is already reached in\npractice."}
{"id": "2509.10581", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.10581", "abs": "https://arxiv.org/abs/2509.10581", "authors": ["Prokash Barman", "Ratul Chowdhury", "Banani Saha"], "title": "Multi-channel secure communication framework for wireless IoT (MCSC-WoT): enhancing security in Internet of Things", "comment": null, "summary": "In modern smart systems, the convergence of the Internet of Things (IoT) and\nWireless of Things (WoT) have been revolutionized by offering a broad level of\nwireless connectivity and communication among various devices. Hitherto, this\ngreater interconnectivity poses important security problems, including the\nquestion of how to securely interconnect different networks, preserve secure\ncommunication channels, and maintain data integrity. However, the traditional\ncryptographic method and frequency hopping technique, although they provide\nsome protection, are not sufficient to defend against Man-In-The-Middle,\njamming, and replay attacks. In addition, synchronization issues in\nmulti-channel communication systems result in increased latency and energy\nconsumption, which make them unsuitable for resource-constrained IoT and WoT\ndevices. This work presents the Multi-Channel Secure Communication (MCSC)\nframework, which integrates advanced cryptographic protocols with dynamic\nchannel-hopping strategies to enhance security with reduced synchronization\noverhead. The MCSC framework maximizes the critical performance metrics, such\nas packet delivery ratio, latency, throughput, and energy efficiency, and\nfulfills the specific requirements of the IoT and WoT networks. A comprehensive\ncomparison of MCSC with well-established methods, including Frequency Hop\nSpread Spectrum, single channel Advanced Encryption Standard, and various\nElliptic Curve Cryptography-based schemes, indicates that MCSC has lower error\nrates and is more resilient to a wider range of cyber attacks. The efficiency\nof the proposed solution to secure IoT and WoT networks without compromising\nthe operational performance is validated under various interference conditions."}
{"id": "2509.10655", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.10655", "abs": "https://arxiv.org/abs/2509.10655", "authors": ["Charankumar Akiri", "Harrison Simpson", "Kshitiz Aryal", "Aarav Khanna", "Maanak Gupta"], "title": "Safety and Security Analysis of Large Language Models: Risk Profile and Harm Potential", "comment": null, "summary": "While the widespread deployment of Large Language Models (LLMs) holds great\npotential for society, their vulnerabilities to adversarial manipulation and\nexploitation can pose serious safety, security, and ethical risks. As new\nthreats continue to emerge, it becomes critically necessary to assess the\nlandscape of LLMs' safety and security against evolving adversarial prompt\ntechniques. To understand the behavior of LLMs, this research provides an\nempirical analysis and risk profile of nine prominent LLMs, Claude Opus 4,\nDeepSeek V3 (both open-source and online), Gemini 2.5 Flash, GPT-4o, Grok 3,\nLlama 4 Scout, Mistral 7B, and Qwen 3 1.7B, against 24 different security and\nsafety categories. These LLMs are evaluated on their ability to produce harmful\nresponses for adversarially crafted prompts (dataset has been made public) for\na broad range of safety and security topics, such as promotion of violent\ncriminal behavior, promotion of non-violent criminal activity, societal harms\nrelated to safety, illegal sexual content, dangerous code generation, and\ncybersecurity threats beyond code. Our study introduces the Risk Severity Index\n(RSI), an agile and scalable evaluation score, to quantify and compare the\nsecurity posture and creating a risk profile of LLMs. As the LLM development\nlandscape progresses, the RSI is intended to be a valuable metric for comparing\nthe risks of LLMs across evolving threats. This research finds widespread\nvulnerabilities in the safety filters of the LLMs tested and highlights the\nurgent need for stronger alignment, responsible deployment practices, and model\ngovernance, particularly for open-access and rapidly iterated models."}
{"id": "2509.10682", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10682", "abs": "https://arxiv.org/abs/2509.10682", "authors": ["Vitor Hugo Galhardo Moia", "Igor Jochem Sanz", "Gabriel Antonio Fontes Rebello", "Rodrigo Duarte de Meneses", "Briland Hitaj", "Ulf Lindqvist"], "title": "LLM in the Middle: A Systematic Review of Threats and Mitigations to Real-World LLM-based Systems", "comment": "37 pages, 8 figures, 13 tables", "summary": "The success and wide adoption of generative AI (GenAI), particularly large\nlanguage models (LLMs), has attracted the attention of cybercriminals seeking\nto abuse models, steal sensitive data, or disrupt services. Moreover, providing\nsecurity to LLM-based systems is a great challenge, as both traditional threats\nto software applications and threats targeting LLMs and their integration must\nbe mitigated. In this survey, we shed light on security and privacy concerns of\nsuch LLM-based systems by performing a systematic review and comprehensive\ncategorization of threats and defensive strategies considering the entire\nsoftware and LLM life cycles. We analyze real-world scenarios with distinct\ncharacteristics of LLM usage, spanning from development to operation. In\naddition, threats are classified according to their severity level and to which\nscenarios they pertain, facilitating the identification of the most relevant\nthreats. Recommended defense strategies are systematically categorized and\nmapped to the corresponding life cycle phase and possible attack strategies\nthey attenuate. This work paves the way for consumers and vendors to understand\nand efficiently mitigate risks during integration of LLMs in their respective\nsolutions or organizations. It also enables the research community to benefit\nfrom the discussion of open challenges and edge cases that may hinder the\nsecure and privacy-preserving adoption of LLM-based systems."}
{"id": "2509.10691", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10691", "abs": "https://arxiv.org/abs/2509.10691", "authors": ["Fardin Jalil Piran", "Zhiling Chen", "Yang Zhang", "Qianyu Zhou", "Jiong Tang", "Farhad Imani"], "title": "Privacy-Preserving Decentralized Federated Learning via Explainable Adaptive Differential Privacy", "comment": "21 pages", "summary": "Decentralized federated learning faces privacy risks because model updates\ncan leak data through inference attacks and membership inference, a concern\nthat grows over many client exchanges. Differential privacy offers principled\nprotection by injecting calibrated noise so confidential information remains\nsecure on resource-limited IoT devices. Yet without transparency, black-box\ntraining cannot track noise already injected by previous clients and rounds,\nwhich forces worst-case additions and harms accuracy. We propose PrivateDFL, an\nexplainable framework that joins hyperdimensional computing with differential\nprivacy and keeps an auditable account of cumulative noise so each client adds\nonly the difference between the required noise and what has already been\naccumulated. We evaluate on MNIST, ISOLET, and UCI-HAR to span image, signal,\nand tabular modalities, and we benchmark against transformer-based and deep\nlearning-based baselines trained centrally with Differentially Private\nStochastic Gradient Descent (DP-SGD) and Renyi Differential Privacy (RDP).\nPrivateDFL delivers higher accuracy, lower latency, and lower energy across IID\nand non-IID partitions while preserving formal (epsilon, delta) guarantees and\noperating without a central server. For example, under non-IID partitions,\nPrivateDFL achieves 24.42% higher accuracy than the Vision Transformer on MNIST\nwhile using about 10x less training time, 76x lower inference latency, and 11x\nless energy, and on ISOLET it exceeds Transformer accuracy by more than 80%\nwith roughly 10x less training time, 40x lower inference latency, and 36x less\ntraining energy. Future work will extend the explainable accounting to\nadversarial clients and adaptive topologies with heterogeneous privacy budgets."}
{"id": "2509.10703", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.10703", "abs": "https://arxiv.org/abs/2509.10703", "authors": ["Seonghun Son", "Chandrika Mukherjee", "Reham Mohamed Aburas", "Berk Gulmezoglu", "Z. Berkay Celik"], "title": "Side-channel Inference of User Activities in AR/VR Using GPU Profiling", "comment": "Accepted to the 2026 Network and Distributed System Security (NDSS)\n  Symposium", "summary": "Over the past decade, AR/VR devices have drastically changed how we interact\nwith the digital world. Users often share sensitive information, such as their\nlocation, browsing history, and even financial data, within third-party apps\ninstalled on these devices, assuming a secure environment protected from\nmalicious actors. Recent research has revealed that malicious apps can exploit\nsuch capabilities and monitor benign apps to track user activities, leveraging\nfine-grained profiling tools, such as performance counter APIs. However,\napp-to-app monitoring is not feasible on all AR/VR devices (e.g., Meta Quest),\nas a concurrent standalone app execution is disabled. In this paper, we present\nOVRWatcher, a novel side-channel primitive for AR/VR devices that infers user\nactivities by monitoring low-resolution (1Hz) GPU usage via a background\nscript, unlike prior work that relies on high-resolution profiling. OVRWatcher\ncaptures correlations between GPU metrics and 3D object interactions under\nvarying speeds, distances, and rendering scenarios, without requiring\nconcurrent app execution, access to application data, or additional SDK\ninstallations. We demonstrate the efficacy of OVRWatcher in fingerprinting both\nstandalone AR/VR and WebXR applications. OVRWatcher also distinguishes virtual\nobjects, such as products in immersive shopping apps selected by real users and\nthe number of participants in virtual meetings, thereby revealing users'\nproduct preferences and potentially exposing confidential information from\nthose meetings. OVRWatcher achieves over 99% accuracy in app fingerprinting and\nover 98% accuracy in object-level inference."}
{"id": "2509.10709", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.10709", "abs": "https://arxiv.org/abs/2509.10709", "authors": ["Shama Maganur", "Yili Jiang", "Jiaqi Huang", "Fangtian Zhong"], "title": "Feature-Centric Approaches to Android Malware Analysis: A Survey", "comment": null, "summary": "Sophisticated malware families exploit the openness of the Android platform\nto infiltrate IoT networks, enabling large-scale disruption, data exfiltration,\nand denial-of-service attacks. This systematic literature review (SLR) examines\ncutting-edge approaches to Android malware analysis with direct implications\nfor securing IoT infrastructures. We analyze feature extraction techniques\nacross static, dynamic, hybrid, and graph-based methods, highlighting their\ntrade-offs: static analysis offers efficiency but is easily evaded through\nobfuscation; dynamic analysis provides stronger resistance to evasive behaviors\nbut incurs high computational costs, often unsuitable for lightweight IoT\ndevices; hybrid approaches balance accuracy with resource considerations; and\ngraph-based methods deliver superior semantic modeling and adversarial\nrobustness. This survey contributes a structured comparison of existing\nmethods, exposes research gaps, and outlines a roadmap for future directions to\nenhance scalability, adaptability, and long-term security in IoT-driven Android\nmalware detection."}
{"id": "2509.10727", "categories": ["cs.CR", "H.1"], "pdf": "https://arxiv.org/pdf/2509.10727", "abs": "https://arxiv.org/abs/2509.10727", "authors": ["Luigi Logrippo"], "title": "Security theory for data flow and access control: From partial orders to lattices and back, a half-century trip", "comment": "7 pages, 1 figure", "summary": "The multi level Bell La Padula model for secure data access and data flow\ncontrol, formulated in the 1970s, was based on the theory of partial orders.\nSince then, another model, based on lattice theory, has prevailed. We present\nreasons why the partial order model is more appropriate. We also show, by\nexample, how non lattice data flow networks can be easily implemented by using\nAttribute-based access control (ABAC)."}
{"id": "2509.10755", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.10755", "abs": "https://arxiv.org/abs/2509.10755", "authors": ["Zhongtang Luo", "Jianting Zhang", "Akshat Neerati", "Aniket Kate"], "title": "Five Minutes of DDoS Brings down Tor: DDoS Attacks on the Tor Directory Protocol and Mitigations", "comment": null, "summary": "The Tor network offers network anonymity to its users by routing their\ntraffic through a sequence of relays. A group of nine directory authorities\nmaintains information about all available relay nodes using a distributed\ndirectory protocol. We observe that the current protocol makes a steep\nsynchrony assumption, which makes it vulnerable to natural as well as\nadversarial non-synchronous communication scenarios over the Internet. In this\npaper, we show that it is possible to cause a failure in the Tor directory\nprotocol by targeting a majority of the authorities for only five minutes using\na well-executed distributed denial-of-service (DDoS) attack. We demonstrate\nthis attack in a controlled environment and show that it is cost-effective for\nas little as \\$53.28 per month to disrupt the protocol and to effectively bring\ndown the entire Tor network. To mitigate this problem, we consider the popular\npartial synchrony assumption for the Tor directory protocol that ensures that\nthe protocol security is hampered even when the network delays are large and\nunknown. We design a new Tor directory protocol that leverages any standard\npartial-synchronous consensus protocol to solve this problem, while also\nproving its security. We have implemented a prototype in Rust, demonstrating\ncomparable performance to the current protocol while resisting similar attacks."}
{"id": "2509.10766", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10766", "abs": "https://arxiv.org/abs/2509.10766", "authors": ["Tong Zhou", "Ruyi Ding", "Gaowen Liu", "Charles Fleming", "Ramana Rao Kompella", "Yunsi Fei", "Xiaolin Xu", "Shaolei Ren"], "title": "A Content-dependent Watermark for Safeguarding Image Attribution", "comment": "18 pages, 13 figures", "summary": "The rapid growth of digital and AI-generated images has amplified the need\nfor secure and verifiable methods of image attribution. While digital\nwatermarking offers more robust protection than metadata-based\napproaches--which can be easily stripped--current watermarking techniques\nremain vulnerable to forgery, creating risks of misattribution that can damage\nthe reputations of AI model developers and the rights of digital artists. These\nvulnerabilities arise from two key issues: (1) content-agnostic watermarks,\nwhich, once learned or leaked, can be transferred across images to fake\nattribution, and (2) reliance on detector-based verification, which is\nunreliable since detectors can be tricked. We present MetaSeal, a novel\nframework for content-dependent watermarking with cryptographic security\nguarantees to safeguard image attribution. Our design provides (1) forgery\nresistance, preventing unauthorized replication and enforcing cryptographic\nverification; (2) robust, self-contained protection, embedding attribution\ndirectly into images while maintaining resilience against benign\ntransformations; and (3) evidence of tampering, making malicious alterations\nvisually detectable. Experiments demonstrate that MetaSeal effectively\nmitigates forgery attempts and applies to both natural and AI-generated images,\nestablishing a new standard for secure image attribution."}
{"id": "2509.10793", "categories": ["cs.CR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.10793", "abs": "https://arxiv.org/abs/2509.10793", "authors": ["Eli Baum", "Sam Buxbaum", "Nitin Mathai", "Muhammad Faisal", "Vasiliki Kalavri", "Mayank Varia", "John Liagouris"], "title": "ORQ: Complex Analytics on Private Data with Strong Security Guarantees", "comment": "14 pages, plus Appendix. To appear at SOSP 2025. Code published at\n  https://github.com/CASP-Systems-BU/orq", "summary": "We present ORQ, a system that enables collaborative analysis of large private\ndatasets using cryptographically secure multi-party computation (MPC). ORQ\nprotects data against semi-honest or malicious parties and can efficiently\nevaluate relational queries with multi-way joins and aggregations that have\nbeen considered notoriously expensive under MPC. To do so, ORQ eliminates the\nquadratic cost of secure joins by leveraging the fact that, in practice, the\nstructure of many real queries allows us to join records and apply the\naggregations \"on the fly\" while keeping the result size bounded. On the system\nside, ORQ contributes generic oblivious operators, a data-parallel vectorized\nquery engine, a communication layer that amortizes MPC network costs, and a\ndataflow API for expressing relational analytics -- all built from the ground\nup.\n  We evaluate ORQ in LAN and WAN deployments on a diverse set of workloads,\nincluding complex queries with multiple joins and custom aggregations. When\ncompared to state-of-the-art solutions, ORQ significantly reduces MPC execution\ntimes and can process one order of magnitude larger datasets. For our most\nchallenging workload, the full TPC-H benchmark, we report results entirely\nunder MPC with Scale Factor 10 -- a scale that had previously been achieved\nonly with information leakage or the use of trusted third parties."}
{"id": "2509.10814", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.10814", "abs": "https://arxiv.org/abs/2509.10814", "authors": ["Yang Zhang", "Wenyi Ouyang", "Yi Zhang", "Liang Cheng", "Chen Wu", "Wenxin Hu"], "title": "Automatic Generation of a Cryptography Misuse Taxonomy Using Large Language Models", "comment": "23 pages, 9 figures", "summary": "The prevalence of cryptographic API misuse (CAM) is compromising the\neffectiveness of cryptography and in turn the security of modern systems and\napplications. Despite extensive efforts to develop CAM detection tools, these\ntools typically rely on a limited set of predefined rules from human-curated\nknowledge. This rigid, rule-based approach hinders adaptation to evolving CAM\npatterns in real practices.\n  We propose leveraging large language models (LLMs), trained on publicly\navailable cryptography-related data, to automatically detect and classify CAMs\nin real-world code to address this limitation. Our method enables the\ndevelopment and continuous expansion of a CAM taxonomy, supporting developers\nand detection tools in tracking and understanding emerging CAM patterns.\nSpecifically, we develop an LLM-agnostic prompt engineering method to guide\nLLMs in detecting CAM instances from C/C++, Java, Python, and Go code, and then\nclassifying them into a hierarchical taxonomy.\n  Using a data set of 3,492 real-world software programs, we demonstrate the\neffectiveness of our approach with mainstream LLMs, including GPT, Llama,\nGemini, and Claude. It also allows us to quantitatively measure and compare the\nperformance of these LLMs in analyzing CAM in realistic code. Our evaluation\nproduced a taxonomy with 279 base CAM categories, 36 of which are not addressed\nby existing taxonomies. To validate its practical value, we encode 11 newly\nidentified CAM types into detection rules and integrate them into existing\ntools. Experiments show that such integration expands the tools' detection\ncapabilities."}
{"id": "2509.10823", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.10823", "abs": "https://arxiv.org/abs/2509.10823", "authors": ["Yury Yanovich", "Sergey Sobolev", "Yash Madhwal", "Kirill Ziborov", "Vladimir Gorgadze", "Victoria Kovalevskay", "Elizaveta Smirnova", "Matvey Mishuris", "Subodh Sharma"], "title": "From Paradigm Shift to Audit Rift: Exploring Vulnerabilities and Audit Tips for TON Smart Contracts", "comment": null, "summary": "The Open Network (TON) is a high-performance blockchain platform designed for\nscalability and efficiency, leveraging an asynchronous execution model and a\nmulti-layered architecture. While TON's design offers significant advantages,\nit also introduces unique challenges for smart contract development and\nsecurity. This paper introduces a comprehensive audit checklist for TON smart\ncontracts, based on an analysis of 34 professional audit reports containing 233\nreal-world vulnerabilities. The checklist addresses TON-specific challenges,\nsuch as asynchronous message handling, and provides actionable insights for\ndevelopers and auditors. We also present detailed case studies of\nvulnerabilities in TON smart contracts, highlighting their implications and\noffering lessons learned. By adopting this checklist, developers and auditors\ncan systematically identify and mitigate vulnerabilities, enhancing the\nsecurity and reliability of TON-based projects. Our work bridges the gap\nbetween Ethereum's mature audit methodologies and the emerging needs of the TON\necosystem, fostering a more secure and robust blockchain environment."}
{"id": "2509.10838", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10838", "abs": "https://arxiv.org/abs/2509.10838", "authors": ["Rishit Agrawal", "Kunal Bhatnagar", "Andrew Do", "Ronnit Rana", "Mark Stamp"], "title": "A Comparison of Selected Image Transformation Techniques for Malware Classification", "comment": null, "summary": "Recently, a considerable amount of malware research has focused on the use of\npowerful image-based machine learning techniques, which generally yield\nimpressive results. However, before image-based techniques can be applied to\nmalware, the samples must be converted to images, and there is no\ngenerally-accepted approach for doing so. The malware-to-image conversion\nstrategies found in the literature often appear to be ad hoc, with little or no\neffort made to take into account properties of executable files. In this paper,\nwe experiment with eight distinct malware-to-image conversion techniques, and\nfor each, we test a variety of learning models. We find that several of these\nimage conversion techniques perform similarly across a range of learning\nmodels, in spite of the image conversion processes being quite different. These\nresults suggest that the effectiveness of image-based malware classification\ntechniques may depend more on the inherent strengths of image analysis\ntechniques, as opposed to the precise details of the image conversion strategy."}
{"id": "2509.10858", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10858", "abs": "https://arxiv.org/abs/2509.10858", "authors": ["Ali Habibzadeh", "Farid Feyzi", "Reza Ebrahimi Atani"], "title": "Large Language Models for Security Operations Centers: A Comprehensive Survey", "comment": null, "summary": "Large Language Models (LLMs) have emerged as powerful tools capable of\nunderstanding and generating human-like text, offering transformative potential\nacross diverse domains. The Security Operations Center (SOC), responsible for\nsafeguarding digital infrastructure, represents one of these domains. SOCs\nserve as the frontline of defense in cybersecurity, tasked with continuous\nmonitoring, detection, and response to incidents. However, SOCs face persistent\nchallenges such as high alert volumes, limited resources, high demand for\nexperts with advanced knowledge, delayed response times, and difficulties in\nleveraging threat intelligence effectively. In this context, LLMs can offer\npromising solutions by automating log analysis, streamlining triage, improving\ndetection accuracy, and providing the required knowledge in less time. This\nsurvey systematically explores the integration of generative AI and more\nspecifically LLMs into SOC workflow, providing a structured perspective on its\ncapabilities, challenges, and future directions. We believe that this survey\noffers researchers and SOC managers a broad overview of the current state of\nLLM integration within academic study. To the best of our knowledge, this is\nthe first comprehensive study to examine LLM applications in SOCs in details."}
{"id": "2509.10895", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.10895", "abs": "https://arxiv.org/abs/2509.10895", "authors": ["Fabian Bäumer", "Marcel Maehren", "Marcus Brinkmann", "Jörg Schwenk"], "title": "Finding SSH Strict Key Exchange Violations by State Learning", "comment": "15 pages, 7 figures, accepted at ACM CCS 2025", "summary": "SSH is an important protocol for secure remote shell access to servers on the\nInternet. At USENIX 2024, B\\\"aumer et al. presented the Terrapin attack on SSH,\nwhich relies on the attacker injecting optional messages during the key\nexchange. To mitigate this attack, SSH vendors adopted an extension developed\nby OpenSSH called strict key exchange (\"strict KEX\"). With strict KEX, optional\nmessages are forbidden during the handshake, preventing the attack. In\npractice, this should simplify the state machine of an SSH handshake to a\nlinear message flow similar to that of TLS.\n  In this work, we analyze the design, implementation, and security of strict\nKEX in popular SSH servers, using black-box state learning, which can uncover\nthe hidden state machine of an implementation. In practice, it is limited by\nthe number of learned messages and the complexity of the state machine. Thus,\nlearning the complete state machine of SSH is infeasible. Previous research on\nSSH, therefore, excluded optional messages, learning only a partial state\nmachine. However, these messages are a critical part of the Terrapin attack. We\npropose to instead learn the complete state machine of the handshake phase of\nan SSH server, but with strict KEX enabled.\n  We investigate the security of ten SSH implementations supporting strict KEX\nfor up to five key exchange algorithms. In total, we learn 33 state machines,\nrevealing significant differences in the implementations. We show that seven\nimplementations violate the strict KEX specification and find two critical\nsecurity vulnerabilities. One results in a rogue session attack in the\nproprietary Tectia SSH implementation. Another affects the official SSH\nimplementation of the Erlang Open Telecom Platform, and enables unauthenticated\nremote code execution in the security context of the SSH server."}
{"id": "2509.11006", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.11006", "abs": "https://arxiv.org/abs/2509.11006", "authors": ["M. Z. Haider", "M. Dias de Assuncao", "Kaiwen Zhang"], "title": "A Range-Based Sharding (RBS) Protocol for Scalable Enterprise Blockchain", "comment": null, "summary": "Blockchain technology offers decentralization and security but struggles with\nscalability, particularly in enterprise settings where efficiency and\ncontrolled access are paramount. Sharding is a promising solution for private\nblockchains, yet existing approaches face challenges in coordinating shards,\nensuring fault tolerance with limited nodes, and minimizing the high overhead\nof consensus mechanisms like PBFT. This paper proposes the Range-Based Sharding\n(RBS) Protocol, a novel sharding mechanism tailored for enterprise blockchains,\nimplemented on Quorum. Unlike traditional sharding models such as OmniLedger\nand non-sharding Corda framework, RBS employs a commit-reveal scheme for secure\nand unbiased shard allocation, ensuring fair validator distribution while\nreducing cross-shard transaction delays. Our approach enhances scalability by\nbalancing computational loads across shards, reducing consensus overhead, and\nimproving parallel transaction execution. Experimental evaluations demonstrate\nthat RBS achieves significantly higher throughput and lower latency compared to\nexisting enterprise sharding frameworks, making it a viable and efficient\nsolution for largescale blockchain deployments."}
{"id": "2509.11120", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.11120", "abs": "https://arxiv.org/abs/2509.11120", "authors": ["Qingzhao Zhang", "Shaocheng Luo", "Z. Morley Mao", "Miroslav Pajic", "Michael K. Reiter"], "title": "SoK: How Sensor Attacks Disrupt Autonomous Vehicles: An End-to-end Analysis, Challenges, and Missed Threats", "comment": null, "summary": "Autonomous vehicles, including self-driving cars, robotic ground vehicles,\nand drones, rely on complex sensor pipelines to ensure safe and reliable\noperation. However, these safety-critical systems remain vulnerable to\nadversarial sensor attacks that can compromise their performance and mission\nsuccess. While extensive research has demonstrated various sensor attack\ntechniques, critical gaps remain in understanding their feasibility in\nreal-world, end-to-end systems. This gap largely stems from the lack of a\nsystematic perspective on how sensor errors propagate through interconnected\nmodules in autonomous systems when autonomous vehicles interact with the\nphysical world.\n  To bridge this gap, we present a comprehensive survey of autonomous vehicle\nsensor attacks across platforms, sensor modalities, and attack methods. Central\nto our analysis is the System Error Propagation Graph (SEPG), a structured\ndemonstration tool that illustrates how sensor attacks propagate through system\npipelines, exposing the conditions and dependencies that determine attack\nfeasibility. With the aid of SEPG, our study distills seven key findings that\nhighlight the feasibility challenges of sensor attacks and uncovers eleven\npreviously overlooked attack vectors exploiting inter-module interactions,\nseveral of which we validate through proof-of-concept experiments.\nAdditionally, we demonstrate how large language models (LLMs) can automate\naspects of SEPG construction and cross-validate expert analysis, showcasing the\npromise of AI-assisted security evaluation."}
{"id": "2509.11123", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.11123", "abs": "https://arxiv.org/abs/2509.11123", "authors": ["Aditya Kulkarni", "Tamal Das", "Vivek Balachandran"], "title": "ODoQ: Oblivious DNS-over-QUIC", "comment": null, "summary": "The Domain Name System (DNS), which converts domain names to their respective\nIP addresses, has advanced enhancements aimed at safeguarding DNS data and\nusers' identity from attackers. The recent privacy-focused advancements have\nenabled the IETF to standardize several protocols. Nevertheless, these\nprotocols tend to focus on either strengthening user privacy (like Oblivious\nDNS and Oblivious DNS-over-HTTPS) or reducing resolution latency (as\ndemonstrated by DNS-over-QUIC). Achieving both within a single protocol remains\na key challenge, which we address in this paper. Our proposed protocol --\n'Oblivious DNS-over-QUIC' (ODoQ) -- leverages the benefits of the QUIC protocol\nand incorporates an intermediary proxy server to protect the client's identity\nfrom exposure to the recursive resolver."}
{"id": "2509.11158", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.11158", "abs": "https://arxiv.org/abs/2509.11158", "authors": ["Qianxue Wang", "Simin Yu"], "title": "Cryptanalysis and design for a family of plaintext non-delayed chaotic ciphers", "comment": null, "summary": "Plaintext non-delayed chaotic cipher (PNDCC) means that in the diffusion\nequation, plaintext has no delay terms while ciphertext has a feedback term. In\nexisting literature, chaotic cipher diffusions invariably take this form. Since\nits introduction, PNDCC has attracted attention but also doubts. Designers of\nchaotic ciphers usually claim PNDCC security by statistical tests, while\nrigorous cryptographic proofs are absent. Thus, it is necessary to re-examine\nits design rationale and empirical security. To address this issue, we present\na typical example of a three-stage permutation-diffusion-permutation PNDCC,\nwhich contains multiple security vulnerabilities. Although all of its\nstatistical indicators show good performance, we are able to break it using\nfour different attacks. The first is a differential attack based on homogeneous\noperations; the second is an S-PTC attack; the third is a novel\nimpulse-step-based differential attack (ISBDA), proposed in this paper, and the\nfourth is a novel chain attack, also introduced here. These results demonstrate\nthat the fulfilment of statistical criteria is not a sufficient condition for\nthe security of PNDCC. Then, based on a mathematical model of multi-stage\nPNDCC, we show that the proposed chain attack can successfully break a class of\nmulti-stage PNDCCs. The key technique of the chain attack depends on how to\nreveal all permutations. To address this key problem, we summarize the chaining\nrules and show that, from the attacker's perspective, if the same decryption\nchain can be reconstructed then all permutations can be deciphered. To that\nend, the entire diffusion process can be broken by solving a system of\nsimultaneous equations. Finally, as a secure improvement, we propose a new\nscheme termed plaintext-delayed chaotic cipher (PDCC) that can resist various\ncryptanalytic attacks."}
{"id": "2509.11173", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11173", "abs": "https://arxiv.org/abs/2509.11173", "authors": ["Simin Chen", "Jinjun Peng", "Yixin He", "Junfeng Yang", "Baishakhi Ray"], "title": "Your Compiler is Backdooring Your Model: Understanding and Exploiting Compilation Inconsistency Vulnerabilities in Deep Learning Compilers", "comment": "This paper is accepted to S&P 2026", "summary": "Deep learning (DL) compilers are core infrastructure in modern DL systems,\noffering flexibility and scalability beyond vendor-specific libraries. This\nwork uncovers a fundamental vulnerability in their design: can an official,\nunmodified compiler alter a model's semantics during compilation and introduce\nhidden backdoors? We study both adversarial and natural settings. In the\nadversarial case, we craft benign models where triggers have no effect\npre-compilation but become effective backdoors after compilation. Tested on six\nmodels, three commercial compilers, and two hardware platforms, our attack\nyields 100% success on triggered inputs while preserving normal accuracy and\nremaining undetected by state-of-the-art detectors. The attack generalizes\nacross compilers, hardware, and floating-point settings. In the natural\nsetting, we analyze the top 100 HuggingFace models (including one with 220M+\ndownloads) and find natural triggers in 31 models. This shows that compilers\ncan introduce risks even without adversarial manipulation.\n  Our results reveal an overlooked threat: unmodified DL compilers can silently\nalter model semantics. To our knowledge, this is the first work to expose\ninherent security risks in DL compiler design, opening a new direction for\nsecure and trustworthy ML."}
{"id": "2509.11187", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.11187", "abs": "https://arxiv.org/abs/2509.11187", "authors": ["Doan Minh Trung", "Tien Duc Anh Hao", "Luong Hoang Minh", "Nghi Hoang Khoa", "Nguyen Tan Cam", "Van-Hau Pham", "Phan The Duy"], "title": "DMLDroid: Deep Multimodal Fusion Framework for Android Malware Detection with Resilience to Code Obfuscation and Adversarial Perturbations", "comment": null, "summary": "In recent years, learning-based Android malware detection has seen\nsignificant advancements, with detectors generally falling into three\ncategories: string-based, image-based, and graph-based approaches. While these\nmethods have shown strong detection performance, they often struggle to sustain\nrobustness in real-world settings, particularly when facing code obfuscation\nand adversarial examples (AEs). Deep multimodal learning has emerged as a\npromising solution, leveraging the strengths of multiple feature types to\nenhance robustness and generalization. However, a systematic investigation of\nmultimodal fusion for both accuracy and resilience remains underexplored. In\nthis study, we propose DMLDroid, an Android malware detection based on\nmultimodal fusion that leverages three different representations of malware\nfeatures, including permissions & intents (tabular-based), DEX file\nrepresentations (image-based), and API calls (graph-derived sequence-based). We\nconduct exhaustive experiments independently on each feature, as well as in\ncombination, using different fusion strategies. Experimental results on the\nCICMalDroid 2020 dataset demonstrate that our multimodal approach with the\ndynamic weighted fusion mechanism achieves high performance, reaching 97.98%\naccuracy and 98.67% F1-score on original malware detection. Notably, the\nproposed method maintains strong robustness, sustaining over 98% accuracy and\n98% F1-score under both obfuscation and adversarial attack scenarios. Our\nfindings highlight the benefits of multimodal fusion in improving both\ndetection accuracy and robustness against evolving Android malware threats."}
{"id": "2509.11237", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.11237", "abs": "https://arxiv.org/abs/2509.11237", "authors": ["Aleksejus Mihalkovič", "Lina Dindiene", "Eligijus Sakalauskas"], "title": "Implementation of Learning with Errors in Non-Commuting Multiplicative Groups", "comment": "18 pages, 1 figure", "summary": "In this paper, we demonstrate a way to generalize learning with errors (LWE)\nto the family of so-called modular-maximal cyclic groups which are\nnon-commuting. Since the group M2t has two cycles of maximal multiplicative\norder, we use this fact to construct an accurate criterion for restoring the\nmessage bit with overwhelming probability. Furthermore, we implement the\noriginal idea by O. Regev in the considered group to gain benefits from the\nnon-commutativity of M2t . Also we prove that using this approach we can\nachieve a level of security comparable to the original idea."}
{"id": "2509.11242", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.11242", "abs": "https://arxiv.org/abs/2509.11242", "authors": ["Zhaofeng Yu", "Dongyang Zhan", "Lin Ye", "Haining Yu", "Hongli Zhang", "Zhihong Tian"], "title": "Exploring and Exploiting the Resource Isolation Attack Surface of WebAssembly Containers", "comment": "18 pages, 2 figures. Accepted at the 34th USENIX Security Symposium\n  (USENIX Security 2025)", "summary": "Recently, the WebAssembly (or Wasm) technology has been rapidly evolving,\nwith many runtimes actively under development, providing cross-platform secure\nsandboxes for Wasm modules to run as portable containers. Compared with Docker,\nwhich isolates applications at the operating system level, Wasm runtimes\nprovide more security mechanisms, such as linear memory, type checking, and\nprotected call stacks. Although Wasm is designed with security in mind and\nconsidered to be a more secure container runtime, various security challenges\nhave arisen, and researchers have focused on the security of Wasm runtimes,\nsuch as discovering vulnerabilities or proposing new security mechanisms to\nachieve robust isolation. However, we have observed that the resource isolation\nis not well protected by the current Wasm runtimes, and attackers can exhaust\nthe host's resources to interfere with the execution of other container\ninstances by exploiting the WASI/WASIX interfaces. And the attack surface has\nnot been well explored and measured. In this paper, we explore the resource\nisolation attack surface of Wasm runtimes systematically by proposing several\nstatic Wasm runtime analysis approaches. Based on the analysis results, we\npropose several exploitation strategies to break the resource isolation of Wasm\nruntimes. The experimental results show that malicious Wasm instances can not\nonly consume large amounts of system resources on their own but also introduce\nhigh workloads into other components of the underlying operating system,\nleading to a substantial performance degradation of the whole system. In\naddition, the mitigation approaches have also been discussed."}
{"id": "2509.11249", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.11249", "abs": "https://arxiv.org/abs/2509.11249", "authors": ["Tao Wang", "Yushu Zhang", "Xiangli Xiao", "Kun Xu", "Lin Yuan", "Wenying Wen", "Yuming Fang"], "title": "Make Identity Unextractable yet Perceptible: Synthesis-Based Privacy Protection for Subject Faces in Photos", "comment": null, "summary": "Deep learning-based face recognition (FR) technology exacerbates privacy\nconcerns in photo sharing. In response, the research community developed a\nsuite of anti-FR methods to block identity extraction by unauthorized FR\nsystems. Benefiting from quasi-imperceptible alteration, perturbation-based\nmethods are well-suited for privacy protection of subject faces in photos, as\nthey allow familiar persons to recognize subjects via naked eyes. However, we\nreveal that perturbation-based methods provide a false sense of privacy through\ntheoretical analysis and experimental validation.\n  Therefore, new alternative solutions should be found to protect subject\nfaces. In this paper, we explore synthesis-based methods as a promising\nsolution, whose challenge is to enable familiar persons to recognize subjects.\nTo solve the challenge, we present a key insight: In most photo sharing\nscenarios, familiar persons recognize subjects through identity perception\nrather than meticulous face analysis. Based on the insight, we propose the\nfirst synthesis-based method dedicated to subject faces, i.e., PerceptFace,\nwhich can make identity unextractable yet perceptible. To enhance identity\nperception, a new perceptual similarity loss is designed for faces, reducing\nthe alteration in regions of high sensitivity to human vision.\n  As a synthesis-based method, PerceptFace can inherently provide reliable\nidentity protection. Meanwhile, out of the confine of meticulous face analysis,\nPerceptFace focuses on identity perception from a more practical scenario,\nwhich is also enhanced by the designed perceptual similarity loss. Sufficient\nexperiments show that PerceptFace achieves a superior trade-off between\nidentity protection and identity perception compared to existing methods. We\nprovide a public API of PerceptFace and believe that it has great potential to\nbecome a practical anti-FR tool."}
{"id": "2509.11250", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.11250", "abs": "https://arxiv.org/abs/2509.11250", "authors": ["Yitong Zhang", "Ximo Li", "Liyi Cai", "Jia Li"], "title": "Realistic Environmental Injection Attacks on GUI Agents", "comment": null, "summary": "GUI agents built on LVLMs are increasingly used to interact with websites.\nHowever, their exposure to open-world content makes them vulnerable to\nEnvironmental Injection Attacks (EIAs) that hijack agent behavior via webpage\nelements. Many recent studies assume the attacker to be a regular user who can\nonly upload a single trigger image, which is more realistic than earlier\nassumptions of website-level administrative control. However, these works still\nfall short of realism: (1) the trigger's position and surrounding context\nremain largely fixed between training and testing, failing to capture the\ndynamic nature of real webpages and (2) the trigger often occupies an\nunrealistically large area, whereas real-world images are typically small. To\nbetter reflect real-world scenarios, we introduce a more realistic threat model\nwhere the attacker is a regular user and the trigger image is small and\nembedded within a dynamically changing environment. As a result, existing\nattacks prove largely ineffective under this threat model.\n  To better expose the vulnerabilities of GUI agents, we propose Chameleon, an\nattack framework with two main novelties. The first is LLM-Driven Environment\nSimulation, which automatically generates diverse and high-fidelity webpage\nsimulations. The second is Attention Black Hole, which transforms attention\nweights into explicit supervisory signals that guide the agent's focus toward\nthe trigger region. We evaluate Chameleon on 6 realistic websites and 4\nrepresentative LVLM-powered GUI agents, where it significantly outperforms\nexisting methods. Ablation studies confirm that both novelties are critical to\nperformance. Our findings reveal underexplored vulnerabilities in modern GUI\nagents and establish a robust foundation for future research on defense in\nopen-world GUI agent systems. The code is publicly available at\nhttps://github.com/zhangyitonggg/attack2gui."}
{"id": "2509.11440", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.11440", "abs": "https://arxiv.org/abs/2509.11440", "authors": ["Robert Dumitru", "Junpeng Wan", "Daniel Genkin", "Rick Kennell", "Dave", "Tian", "Yuval Yarom"], "title": "Thunderhammer: Rowhammer Bitflips via PCIe and Thunderbolt (USB-C)", "comment": null, "summary": "In recent years, Rowhammer has attracted significant attention from academia\nand industry alike. This technique, first published in 2014, flips bits in\nmemory by repeatedly accessing neighbouring memory locations. Since its\ndiscovery, researchers have developed a substantial body of work exploiting\nRowhammer and proposing countermeasures. These works demonstrate that Rowhammer\ncan be mounted not only through native code, but also via remote code\nexecution, such as JavaScript in browsers, and over networks.\n  In this work, we uncover a previously unexplored Rowhammer vector. We present\nThunderhammer, an attack that induces DRAM bitflips from malicious peripherals\nconnected via PCIe or Thunderbolt (which tunnels PCIe). On modern DDR4 systems,\nwe observe that triggering bitflips through PCIe requests requires precisely\ntimed access patterns tailored to the target system. We design a custom device\nto reverse engineer critical architectural parameters that shape PCIe request\nscheduling, and to execute effective hammering access patterns. Leveraging this\nknowledge, we successfully demonstrate Rowhammer-induced bitflips in DDR4\nmemory modules via both PCIe slot connections and Thunderbolt ports tunnelling\nPCIe."}
{"id": "2509.11451", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.11451", "abs": "https://arxiv.org/abs/2509.11451", "authors": ["Ahaan Dabholkar", "Atul Sharma", "Z. Berkay Celik", "Saurabh Bagchi"], "title": "MAUI: Reconstructing Private Client Data in Federated Transfer Learning", "comment": null, "summary": "Recent works in federated learning (FL) have shown the utility of leveraging\ntransfer learning for balancing the benefits of FL and centralized learning. In\nthis setting, federated training happens after a stable point has been reached\nthrough conventional training. Global model weights are first centrally\npretrained by the server on a public dataset following which only the last few\nlinear layers (the classification head) of the model are finetuned across\nclients. In this scenario, existing data reconstruction attacks (DRAs) in FL\nshow two key weaknesses. First, strongly input-correlated gradient information\nfrom the initial model layers is never shared, significantly degrading\nreconstruction accuracy. Second, DRAs in which the server makes highly\nspecific, handcrafted manipulations to the model structure or parameters (for\ne.g., layers with all zero weights, identity mappings and rows with identical\nweight patterns) are easily detectable by an active client.\n  Improving on these, we propose MAUI, a stealthy DRA that does not require any\novert manipulations to the model architecture or weights, and relies solely on\nthe gradients of the classification head. MAUI first extracts \"robust\" feature\nrepresentations of the input batch from the gradients of the classification\nhead and subsequently inverts these representations to the original inputs. We\nreport highly accurate reconstructions on the CIFAR10 and ImageNet datasets on\na variety of model architectures including convolution networks (CNN, VGG11),\nResNets (18, 50), ShuffleNet-V2 and Vision Transformer (ViT B-32), regardless\nof the batch size. MAUI significantly outperforms prior DRAs in reconstruction\nquality, achieving 40-120% higher PSNR scores."}
{"id": "2509.11555", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11555", "abs": "https://arxiv.org/abs/2509.11555", "authors": ["Shunfan Zhou", "Kevin Wang", "Hang Yin"], "title": "Dstack: A Zero Trust Framework for Confidential Containers", "comment": null, "summary": "Web3 applications require execution platforms that maintain confidentiality\nand integrity without relying on centralized trust authorities. While Trusted\nExecution Environments (TEEs) offer promising capabilities for confidential\ncomputing, current implementations face significant limitations when applied to\nWeb3 contexts, particularly in security reliability, censorship resistance, and\nvendor independence.\n  This paper presents dstack, a comprehensive framework that transforms raw TEE\ntechnology into a true Zero Trust platform. We introduce three key innovations:\n(1) Portable Confidential Containers that enable seamless workload migration\nacross heterogeneous TEE environments while maintaining security guarantees,\n(2) Decentralized Code Management that leverages smart contracts for\ntransparent governance of TEE applications, and (3) Verifiable Domain\nManagement that ensures secure and verifiable application identity without\ncentralized authorities.\n  These innovations are implemented through three core components: dstack-OS,\ndstack-KMS, and dstack-Gateway. Together, they demonstrate how to achieve both\nthe performance advantages of VM-level TEE solutions and the trustless\nguarantees required by Web3 applications. Our evaluation shows that dstack\nprovides comprehensive security guarantees while maintaining practical\nusability for real-world applications."}
{"id": "2509.11559", "categories": ["cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.11559", "abs": "https://arxiv.org/abs/2509.11559", "authors": ["Tarakaram Gollamudi", "Anitha Gollamudi", "Joshua Gancher"], "title": "ILA: Correctness via Type Checking for Fully Homomorphic Encryption", "comment": null, "summary": "RLWE-based Fully Homomorphic Encryption (FHE) schemes add some small\n\\emph{noise} to the message during encryption. The noise accumulates with each\nhomomorphic operation. When the noise exceeds a critical value, the FHE circuit\nproduces an incorrect output. This makes developing FHE applications quite\nsubtle, as one must closely track the noise to ensure correctness. However,\nexisting libraries and compilers offer limited support to statically track the\nnoise. Additionally, FHE circuits are also plagued by wraparound errors that\nare common in finite modulus arithmetic. These two limitations of existing\ncompilers and libraries make FHE applications too difficult to develop with\nconfidence.\n  In this work, we present a \\emph{correctness-oriented} IR, Intermediate\nLanguage for Arithmetic circuits, for type-checking circuits intended for\nhomomorphic evaluation. Our IR is backed by a type system that tracks low-level\nquantitative bounds (e.g., ciphertext noise) without using the secret key.\nUsing our type system, we identify and prove a strong \\emph{functional\ncorrectness} criterion for \\ila circuits. Additionally, we have designed \\ila\nto be maximally general: our core type system does not directly assume a\nparticular FHE scheme, but instead axiomatizes a \\emph{model} of FHE. We\ninstantiate this model with the exact FHE schemes (BGV, BFV and TFHE), and\nobtain functional correctness for free."}
{"id": "2509.11615", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.11615", "abs": "https://arxiv.org/abs/2509.11615", "authors": ["Rimsha Kanwal", "Umara Noor", "Zafar Iqbal", "Zahid Rashid"], "title": "Cyber Threat Hunting: Non-Parametric Mining of Attack Patterns from Cyber Threat Intelligence for Precise Threats Attribution", "comment": null, "summary": "With the ever-changing landscape of cyber threats, identifying their origin\nhas become paramount, surpassing the simple task of attack classification.\nCyber threat attribution gives security analysts the insights they need to\ndevice effective threat mitigation strategies. Such strategies empower\nenterprises to proactively detect and defend against future cyber-attacks.\nHowever, existing approaches exhibit limitations in accurately identifying\nthreat actors, leading to low precision and a significant occurrence of false\npositives. Machine learning offers the potential to automate certain aspects of\ncyber threat attribution. The distributed nature of information regarding cyber\nthreat actors and their intricate attack methodologies has hindered substantial\nprogress in this domain. Cybersecurity analysts deal with an ever-expanding\ncollection of cyber threat intelligence documents. While these documents hold\nvaluable insights, their sheer volume challenges efficient organization and\nretrieval of pertinent information. To assist the cybersecurity analyst\nactivities, we propose a machine learning based approach featuring visually\ninteractive analytics tool named the Cyber-Attack Pattern Explorer (CAPE),\ndesigned to facilitate efficient information discovery by employing interactive\nvisualization and mining techniques. In the proposed system, a non-parametric\nmining technique is proposed to create a dataset for identifying the attack\npatterns within cyber threat intelligence documents. These attack patterns\nalign semantically with commonly employed themes ensuring ease of\ninterpretation. The extracted dataset is used for training of proposed machine\nlearning algorithms that enables the attribution of cyber threats with\nrespective to the actors."}
{"id": "2509.11668", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.11668", "abs": "https://arxiv.org/abs/2509.11668", "authors": ["Fizza Khurshid", "Umara Noor", "Zahid Rashid"], "title": "Cyber Attack Mitigation Framework for Denial of Service (DoS) Attacks in Fog Computing", "comment": null, "summary": "Innovative solutions to cyber security issues are shaped by the ever-changing\nlandscape of cyber threats. Automating the mitigation of these threats can be\nachieved through a new methodology that addresses the domain of mitigation\nautomation, which is often overlooked. This literature overview emphasizes the\nlack of scholarly work focusing specifically on automated cyber threat\nmitigation, particularly in addressing challenges beyond detection. The\nproposed methodology comprise of the development of an automatic cyber threat\nmitigation framework tailored for Distributed Denial-of-Service (DDoS) attacks.\nThis framework adopts a multi-layer security approach, utilizing smart devices\nat the device layer, and leveraging fog network and cloud computing layers for\ndeeper understanding and technological adaptability. Initially, firewall\nrule-based packet inspection is conducted on simulated attack traffic to filter\nout DoS packets, forwarding legitimate packets to the fog. The methodology\nemphasizes the integration of fog detection through statistical and behavioral\nanalysis, specification-based detection, and deep packet inspection, resulting\nin a comprehensive cyber protection system. Furthermore, cloud-level inspection\nis performed to confirm and mitigate attacks using firewalls, enhancing\nstrategic defense and increasing robustness against cyber threats. These\nenhancements enhance understanding of the research framework's practical\nimplementation and assessment strategies, substantiating its importance in\naddressing current cyber security challenges and shaping future automation\nmitigation approaches."}
{"id": "2509.11683", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.11683", "abs": "https://arxiv.org/abs/2509.11683", "authors": ["Sawera Shahid", "Umara Noor", "Zahid Rashid"], "title": "An Unsupervised Learning Approach For A Reliable Profiling Of Cyber Threat Actors Reported Globally Based On Complete Contextual Information Of Cyber Attacks", "comment": null, "summary": "Cyber attacks are rapidly increasing with the advancement of technology and\nthere is no protection for our information. To prevent future cyberattacks it\nis critical to promptly recognize cyberattacks and establish strong defense\nmechanisms against them. To respond to cybersecurity threats immediately, it is\nessential to examine the attackers skills, knowledge, and behaviors with the\ngoal of evaluating their impact on the system and comprehending the traits\nassociated with these attacks. Creating a profile of cyber threat actors based\non their traits or patterns of behavior can help to create effective defenses\nagainst cyberattacks in advance. In the current literature, multiple supervised\nmachine learning based approaches considered a smaller number of features for\nattacker profiling that are reported in textual cyber threat incident documents\nalthough these profiles have been developed based on the security experts own\nperception, we cannot rely on them. Supervised machine learning approaches\nstrictly depend upon the structure data set. This usually leads to a two step\nprocess where we first have to establish a structured data set before we can\nanalyze it and then employ it to construct defense mechanisms, which takes\ntime. In this paper, an unsupervised efficient agglomerative hierarchal\nclustering technique is proposed for profiling cybercriminal groups based on\ntheir comprehensive contextual threat information in order to address the\naforementioned issues. The main objective of this report is to identify the\nrelationship between cyber threat actors based on their common features,\naggregate them, and also profile cyber criminal groups."}
{"id": "2509.11695", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.11695", "abs": "https://arxiv.org/abs/2509.11695", "authors": ["Daniel Herzinger", "Linus Heise", "Daniel Loebenberger", "Matthias Söllner"], "title": "Time-Based State-Management of Hash-Based Signature CAs for VPN-Authentication", "comment": "17 pages, 6 figures", "summary": "Advances in quantum computing necessitate migrating the entire technology\nstack to post-quantum cryptography. This includes IPsec-based VPN connection\nauthentication. Although there is an RFC draft for post-quantum authentication\nin this setting, the draft does not consider (stateful) hash-based signatures\ndespite their small signature size and trusted long-term security.\n  We propose a design with time-based state-management that assigns VPN devices\na certificate authority (CA) based on the hash-based signature scheme XMSS. The\nCA then issues leaf certificates which are based on classical cryptography but\nhave a short validity time, e. g., four hours. It is to be expected that even\nlarge quantum computers will take significantly longer to break the\ncryptography, making the design quantum-secure. We propose strategies to make\nthe timekeeping more resilient to faults and tampering, as well as strategies\nto recognize a wrong system time, minimize its potential damage, and quickly\nrecover.\n  The result is an OpenBSD implementation of a quantum-safe and, regarding the\nleaf certificates, highly flexible VPN authentication design that requires\nsignificantly less bandwidth and computational resources compared to existing\nalternatives."}
{"id": "2509.11712", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11712", "abs": "https://arxiv.org/abs/2509.11712", "authors": ["Mohammad Olid Ali Akash", "Priyangana Saha"], "title": "A Holistic Approach to E-Commerce Innovation: Redefining Security and User Experience", "comment": null, "summary": "In the modern, fast-moving world of e-commerce, many Android apps face\nchallenges in providing a simple and secure shopping experience. Many of these\napps, often enough, have complicated designs that prevent users from finding\nwhat they want quickly, thus frustrating them and wasting their precious time.\nAnother major issue is that of security; with the limitation of payment options\nand weak authentication mechanisms, users' sensitive information can be\ncompromised. This research presents a new e-commerce platform that responds to\nthe above challenges with an intuitive interface and strong security measures.\nThe platform makes online shopping easy with well-organized categories of\nproducts and a fast, efficient checkout process. It also gives priority to\nsecurity by incorporating features such as Google authentication and\nSSL-secured payment gateways to protect user data and ensure secure\ntransactions. This paper discusses how a focus on user-friendliness, security,\nand personalization steps up the game for e-commerce platforms, providing\nworkable frameworks that match modern user needs and expectations. The findings\nshow the e-commerce user experience can be remodelled by the platform, hence\nopening ways for future developments in that respect."}
{"id": "2509.11745", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.11745", "abs": "https://arxiv.org/abs/2509.11745", "authors": ["De Zhang Lee", "Han Fang", "Hanyi Wang", "Ee-Chien Chang"], "title": "Removal Attack and Defense on AI-generated Content Latent-based Watermarking", "comment": null, "summary": "Digital watermarks can be embedded into AI-generated content (AIGC) by\ninitializing the generation process with starting points sampled from a secret\ndistribution. When combined with pseudorandom error-correcting codes, such\nwatermarked outputs can remain indistinguishable from unwatermarked objects,\nwhile maintaining robustness under whitenoise. In this paper, we go beyond\nindistinguishability and investigate security under removal attacks. We\ndemonstrate that indistinguishability alone does not necessarily guarantee\nresistance to adversarial removal. Specifically, we propose a novel attack that\nexploits boundary information leaked by the locations of watermarked objects.\nThis attack significantly reduces the distortion required to remove watermarks\n-- by up to a factor of $15 \\times$ compared to a baseline whitenoise attack\nunder certain settings. To mitigate such attacks, we introduce a defense\nmechanism that applies a secret transformation to hide the boundary, and prove\nthat the secret transformation effectively rendering any attacker's\nperturbations equivalent to those of a naive whitenoise adversary. Our\nempirical evaluations, conducted on multiple versions of Stable Diffusion,\nvalidate the effectiveness of both the attack and the proposed defense,\nhighlighting the importance of addressing boundary leakage in latent-based\nwatermarking schemes."}
{"id": "2509.11761", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.11761", "abs": "https://arxiv.org/abs/2509.11761", "authors": ["Manish Bansal", "Pramsu Shrivastava", "J. Harshan"], "title": "On Spatial-Provenance Recovery in Wireless Networks with Relaxed-Privacy Constraints", "comment": "Accepted for publication in IEEE Transactions on Dependable and\n  Secure Computing, September 2025", "summary": "In Vehicle-to-Everything (V2X) networks with multi-hop communication, Road\nSide Units (RSUs) intend to gather location data from the vehicles to offer\nvarious location-based services. Although vehicles use the Global Positioning\nSystem (GPS) for navigation, they may refrain from sharing their exact GPS\ncoordinates to the RSUs due to privacy considerations. Thus, to address the\nlocalization expectations of the RSUs and the privacy concerns of the vehicles,\nwe introduce a relaxed-privacy model wherein the vehicles share their partial\nlocation information in order to avail the location-based services. To\nimplement this notion of relaxed-privacy, we propose a low-latency protocol for\nspatial-provenance recovery, wherein vehicles use correlated linear Bloom\nfilters to embed their position information. Our proposed spatial-provenance\nrecovery process takes into account the resolution of localization, the\nunderlying ad hoc protocol, and the coverage range of the wireless technology\nused by the vehicles. Through a rigorous theoretical analysis, we present\nextensive analysis on the underlying trade-off between relaxed-privacy and the\ncommunication-overhead of the protocol. Finally, using a wireless testbed, we\nshow that our proposed method requires a few bits in the packet header to\nprovide security features such as localizing a low-power jammer executing a\ndenial-of-service attack."}
{"id": "2509.11786", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.11786", "abs": "https://arxiv.org/abs/2509.11786", "authors": ["Dongyang Zhan", "Wenqi Zhang", "Lin Ye", "Xiangzhan Yu", "Hongli Zhang", "Zheng He"], "title": "Anomaly Detection in Industrial Control Systems Based on Cross-Domain Representation Learning", "comment": null, "summary": "Industrial control systems (ICSs) are widely used in industry, and their\nsecurity and stability are very important. Once the ICS is attacked, it may\ncause serious damage. Therefore, it is very important to detect anomalies in\nICSs. ICS can monitor and manage physical devices remotely using communication\nnetworks. The existing anomaly detection approaches mainly focus on analyzing\nthe security of network traffic or sensor data. However, the behaviors of\ndifferent domains (e.g., network traffic and sensor physical status) of ICSs\nare correlated, so it is difficult to comprehensively identify anomalies by\nanalyzing only a single domain. In this paper, an anomaly detection approach\nbased on cross-domain representation learning in ICSs is proposed, which can\nlearn the joint features of multi-domain behaviors and detect anomalies within\ndifferent domains. After constructing a cross-domain graph that can represent\nthe behaviors of multiple domains in ICSs, our approach can learn the joint\nfeatures of them by leveraging graph neural networks. Since anomalies behave\ndifferently in different domains, we leverage a multi-task learning approach to\nidentify anomalies in different domains separately and perform joint training.\nThe experimental results show that the performance of our approach is better\nthan existing approaches for identifying anomalies in ICSs."}
{"id": "2509.11833", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.11833", "abs": "https://arxiv.org/abs/2509.11833", "authors": ["Xuewei Feng", "Zhaoxi Li", "Qi Li", "Ziqiang Wang", "Kun Sun", "Ke Xu"], "title": "Off-Path TCP Exploits: PMTUD Breaks TCP Connection Isolation in IP Address Sharing Scenarios", "comment": null, "summary": "Path MTU Discovery (PMTUD) and IP address sharing are integral aspects of\nmodern Internet infrastructure. In this paper, we investigate the security\nvulnerabilities associated with PMTUD within the context of prevalent IP\naddress sharing practices. We reveal that PMTUD is inadequately designed to\nhandle IP address sharing, creating vulnerabilities that attackers can exploit\nto perform off-path TCP hijacking attacks. We demonstrate that by observing the\npath MTU value determined by a server for a public IP address (shared among\nmultiple devices), an off-path attacker on the Internet, in collaboration with\na malicious device, can infer the sequence numbers of TCP connections\nestablished by other legitimate devices sharing the same IP address. This\nvulnerability enables the attacker to perform off-path TCP hijacking attacks,\nsignificantly compromising the security of the affected TCP connections. Our\nattack involves first identifying a target TCP connection originating from the\nshared IP address, followed by inferring the sequence numbers of the identified\nconnection. We thoroughly assess the impacts of our attack under various\nnetwork configurations. Experimental results reveal that the attack can be\nexecuted within an average time of 220 seconds, achieving a success rate of\n70%.Case studies, including SSH DoS, FTP traffic poisoning, and HTTP injection,\nhighlight the threat it poses to various applications. Additionally, we\nevaluate our attack across 50 real-world networks with IP address\nsharing--including public Wi-Fi, VPNs, and 5G--and find 38 vulnerable. Finally,\nwe responsibly disclose the vulnerabilities, receive recognition from\norganizations such as IETF, Linux, and Cisco, and propose our countermeasures."}
{"id": "2509.11836", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11836", "abs": "https://arxiv.org/abs/2509.11836", "authors": ["Kai Tan", "Dongyang Zhan", "Lin Ye", "Hongli Zhang", "Binxing Fang"], "title": "A Practical Adversarial Attack against Sequence-based Deep Learning Malware Classifiers", "comment": null, "summary": "Sequence-based deep learning models (e.g., RNNs), can detect malware by\nanalyzing its behavioral sequences. Meanwhile, these models are susceptible to\nadversarial attacks. Attackers can create adversarial samples that alter the\nsequence characteristics of behavior sequences to deceive malware classifiers.\nThe existing methods for generating adversarial samples typically involve\ndeleting or replacing crucial behaviors in the original data sequences, or\ninserting benign behaviors that may violate the behavior constraints. However,\nthese methods that directly manipulate sequences make adversarial samples\ndifficult to implement or apply in practice. In this paper, we propose an\nadversarial attack approach based on Deep Q-Network and a heuristic\nbacktracking search strategy, which can generate perturbation sequences that\nsatisfy practical conditions for successful attacks. Subsequently, we utilize a\nnovel transformation approach that maps modifications back to the source code,\nthereby avoiding the need to directly modify the behavior log sequences. We\nconduct an evaluation of our approach, and the results confirm its\neffectiveness in generating adversarial samples from real-world malware\nbehavior sequences, which have a high success rate in evading anomaly detection\nmodels. Furthermore, our approach is practical and can generate adversarial\nsamples while maintaining the functionality of the modified software."}
{"id": "2509.11864", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.11864", "abs": "https://arxiv.org/abs/2509.11864", "authors": ["Lichao Wu", "Sasha Behrouzi", "Mohamadreza Rostami", "Maximilian Thang", "Stjepan Picek", "Ahmad-Reza Sadeghi"], "title": "NeuroStrike: Neuron-Level Attacks on Aligned LLMs", "comment": null, "summary": "Safety alignment is critical for the ethical deployment of large language\nmodels (LLMs), guiding them to avoid generating harmful or unethical content.\nCurrent alignment techniques, such as supervised fine-tuning and reinforcement\nlearning from human feedback, remain fragile and can be bypassed by carefully\ncrafted adversarial prompts. Unfortunately, such attacks rely on trial and\nerror, lack generalizability across models, and are constrained by scalability\nand reliability.\n  This paper presents NeuroStrike, a novel and generalizable attack framework\nthat exploits a fundamental vulnerability introduced by alignment techniques:\nthe reliance on sparse, specialized safety neurons responsible for detecting\nand suppressing harmful inputs. We apply NeuroStrike to both white-box and\nblack-box settings: In the white-box setting, NeuroStrike identifies safety\nneurons through feedforward activation analysis and prunes them during\ninference to disable safety mechanisms. In the black-box setting, we propose\nthe first LLM profiling attack, which leverages safety neuron transferability\nby training adversarial prompt generators on open-weight surrogate models and\nthen deploying them against black-box and proprietary targets. We evaluate\nNeuroStrike on over 20 open-weight LLMs from major LLM developers. By removing\nless than 0.6% of neurons in targeted layers, NeuroStrike achieves an average\nattack success rate (ASR) of 76.9% using only vanilla malicious prompts.\nMoreover, Neurostrike generalizes to four multimodal LLMs with 100% ASR on\nunsafe image inputs. Safety neurons transfer effectively across architectures,\nraising ASR to 78.5% on 11 fine-tuned models and 77.7% on five distilled\nmodels. The black-box LLM profiling attack achieves an average ASR of 63.7%\nacross five black-box models, including the Google Gemini family."}
{"id": "2509.11870", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.11870", "abs": "https://arxiv.org/abs/2509.11870", "authors": ["Xian Qin", "Xue Yang", "Xiaohu Tang"], "title": "Efficient Byzantine-Robust Privacy-Preserving Federated Learning via Dimension Compression", "comment": null, "summary": "Federated Learning (FL) allows collaborative model training across\ndistributed clients without sharing raw data, thus preserving privacy. However,\nthe system remains vulnerable to privacy leakage from gradient updates and\nByzantine attacks from malicious clients. Existing solutions face a critical\ntrade-off among privacy preservation, Byzantine robustness, and computational\nefficiency. We propose a novel scheme that effectively balances these competing\nobjectives by integrating homomorphic encryption with dimension compression\nbased on the Johnson-Lindenstrauss transformation. Our approach employs a\ndual-server architecture that enables secure Byzantine defense in the\nciphertext domain while dramatically reducing computational overhead through\ngradient compression. The dimension compression technique preserves the\ngeometric relationships necessary for Byzantine defence while reducing\ncomputation complexity from $O(dn)$ to $O(kn)$ cryptographic operations, where\n$k \\ll d$. Extensive experiments across diverse datasets demonstrate that our\napproach maintains model accuracy comparable to non-private FL while\neffectively defending against Byzantine clients comprising up to $40\\%$ of the\nnetwork."}
{"id": "2509.11934", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.11934", "abs": "https://arxiv.org/abs/2509.11934", "authors": ["Praveensankar Manimaran", "Mayank Raikwar", "Thiago Garrett", "Arlindo F. da Conceição", "Leander Jehl", "Roman Vitenberg"], "title": "zkToken: Empowering Holders to Limit Revocation Checks for Verifiable Credentials", "comment": null, "summary": "Systems managing Verifiable Credentials are becoming increasingly popular.\nUnfortunately, their support for revoking previously issued credentials allows\nverifiers to effectively monitor the validity of the credentials, which is\nsensitive information. While the issue started to gain recognition, no adequate\nsolution has been proposed so far.\n  In this work, we propose a novel framework for time-limited continuous\nverification. The holder is able to individually configure the verification\nperiod when sharing information with the verifier, and the system guarantees\nproven untraceability of the revocation status after the verification period\nexpires. Different from existing systems, the implementation adopts a more\nscalable blacklist approach where tokens corresponding to revoked credentials\nare stored in the registry. The approach employs ZK proofs that allow holders\nto prove non-membership in the blacklist. In addition to theoretically proving\nsecurity, we evaluate the approach analytically and experimentally and show\nthat it significantly improves bandwidth consumption on the holder while being\non par with state-of-the-art solutions with respect to the other performance\nmetrics."}
{"id": "2509.11974", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11974", "abs": "https://arxiv.org/abs/2509.11974", "authors": ["Soumia Zohra El Mestari", "Maciej Krzysztof Zuziak", "Gabriele Lenzini"], "title": "Poison to Detect: Detection of Targeted Overfitting in Federated Learning", "comment": null, "summary": "Federated Learning (FL) enables collaborative model training across\ndecentralised clients while keeping local data private, making it a widely\nadopted privacy-enhancing technology (PET). Despite its privacy benefits, FL\nremains vulnerable to privacy attacks, including those targeting specific\nclients. In this paper, we study an underexplored threat where a dishonest\norchestrator intentionally manipulates the aggregation process to induce\ntargeted overfitting in the local models of specific clients. Whereas many\nstudies in this area predominantly focus on reducing the amount of information\nleakage during training, we focus on enabling an early client-side detection of\ntargeted overfitting, thereby allowing clients to disengage before significant\nharm occurs. In line with this, we propose three detection techniques - (a)\nlabel flipping, (b) backdoor trigger injection, and (c) model fingerprinting -\nthat enable clients to verify the integrity of the global aggregation. We\nevaluated our methods on multiple datasets under different attack scenarios.\nOur results show that the three methods reliably detect targeted overfitting\ninduced by the orchestrator, but they differ in terms of computational\ncomplexity, detection latency, and false-positive rates."}
{"id": "2509.12181", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.12181", "abs": "https://arxiv.org/abs/2509.12181", "authors": ["Pujan Paudel", "Gianluca Stringhini"], "title": "LOKI: Proactively Discovering Online Scam Websites by Mining Toxic Search Queries", "comment": "Published in the Proceedings of the 33rd Network and Distributed\n  System Security Symposium (NDSS 2026)", "summary": "Online e-commerce scams, ranging from shopping scams to pet scams, globally\ncause millions of dollars in financial damage every year. In response, the\nsecurity community has developed highly accurate detection systems able to\ndetermine if a website is fraudulent. However, finding candidate scam websites\nthat can be passed as input to these downstream detection systems is\nchallenging: relying on user reports is inherently reactive and slow, and\nproactive systems issuing search engine queries to return candidate websites\nsuffer from low coverage and do not generalize to new scam types. In this\npaper, we present LOKI, a system designed to identify search engine queries\nlikely to return a high fraction of fraudulent websites. LOKI implements a\nkeyword scoring model grounded in Learning Under Privileged Information (LUPI)\nand feature distillation from Search Engine Result Pages (SERPs). We rigorously\nvalidate LOKI across 10 major scam categories and demonstrate a 20.58 times\nimprovement in discovery over both heuristic and data-driven baselines across\nall categories. Leveraging a small seed set of only 1,663 known scam sites, we\nuse the keywords identified by our method to discover 52,493 previously\nunreported scams in the wild. Finally, we show that LOKI generalizes to\npreviously-unseen scam categories, highlighting its utility in surfacing\nemerging threats."}
