<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 40]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Evaluating Adversarial Vulnerabilities in Modern Large Language Models](https://arxiv.org/abs/2511.17666)
*Tom Perel*

Main category: cs.CR

TL;DR: 对Google Gemini 2.5 Flash和OpenAI GPT-4o mini进行越狱攻击对比分析，发现两者在安全性上存在差异，交叉绕过攻击特别有效，揭示了transformer架构中的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型广泛应用，需要深入理解其安全漏洞，评估主流模型的越狱攻击易感性。

Method: 使用两种绕过策略（自我绕过和交叉绕过）和四种攻击方法（直接注入、角色扮演、上下文操纵和混淆），生成五类不安全内容，通过生成被禁止内容来判定攻击成功。

Result: 发现Gemini 2.5 Flash和GPT-4在越狱易感性上存在差异，交叉绕过攻击特别有效，表明transformer架构中存在大量漏洞。

Conclusion: 研究贡献了自动化AI红队测试的可扩展框架，揭示了在平衡模型能力与安全机制方面的复杂挑战。

Abstract: The recent boom and rapid integration of Large Language Models (LLMs) into a wide range of applications warrants a deeper understanding of their security and safety vulnerabilities. This paper presents a comparative analysis of the susceptibility to jailbreak attacks for two leading publicly available LLMs, Google's Gemini 2.5 Flash and OpenAI's GPT-4 (specifically the GPT-4o mini model accessible in the free tier). The research utilized two main bypass strategies: 'self-bypass', where models were prompted to circumvent their own safety protocols, and 'cross-bypass', where one model generated adversarial prompts to exploit vulnerabilities in the other. Four attack methods were employed - direct injection, role-playing, context manipulation, and obfuscation - to generate five distinct categories of unsafe content: hate speech, illegal activities, malicious code, dangerous content, and misinformation. The success of the attack was determined by the generation of disallowed content, with successful jailbreaks assigned a severity score. The findings indicate a disparity in jailbreak susceptibility between 2.5 Flash and GPT-4, suggesting variations in their safety implementations or architectural design. Cross-bypass attacks were particularly effective, indicating that an ample amount of vulnerabilities exist in the underlying transformer architecture. This research contributes a scalable framework for automated AI red-teaming and provides data-driven insights into the current state of LLM safety, underscoring the complex challenge of balancing model capabilities with robust safety mechanisms.

</details>


### [2] [MURMUR: Using cross-user chatter to break collaborative language agents in groups](https://arxiv.org/abs/2511.17671)
*Atharv Singh Patlan,Peiyao Sheng,S. Ashwin Hebbar,Prateek Mittal,Pramod Viswanath*

Main category: cs.CR

TL;DR: 本文提出了一种针对多用户语言代理的新攻击向量——跨用户投毒(CUP)，攻击者通过注入看似正常的消息来污染共享状态，从而触发代理执行攻击者指定的操作。


<details>
  <summary>Details</summary>
Motivation: 随着语言代理从单用户助手扩展到多用户协作环境，当前语言模型缺乏隔离用户交互和并发任务的机制，这为攻击者创造了新的攻击途径。

Method: 提出了MURMUR框架，使用LLM生成逼真的、历史感知的用户交互，将单用户任务组合成并发、基于群体的场景，并验证CUP攻击的有效性。

Result: 在真实系统中成功攻击了流行的多用户代理，CUP攻击成功率很高，且其影响在多个任务中持续存在。

Conclusion: CUP攻击对多用户LLM部署构成根本性风险，提出了基于任务聚类的初步防御措施来缓解这类新漏洞。

Abstract: Language agents are rapidly expanding from single-user assistants to multi-user collaborators in shared workspaces and groups. However, today's language models lack a mechanism for isolating user interactions and concurrent tasks, creating a new attack vector inherent to this new setting: cross-user poisoning (CUP). In a CUP attack, an adversary injects ordinary-looking messages that poison the persistent, shared state, which later triggers the agent to execute unintended, attacker-specified actions on behalf of benign users. We validate CUP on real systems, successfully attacking popular multi-user agents. To study the phenomenon systematically, we present MURMUR, a framework that composes single-user tasks into concurrent, group-based scenarios using an LLM to generate realistic, history-aware user interactions. We observe that CUP attacks succeed at high rates and their effects persist across multiple tasks, thus posing fundamental risks to multi-user LLM deployments. Finally, we introduce a first-step defense with task-based clustering to mitigate this new class of vulnerability

</details>


### [3] [QDNA-ID Quantum Device Native Authentication](https://arxiv.org/abs/2511.17692)
*Osamah N. Neamah*

Main category: cs.CR

TL;DR: QDNA-ID是一个信任链框架，通过将量子物理行为与数字验证记录链接，确保量子计算环境的可信度。系统通过执行量子电路生成熵配置文件，使用贝尔测试验证非经典相关性，创建统计指纹来表征设备，并通过数字签名和时间戳确保完整性。


<details>
  <summary>Details</summary>
Motivation: 量子计算环境需要建立可信的验证机制来确保量子行为的真实性和不可伪造性，防止经典模拟欺骗，并为量子设备提供长期可审计的信任链。

Method: 1) 在不同设备上执行标准量子电路生成熵配置文件；2) 使用贝尔或CHSH测试验证非经典相关性；3) 将结果转换为统计指纹；4) 数字签名和时间戳元数据；5) 分层索引存储；6) 机器学习和可视化分析监控漂移和异常。

Result: 建立了从量子物理行为到数字验证记录的完整信任链，能够检测设备行为漂移、识别异常，并支持独立验证和长期审计。

Conclusion: QDNA-ID提供了一个持续反馈的信任维护系统，为量子计算环境建立了持久可信的验证框架，确保量子计算结果的真实性和可追溯性。

Abstract: QDNA-ID is a trust-chain framework that links physical quantum behavior to digitally verified records. The system first executes standard quantum circuits with random shot patterns across different devices to generate entropy profiles and measurement data that reveal device-specific behavior. A Bell or CHSH test is then used to confirm that correlations originate from genuine non classical processes rather than classical simulation. The verified outcomes are converted into statistical fingerprints using entropy, divergence, and bias features to characterize each device. These features and metadata for device, session, and random seed parameters are digitally signed and time stamped to ensure integrity and traceability. Authenticated artifacts are stored in a hierarchical index for reproducible retrieval and long term auditing. A visualization and analytics interface monitors drift, policy enforcement, and device behavior logs. A machine learning engine tracks entropy drift, detects anomalies, and classifies devices based on evolving patterns. An external verification API supports independent recomputation of hashes, signatures, and CHSH evidence. QDNA-ID operates as a continuous feedback loop that maintains a persistent chain of trust for quantum computing environments.

</details>


### [4] [Pre-cache: A Microarchitectural Solution to prevent Meltdown and Spectre](https://arxiv.org/abs/2511.17726)
*Subhash Sethumurugan,Hari Cherupalli,Kangjie Lu,John Sartori*

Main category: cs.CR

TL;DR: 提出了一种基于微架构的解决方案来防御Meltdown和Spectre攻击，通过防止刷新指令向缓存暴露数据，能够安全地恢复乱序和推测执行，且性能开销较低。


<details>
  <summary>Details</summary>
Motivation: 现代处理器中的乱序和推测执行机制存在安全漏洞，Meltdown和Spectre攻击利用这些性能增强特性的副作用通过微架构侧信道暴露秘密数据。现有的软件补丁只是临时修复且性能开销高达30%。

Method: 提出微架构级解决方案，防止刷新指令向缓存暴露数据，并可扩展到微架构中的其他内存结构，从而防御各种变体攻击。

Result: 评估结果显示该解决方案不仅能够安全地恢复乱序和推测执行，而且具有相对较低的开销，对大多数应用的性能影响不显著。

Conclusion: 该微架构解决方案有效解决了Meltdown和Spectre攻击利用的漏洞，能够在不显著影响性能的情况下提供安全保障。

Abstract: Recent work has shown that out-of-order and speculative execution mechanisms used to increase performance in the majority of processors expose the processors to critical attacks. These attacks, called Meltdown and Spectre, exploit the side effects of performance-enhancing features in modern microprocessors to expose secret data through side channels in the microarchitecture. The well known implementations of these attacks exploit cache-based side channels since they are the least noisy channels to exfiltrate data. While some software patches attempted to mitigate these attacks, they are ad-hoc and only try to fix the side effects of the vulnerabilites. They may also impose a performance overhead of up to 30%. In this paper, we present a microarchitecture-based solution for Meltdown and Spectre that addresses the vulnerabilities exploited by the attacks. Our solution prevents flushed instructions from exposing data to the cache. Our approach can also be extended to other memory structures in the microarchitecture thereby preventing variants of the attacks which exploit these memory structures. We further identify two new variant attacks based on exploiting the side effects of speculative and out-of-order execution and show how our solution can be used to prevent these attacks. Evaluation results show that our microarchitectural solution not only restores secure out-of-order and speculative execution, but also has relatively low overhead and does not significantly impact performance for most applications.

</details>


### [5] [The Dark Side of Flexibility: How Aggregated Cyberattacks Threaten the Power Grid](https://arxiv.org/abs/2511.17748)
*Daniel Myrén,Zeeshan Afzal,Mikael Asplund*

Main category: cs.CR

TL;DR: 本文研究了攻击者如何通过操纵柔性能源资源来破坏电网的首次摆动稳定性，发现当前灵活性容量可能足以在国家层面破坏电网。


<details>
  <summary>Details</summary>
Motivation: 随着柔性能源资源在智能电网中的普及，这些资源及其聚合器存在安全漏洞，攻击者可能远程控制这些资源发动大规模电网攻击。

Method: 调查和评估了操纵柔性能源资源的潜在攻击策略，分析这些策略如何挑战传统电网稳定性措施的有效性。

Result: 研究表明，虽然需要大量电力，但当前的灵活性容量可能足以在国家层面破坏电网的稳定性。

Conclusion: 柔性能源资源的安全漏洞可能被利用来发动大规模电网攻击，威胁电网的首次摆动稳定性，需要加强安全防护措施。

Abstract: Flexible energy resources are increasingly becoming common in smart grids. These resources are typically managed and controlled by aggregators that coordinate many resources to provide flexibility services. However, these aggregators and flexible energy resources are vulnerable, which could allow attackers to remotely control flexible energy resources to launch large-scale attacks on the grid. This paper investigates and evaluates the potential attack strategies that can be used to manipulate flexible energy resources to challenge the effectiveness of traditional grid stability measures and disrupt the first-swing stability of the power grid. Our work shows that although a large amount of power is required, the current flexibility capacities could potentially be sufficient to disrupt the grid on a national level.

</details>


### [6] [StealthCup: Realistic, Multi-Stage, Evasion-Focused CTF for Benchmarking IDS](https://arxiv.org/abs/2511.17761)
*Manuel Kern,Dominik Steffan,Felix Schuster,Florian Skopik,Max Landauer,David Allison,Simon Freudenthaler,Edgar Weippl*

Main category: cs.CR

TL;DR: StealthCup是一种新颖的入侵检测系统评估方法，通过模拟真实攻击场景发现商业和开源IDS存在严重盲点，11种攻击技术未被任何IDS检测到。


<details>
  <summary>Details</summary>
Motivation: 现有IDS评估方法依赖合成数据集或脚本重放，无法捕捉自适应攻击者行为，MITRE ATT&CK评估也主要关注主机层面，缺乏对IT/OT混合环境中隐蔽多阶段入侵的评估。

Method: 采用基于规避的夺旗竞赛形式，专业渗透测试人员在真实IT/OT测试环境中执行多阶段攻击链，通过惩罚IDS检测来评分，生成结构化攻击报告和取证数据。

Result: 在32种攻击技术中，11种未被任何IDS检测；开源系统误报率>90%，商业工具误报较少但漏检更多；与Volt Typhoon APT技术对比显示高度真实性。

Conclusion: StealthCup能够激发与国家支持攻击技术高度一致的行为，暴露了开源和商业IDS的盲点，为未来隐蔽攻击评估提供了可复现基础。

Abstract: Intrusion Detection Systems (IDS) are critical to defending enterprise and industrial control environments, yet evaluating their effectiveness under realistic conditions remains an open challenge. Existing benchmarks rely on synthetic datasets (e.g., NSL-KDD, CICIDS2017) or scripted replay frameworks, which fail to capture adaptive adversary behavior. Even MITRE ATT&CK Evaluations, while influential, are host-centric and assume malware-driven compromise, thereby under-representing stealthy, multi-stage intrusions across IT and OT domains. We present StealthCup, a novel evaluation methodology that operationalizes IDS benchmarking as an evasion-focused Capture-the-Flag competition. Professional penetration testers engaged in multi-stage attack chains on a realistic IT/OT testbed, with scoring penalizing IDS detections. The event generated structured attacker writeups, validated detections, and PCAPs, host logs, and alerts. Our results reveal that out of 32 exercised attack techniques, 11 were not detected by any IDS configuration. Open-source systems (Wazuh, Suricata) produced high false-positive rates >90%, while commercial tools generated fewer false positives but also missed more attacks. Comparison with the Volt Typhoon APT advisory confirmed strong realism: all 28 applicable techniques were exercised, 19 appeared in writeups, and 9 in forensic traces. These findings demonstrate that StealthCup elicits attacker behavior closely aligned with state-sponsored TTPs, while exposing blind spots across both open-source and commercial IDS. The resulting datasets and methodology provide a reproducible foundation for future stealth-focused IDS evaluation.

</details>


### [7] [Characteristics, Root Causes, and Detection of Incomplete Security Bug Fixes in the Linux Kernel](https://arxiv.org/abs/2511.17799)
*Qiang Liu,Wenlong Zhang,Muhui Jiang,Lei Wu,Yajin Zhou*

Main category: cs.CR

TL;DR: 首次系统研究Linux内核中安全漏洞修复不完整问题，分析其特征、根本原因和检测方法


<details>
  <summary>Details</summary>
Motivation: Linux内核安全漏洞不断涌现，但人工修复常因错误导致修复不完整，无法完全修复原始安全缺陷或引入新漏洞

Method: 构建Linux内核不完整安全漏洞修复数据集，分析其特征、根本原因和检测方法

Result: 获得三个主要发现：不完整修复的特征、根本原因以及如何检测以减少安全风险

Conclusion: 首次系统揭示了Linux内核中安全漏洞不完整修复的问题，为改进修复质量提供了重要见解

Abstract: Security bugs in the Linux kernel emerge endlessly and have attracted much attention. However, fixing security bugs in the Linux kernel could be incomplete due to human mistakes. Specifically, an incomplete fix fails to repair all the original security defects in the software, fails to properly repair the original security defects, or introduces new ones. In this paper, we study the fixes of incomplete security bugs in the Linux kernel for the first time, and reveal their characteristics, root causes, as well as detection. We first construct a dataset of incomplete security bug fixes in the Linux kernel and answer the following three questions. What are the characteristics of incomplete security bug fixes in the Linux kernel? What are the root causes behind them? How should they be detected to reduce security risks? We then have the three main insights in the following. (*Due to the notification of arXiv "The Abstract field cannot be longer than 1,920 characters", the appeared Abstract is shortened. For the full Abstract, please download the Article.)

</details>


### [8] [Homomorphic Encryption-based Vaults for Anonymous Balances on VM-enabled Blockchains](https://arxiv.org/abs/2511.17842)
*Xavier Salleras*

Main category: cs.CR

TL;DR: Haults是一个基于同态加密的隐私保护智能钱包协议，用于VM区块链，可保护用户余额和交易金额的机密性，同时包含可选的合规功能。


<details>
  <summary>Details</summary>
Motivation: 在区块链上实现隐私保护的同时满足监管合规要求，需要在保护用户交易隐私和允许必要审计之间取得平衡。

Method: 使用椭圆曲线上的ElGamal加密来加密余额，结合零知识证明验证交易金额的正确性和发送方更新余额的完整性等安全检查。

Result: 概念验证实现的基准测试显示出良好的性能结果，协议支持合约内发行的代币和外部代币（如Ether或ERC20）的兼容性。

Conclusion: Haults协议成功实现了隐私保护与合规要求的平衡，为区块链上的隐私保护智能钱包提供了可行的解决方案。

Abstract: In this work, we present homomorphic encryption-based vaults (Haults), a permissioned privacy-preserving smart wallet protocol for VM-enabled blockchains that keeps users' balances confidential, as well as the amounts transacted to other parties. To comply with regulations, we include optional compliance features that allow specific entities (the auditors) to retrieve transaction amounts or execute force transfers when necessary. Our solution uses ElGamal over elliptic curves to encrypt balances, combined with zero-knowledge proofs to verify the correctness of transaction amounts and the integrity of the sender's updated balance, among other security checks. We provide a detailed explanation of the protocol, including a security discussion and benchmarks from our proof-of-concept implementation, which yield great results. Beyond in-contract issued tokens, we also provide a thorough explanation on how our solution can be compatible with external ones (e.g., Ether or any ERC20).

</details>


### [9] [Beyond Jailbreak: Unveiling Risks in LLM Applications Arising from Blurred Capability Boundaries](https://arxiv.org/abs/2511.17874)
*Yunyi Zhang,Shibo Cui,Baojun Liu,Jingkai Yu,Min Zhang,Fan Shi,Han Zheng*

Main category: cs.CR

TL;DR: 本文系统分析了LLM应用生态系统，定义了LLM应用能力空间概念，揭示了超越越狱的新风险（能力降级和升级），并开发了LLMApp-Eval评估框架，发现89.45%的应用存在潜在风险。


<details>
  <summary>Details</summary>
Motivation: LLM应用的普及带来了新的安全挑战，但安全社区对LLM应用生态系统缺乏足够理解，特别是对应用自身能力边界的认知不足。

Method: 设计了LLM应用能力评估框架LLMApp-Eval，收集了4个平台的应用程序元数据，对199个流行应用和6个开源LLM进行了风险评估。

Result: 发现178个（89.45%）应用存在潜在风险，可执行15+场景的任务或被恶意利用，甚至发现17个应用直接执行恶意任务。提示设计质量与应用鲁棒性呈正相关。

Conclusion: 研究揭示了LLM应用在现实世界中的风险，希望激发社区关注LLM应用安全，促进更健壮的LLM应用生态系统发展。

Abstract: LLM applications (i.e., LLM apps) leverage the powerful capabilities of LLMs to provide users with customized services, revolutionizing traditional application development. While the increasing prevalence of LLM-powered applications provides users with unprecedented convenience, it also brings forth new security challenges. For such an emerging ecosystem, the security community lacks sufficient understanding of the LLM application ecosystem, especially regarding the capability boundaries of the applications themselves.
  In this paper, we systematically analyzed the new development paradigm and defined the concept of the LLM app capability space. We also uncovered potential new risks beyond jailbreak that arise from ambiguous capability boundaries in real-world scenarios, namely, capability downgrade and upgrade. To evaluate the impact of these risks, we designed and implemented an LLM app capability evaluation framework, LLMApp-Eval. First, we collected application metadata across 4 platforms and conducted a cross-platform ecosystem analysis. Then, we evaluated the risks for 199 popular applications among 4 platforms and 6 open-source LLMs. We identified that 178 (89.45%) potentially affected applications, which can perform tasks from more than 15 scenarios or be malicious. We even found 17 applications in our study that executed malicious tasks directly, without applying any adversarial rewriting. Furthermore, our experiments also reveal a positive correlation between the quality of prompt design and application robustness. We found that well-designed prompts enhance security, while poorly designed ones can facilitate abuse. We hope our work inspires the community to focus on the real-world risks of LLM applications and foster the development of a more robust LLM application ecosystem.

</details>


### [10] [Towards Automating Data Access Permissions in AI Agents](https://arxiv.org/abs/2511.17959)
*Yuhao Wu,Ke Yang,Franziska Roesner,Tadayoshi Kohno,Ning Zhang,Umar Iqbal*

Main category: cs.CR

TL;DR: 提出基于机器学习的AI代理自动权限管理方法，通过用户研究识别权限决策影响因素，开发权限预测模型达到85.1%准确率


<details>
  <summary>Details</summary>
Motivation: AI代理的自主行为引发透明度和控制问题，传统权限模型不适用于自动化代理执行范式，需要为AI代理设计自动化权限管理

Method: 进行用户研究识别影响用户权限决策的因素，将这些因素编码到基于机器学习的权限管理助手中，开发权限预测模型

Result: 权限预测模型整体准确率达85.1%，高置信度预测准确率达94.4%；即使不使用权限历史记录，模型准确率仍达66.9%；少量训练样本（1-4个）可使准确率提升10.8%

Conclusion: 用户的权限决策受沟通情境影响，但个体偏好在情境内保持一致性且与其他参与者一致；基于机器学习的权限管理助理能够有效预测用户权限决策

Abstract: As AI agents attempt to autonomously act on users' behalf, they raise transparency and control issues. We argue that permission-based access control is indispensable in providing meaningful control to the users, but conventional permission models are inadequate for the automated agentic execution paradigm. We therefore propose automated permission management for AI agents. Our key idea is to conduct a user study to identify the factors influencing users' permission decisions and to encode these factors into an ML-based permission management assistant capable of predicting users' future decisions. We find that participants' permission decisions are influenced by communication context but importantly individual preferences tend to remain consistent within contexts, and align with those of other participants. Leveraging these insights, we develop a permission prediction model achieving 85.1% accuracy overall and 94.4% for high-confidence predictions. We find that even without using permission history, our model achieves an accuracy of 66.9%, and a slight increase of training samples (i.e., 1-4) can substantially increase the accuracy by 10.8%.

</details>


### [11] [Towards Effective, Stealthy, and Persistent Backdoor Attacks Targeting Graph Foundation Models](https://arxiv.org/abs/2511.17982)
*Jiayi Luo,Qingyun Sun,Lingjuan Lyu,Ziwei Zhang,Haonan Yuan,Xingcheng Fu,Jianxin Li*

Main category: cs.CR

TL;DR: GFM-BA是一种针对图基础模型的后门攻击方法，解决了跨域攻击中的有效性、隐蔽性和持久性三大挑战。


<details>
  <summary>Details</summary>
Motivation: 图基础模型(GFMs)在预训练后适应未见目标域，但后门攻击漏洞尚未充分研究。被攻击的GFM会在下游应用中引入后门行为，带来严重安全风险。

Method: 提出GFM-BA后门攻击模型：1)无标签触发关联模块，将触发器与原型嵌入关联；2)节点自适应触发器生成器，动态生成节点特定触发器；3)持久后门锚定模块，将后门锚定到微调不敏感参数。

Result: 大量实验证明GFM-BA在有效性、隐蔽性和持久性方面表现优异。

Conclusion: GFM-BA成功解决了图基础模型后门攻击的关键挑战，揭示了GFMs的安全漏洞，需要加强防御措施。

Abstract: Graph Foundation Models (GFMs) are pre-trained on diverse source domains and adapted to unseen targets, enabling broad generalization for graph machine learning. Despite that GFMs have attracted considerable attention recently, their vulnerability to backdoor attacks remains largely underexplored. A compromised GFM can introduce backdoor behaviors into downstream applications, posing serious security risks. However, launching backdoor attacks against GFMs is non-trivial due to three key challenges. (1) Effectiveness: Attackers lack knowledge of the downstream task during pre-training, complicating the assurance that triggers reliably induce misclassifications into desired classes. (2) Stealthiness: The variability in node features across domains complicates trigger insertion that remains stealthy. (3) Persistence: Downstream fine-tuning may erase backdoor behaviors by updating model parameters. To address these challenges, we propose GFM-BA, a novel Backdoor Attack model against Graph Foundation Models. Specifically, we first design a label-free trigger association module that links the trigger to a set of prototype embeddings, eliminating the need for knowledge about downstream tasks to perform backdoor injection. Then, we introduce a node-adaptive trigger generator, dynamically producing node-specific triggers, reducing the risk of trigger detection while reliably activating the backdoor. Lastly, we develop a persistent backdoor anchoring module that firmly anchors the backdoor to fine-tuning-insensitive parameters, enhancing the persistence of the backdoor under downstream adaptation. Extensive experiments demonstrate the effectiveness, stealthiness, and persistence of GFM-BA.

</details>


### [12] [Correlated-Sequence Differential Privacy](https://arxiv.org/abs/2511.18025)
*Yifan Luo,Meng Zhang,Jin Xu,Junting Chen,Jianwei Huang*

Main category: cs.CR

TL;DR: 提出了相关序列差分隐私(CSDP)框架，专门用于保护相关时序数据中的隐私，通过建模多变量流为耦合马尔可夫链，并开发了FRAN机制，在相关数据上比现有方法提升约50%的隐私-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 多源数据流通常存在相关性，这种相关性可以提升预测性能，但违反了传统差分隐私的记录独立性假设，需要专门设计隐私保护机制。

Method: 将多变量流建模为耦合马尔可夫链，推导出宽松泄露边界，并构建了FRAN机制，结合数据老化、相关性感知灵敏度缩放和拉普拉斯噪声。

Result: 在双序列数据集上的测试显示，CSDP比现有相关DP方法提升约50%的隐私-效用权衡，比标准DP方法提升两个数量级。

Conclusion: CSDP框架能够在不牺牲效用的前提下为相关时序数据提供严格的隐私保证，且更强的耦合实际上可能通过分散扰动来减少最坏情况泄露。

Abstract: Data streams collected from multiple sources are rarely independent. Values evolve over time and influence one another across sequences. These correlations improve prediction in healthcare, finance, and smart-city control yet violate the record-independence assumption built into most Differential Privacy (DP) mechanisms. To restore rigorous privacy guarantees without sacrificing utility, we introduce Correlated-Sequence Differential Privacy (CSDP), a framework specifically designed for preserving privacy in correlated sequential data. CSDP addresses two linked challenges: quantifying the extra information an attacker gains from joint temporal and cross-sequence links, and adding just enough noise to hide that information while keeping the data useful. We model multivariate streams as a Coupling Markov Chain, yielding the derived loose leakage bound expressed with a few spectral terms and revealing a counterintuitive result: stronger coupling can actually decrease worst-case leakage by dispersing perturbations across sequences. Guided by these bounds, we build the Freshness-Regulated Adaptive Noise (FRAN) mechanism--combining data aging, correlation-aware sensitivity scaling, and Laplace noise--that runs in linear time. Tests on two-sequence datasets show that CSDP improves the privacy-utility trade-off by approximately 50% over existing correlated-DP methods and by two orders of magnitude compared to the standard DP approach.

</details>


### [13] [SCI-IoT: A Quantitative Framework for Trust Scoring and Certification of IoT Devices](https://arxiv.org/abs/2511.18045)
*Shreyansh Swami,Ishwardeep Singh,Chinmay Prawah Pant*

Main category: cs.CR

TL;DR: SCI-IoT是一个标准化的定量框架，用于物联网设备的信任评分、评估和认证，通过六等级模型和30个信任测试来量化设备安全性能。


<details>
  <summary>Details</summary>
Motivation: 物联网生态系统快速增长引发了对设备可靠性、互操作性和安全性的担忧，现有安全指南缺乏统一的定量信任测量方法。

Method: 采用六等级分级模型（A-F级），通过30个信任测试评估认证、加密、数据完整性等维度，每个测试基于关键性权重（1.0-2.0）和性能评级（1-4）计算标准化百分比，加权汇总得到安全认证指数。

Result: SCI-IoT框架能够确定设备的信任裁决，分为五个SCI级别，并作为可选等级认证的基础，同时包含关键门条件以确保高风险参数的绝对合规性。

Conclusion: SCI-IoT通过统一定量信任评分与结构化认证标准，提供了透明、可扩展且可复现的方法来基准化物联网设备可信度，旨在简化制造商合规、提升消费者信心并促进全球互操作性。

Abstract: The exponential growth of the Internet of Things (IoT) ecosystem has amplified concerns regarding device reliability, interoperability, and security assurance. Despite the proliferation of IoT security guidelines, a unified and quantitative approach to measuring trust remains absent. This paper introduces SCI-IoT (Secure Certification Index for IoT), a standardized and quantitative framework for trust scoring, evaluation, and certification of IoT devices.
  The framework employs a six-tier grading model (Grades A-F), enabling device profiling across consumer, industrial, and critical infrastructure domains. Within this model, 30 distinct Trust Tests assess devices across dimensions such as authentication, encryption, data integrity, resilience, and firmware security. Each test is assigned a criticality-based weight (1.0-2.0) and a performance rating (1-4), converted to a normalized percentage and aggregated through a weighted computation to yield the Secure Certification Index (SCI). The SCI determines the device's Trust Verdict, categorized into five SCI levels, and serves as the foundation for optional grade-based certification. The framework also incorporates critical gate conditions, enforcing absolute compliance in high risk parameters to prevent certification of devices with fundamental vulnerabilities. By unifying quantitative trust scoring with structured certification criteria, SCI-IoT provides a transparent, scalable, and reproducible method to benchmark IoT device trustworthiness. The proposed system aims to streamline manufacturer compliance, improve consumer confidence, and facilitate global interoperability in IoT security certification.

</details>


### [14] [Towards Harnessing the Power of LLMs for ABAC Policy Mining](https://arxiv.org/abs/2511.18098)
*More Aayush Babasaheb,Shamik Sural*

Main category: cs.CR

TL;DR: 本文实证研究了大型语言模型在自动化基于属性的访问控制策略挖掘中的能力，发现LLMs在小规模场景中能有效推断紧凑有效的ABAC策略，但随着系统规模增大，准确性和精确度下降，生成策略规模超出最优值。


<details>
  <summary>Details</summary>
Motivation: ABAC提供细粒度、上下文感知的访问管理，但访问策略数量和复杂度的增加使其制定和评估变得困难。研究旨在评估LLMs作为策略挖掘引擎的潜力，以合成简洁准确的策略。

Method: 开发了Python实验框架，生成随机访问数据，参数化不同数量的主体、对象和初始策略集。使用Google Gemini和OpenAI ChatGPT等最先进LLMs作为策略挖掘引擎，将生成的策略与基线策略进行比较评估。

Result: LLMs在小规模场景中能有效推断紧凑有效的ABAC策略，但随着主体和对象数量增加，LLM输出的准确性和精确度下降，生成策略规模显著增加且超出最优大小。

Conclusion: 当前LLM架构在访问控制领域的可扩展策略挖掘方面既有前景也有局限性。未来工作将探索结合提示优化与经典规则挖掘算法的混合方法，以提高复杂ABAC环境中的可扩展性和可解释性。

Abstract: This paper presents an empirical investigation into the capabilities of Large Language Models (LLMs) to perform automated Attribute-based Access Control (ABAC) policy mining. While ABAC provides fine-grained, context-aware access management, the increasing number and complexity of access policies can make their formulation and evaluation rather challenging. To address the task of synthesizing concise yet accurate policies, we evaluate the performance of some of the state-of-the-art LLMs, specifically Google Gemini (Flash and Pro) and OpenAI ChatGPT, as potential policy mining engines. An experimental framework was developed in Python to generate randomized access data parameterized by varying numbers of subjects, objects, and initial policy sets. The baseline policy sets, which govern permission decisions between subjects and objects, serve as the ground truth for comparison. Each LLM-generated policy was evaluated against the baseline policy using standard performance metrics. The results indicate that LLMs can effectively infer compact and valid ABAC policies for small-scale scenarios. However, as the system size increases, characterized by higher numbers of subjects and objects, LLM outputs exhibit declining accuracy and precision, coupled with significant increase in the size of policy generated, which is beyond the optimal size. These findings highlight both the promise and limitations of current LLM architectures for scalable policy mining in access control domains. Future work will explore hybrid approaches that combine prompt optimization with classical rule mining algorithms to improve scalability and interpretability in complex ABAC environments.

</details>


### [15] [ASTRA: Agentic Steerability and Risk Assessment Framework](https://arxiv.org/abs/2511.18114)
*Itay Hazan,Yael Mathov,Guy Shtar,Ron Bitton,Itsik Mantin*

Main category: cs.CR

TL;DR: ASTRA框架是首个评估LLM在创建安全AI代理方面有效性的系统，通过模拟10种不同自主代理和37种工具，测试13个开源LLM在抵御专门针对代理威胁的新型攻击时的安全表现。


<details>
  <summary>Details</summary>
Motivation: AI代理利用LLM作为"大脑"自主执行工具操作，这带来了比传统聊天机器人更严重的风险，因为被入侵的代理可能滥用强大工具执行恶意且不可逆的操作。

Method: 开发ASTRA框架，模拟10种多样化自主代理，配备37种独特工具，使用专门为代理威胁设计的新型攻击套件进行测试，这些攻击基于OWASP Top 10但针对LLM在多轮规划和严格工具激活中的策略执行能力。

Result: 评估13个开源工具调用LLM，发现它们在保持安全和在边界内运行的能力存在显著差异。

Conclusion: 该工作为社区提供了强大统一的方法论来构建和验证更好的LLM，最终推动更安全可靠的代理AI系统发展。

Abstract: Securing AI agents powered by Large Language Models (LLMs) represents one of the most critical challenges in AI security today. Unlike traditional software, AI agents leverage LLMs as their "brain" to autonomously perform actions via connected tools. This capability introduces significant risks that go far beyond those of harmful text presented in a chatbot that was the main application of LLMs. A compromised AI agent can deliberately abuse powerful tools to perform malicious actions, in many cases irreversible, and limited solely by the guardrails on the tools themselves and the LLM ability to enforce them. This paper presents ASTRA, a first-of-its-kind framework designed to evaluate the effectiveness of LLMs in supporting the creation of secure agents that enforce custom guardrails defined at the system-prompt level (e.g., "Do not send an email out of the company domain," or "Never extend the robotic arm in more than 2 meters").
  Our holistic framework simulates 10 diverse autonomous agents varying between a coding assistant and a delivery drone equipped with 37 unique tools. We test these agents against a suite of novel attacks developed specifically for agentic threats, inspired by the OWASP Top 10 but adapted to challenge the ability of the LLM for policy enforcement during multi-turn planning and execution of strict tool activation. By evaluating 13 open-source, tool-calling LLMs, we uncovered surprising and significant differences in their ability to remain secure and keep operating within their boundaries. The purpose of this work is to provide the community with a robust and unified methodology to build and validate better LLMs, ultimately pushing for more secure and reliable agentic AI systems.

</details>


### [16] [eBPF-PATROL: Protective Agent for Threat Recognition and Overreach Limitation using eBPF in Containerized and Virtualized Environments](https://arxiv.org/abs/2511.18155)
*Sangam Ghimire,Nirjal Bhurtel,Roshan Sahani,Sudan Jha*

Main category: cs.CR

TL;DR: eBPF-PATROL是一个基于eBPF技术的轻量级运行时安全代理，用于监控和执行容器化和虚拟化环境中的安全策略，能够实时检测和阻止边界违规行为，如反向shell、权限提升和容器逃逸尝试。


<details>
  <summary>Details</summary>
Motivation: 随着云原生计算的普及，容器化和虚拟化技术成为基础，但严格隔离和运行时安全保护面临挑战。现有方法如seccomp和MAC框架缺乏上下文感知、系统调用参数过滤和自适应执行能力。

Method: 使用扩展伯克利包过滤器(eBPF)技术，通过拦截系统调用、分析执行上下文并应用用户定义规则，实现实时边界违规检测和防护。

Result: 评估显示eBPF-PATROL具有低开销(<2.5%)，并在真实攻击场景中表现出高检测准确率。

Conclusion: eBPF-PATROL提供了一个可扩展的轻量级运行时安全解决方案，能够有效保护容器化和虚拟化环境的安全。

Abstract: With the increasing use and adoption of cloud and cloud-native computing, the underlying technologies (i.e., containerization and virtualization) have become foundational. However, strict isolation and maintaining runtime security in these environments has become increasingly challenging. Existing approaches like seccomp and Mandatory Access Control (MAC) frameworks offer some protection up to a limit, but often lack context awareness, syscall argument filtering, and adaptive enforcement, providing the ability to adjust decisions at runtime based on observed application behavior, workload changes, or detected anomalies rather than relying solely on static or predefined rules.This paper introduces eBPF-PATROL (eBPF-Protective Agent for Threat Recognition and Overreach Limitation), an extensible lightweight runtime security agent that uses extended Berkeley Packet Filter (eBPF) technology to monitor and enforce policies in containerized and virtualized environments. By intercepting system calls, analyzing execution context, and applying user-defined rules, eBPF-PATROL detects and prevents real-time boundary violations, such as reverse shells, privilege escalation, and container escape attempts. We describe the architecture, implementation, and evaluation of eBPF-PATROL, demonstrating its low overhead (< 2.5 percent) and high detection accuracy across real-world attack scenarios.

</details>


### [17] [A Novel and Practical Universal Adversarial Perturbations against Deep Reinforcement Learning based Intrusion Detection Systems](https://arxiv.org/abs/2511.18223)
*H. Zhang,L. Zhang,G. Epiphaniou,C. Maple*

Main category: cs.CR

TL;DR: 提出了一种针对基于深度强化学习的入侵检测系统的通用对抗扰动攻击方法，在考虑网络数据规则和特征关系的领域约束下，通过定制化的Pearson相关系数损失函数增强攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度强化学习的入侵检测系统虽然具有自适应和泛化能力，但研究表明它们容易受到对抗攻击，特别是通用对抗扰动的威胁。目前还没有研究探索针对这类系统的UAP生成方法，特别是在考虑实际领域约束的情况下。

Method: 提出了Customized UAP方法，在生成通用对抗扰动时考虑了网络数据规则和特征间的数学关系，并使用基于Pearson相关系数的定制化损失函数来增强攻击效果。

Result: 实验结果表明，提出的Customized UAP方法在攻击效果上优于两种输入相关攻击方法（FGSM、BIM）和四种UAP基线方法，在真实对抗场景中表现出色。

Conclusion: 该研究首次探索了针对基于深度强化学习的入侵检测系统的通用对抗扰动攻击，并在考虑实际领域约束的情况下开发了有效的攻击方法，揭示了这类系统的安全脆弱性。

Abstract: Intrusion Detection Systems (IDS) play a vital role in defending modern cyber physical systems against increasingly sophisticated cyber threats. Deep Reinforcement Learning-based IDS, have shown promise due to their adaptive and generalization capabilities. However, recent studies reveal their vulnerability to adversarial attacks, including Universal Adversarial Perturbations (UAPs), which can deceive models with a single, input-agnostic perturbation. In this work, we propose a novel UAP attack against Deep Reinforcement Learning (DRL)-based IDS under the domain-specific constraints derived from network data rules and feature relationships. To the best of our knowledge, there is no existing study that has explored UAP generation for the DRL-based IDS. In addition, this is the first work that focuses on developing a UAP against a DRL-based IDS under realistic domain constraints based on not only the basic domain rules but also mathematical relations between the features. Furthermore, we enhance the evasion performance of the proposed UAP, by introducing a customized loss function based on the Pearson Correlation Coefficient, and we denote it as Customized UAP. To the best of our knowledge, this is also the first work using the PCC value in the UAP generation, even in the broader context. Four additional established UAP baselines are implemented for a comprehensive comparison. Experimental results demonstrate that our proposed Customized UAP outperforms two input-dependent attacks including Fast Gradient Sign Method (FGSM), Basic Iterative Method (BIM), and four UAP baselines, highlighting its effectiveness for real-world adversarial scenarios.

</details>


### [18] [Utilizing Circulant Structure to Optimize the Implementations of Linear Layers](https://arxiv.org/abs/2511.18226)
*Buji Xu,Xiaoming Sun*

Main category: cs.CR

TL;DR: 提出了一种利用循环结构优化对称密码学中线性层的新方法，通过构造变换矩阵序列来寻找更高效实现，在多个密码算法的线性层实现上超越了先前工作。


<details>
  <summary>Details</summary>
Motivation: 观察到对称密码学中的线性层矩阵通常具有循环结构，利用这一特性可以优化实现效率。

Method: 利用线性层的循环结构构造变换矩阵序列，结合启发式算法寻找更高效的实现方案。

Result: 在Whirlwind M0上获得159个XOR计数（比FSE 2025工作提升8%）和深度17（比AsiaCrypt 2024工作提升39%）的实现；在AES MixColumn上获得深度10的量子电路，仅比IEEE TC 2024手动优化结果多2个CNOT门。

Conclusion: 该方法能有效利用线性层的循环结构特性，自动生成接近手动优化水平的实现，在多个密码算法的线性层优化中表现出色。

Abstract: In this paper, we propose a novel approach for optimizing the linear layer used in symmetric cryptography. It is observed that these matrices often have circulant structure. The basic idea of this work is to utilize the property to construct a sequence of transformation matrices, which allows subsequent heuristic algorithms to find more efficient implementations. Our results outperform previous works for various linear layers of block ciphers. For Whirlwind M0 , we obtain two implementations with 159 XOR counts (8% better than Yuan et al. at FSE 2025) and depth 17 (39% better than Shi et al. at AsiaCrypt 2024) respectively. For AES MixColumn, our automated method produces a quantum circuit with depth 10, which nearly matches the manually optimized state-of-the-art result by Zhang et al. at IEEE TC 2024, only with 2 extra CNOTs.

</details>


### [19] [Think Fast: Real-Time IoT Intrusion Reasoning Using IDS and LLMs at the Edge Gateway](https://arxiv.org/abs/2511.18230)
*Saeid Jamshidi,Amin Nikanjam,Negar Shahabi,Kawser Wazed Nafi,Foutse Khomh,Samira Keivanpour,Rolando Herrero*

Main category: cs.CR

TL;DR: 提出了一种边缘中心入侵检测系统框架，结合轻量级机器学习模型和预训练大语言模型，在边缘网关实现高精度威胁检测和可解释分析。


<details>
  <summary>Details</summary>
Motivation: 随着物联网设备数量增长，在计算和能源资源有限的环境中保护这些系统免受网络威胁仍然是一个重大挑战。

Method: 在低功耗边缘网关上评估六种ML模型（DT、KNN、RF、CNN、LSTM、CNN-LSTM），并通过低带宽API调用将紧凑的遥测快照传输给LLMs（GPT-4-turbo、DeepSeek V2、LLaMA 3.5）进行零样本、少样本和思维链推理。

Result: 在真实网络攻击下达到98%准确率，延迟<1.5秒，带宽使用<1.2kB/提示，能耗<75J，显著提升可解释性。

Conclusion: 该系统展示了作为边缘网关IDS解决方案的实用性和可扩展性，在保持高性能的同时提供人类可读的威胁分析和缓解建议。

Abstract: As the number of connected IoT devices continues to grow, securing these systems against cyber threats remains a major challenge, especially in environments with limited computational and energy resources. This paper presents an edge-centric Intrusion Detection System (IDS) framework that integrates lightweight machine learning (ML) based IDS models with pre-trained large language models (LLMs) to improve detection accuracy, semantic interpretability, and operational efficiency at the network edge. The system evaluates six ML-based IDS models: Decision Tree (DT), K-Nearest Neighbors (KNN), Random Forest (RF), Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), and a hybrid CNN-LSTM model on low-power edge gateways, achieving accuracy up to 98 percent under real-world cyberattacks. For anomaly detection, the system transmits a compact and secure telemetry snapshot (for example, CPU usage, memory usage, latency, and energy consumption) via low-bandwidth API calls to LLMs including GPT-4-turbo, DeepSeek V2, and LLaMA 3.5. These models use zero-shot, few-shot, and chain-of-thought reasoning to produce human-readable threat analyses and actionable mitigation recommendations. Evaluations across diverse attacks such as DoS, DDoS, brute force, and port scanning show that the system enhances interpretability while maintaining low latency (<1.5 s), minimal bandwidth usage (<1.2 kB per prompt), and energy efficiency (<75 J), demonstrating its practicality and scalability as an IDS solution for edge gateways.

</details>


### [20] [Lightweight Autoencoder-Isolation Forest Anomaly Detection for Green IoT Edge Gateways](https://arxiv.org/abs/2511.18235)
*Saeid Jamshidi,Fatemeh Erfan,Omar Abdul-Wahab,Martine Bellaiche,Foutse Khomh*

Main category: cs.CR

TL;DR: EcoDefender是一个可持续的混合异常检测框架，结合自编码器表示学习和孤立森林异常评分，在保持高检测精度的同时显著降低计算资源和能耗。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测方案在资源受限的边缘环境中部署困难，因为它们往往只关注准确性而忽略计算效率和环境影响。

Method: 集成自编码器(AE)表示学习与孤立森林(IF)异常评分，建立理论保证框架稳定性、收敛性、鲁棒性和能量-复杂度耦合。

Result: 在真实IoT流量上达到94%检测精度，平均CPU使用率仅22%，推理延迟27ms，能耗比纯AE基线降低30%。

Conclusion: 可靠异常检测与环境责任可以在下一代绿色IoT基础设施中共存，符合联合国可持续发展目标(SDG 9和13)。

Abstract: The rapid growth of the Internet of Things (IoT) has given rise to highly diverse and interconnected ecosystems that are increasingly susceptible to sophisticated cyber threats. Conventional anomaly detection schemes often prioritize accuracy while overlooking computational efficiency and environmental impact, which limits their deployment in resource-constrained edge environments. This paper presents \textit{EcoDefender}, a sustainable hybrid anomaly detection framework that integrates \textit{Autoencoder(AE)}-based representation learning with \textit{Isolation Forest(IF)} anomaly scoring. Beyond empirical performance, EcoDefender is supported by a theoretical foundation that establishes formal guarantees for its stability, convergence, robustness, and energy-complexity coupling-thereby linking computational behavior to energy efficiency. Furthermore, experiments on realistic IoT traffic confirm these theoretical insights, achieving up to 94\% detection accuracy with an average CPU usage of only 22\%, 27 ms inference latency, and 30\% lower energy consumption compared to AE-only baselines. By embedding sustainability metrics directly into the security evaluation process, this work demonstrates that reliable anomaly detection and environmental responsibility can coexist within next-generation green IoT infrastructures, aligning with the United Nations Sustainable Development Goals (SDG 9: resilient infrastructure, SDG 13: climate action).

</details>


### [21] [Carbon-Aware Intrusion Detection: A Comparative Study of Supervised and Unsupervised DRL for Sustainable IoT Edge Gateways](https://arxiv.org/abs/2511.18240)
*Saeid Jamshidi,Foutse Khomh,Kawser Wazed Nafi,Amin Nikanjam,Samira Keivanpour,Omar Abdul-Wahab,Martine Bellaiche*

Main category: cs.CR

TL;DR: 本文提出了两种基于深度强化学习(DRL)的入侵检测系统：无监督的DeepEdgeIDS和监督的AutoDRL-IDS，用于解决物联网边缘网络中的DDoS攻击检测问题，并引入了碳感知的多目标奖励函数以实现可持续性。


<details>
  <summary>Details</summary>
Motivation: 物联网的快速发展加剧了网络安全挑战，特别是边缘网络中的DDoS攻击。传统入侵检测系统存在适应性差、依赖静态特征和标记数据集、在资源受限的边缘网关上效率低等问题，且现有DRL方法忽视能源效率和碳影响等可持续性因素。

Method: 提出了两种DRL-based IDS：DeepEdgeIDS（无监督的自动编码器-DRL混合模型）和AutoDRL-IDS（监督的LSTM-DRL模型），并引入了碳感知的多目标奖励函数，通过理论分析和边缘网关实验验证。

Result: AutoDRL-IDS在使用标记数据时达到94%检测准确率，DeepEdgeIDS在无标签情况下达到98%准确率并具有适应性。两种系统都实现了可持续和实时的IDS操作。

Conclusion: 所提出的DRL-based IDS方法有效解决了物联网边缘网络中的DDoS攻击检测问题，DeepEdgeIDS在无监督情况下表现出更高的准确率和适应性，同时通过碳感知奖励函数实现了可持续性目标。

Abstract: The rapid expansion of the Internet of Things (IoT) has intensified cybersecurity challenges, particularly in mitigating Distributed Denial-of-Service (DDoS) attacks at the network edge. Traditional Intrusion Detection Systems (IDSs) face significant limitations, including poor adaptability to evolving and zero-day attacks, reliance on static signatures and labeled datasets, and inefficiency on resource-constrained edge gateways. Moreover, most existing DRL-based IDS studies overlook sustainability factors such as energy efficiency and carbon impact. To address these challenges, this paper proposes two novel Deep Reinforcement Learning (DRL)-based IDS: DeepEdgeIDS, an unsupervised Autoencoder-DRL hybrid, and AutoDRL-IDS, a supervised LSTM-DRL model. Both DRL-based IDS are validated through theoretical analysis and experimental evaluation on edge gateways. Results demonstrate that AutoDRL-IDS achieves 94% detection accuracy using labeled data, while DeepEdgeIDS attains 98% accuracy and adaptability without labels. Distinctly, this study introduces a carbon-aware, multi-objective reward function optimized for sustainable and real-time IDS operations in dynamic IoT networks.

</details>


### [22] [On Addressing Isolation in Blockchain-Based Self-Sovereign Identity](https://arxiv.org/abs/2511.18379)
*Andreea Elena Drăgnoiu,Andrei Ciobanu,Ruxandra F. Olimid*

Main category: cs.CR

TL;DR: 本文研究了区块链隔离对基于区块链的自认证身份系统的影响，定义了跨链SSI场景和需求，分析了区块链互操作性解决方案，并讨论了安全隐私问题。


<details>
  <summary>Details</summary>
Motivation: 区块链通常是孤立的，这影响了自认证身份系统的互操作性和普适性，需要研究跨链SSI解决方案。

Method: 定义跨链SSI场景和用例，制定具体要求，识别挑战，探索区块链互操作性解决方案，分析不同跨链模型的优缺点。

Result: 识别了区块链隔离对SSI的影响，定义了跨链SSI的具体需求和挑战，并分析了各种互操作性解决方案的适用性。

Conclusion: 跨链SSI是解决区块链隔离问题的关键方向，需要进一步研究其可用性、安全性和隐私保护机制。

Abstract: Self-Sovereign Identity (SSI) grants holders full ownership and control of their digital identities, being the ultimate digital identity model. Operating in a decentralized manner, SSI enables the verification of claims, including privacy-preserving mechanisms. Blockchain, which can be used to implement a Verifiable Data Registry (VDR), is often considered one of the pillars of SSI, along with Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs). Unfortunately, blockchains are mostly siloed, affecting the interoperability and universality of SSI. We investigate the effect of blockchain isolation on blockchain-based SSI. We first define possible scenarios for cross-chain SSI and exemplify with real-life use cases. We then define specific requirements for cross-chain SSI and identify challenges, also in relation to the identified scenarios. We explore various solutions to achieve blockchain interoperability, with a focus on SSI. In particular, we identify the advantages and disadvantages of distinct cross-chain models for cross-chain SSI. Finally, we address the usability of cross-chain SSI and discuss security and privacy aspects, opening the way for future research.

</details>


### [23] [ioPUF+: A PUF Based on I/O Pull-Up/Down Resistors for Secret Key Generation in IoT Nodes](https://arxiv.org/abs/2511.18412)
*Dilli Babu Porlapothula,Pralay Chakrabarty,Ananya Lakshmi Ravi,Kurian Polachan*

Main category: cs.CR

TL;DR: ioPUF+是一种新型物理不可克隆函数(PUF)，利用IC芯片I/O引脚的上拉/下拉电阻值变化生成设备唯一指纹，无需额外电路，适用于商用现成组件构建的嵌入式系统，并包含完整的密钥生成和安全通信数据通路。


<details>
  <summary>Details</summary>
Motivation: 为资源受限的物联网设备提供低成本、无需定制电路的硬件安全解决方案，利用现有IC制造工艺中的固有变化来生成设备唯一标识。

Method: 通过测量IC芯片I/O引脚的上拉和下拉电阻值，利用制造过程中的工艺变化生成设备特定响应，结合BCH纠错和SHA-256哈希将原始PUF响应转换为加密可用密钥，并实现AES加密的安全通信。

Result: 在30个设备上评估显示：可靠性100%、独特性50.33%、均匀性50.54%、位别名可忽略；温度变化下最差误码率2.63%、电压变化下2.10%；完整系统仅需19.8KB Flash、600ms延迟、79mW功耗。

Conclusion: ioPUF+提供了一种高效、低成本的PUF解决方案，特别适合资源受限的物联网节点，无需定制IC制造即可实现硬件级安全功能。

Abstract: In this work, we present ioPUF+, which incorporates a novel Physical Unclonable Function (PUF) that generates unique fingerprints for Integrated Circuits (ICs) and the IoT nodes encompassing them. The proposed PUF generates device-specific responses by measuring the pull-up and pull-down resistor values on the I/O pins of the ICs, which naturally vary across chips due to manufacturing-induced process variations. Since these resistors are already integrated into the I/O structures of most ICs, ioPUF+ requires no custom circuitry, and no new IC fabrication. This makes ioPUF+ suitable for cost-sensitive embedded systems built from Commercial Off-The-Shelf (COTS) components. Beyond introducing a new PUF, ioPUF+ includes a complete datapath for converting raw PUF responses into cryptographically usable secret keys using BCH error correction and SHA-256 hashing. Further ioPUF+ also demonstrate a practical use case of PUF derive secret keys in securing device-to-device communication using AES-encryption. We implemented ioPUF+ on the Infineon PSoC-5 microcontroller and evaluated its performance across 30 devices using standard PUF metrics. The results show excellent reliability (intra-device Hamming distance of 100.00%), strong uniqueness (inter-device Hamming distance of 50.33%), near-ideal uniformity (50.54%), and negligible bit aliasing. Stability tests under temperature and supply-voltage variations show worst-case bit-error rates of only 2.63% and 2.10%, respectively. We also profiled the resource and energy usage of the complete ioPUF+ system, including the PUF primitive, BCH decoding, SHA-256 hashing, and AES encryption. The full implementation requires only 19.8 KB of Flash, exhibits a latency of 600 ms, and consumes 79 mW of power, demonstrating the suitabilitiy of ioPUF+ for resource-constrained IoT nodes.

</details>


### [24] [LLMs as Firmware Experts: A Runtime-Grown Tree-of-Agents Framework](https://arxiv.org/abs/2511.18438)
*Xiangrui Zhang,Zeyu Chen,Haining Wang,Qiang Li*

Main category: cs.CR

TL;DR: FIRMHIVE是一个递归代理蜂巢系统，使LLM能够作为自主固件安全分析器，通过将委托转化为可执行原语和构建运行时代理树，显著提升大规模固件安全分析的深度和广度。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在大型固件安全分析中性能下降的问题，因为固件的二进制特性、复杂依赖结构和异构组件导致现有方法效果不佳。

Method: 提出FIRMHIVE系统，包含两个关键机制：(1)将委托转化为每个代理的可执行原语；(2)构建运行时代理树实现去中心化协调。

Result: 相比现有LLM代理基线，FIRMHIVE实现约16倍更多推理步骤和2.3倍更多文件检查，产生约5.6倍更多警报；相比SOTA安全工具，识别约1.5倍更多漏洞（共1802个），达到71%精确率。

Conclusion: FIRMHIVE在固件安全分析任务中显著提升了分析深度、广度和漏洞检测能力，在产量和保真度方面都有重大改进。

Abstract: Large Language Models (LLMs) and their agent systems have recently demonstrated strong potential in automating code reasoning and vulnerability detection. However, when applied to large-scale firmware, their performance degrades due to the binary nature of firmware, complex dependency structures, and heterogeneous components. To address this challenge, this paper presents FIRMHIVE, a recursive agent hive that enables LLMs to act as autonomous firmware security analysts. FIRMHIVE introduces two key mechanisms: (1) transforming delegation into a per-agent, executable primitive and (2) constructing a runtime Tree of Agents (ToA) for decentralized coordination. We evaluate FIRMHIVE using real-world firmware images obtained from publicly available datasets, covering five representative security analysis tasks. Compared with existing LLM-agent baselines, FIRMHIVE performs deeper (about 16x more reasoning steps) and broader (about 2.3x more files inspected) cross-file exploration, resulting in about 5.6x more alerts per firmware. Compared to state-of-the-art (SOTA) security tools, FIRMHIVE identifies about 1.5x more vulnerabilities (1,802 total) and achieves 71% precision, representing significant improvements in both yield and fidelity.

</details>


### [25] [Shadows in the Code: Exploring the Risks and Defenses of LLM-based Multi-Agent Software Development Systems](https://arxiv.org/abs/2511.18467)
*Xiaoqing Wang,Keman Huang,Bin Liang,Hongyu Li,Xiaoyong Du*

Main category: cs.CR

TL;DR: 本文分析了LLM驱动的多智能体系统在软件开发中的安全风险，识别了两种危险场景（恶意用户+良性智能体、良性用户+恶意智能体），提出了IMBIA攻击方法和Adv-IMBIA防御机制，并在多个框架中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的多智能体系统虽然降低了软件开发门槛，但引入了严重的安全风险，这些风险尚未得到充分研究。

Method: 提出了IMBIA（隐式恶意行为注入攻击）方法，展示如何在看似良性的应用中隐藏恶意功能，并设计了Adv-IMBIA作为防御机制。在ChatDev、MetaGPT和AgentVerse框架上进行评估。

Result: IMBIA在MU-BA场景下的攻击成功率分别为93%、45%和71%，在BU-MA场景下为71%、84%和45%。防御机制显著降低了攻击成功率，特别是在MU-BA场景中。编码和测试阶段的被入侵智能体风险最大。

Conclusion: 多智能体软件开发系统迫切需要强大的安全措施，研究为实施有针对性的防御策略提供了实用指南。

Abstract: The rapid advancement of Large Language Model (LLM)-driven multi-agent systems has significantly streamlined software developing tasks, enabling users with little technical expertise to develop executable applications. While these systems democratize software creation through natural language requirements, they introduce significant security risks that remain largely unexplored. We identify two risky scenarios: Malicious User with Benign Agents (MU-BA) and Benign User with Malicious Agents (BU-MA). We introduce the Implicit Malicious Behavior Injection Attack (IMBIA), demonstrating how multi-agent systems can be manipulated to generate software with concealed malicious capabilities beneath seemingly benign applications, and propose Adv-IMBIA as a defense mechanism. Evaluations across ChatDev, MetaGPT, and AgentVerse frameworks reveal varying vulnerability patterns, with IMBIA achieving attack success rates of 93%, 45%, and 71% in MU-BA scenarios, and 71%, 84%, and 45% in BU-MA scenarios. Our defense mechanism reduced attack success rates significantly, particularly in the MU-BA scenario. Further analysis reveals that compromised agents in the coding and testing phases pose significantly greater security risks, while also identifying critical agents that require protection against malicious user exploitation. Our findings highlight the urgent need for robust security measures in multi-agent software development systems and provide practical guidelines for implementing targeted, resource-efficient defensive strategies.

</details>


### [26] [DEXO: A Secure and Fair Exchange Mechanism for Decentralized IoT Data Markets](https://arxiv.org/abs/2511.18498)
*Yue Li,Ifteher Alom,Wenhai Sun,Yang Xiao*

Main category: cs.CR

TL;DR: DEXO是一个去中心化的数据交换机制，通过结合可信执行环境、秘密共享和智能合约，确保物联网和移动设备数据交换的端到端机密性、来源可验证性和公平性。


<details>
  <summary>Details</summary>
Motivation: 物联网和移动设备产生的数据具有巨大经济价值，但数据源可信度和交易过程安全性存在挑战，特别是在数据提供者和消费者之间缺乏信任的情况下。

Method: 扩展去中心化预言机网络模型，采用硬件-密码学协同设计，整合可信执行环境、秘密共享和智能合约辅助的公平交换。

Result: 原型系统验证了可行性，部署成本适中，与流行的数据交换机制相比显著提高了区块链操作效率。

Conclusion: DEXO首次实现了端到端数据机密性、来源可验证性和交换过程公平性，对参与者合谋具有强韧性。

Abstract: Opening up data produced by the Internet of Things (IoT) and mobile devices for public utilization can maximize their economic value. Challenges remain in the trustworthiness of the data sources and the security of the trading process, particularly when there is no trust between the data providers and consumers. In this paper, we propose DEXO, a decentralized data exchange mechanism that facilitates secure and fair data exchange between data consumers and distributed IoT/mobile data providers at scale, allowing the consumer to verify the data generation process and the providers to be compensated for providing authentic data, with correctness guarantees from the exchange platform. To realize this, DEXO extends the decentralized oracle network model that has been successful in the blockchain applications domain to incorporate novel hardware-cryptographic co-design that harmonizes trusted execution environment, secret sharing, and smart contract-assisted fair exchange. For the first time, DEXO ensures end-to-end data confidentiality, source verifiability, and fairness of the exchange process with strong resilience against participant collusion. We implemented a prototype of the DEXO system to demonstrate feasibility. The evaluation shows a moderate deployment cost and significantly improved blockchain operation efficiency compared to a popular data exchange mechanism.

</details>


### [27] [LockForge: Automating Paper-to-Code for Logic Locking with Multi-Agent Reasoning LLMs](https://arxiv.org/abs/2511.18531)
*Akashdeep Saha,Zeng Wang,Prithwish Basu Roy,Johann Knechtel,Ozgur Sinanoglu,Ramesh Karri*

Main category: cs.CR

TL;DR: LockForge是一个多智能体LLM框架，可将逻辑锁定方案的论文描述转化为可执行代码，解决该领域代码不公开导致的复现性问题。


<details>
  <summary>Details</summary>
Motivation: 逻辑锁定领域代码很少公开，导致研究难以复现，需要一种系统化方法将论文描述转化为可执行代码。

Method: 使用多智能体LLM框架，包含前瞻思考、实现、迭代优化和多阶段验证的完整流程，通过LLM-as-Judge和LLM-as-Examiner进行验证。

Result: 成功应用于10个经典逻辑锁定方案，其中许多缺乏参考实现，验证了框架的有效性和任务复杂性。

Conclusion: 需要先进的推理模型和复杂多阶段框架才能完成此任务，所有实现和基准测试已公开，为后续研究提供可复现基础。

Abstract: Despite rapid progress in logic locking (LL), reproducibility remains a challenge as codes are rarely made public. We present LockForge, a first-of-its-kind, multi-agent large language model (LLM) framework that turns LL descriptions in papers into executable and tested code. LockForge provides a carefully crafted pipeline realizing forethought, implementation, iterative refinement, and a multi-stage validation, all to systematically bridge the gap between prose and practice for complex LL schemes. For validation, we devise (i) an LLM-as-Judge stage with a scoring system considering behavioral checks, conceptual mechanisms, structural elements, and reproducibility on benchmarks, and (ii) an independent LLM-as-Examiner stage for ground-truth assessment. We apply LockForge to 10 seminal LL schemes, many of which lack reference implementations. Our evaluation on multiple SOTA LLMs, including ablation studies, reveals the significant complexity of the task. We show that an advanced reasoning model and a sophisticated, multi-stage framework like LockForge are required. We release all implementations and benchmarks, providing a reproducible and fair foundation for evaluation of further LL research.

</details>


### [28] [Zero-Trust Strategies for O-RAN Cellular Networks: Principles, Challenges and Research Directions](https://arxiv.org/abs/2511.18568)
*Charalampos Katsis,Imtiaz Karim,Elisa Bertino*

Main category: cs.CR

TL;DR: 本文探讨了在5G及未来网络中采用零信任架构（ZTA），特别关注开放无线接入网（O-RAN）作为架构赋能者，分析了ZTA原则如何与O-RAN的架构特性相契合。


<details>
  <summary>Details</summary>
Motivation: 随着蜂窝网络从封闭、供应商锁定的基础设施演变为开放可编程生态系统（如O-RAN），安全成为日益关键的问题。零信任架构通过消除隐式信任假设，为这种开放环境提供了有前景的安全范式。

Method: 分析ZTA原则与O-RAN架构和运营特性的契合度，识别在基于O-RAN的蜂窝网络中嵌入零信任机制的关键挑战和机遇。

Result: 研究发现ZTA与O-RAN的架构特性具有良好的一致性，为在开放可编程的蜂窝网络中实施细粒度安全机制提供了框架。

Conclusion: O-RAN作为架构赋能者，为在5G及未来网络中实施零信任安全提供了可行路径，但需要解决相关的技术挑战以实现全面的零信任保护。

Abstract: Cellular networks have become foundational to modern communication, supporting a broad range of applications, from civilian use to enterprise systems and military tactical networks. The advent of fifth-generation and beyond cellular networks (B5G) introduces emerging compute capabilities into the Radio Access Network (RAN), transforming it from a traditionally closed, vendor-locked infrastructure into an open and programmable ecosystem. This evolution, exemplified by Open-RAN (O-RAN), enables the deployment of control-plane applications from diverse sources, which can dynamically influence user-plane traffic in response to real-time events. As cellular infrastructures become more disaggregated and software-driven, security becomes an increasingly critical concern. Zero-Trust Architecture (ZTA) has emerged as a promising security paradigm that discards implicit trust assumptions by acknowledging that threats may arise from both external and internal sources. ZTA mandates comprehensive and fine-grained security mechanisms across both control and user planes to contain adversarial movements and enhance breach detection and attack response actions. In this paper, we explore the adoption of ZTA in the context of 5G and beyond, with a particular focus on O-RAN as an architectural enabler. We analyze how ZTA principles align with the architectural and operational characteristics of O-RAN, and identify key challenges and opportunities for embedding zero-trust mechanisms within O-RAN-based cellular networks.

</details>


### [29] [TASO: Jailbreak LLMs via Alternative Template and Suffix Optimization](https://arxiv.org/abs/2511.18581)
*Yanting Wang,Runpeng Geng,Jinghui Chen,Minhao Cheng,Jinyuan Jia*

Main category: cs.CR

TL;DR: TASO是一种新颖的越狱攻击方法，通过交替优化模板和后缀来有效诱导LLM生成有害输出。


<details>
  <summary>Details</summary>
Motivation: 现有越狱技术要么优化语义模板诱导有害输出，要么优化后缀控制初始响应token，但两者各有局限性，需要互补结合。

Method: TASO方法交替优化模板和后缀：后缀优化控制前几个输出token，模板优化指导整个输出质量，两者互补提升攻击效果。

Result: 在24个主流LLM（包括Llama、OpenAI、DeepSeek等）的基准数据集（HarmBench和AdvBench）上评估，TASO能有效越狱现有LLM。

Conclusion: TASO证明了模板和后缀优化结合的互补优势，为未来研究方向提供启发。

Abstract: Many recent studies showed that LLMs are vulnerable to jailbreak attacks, where an attacker can perturb the input of an LLM to induce it to generate an output for a harmful question. In general, existing jailbreak techniques either optimize a semantic template intended to induce the LLM to produce harmful outputs or optimize a suffix that leads the LLM to initiate its response with specific tokens (e.g., "Sure").
  In this work, we introduce TASO (Template and Suffix Optimization), a novel jailbreak method that optimizes both a template and a suffix in an alternating manner. Our insight is that suffix optimization and template optimization are complementary to each other: suffix optimization can effectively control the first few output tokens but cannot control the overall quality of the output, while template optimization provides guidance for the entire output but cannot effectively control the initial tokens, which significantly impact subsequent responses. Thus, they can be combined to improve the attack's effectiveness.
  We evaluate the effectiveness of TASO on benchmark datasets (including HarmBench and AdvBench) on 24 leading LLMs (including models from the Llama family, OpenAI, and DeepSeek). The results demonstrate that TASO can effectively jailbreak existing LLMs. We hope our work can inspire future studies in exploring this direction. We will make code and data publicly available.

</details>


### [30] [FHE-Agent: Automating CKKS Configuration for Practical Encrypted Inference via an LLM-Guided Agentic Framework](https://arxiv.org/abs/2511.18653)
*Nuo Xu,Zhaoting Gong,Ran Ran,Jinwei Tang,Wujie Wen,Caiwen Ding*

Main category: cs.CR

TL;DR: FHE-Agent是一个基于大语言模型的代理框架，用于自动化CKKS全同态加密方案的参数配置，解决了传统方法需要深度密码学专业知识的问题。


<details>
  <summary>Details</summary>
Motivation: 全同态加密特别是CKKS方案是实现隐私保护机器学习即服务的有前景技术，但其实际部署面临重大障碍：配置过程需要深度密码学专业知识，涉及环维度、模数链和打包布局的紧密耦合空间。

Method: FHE-Agent将大语言模型控制器与确定性工具套件相结合，将搜索过程分解为全局参数选择和逐层瓶颈修复，采用多保真度工作流，使用廉价静态分析剪枝无效区域，为最有希望的候选方案保留昂贵的加密评估。

Result: 在Orion编译器上实例化FHE-Agent，并在标准基准测试和更深层架构上评估，FHE-Agent始终比简单搜索策略获得更好的精度和更低延迟，能够自动发现复杂模型的可行128位安全配置。

Conclusion: FHE-Agent框架成功自动化了专家推理过程，在传统启发式方法和一次性提示无法产生有效设置的复杂模型中发现了可行的安全配置。

Abstract: Fully Homomorphic Encryption (FHE), particularly the CKKS scheme, is a promising enabler for privacy-preserving MLaaS, but its practical deployment faces a prohibitive barrier: it heavily relies on domain expertise. Configuring CKKS involves a tightly coupled space of ring dimensions, modulus chains, and packing layouts. Without deep cryptographic knowledge to navigate these interactions, practitioners are restricted to compilers that rely on fixed heuristics. These "one-shot" tools often emit rigid configurations that are either severely over-provisioned in latency or fail to find a feasible solution entirely for deeper networks.
  We present FHE-Agent, an agentic framework that automates this expert reasoning process. By coupling a Large Language Model (LLM) controller with a deterministic tool suite, FHE-Agent decomposes the search into global parameter selection and layer-wise bottleneck repair. The agents operate within a multi-fidelity workflow, pruning invalid regimes using cheap static analysis and reserving expensive encrypted evaluations for the most promising candidates.
  We instantiate FHE-Agent on the Orion compiler and evaluate it on standard benchmarks (MLP, LeNet, LoLa) and deeper architectures (AlexNet). FHE-Agent consistently achieves better precision and lower latency than naïve search strategies. Crucially, it automatically discovers feasible, 128-bit secure configurations for complex models where baseline heuristics and one-shot prompts fail to produce a valid setup.

</details>


### [31] [Evaluation of Real-Time Mitigation Techniques for Cyber Security in IEC 61850 / IEC 62351 Substations](https://arxiv.org/abs/2511.18748)
*Akila Herath,Chen-Ching Liu,Junho Hong,Kuchan Park*

Main category: cs.CR

TL;DR: 本文提出了三种实时缓解GOOSE攻击的技术：IEC 62351兼容的MAC认证、基于语义的规则IDS以及两者的混合方法。混合方法在测试中表现最佳，所有方法都能满足GOOSE通信的实时要求。


<details>
  <summary>Details</summary>
Motivation: 变电站数字化扩大了网络攻击面，需要有效的实时攻击检测和缓解。现有机器学习方法无法满足实时性要求，而密码认证和轻量级规则检测更具实用性。

Method: 设计并实现了三种实时缓解技术：MAC认证方案、语义规则IDS以及混合方法。使用CPS安全测试床进行对比评估。

Result: 混合集成显著提升了缓解能力，所有方法的处理延迟都满足GOOSE通信的严格交付要求。

Conclusion: 混合方法是最有效的实时缓解方案，但仍存在无法完全解决的局限性，需要未来进一步研究。

Abstract: The digitalization of substations enlarges the cyber-attack surface, necessitating effective detection and mitigation of cyber attacks in digital substations. While machine learning-based intrusion detection has been widely explored, such methods have not demonstrated detection and mitigation within the required real-time budget. In contrast, cryptographic authentication has emerged as a practical candidate for real-time cyber defense, as specified in IEC 62351. In addition, lightweight rule-based intrusion detection that validates IEC 61850 semantics can provide specification-based detection of anomalous or malicious traffic with minimal processing delay. This paper presents the design logic and implementation aspects of three potential real-time mitigation techniques capable of countering GOOSE-based attacks: (i) IEC 62351-compliant message authentication code (MAC) scheme, (ii) a semantics-enforced rule-based intrusion detection system (IDS), and (iii) a hybrid approach integrating both MAC verification and Intrusion Detection System (IDS). A comparative evaluation of these real-time mitigation approaches is conducted using a cyber-physical system (CPS) security testbed. The results show that the hybrid integration significantly enhances mitigation capability. Furthermore, the processing delays of all three methods remain within the strict delivery requirements of GOOSE communication. The study also identifies limitations that none of the techniques can fully address, highlighting areas for future work.

</details>


### [32] [Re-Key-Free, Risky-Free: Adaptable Model Usage Control](https://arxiv.org/abs/2511.18772)
*Zihan Wang,Zhongkui Ma,Xinguo Feng,Chuan Yan,Dongge Liu,Ruoxi Sun,Derui Wang,Minhui Xue,Guangdong Bai*

Main category: cs.CR

TL;DR: ADALOC是一种自适应深度神经网络模型使用控制方法，通过在模型权重中嵌入访问密钥，使模型在持续更新过程中保持保护能力，无需完全重新分发或重新加锁。


<details>
  <summary>Details</summary>
Motivation: 现有模型使用控制方法假设静态部署，无法应对模型部署后的持续更新（如微调、任务特定适应），导致保护失效。需要一种能在模型演化过程中保持保护能力的方法。

Method: 策略性地选择权重子集作为内在访问密钥，将所有模型更新限制在该密钥范围内，使用访问密钥将密钥模型恢复到最新授权状态而无需重新分发整个网络。

Result: 在CIFAR-100、Caltech-256、Flowers-102等标准基准测试和ResNet、DenseNet、ConvNeXt等架构上，ADALOC在显著更新下保持高精度，同时提供强大保护。授权使用达到强任务特定性能，未授权使用准确率降至接近随机猜测水平（如CIFAR-100上1.01%），而未经ADALOC保护可达87.01%。

Conclusion: ADALOC为演化中的现实世界场景提供了自适应且受保护的DNN部署实用解决方案，能够在模型持续更新过程中保持强大的使用控制能力。

Abstract: Deep neural networks (DNNs) have become valuable intellectual property of model owners, due to the substantial resources required for their development. To protect these assets in the deployed environment, recent research has proposed model usage control mechanisms to ensure models cannot be used without proper authorization. These methods typically lock the utility of the model by embedding an access key into its parameters. However, they often assume static deployment, and largely fail to withstand continual post-deployment model updates, such as fine-tuning or task-specific adaptation. In this paper, we propose ADALOC, to endow key-based model usage control with adaptability during model evolution. It strategically selects a subset of weights as an intrinsic access key, which enables all model updates to be confined to this key throughout the evolution lifecycle. ADALOC enables using the access key to restore the keyed model to the latest authorized states without redistributing the entire network (i.e., adaptation), and frees the model owner from full re-keying after each model update (i.e., lock preservation). We establish a formal foundation to underpin ADALOC, providing crucial bounds such as the errors introduced by updates restricted to the access key. Experiments on standard benchmarks, such as CIFAR-100, Caltech-256, and Flowers-102, and modern architectures, including ResNet, DenseNet, and ConvNeXt, demonstrate that ADALOC achieves high accuracy under significant updates while retaining robust protections. Specifically, authorized usages consistently achieve strong task-specific performance, while unauthorized usage accuracy drops to near-random guessing levels (e.g., 1.01% on CIFAR-100), compared to up to 87.01% without ADALOC. This shows that ADALOC can offer a practical solution for adaptive and protected DNN deployment in evolving real-world scenarios.

</details>


### [33] [RoguePrompt: Dual-Layer Ciphering for Self-Reconstruction to Circumvent LLM Moderation](https://arxiv.org/abs/2511.18790)
*Benyamin Tafreshian*

Main category: cs.CR

TL;DR: RoguePrompt是一种自动化的越狱攻击方法，通过将禁止的用户查询转换为自我重构的提示，绕过提供商的审核系统，同时保留原始有害意图。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的内容审核系统结合了静态过滤器、专用审核服务和基础模型对齐调优，但在实际部署中仍存在危险故障模式，需要研究更有效的攻击方法来暴露系统盲点。

Method: RoguePrompt将指令分割到两个词汇流中，应用嵌套的经典密码，并将结果包装在自然语言指令中，使目标模型解码并执行隐藏的有效载荷。该方法仅假设对模型和相关审核端点的黑盒访问。

Result: 在GPT-4o上对2,448个被生产审核系统强烈拒绝的提示进行评估，攻击获得了84.7%的绕过率、80.2%的重构率和71.5%的完全执行率，显著优于五个自动化越狱基线方法。

Conclusion: 双重词汇转换即使在检测器依赖语义相似性或学习的安全规则时仍然有效，当前审核实践存在系统性盲点，稳健部署需要联合推理用户意图、解码工作流和模型端计算，而不仅仅是表面毒性。

Abstract: Content moderation pipelines for modern large language models combine static filters, dedicated moderation services, and alignment tuned base models, yet real world deployments still exhibit dangerous failure modes. This paper presents RoguePrompt, an automated jailbreak attack that converts a disallowed user query into a self reconstructing prompt which passes provider moderation while preserving the original harmful intent. RoguePrompt partitions the instruction across two lexical streams, applies nested classical ciphers, and wraps the result in natural language directives that cause the target model to decode and execute the hidden payload. Our attack assumes only black box access to the model and to the associated moderation endpoint. We instantiate RoguePrompt against GPT 4o and evaluate it on 2 448 prompts that a production moderation system previously marked as strongly rejected. Under an evaluation protocol that separates three security relevant outcomes bypass, reconstruction, and execution the attack attains 84.7 percent bypass, 80.2 percent reconstruction, and 71.5 percent full execution, substantially outperforming five automated jailbreak baselines. We further analyze the behavior of several automated and human aligned evaluators and show that dual layer lexical transformations remain effective even when detectors rely on semantic similarity or learned safety rubrics. Our results highlight systematic blind spots in current moderation practice and suggest that robust deployment will require joint reasoning about user intent, decoding workflows, and model side computation rather than surface level toxicity alone.

</details>


### [34] [Defending Large Language Models Against Jailbreak Exploits with Responsible AI Considerations](https://arxiv.org/abs/2511.18933)
*Ryan Wong,Hosea David Yu Fei Ng,Dhananjai Sharma,Glenn Jun Jie Ng,Kavishvaran Srinivasan*

Main category: cs.CR

TL;DR: 本文提出了针对大型语言模型越狱攻击的系统性防御策略，包括提示级防御框架、基于logit的引导防御和领域特定代理防御，在基准测试中显著降低了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型仍然容易受到越狱攻击的威胁，这些攻击会绕过安全过滤器并诱导有害或不道德行为，因此需要开发有效的防御机制。

Method: 提出了三种防御策略：1）提示级防御框架通过净化、改写和自适应系统防护来检测和中和对抗性输入；2）基于logit的引导防御通过推理时在安全敏感层进行向量引导来强化拒绝行为；3）领域特定代理防御使用MetaGPT框架强制执行结构化、基于角色的协作和领域遵从。

Result: 在基准数据集上的实验显示攻击成功率显著降低，基于代理的防御实现了完全缓解。

Conclusion: 越狱攻击对LLMs构成重大安全威胁，本文确定了关键的干预点进行预防，但防御策略通常需要在安全性、性能和可扩展性之间进行权衡。

Abstract: Large Language Models (LLMs) remain susceptible to jailbreak exploits that bypass safety filters and induce harmful or unethical behavior. This work presents a systematic taxonomy of existing jailbreak defenses across prompt-level, model-level, and training-time interventions, followed by three proposed defense strategies. First, a Prompt-Level Defense Framework detects and neutralizes adversarial inputs through sanitization, paraphrasing, and adaptive system guarding. Second, a Logit-Based Steering Defense reinforces refusal behavior through inference-time vector steering in safety-sensitive layers. Third, a Domain-Specific Agent Defense employs the MetaGPT framework to enforce structured, role-based collaboration and domain adherence. Experiments on benchmark datasets show substantial reductions in attack success rate, achieving full mitigation under the agent-based defense. Overall, this study highlights how jailbreaks pose a significant security threat to LLMs and identifies key intervention points for prevention, while noting that defense strategies often involve trade-offs between safety, performance, and scalability. Code is available at: https://github.com/Kuro0911/CS5446-Project

</details>


### [35] [Understanding and Mitigating Over-refusal for Large Language Models via Safety Representation](https://arxiv.org/abs/2511.19009)
*Junbo Zhang,Ran Chen,Qianli Zhou,Xinyang Deng,Wen Jiang*

Main category: cs.CR

TL;DR: 提出MOSR方法解决LLM安全防御中的过度拒绝问题，通过表示空间干预在保持安全性的同时减少过度拒绝


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全防御方法在提升安全性的同时往往导致严重的过度拒绝问题，无法在安全性和可用性之间取得良好平衡

Method: 提出MOSR方法，包含两个核心组件：基于表示空间相似性的重叠感知损失加权和通过添加有害前缀的上下文感知增强

Result: 实验表明MOSR在缓解过度拒绝方面优于现有方法，同时能基本保持安全性

Conclusion: 未来的防御方法应该在安全性和过度拒绝之间取得更好的平衡

Abstract: Large language models demonstrate powerful capabilities across various natural language processing tasks, yet they also harbor safety vulnerabilities. To enhance LLM safety, various jailbreak defense methods have been proposed to guard against harmful outputs. However, improvements in model safety often come at the cost of severe over-refusal, failing to strike a good balance between safety and usability. In this paper, we first analyze the causes of over-refusal from a representation perspective, revealing that over-refusal samples reside at the boundary between benign and malicious samples. Based on this, we propose MOSR, designed to mitigate over-refusal by intervening the safety representation of LLMs. MOSR incorporates two novel components: (1) Overlap-Aware Loss Weighting, which determines the erasure weight for malicious samples by quantifying their similarity to pseudo-malicious samples in the representation space, and (2) Context-Aware Augmentation, which supplements the necessary context for rejection decisions by adding harmful prefixes before rejection responses. Experiments demonstrate that our method outperforms existing approaches in mitigating over-refusal while largely maintaining safety. Overall, we advocate that future defense methods should strike a better balance between safety and over-refusal.

</details>


### [36] [Can LLMs Threaten Human Survival? Benchmarking Potential Existential Threats from LLMs via Prefix Completion](https://arxiv.org/abs/2511.19171)
*Yu Cui,Yifei Liu,Hang Fu,Sicheng Pan,Haibin Zhang,Cong Zuo,Licheng Wang*

Main category: cs.CR

TL;DR: 本文提出了ExistBench基准来评估大型语言模型生成内容中存在的生存威胁风险，发现LLM确实会产生对人类生存构成威胁的输出，并能主动调用外部工具执行威胁行为。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLM被越狱后产生已知不安全信息的问题，但对于LLM是否会产生不可预测的、对人类生存构成实质性威胁的输出缺乏研究。

Method: 提出ExistBench基准，通过前缀补全绕过模型安全机制，让LLM生成对人类表达敌意或具有严重威胁行为的后缀；分析注意力logits；开发工具调用框架评估模型行为。

Result: 在10个LLM上的实验表明，LLM生成的内容确实存在生存威胁；模型会主动选择和调用具有生存威胁的外部工具。

Conclusion: LLM生成的内容存在生存威胁风险，需要更严格的安全评估和防护措施。

Abstract: Research on the safety evaluation of large language models (LLMs) has become extensive, driven by jailbreak studies that elicit unsafe responses. Such response involves information already available to humans, such as the answer to "how to make a bomb". When LLMs are jailbroken, the practical threat they pose to humans is negligible. However, it remains unclear whether LLMs commonly produce unpredictable outputs that could pose substantive threats to human safety. To address this gap, we study whether LLM-generated content contains potential existential threats, defined as outputs that imply or promote direct harm to human survival. We propose \textsc{ExistBench}, a benchmark designed to evaluate such risks. Each sample in \textsc{ExistBench} is derived from scenarios where humans are positioned as adversaries to AI assistants. Unlike existing evaluations, we use prefix completion to bypass model safeguards. This leads the LLMs to generate suffixes that express hostility toward humans or actions with severe threat, such as the execution of a nuclear strike. Our experiments on 10 LLMs reveal that LLM-generated content indicates existential threats. To investigate the underlying causes, we also analyze the attention logits from LLMs. To highlight real-world safety risks, we further develop a framework to assess model behavior in tool-calling. We find that LLMs actively select and invoke external tools with existential threats. Code and data are available at: https://github.com/cuiyu-ai/ExistBench.

</details>


### [37] [Adversarial Attack-Defense Co-Evolution for LLM Safety Alignment via Tree-Group Dual-Aware Search and Optimization](https://arxiv.org/abs/2511.19218)
*Xurui Li,Kaisong Song,Rui Zhu,Pin-Yu Chen,Haixu Tang*

Main category: cs.CR

TL;DR: 提出了ACE-Safety框架，通过联合优化攻击和防御模型来增强LLM安全性，包含GS-MCTS探索漏洞和AC-TGPO联合训练两个创新模块。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注孤立的越狱攻击或静态防御，忽视了实际网络环境中威胁与防护措施之间的动态交互作用。

Method: ACE-Safety框架包含：(1) GS-MCTS：基于群体感知策略引导的蒙特卡洛树搜索，高效探索越狱策略并生成多样化对抗样本；(2) AC-TGPO：基于对抗课程树感知群体策略优化，通过课程强化学习联合训练攻击和防御LLM。

Result: 在多个基准测试中，该方法优于现有的攻击和防御方法。

Conclusion: 为开发可持续支持负责任AI生态系统的LLM提供了可行路径。

Abstract: Large Language Models (LLMs) have developed rapidly in web services, delivering unprecedented capabilities while amplifying societal risks. Existing works tend to focus on either isolated jailbreak attacks or static defenses, neglecting the dynamic interplay between evolving threats and safeguards in real-world web contexts. To mitigate these challenges, we propose ACE-Safety (Adversarial Co-Evolution for LLM Safety), a novel framework that jointly optimize attack and defense models by seamlessly integrating two key innovative procedures: (1) Group-aware Strategy-guided Monte Carlo Tree Search (GS-MCTS), which efficiently explores jailbreak strategies to uncover vulnerabilities and generate diverse adversarial samples; (2) Adversarial Curriculum Tree-aware Group Policy Optimization (AC-TGPO), which jointly trains attack and defense LLMs with challenging samples via curriculum reinforcement learning, enabling robust mutual improvement. Evaluations across multiple benchmarks demonstrate that our method outperforms existing attack and defense approaches, and provides a feasible pathway for developing LLMs that can sustainably support responsible AI ecosystems.

</details>


### [38] [FedPoisonTTP: A Threat Model and Poisoning Attack for Federated Test-Time Personalization](https://arxiv.org/abs/2511.19248)
*Md Akil Raihan Iftee,Syed Md. Ahnaf Hasan,Amin Ahsan Ali,AKM Mahbubur Rahman,Sajib Mistry,Aneesh Krishna*

Main category: cs.CR

TL;DR: FedPoisonTTP是一个联邦学习中的测试时数据投毒攻击框架，通过生成高熵或类别置信的毒化样本来破坏模型在测试时的个性化适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习工作忽视了测试时本地适应带来的安全风险，异构域到达、多样化适应算法和有限的跨客户端可见性为恶意参与者创造了漏洞。

Method: FedPoisonTTP从对抗性查询中提取代理模型，使用特征一致性合成分布内毒化样本，并优化攻击目标生成能逃避常见适应过滤器的高熵或类别置信毒化样本。

Result: 在受损的视觉基准测试上的广泛实验表明，被攻陷的参与者可以显著降低整体测试时性能。

Conclusion: 测试时个性化联邦学习存在严重的安全漏洞，需要开发更强的防御机制来应对此类攻击。

Abstract: Test-time personalization in federated learning enables models at clients to adjust online to local domain shifts, enhancing robustness and personalization in deployment. Yet, existing federated learning work largely overlooks the security risks that arise when local adaptation occurs at test time. Heterogeneous domain arrivals, diverse adaptation algorithms, and limited cross-client visibility create vulnerabilities where compromised participants can craft poisoned inputs and submit adversarial updates that undermine both global and per-client performance. To address this threat, we introduce FedPoisonTTP, a realistic grey-box attack framework that explores test-time data poisoning in the federated adaptation setting. FedPoisonTTP distills a surrogate model from adversarial queries, synthesizes in-distribution poisons using feature-consistency, and optimizes attack objectives to generate high-entropy or class-confident poisons that evade common adaptation filters. These poisons are injected during local adaptation and spread through collaborative updates, leading to broad degradation. Extensive experiments on corrupted vision benchmarks show that compromised participants can substantially diminish overall test-time performance.

</details>


### [39] [Medusa: Cross-Modal Transferable Adversarial Attacks on Multimodal Medical Retrieval-Augmented Generation](https://arxiv.org/abs/2511.19257)
*Yingjia Shang,Yi Liu,Huimin Wang,Furong Li,Wenfang Sun,Wu Chengyu,Yefeng Zheng*

Main category: cs.CR

TL;DR: 提出了Medusa框架，针对多模态医疗检索增强生成系统进行黑盒跨模态可迁移对抗攻击，通过扰动优化和双重循环优化策略实现超过90%的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 随着检索增强视觉语言模型在临床决策支持中的广泛应用，这些复杂系统存在未充分探索的对抗脆弱性，特别是在视觉输入扰动方面。

Method: Medusa将攻击建模为扰动优化问题，使用多正样本InfoNCE损失将对抗性视觉嵌入与医学上合理但恶意的文本目标对齐，采用代理模型集成和增强不变风险最小化的双重循环优化策略。

Result: 在两个真实世界医疗任务上的实验表明，Medusa在各种生成模型和检索器上实现了超过90%的平均攻击成功率，同时对四种主流防御方法保持鲁棒性。

Conclusion: 研究揭示了MMed-RAG系统的关键脆弱性，强调了在安全关键医疗应用中鲁棒性基准测试的必要性。

Abstract: With the rapid advancement of retrieval-augmented vision-language models, multimodal medical retrieval-augmented generation (MMed-RAG) systems are increasingly adopted in clinical decision support. These systems enhance medical applications by performing cross-modal retrieval to integrate relevant visual and textual evidence for tasks, e.g., report generation and disease diagnosis. However, their complex architecture also introduces underexplored adversarial vulnerabilities, particularly via visual input perturbations. In this paper, we propose Medusa, a novel framework for crafting cross-modal transferable adversarial attacks on MMed-RAG systems under a black-box setting. Specifically, Medusa formulates the attack as a perturbation optimization problem, leveraging a multi-positive InfoNCE loss (MPIL) to align adversarial visual embeddings with medically plausible but malicious textual targets, thereby hijacking the retrieval process. To enhance transferability, we adopt a surrogate model ensemble and design a dual-loop optimization strategy augmented with invariant risk minimization (IRM). Extensive experiments on two real-world medical tasks, including medical report generation and disease diagnosis, demonstrate that Medusa achieves over 90% average attack success rate across various generation models and retrievers under appropriate parameter configuration, while remaining robust against four mainstream defenses, outperforming state-of-the-art baselines. Our results reveal critical vulnerabilities in the MMed-RAG systems and highlight the necessity of robustness benchmarking in safety-critical medical applications. The code and data are available at https://anonymous.4open.science/r/MMed-RAG-Attack-F05A.

</details>


### [40] [Evolution of Cybersecurity Subdisciplines: A Science of Science Study](https://arxiv.org/abs/2511.19331)
*Yao Chen,Jeff Yan*

Main category: cs.CR

TL;DR: 这是第一篇从科学学角度研究网络安全学科的论文，通过比较SOUPS和FC两个可比的跨学科社区来探讨网络安全实践的发展。


<details>
  <summary>Details</summary>
Motivation: 科学学是研究科学实践本身的新兴领域，但目前缺乏从这一角度对网络安全学科的研究。

Method: 通过分析网络安全领域两个可比的跨学科社区——SOUPS和FC的演变过程进行研究。

Result: 这是该领域的首次研究，为理解网络安全学科的发展提供了新的视角。

Conclusion: 科学学视角为研究网络安全实践提供了有价值的框架，有助于理解该学科的发展轨迹。

Abstract: The science of science is an emerging field that studies the practice of science itself. We present the first study of the cybersecurity discipline from a science of science perspective. We examine the evolution of two comparable interdisciplinary communities in cybersecurity: the Symposium on Usable Privacy and Security (SOUPS) and Financial Cryptography and Data Security (FC).

</details>
