<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 17]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Automated Red-Teaming Framework for Large Language Model Security Assessment: A Comprehensive Attack Generation and Detection System](https://arxiv.org/abs/2512.20677)
*Zhang Wei,Peilu Hu,Shengning Lang,Hao Yan,Li Mei,Yichao Zhang,Chen Yang,Junfeng Hao,Zhimo Han*

Main category: cs.CR

TL;DR: 提出自动化红队框架，系统生成对抗性提示以发现LLM安全漏洞，相比人工测试发现率提升3.9倍


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在关键领域部署，确保其安全性和对齐性成为重要挑战。现有红队测试主要依赖人工，可扩展性差且无法全面覆盖潜在对抗行为空间。

Method: 提出自动化红队框架，集成基于元提示的攻击合成、多模态漏洞检测和标准化评估协议，覆盖六大威胁类别：奖励黑客、欺骗性对齐、数据窃取、消极抵抗、不当工具使用和思维链操纵。

Result: 在GPT-OSS-20B模型上发现47个不同漏洞，包括21个高严重性和12个新颖攻击模式，相比人工专家测试漏洞发现率提升3.9倍，同时保持89%的检测准确率。

Conclusion: 该框架实现了可扩展、系统和可复现的AI安全评估，为提升对齐鲁棒性提供可行见解，推动自动化LLM红队测试发展，有助于构建安全可信的AI系统。

Abstract: As large language models (LLMs) are increasingly deployed in high-stakes domains, ensuring their security and alignment has become a critical challenge. Existing red-teaming practices depend heavily on manual testing, which limits scalability and fails to comprehensively cover the vast space of potential adversarial behaviors. This paper introduces an automated red-teaming framework that systematically generates, executes, and evaluates adversarial prompts to uncover security vulnerabilities in LLMs. Our framework integrates meta-prompting-based attack synthesis, multi-modal vulnerability detection, and standardized evaluation protocols spanning six major threat categories -- reward hacking, deceptive alignment, data exfiltration, sandbagging, inappropriate tool use, and chain-of-thought manipulation. Experiments on the GPT-OSS-20B model reveal 47 distinct vulnerabilities, including 21 high-severity and 12 novel attack patterns, achieving a $3.9\times$ improvement in vulnerability discovery rate over manual expert testing while maintaining 89\% detection accuracy. These results demonstrate the framework's effectiveness in enabling scalable, systematic, and reproducible AI safety evaluations. By providing actionable insights for improving alignment robustness, this work advances the state of automated LLM red-teaming and contributes to the broader goal of building secure and trustworthy AI systems.

</details>


### [2] [Anota: Identifying Business Logic Vulnerabilities via Annotation-Based Sanitization](https://arxiv.org/abs/2512.20705)
*Meng Wang,Philipp Görz,Joschua Schilling,Keno Hassler,Liwei Guo,Thorsten Holz,Ali Abbasi*

Main category: cs.CR

TL;DR: ANOTA是一个人类参与循环的sanitizer框架，通过轻量级注释系统让用户编码领域知识，运行时监控程序行为与注释定义策略对比，检测业务逻辑漏洞。


<details>
  <summary>Details</summary>
Motivation: 业务逻辑漏洞是软件安全的关键挑战，传统模糊测试工具擅长发现内存安全漏洞，但无法检测需要理解应用特定语义上下文的业务逻辑漏洞。现有方法依赖启发式和不可移植语言特性，存在脆弱性和不完整性。业务逻辑漏洞占最危险软件弱点的大多数，这是现有工具的盲区。

Method: ANOTA采用人类参与循环的sanitizer框架，引入轻量级、用户友好的注释系统，让用户直接编码领域特定知识作为定义应用预期行为的轻量级注释。运行时执行监控器观察程序行为，与注释定义的政策进行比较，识别指示漏洞的偏差。

Result: ANOTA与最先进的模糊测试器结合，相比其他兼容相同目标的流行bug发现方法表现更优。ANOTA+FUZZER成功复现了43个已知漏洞，并在评估期间发现了22个先前未知的漏洞（其中17个获得CVE编号）。

Conclusion: ANOTA为发现传统安全测试技术经常遗漏的复杂业务逻辑缺陷提供了实用有效的方法，通过人类参与循环的注释系统填补了现有工具在业务逻辑漏洞检测方面的空白。

Abstract: Detecting business logic vulnerabilities is a critical challenge in software security. These flaws come from mistakes in an application's design or implementation and allow attackers to trigger unintended application behavior. Traditional fuzzing sanitizers for dynamic analysis excel at finding vulnerabilities related to memory safety violations but largely fail to detect business logic vulnerabilities, as these flaws require understanding application-specific semantic context. Recent attempts to infer this context, due to their reliance on heuristics and non-portable language features, are inherently brittle and incomplete. As business logic vulnerabilities constitute a majority (27/40) of the most dangerous software weaknesses in practice, this is a worrying blind spot of existing tools. In this paper, we tackle this challenge with ANOTA, a novel human-in-the-loop sanitizer framework. ANOTA introduces a lightweight, user-friendly annotation system that enables users to directly encode their domain-specific knowledge as lightweight annotations that define an application's intended behavior. A runtime execution monitor then observes program behavior, comparing it against the policies defined by the annotations, thereby identifying deviations that indicate vulnerabilities. To evaluate the effectiveness of ANOTA, we combine ANOTA with a state-of-the-art fuzzer and compare it against other popular bug finding methods compatible with the same targets. The results show that ANOTA+FUZZER outperforms them in terms of effectiveness. More specifically, ANOTA+FUZZER can successfully reproduce 43 known vulnerabilities, and discovered 22 previously unknown vulnerabilities (17 CVEs assigned) during the evaluation. These results demonstrate that ANOTA provides a practical and effective approach for uncovering complex business logic flaws often missed by traditional security testing techniques.

</details>


### [3] [Real-World Adversarial Attacks on RF-Based Drone Detectors](https://arxiv.org/abs/2512.20712)
*Omer Gazit,Yael Itzhakev,Yuval Elovici,Asaf Shabtai*

Main category: cs.CR

TL;DR: 首次针对基于RF图像的无人机检测系统提出物理攻击，通过优化I/Q扰动波形在真实RF链中实现，能可靠降低目标无人机检测率同时保持合法无人机检测


<details>
  <summary>Details</summary>
Motivation: 现有针对图像模型的RF攻击主要修改数字特征，难以在真实无线环境中实现，因为数字扰动转换为可传输波形时面临同步误差、干扰和硬件限制等问题

Method: 优化类别特定的通用复数基带(I/Q)扰动波形，在合法通信信号旁同时传输；使用RF记录和OTA实验评估，涵盖四种无人机类型

Result: 适度、结构化的I/Q扰动与标准RF链兼容，能可靠降低目标无人机检测率，同时保持对合法无人机的检测能力

Conclusion: 成功实现了首个针对RF图像无人机检测器的物理攻击，证明了在真实无线环境中实施对抗性攻击的可行性

Abstract: Radio frequency (RF) based systems are increasingly used to detect drones by analyzing their RF signal patterns, converting them into spectrogram images which are processed by object detection models. Existing RF attacks against image based models alter digital features, making over-the-air (OTA) implementation difficult due to the challenge of converting digital perturbations to transmittable waveforms that may introduce synchronization errors and interference, and encounter hardware limitations. We present the first physical attack on RF image based drone detectors, optimizing class-specific universal complex baseband (I/Q) perturbation waveforms that are transmitted alongside legitimate communications. We evaluated the attack using RF recordings and OTA experiments with four types of drones. Our results show that modest, structured I/Q perturbations are compatible with standard RF chains and reliably reduce target drone detection while preserving detection of legitimate drones.

</details>


### [4] [SoK: Speedy Secure Finality](https://arxiv.org/abs/2512.20715)
*Yash Saraswat,Abhimanyu Nag*

Main category: cs.CR

TL;DR: 该论文综述了以太坊快速最终性协议设计的最新进展，分析了从Goldfish到RLMD-GHOST的理论基础，探讨了单槽最终性在大规模验证者环境中的通信瓶颈，并评估了3槽最终性协议作为实用解决方案的平衡设计。


<details>
  <summary>Details</summary>
Motivation: 以太坊当前Gasper协议存在约15分钟的最终性延迟，这导致网络面临重组攻击风险、MEV提取问题，并限制了经济结算效率。这些限制推动了快速安全最终性（SSF）协议的研究，旨在最小化确认延迟而不削弱正式安全保证。

Method: 论文采用综述研究方法：1）介绍快速最终性协议设计的核心理论原语，包括重组弹性和广义休眠模型；2）追踪从Goldfish到RLMD-GHOST的技术发展路径；3）分析单槽最终性协议在大规模验证者环境中的通信和聚合瓶颈；4）评估3槽最终性协议作为实用工程解决方案的设计平衡。

Result: 论文系统梳理了快速最终性协议设计的技术演进，识别了单槽最终性在大规模验证者环境中的实际限制，并展示了3槽最终性协议如何在快速最终性和以太坊网络工程约束之间取得平衡。

Conclusion: 快速最终性协议设计需要在理论安全性和工程可行性之间取得平衡。3槽最终性协议代表了当前最实用的解决方案，能够在保持以太坊网络稳定性的同时显著减少最终性延迟，为未来协议升级提供了可行的技术路径。

Abstract: While Ethereum has successfully achieved dynamic availability together with safety, a fundamental delay remains between transaction execution and immutable finality. In Ethereum's current Gasper protocol, this latency is on the order of 15 minutes, exposing the network to ex ante reorganization attacks, enabling MEV extraction, and limiting the efficiency of economic settlement. These limitations have motivated a growing body of work on Speedy Secure Finality (SSF), which aims to minimize confirmation latency without weakening formal security guarantees.
  This paper surveys the state of the art in fast finality protocol design. We introduce the core theoretical primitives underlying this space, including reorganization resilience and the generalized sleepy model, and trace their development from Goldfish to RLMD-GHOST. We then analyze the communication and aggregation bottlenecks faced by single-slot finality protocols in large validator settings. Finally, we survey the 3-slot finality (3SF) protocol as a practical synthesis that balances fast finality with the engineering constraints of the Ethereum network.

</details>


### [5] [Towards a Security Plane for 6G Ecosystems](https://arxiv.org/abs/2512.20733)
*Xavi Masip-Bruin,Eva Rodríguez,Admela Jukan,Panos Trakadas*

Main category: cs.CR

TL;DR: 提出基于软件化安全平面和可编程安全功能的主动安全策略，以应对6G网络不确定的安全挑战


<details>
  <summary>Details</summary>
Motivation: 6G网络需要支持高要求服务，但传统针对特定攻击的安全方案无法应对尚在开发中、攻击面未完全确定的6G技术不确定性

Method: 提出软件化解决方案，定义基于可编程、自适应实时安全功能的安全平面，采用主动策略，并引入预评估场景来验证预测模型的准确性

Result: 虽然需要更多开发工作，但这种范式转变被认为是应对6G生态系统安全供应挑战的唯一途径

Conclusion: 需要从传统安全方法转向基于软件化安全平面和主动策略的新范式，以应对6G网络的安全挑战

Abstract: 6G networks promise to be the proper technology to support a wide deployment of highly demanding services, satisfying key users-related aspects such as extremely high quality, and persistent communications. However, there is no service to support if the network is not reliable enough. In this direction, it is with no doubt that security guarantees become a must. Traditional security approaches have focused on providing specific and attack-tailored solutions that will not properly meet the uncertainties driven by a technology yet under development and showing an attack surface not completely identified either. In this positioning paper we propose a softwarized solution, defining a Security Plane built on a top of programmable and adaptable set of live Security Functions under a proactive strategy. In addition, in order to address the inaccuracies driven by the predictive models a pre-assessment scenario is also considered ensuring that no action will be deployed if not previously verified. Although more efforts are required to develop this initiative, we think that such a shift paradigm is the only way to face security provisioning challenges in 6G ecosystems.

</details>


### [6] [Sark: Oblivious Integrity Without Global State](https://arxiv.org/abs/2512.20775)
*Alex Lynham,David Alesch,Ziyi Li,Geoff Goodell*

Main category: cs.CR

TL;DR: Sark是一个实现USO资产系统的参考架构，包含Sloop（许可CFT区块链）和Porters（客户端承诺聚合子系统），分析了CIA三元组安全特性，引入完整性轨迹概念解决去中心化设计权衡。


<details>
  <summary>Details</summary>
Motivation: 实现Goodell、Toliver和Nakib提出的USO（不可伪造、有状态、无感知）资产系统，构建一个具有机密性、可用性和完整性的安全资产管理系统。

Method: 设计Sark参考架构，包含两个核心子系统：1) Sloop - 许可制的崩溃容错区块链；2) Porters - 聚合和汇总客户端承诺的子系统。使用CIA三元组分析系统运行，引入完整性轨迹概念处理去中心化设计权衡。

Result: 提出了完整的Sark系统架构设计，通过CIA三元组分析验证了系统的安全特性，使用完整性轨迹概念解决了去中心化设计中的关键权衡问题。

Conclusion: Sark成功实现了USO资产系统的参考架构，未来工作将集中在实现拜占庭容错（BFT）和缓解Porters的本地中心化问题。

Abstract: In this paper, we introduce Sark, a reference architecture implementing the Unforgeable, Stateful, and Oblivious (USO) asset system as described by Goodell, Toliver, and Nakib. We describe the motivation, design, and implementation of Sloop, a permissioned, crash fault-tolerant (CFT) blockchain that forms a subsystem of Sark, and the other core subsystems, Porters, which accumulate and roll-up commitments from Clients. We analyse the operation of the system using the 'CIA Triad': Confidentiality, Availability, and Integrity. We then introduce the concept of Integrity Locus and use it to address design trade-offs related to decentralization. Finally, we point to future work on Byzantine fault-tolerance (BFT), and mitigating the local centrality of Porters.

</details>


### [7] [pokiSEC: A Multi-Architecture, Containerized Ephemeral Malware Detonation Sandbox](https://arxiv.org/abs/2512.20860)
*Alejandro Avina,Yashas Hariprasad,Naveen Kumar Chaudhary*

Main category: cs.CR

TL;DR: pokiSEC是一个轻量级、临时性的恶意软件引爆沙箱，将完整虚拟化堆栈打包在Docker容器中，支持跨架构（ARM64/x86_64）运行Windows恶意软件分析。


<details>
  <summary>Details</summary>
Motivation: 当前恶意软件动态分析通常依赖重量级虚拟机或专用物理实验室，缺乏可移植性和自动化。随着ARM64硬件（如Apple Silicon）的普及，现有x86_64沙箱方案无法跨架构运行，需要一种轻量、可移植的解决方案。

Method: pokiSEC将QEMU虚拟化与硬件加速（KVM）集成到Docker容器中，提供基于浏览器的分析工作流。核心创新是"通用入口点"，通过运行时主机架构检测自动选择经过验证的虚拟机配置（机器类型、加速模式、设备配置文件）。

Result: 在Apple Silicon（ARM64）和Ubuntu（AMD64）上验证成功，展示出适合分析师工作流的交互性能，并通过临时容器生命周期实现一致的清理语义。

Conclusion: pokiSEC提供了一种轻量级、可移植的恶意软件分析解决方案，解决了跨架构沙箱部署的挑战，支持在ARM64和x86_64主机上运行Windows恶意软件分析工作流。

Abstract: Dynamic malware analysis requires executing untrusted binaries inside strongly isolated, rapidly resettable environments. In practice, many detonation workflows remain tied to heavyweight hypervisors or dedicated bare-metal labs, limiting portability and automation. This challenge has intensified with the adoption of ARM64 developer hardware (e.g., Apple Silicon), where common open-source sandbox recipes and pre-built environments frequently assume x86_64 hosts and do not translate cleanly across architectures. This paper presents pokiSEC, a lightweight, ephemeral malware detonation sandbox that packages the full virtualization and access stack inside a Docker container. pokiSEC integrates QEMU with hardware acceleration (KVM when available) and exposes a browser-based workflow that supports bring-your-own Windows disk images. The key contribution is a Universal Entrypoint that performs runtime host-architecture detection and selects validated hypervisor configurations (machine types, acceleration modes, and device profiles), enabling a single container image and codebase to launch Windows guests on both ARM64 and x86_64 hosts. We validate pokiSEC on Apple Silicon (ARM64) and Ubuntu (AMD64), demonstrating interactive performance suitable for analyst workflows and consistent teardown semantics via ephemeral container lifecycles.

</details>


### [8] [Better Call Graphs: A New Dataset of Function Call Graphs for Malware Classification](https://arxiv.org/abs/2512.20872)
*Jakir Hossain,Gurvinder Singh,Lukasz Ziarek,Ahmet Erdem Sarıyüce*

Main category: cs.CR

TL;DR: 本文介绍了BCG数据集，这是一个针对Android恶意软件检测的大规模、高质量函数调用图数据集，解决了现有数据集过时、冗余和多样性不足的问题。


<details>
  <summary>Details</summary>
Motivation: Android平台恶意软件检测中，函数调用图(FCG)是一种有效的抽象方法，但缺乏大规模、高质量的Android专用FCG数据集。现有数据集通常过时、包含大量由应用重打包导致的小型或冗余图，无法反映真实恶意软件的多样性，导致基于图的分类方法过拟合和评估不可靠。

Method: 作者创建了Better Call Graphs (BCG)数据集，从最近的Android应用包(APK)中提取大规模且独特的函数调用图。数据集包含良性样本和恶意样本，涵盖多种家族和类型，并为每个APK提供图级特征。

Result: 通过使用基线分类器进行广泛实验，证明了BCG相比现有数据集的必要性和价值。BCG数据集已公开可用。

Conclusion: BCG是一个全面的大规模Android函数调用图数据集，解决了现有数据集的局限性，为基于图的Android恶意软件分类研究提供了更好的评估基准。

Abstract: Function call graphs (FCGs) have emerged as a powerful abstraction for malware detection, capturing the behavioral structure of applications beyond surface-level signatures. Their utility in traditional program analysis has been well established, enabling effective classification and analysis of malicious software. In the mobile domain, especially in the Android ecosystem, FCG-based malware classification is particularly critical due to the platform's widespread adoption and the complex, component-based structure of Android apps. However, progress in this direction is hindered by the lack of large-scale, high-quality Android-specific FCG datasets. Existing datasets are often outdated, dominated by small or redundant graphs resulting from app repackaging, and fail to reflect the diversity of real-world malware. These limitations lead to overfitting and unreliable evaluation of graph-based classification methods. To address this gap, we introduce Better Call Graphs (BCG), a comprehensive dataset of large and unique FCGs extracted from recent Android application packages (APKs). BCG includes both benign and malicious samples spanning various families and types, along with graph-level features for each APK. Through extensive experiments using baseline classifiers, we demonstrate the necessity and value of BCG compared to existing datasets. BCG is publicly available at https://erdemub.github.io/BCG-dataset.

</details>


### [9] [Neutralization of IMU-Based GPS Spoofing Detection using external IMU sensor and feedback methodology](https://arxiv.org/abs/2512.20964)
*Ji Hyuk Jung,Ji Won Yoon*

Main category: cs.CR

TL;DR: 本文提出了一种针对自动驾驶车辆GPS欺骗攻击的IMU传感器检测绕过方法，通过窃取内部动态状态信息进行GPS欺骗，实验证明攻击值可注入而不被检测


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆依赖GPS进行位置感知，而GPS欺骗攻击威胁其安全。现有基于IMU传感器的检测方法被认为是最有效的防御机制，因此需要研究如何绕过这些检测方法

Method: 提出一种欺骗攻击系统，通过外部IMU传感器窃取内部动态状态信息，基于EKF传感器融合技术，设计攻击模型来绕过IMU传感器检测

Result: 实验分析显示，提出的方法能够减少目标系统中的异常检测，攻击值可以注入而不被检测到

Conclusion: 本文成功展示了如何绕过自动驾驶车辆中基于IMU传感器的GPS欺骗检测，揭示了现有防御机制的局限性，为改进自动驾驶安全系统提供了重要参考

Abstract: Autonomous Vehicles (AVs) refer to systems capable of perceiving their states and moving without human intervention. Among the factors required for autonomous decision-making in mobility, positional awareness of the vehicle itself is the most critical. Accordingly, extensive research has been conducted on defense mechanisms against GPS spoofing attacks, which threaten AVs by disrupting position recognition. Among these, detection methods based on internal IMU sensors are regarded as some of the most effective. In this paper, we propose a spoofing attack system designed to neutralize IMU sensor-based detection. First, we present an attack modeling approach for bypassing such detection. Then, based on EKF sensor fusion, we experimentally analyze both the impact of GPS spoofing values on the internal target system and how our proposed methodology reduces anomaly detection within the target system. To this end, this paper proposes an attack model that performs GPS spoofing by stealing internal dynamic state information using an external IMU sensor, and the experimental results demonstrate that attack values can be injected without being detected.

</details>


### [10] [AegisAgent: An Autonomous Defense Agent Against Prompt Injection Attacks in LLM-HARs](https://arxiv.org/abs/2512.20986)
*Yihan Wang,Huanqi Yang,Shantanu Pal,Weitao Xu*

Main category: cs.CR

TL;DR: AegisAgent：一种用于保护LLM驱动的人体活动识别系统免受提示注入攻击的自主代理系统，通过主动感知、推理和验证来确保系统安全。


<details>
  <summary>Details</summary>
Motivation: LLM在可穿戴传感中的应用面临提示注入攻击的严重威胁，传统基于静态过滤和固定规则的防御方法无法应对语义复杂的攻击，需要从被动过滤转向主动保护的新范式。

Method: 提出AegisAgent作为认知守护者，能自主感知语义不一致性，通过动态记忆库推理用户真实意图，生成并执行多步验证和修复计划，实现轻量级全栈原型系统。

Result: 在3个公共数据集上对5个最先进的LLM-based HAR系统进行15种常见攻击评估，结果显示平均降低30%攻击成功率，在GPU工作站上仅增加78.6毫秒延迟开销。

Conclusion: AegisAgent为构建安全可信的LLM驱动HAR系统迈出了重要一步，展示了从被动防御向主动认知保护的范式转变的有效性。

Abstract: The integration of Large Language Models (LLMs) into wearable sensing is creating a new class of mobile applications capable of nuanced human activity understanding. However, the reliability of these systems is critically undermined by their vulnerability to prompt injection attacks, where attackers deliberately input deceptive instructions into LLMs. Traditional defenses, based on static filters and rigid rules, are insufficient to address the semantic complexity of these new attacks. We argue that a paradigm shift is needed -- from passive filtering to active protection and autonomous reasoning. We introduce AegisAgent, an autonomous agent system designed to ensure the security of LLM-driven HAR systems. Instead of merely blocking threats, AegisAgent functions as a cognitive guardian. It autonomously perceives potential semantic inconsistencies, reasons about the user's true intent by consulting a dynamic memory of past interactions, and acts by generating and executing a multi-step verification and repair plan. We implement AegisAgent as a lightweight, full-stack prototype and conduct a systematic evaluation on 15 common attacks with five state-of-the-art LLM-based HAR systems on three public datasets. Results show it reduces attack success rate by 30\% on average while incurring only 78.6 ms of latency overhead on a GPU workstation. Our work makes the first step towards building secure and trustworthy LLM-driven HAR systems.

</details>


### [11] [GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs](https://arxiv.org/abs/2512.21008)
*Lichao Wu,Sasha Behrouzi,Mohamadreza Rostami,Stjepan Picek,Ahmad-Reza Sadeghi*

Main category: cs.CR

TL;DR: GateBreaker是一种无需训练、轻量级、架构无关的攻击框架，专门针对MoE LLMs的安全对齐机制，通过识别和禁用安全专家中的关键神经元来破坏模型安全性。


<details>
  <summary>Details</summary>
Motivation: 随着MoE架构在关键领域部署增加，理解其安全机制变得至关重要。现有安全研究主要集中在密集架构，MoE的稀疏激活设计可能导致安全机制运行方式不同，其鲁棒性存在疑问。

Method: GateBreaker采用三阶段攻击：1) 门级分析识别有害输入上被过度路由的安全专家；2) 专家级定位在安全专家内部定位安全结构；3) 目标安全移除禁用识别的安全结构以破坏安全对齐。

Result: 攻击成功率从7.4%提升至64.9%，仅需禁用目标专家层中约3%的神经元。安全神经元在相同模型家族内可迁移，单次迁移攻击使成功率从17.9%提升至67.7%。方法可泛化到MoE视觉语言模型，在危险图像输入上达到60.9%成功率。

Conclusion: MoE的安全性集中在由稀疏路由协调的小部分神经元中，这些神经元容易受到针对性攻击。GateBreaker揭示了MoE安全对齐的脆弱性，为未来安全研究提供了重要方向。

Abstract: Mixture-of-Experts (MoE) architectures have advanced the scaling of Large Language Models (LLMs) by activating only a sparse subset of parameters per input, enabling state-of-the-art performance with reduced computational cost. As these models are increasingly deployed in critical domains, understanding and strengthening their alignment mechanisms is essential to prevent harmful outputs. However, existing LLM safety research has focused almost exclusively on dense architectures, leaving the unique safety properties of MoEs largely unexamined. The modular, sparsely-activated design of MoEs suggests that safety mechanisms may operate differently than in dense models, raising questions about their robustness.
  In this paper, we present GateBreaker, the first training-free, lightweight, and architecture-agnostic attack framework that compromises the safety alignment of modern MoE LLMs at inference time. GateBreaker operates in three stages: (i) gate-level profiling, which identifies safety experts disproportionately routed on harmful inputs, (ii) expert-level localization, which localizes the safety structure within safety experts, and (iii) targeted safety removal, which disables the identified safety structure to compromise the safety alignment. Our study shows that MoE safety concentrates within a small subset of neurons coordinated by sparse routing. Selective disabling of these neurons, approximately 3% of neurons in the targeted expert layers, significantly increases the averaged attack success rate (ASR) from 7.4% to 64.9% against the eight latest aligned MoE LLMs with limited utility degradation. These safety neurons transfer across models within the same family, raising ASR from 17.9% to 67.7% with one-shot transfer attack. Furthermore, GateBreaker generalizes to five MoE vision language models (VLMs) with 60.9% ASR on unsafe image inputs.

</details>


### [12] [zkFL-Health: Blockchain-Enabled Zero-Knowledge Federated Learning for Medical AI Privacy](https://arxiv.org/abs/2512.21048)
*Savvy Sharma,George Petrovic,Sarthak Kaushik*

Main category: cs.CR

TL;DR: zkFL-Health结合联邦学习、零知识证明和可信执行环境，为医疗AI提供隐私保护、可验证正确的协作训练框架，解决数据共享的隐私和信任问题。


<details>
  <summary>Details</summary>
Motivation: 医疗AI需要大规模多样化数据集，但严格的隐私和治理限制阻碍了机构间原始数据共享。联邦学习虽然通过本地训练和模型更新交换缓解了这一问题，但仍面临两个核心风险：1) 通过梯度或更新进行隐私泄露（成员推断、梯度反转）；2) 对聚合器的信任问题，聚合器作为单点故障可能丢弃、篡改或注入贡献而不被检测。

Method: 提出zkFL-Health架构，结合联邦学习、零知识证明和可信执行环境。客户端本地训练并提交更新承诺；聚合器在TEE内运行，计算全局更新并生成简洁的ZK证明（使用Halo2/Nova），证明其使用了确切的承诺输入和正确的聚合规则，而不向主机泄露任何客户端更新。验证节点验证证明并在链上记录加密承诺，提供不可变的审计追踪。

Result: 该框架为医疗AI提供了具有强保密性、完整性和可审计性的多机构协作训练方案，包括针对医疗场景定制的系统和威胁模型、zkFL-Health协议、安全/隐私保证以及涵盖准确性、隐私风险、延迟和成本的性能评估计划。

Conclusion: zkFL-Health框架实现了多机构医疗AI的强保密性、完整性和可审计性，这些关键特性对于临床采用和监管合规至关重要，消除了对任何单一方的信任需求。

Abstract: Healthcare AI needs large, diverse datasets, yet strict privacy and governance constraints prevent raw data sharing across institutions. Federated learning (FL) mitigates this by training where data reside and exchanging only model updates, but practical deployments still face two core risks: (1) privacy leakage via gradients or updates (membership inference, gradient inversion) and (2) trust in the aggregator, a single point of failure that can drop, alter, or inject contributions undetected. We present zkFL-Health, an architecture that combines FL with zero-knowledge proofs (ZKPs) and Trusted Execution Environments (TEEs) to deliver privacy-preserving, verifiably correct collaborative training for medical AI. Clients locally train and commit their updates; the aggregator operates within a TEE to compute the global update and produces a succinct ZK proof (via Halo2/Nova) that it used exactly the committed inputs and the correct aggregation rule, without revealing any client update to the host. Verifier nodes validate the proof and record cryptographic commitments on-chain, providing an immutable audit trail and removing the need to trust any single party. We outline system and threat models tailored to healthcare, the zkFL-Health protocol, security/privacy guarantees, and a performance evaluation plan spanning accuracy, privacy risk, latency, and cost. This framework enables multi-institutional medical AI with strong confidentiality, integrity, and auditability, key properties for clinical adoption and regulatory compliance.

</details>


### [13] [AutoBaxBuilder: Bootstrapping Code Security Benchmarking](https://arxiv.org/abs/2512.21132)
*Tobias von Arx,Niels Mündler,Mark Vero,Maximilian Baader,Martin Vechev*

Main category: cs.CR

TL;DR: AutoBaxBuilder：一个自动生成代码安全基准测试的框架，无需人工标注，能在2小时内以低于10美元成本生成新任务


<details>
  <summary>Details</summary>
Motivation: 现有代码安全基准测试依赖专家手动构建，存在三个问题：(1) 训练数据污染，(2) 难以扩展到新任务，(3) 难度不足以挑战更强大的LLM。需要自动化的基准生成方法。

Method: 提出AutoBaxBuilder框架，利用LLM的代码理解能力自动生成安全基准测试任务。包含细粒度合理性检查、功能测试构建和端到端安全探测利用生成。

Result: 成功构建了AutoBaxBench基准测试集，新任务生成时间小于2小时，成本低于10美元。通过定性和定量分析验证了生成任务的质量，并与专家构建任务进行了比较。

Conclusion: AutoBaxBuilder能够高效、低成本地生成高质量的代码安全基准测试任务，解决了手动基准测试的局限性，为评估LLM生成代码的安全性提供了可持续的解决方案。

Abstract: As LLMs see wide adoption in software engineering, the reliable assessment of the correctness and security of LLM-generated code is crucial. Notably, prior work has demonstrated that security is often overlooked, exposing that LLMs are prone to generating code with security vulnerabilities. These insights were enabled by specialized benchmarks, crafted through significant manual effort by security experts. However, relying on manually-crafted benchmarks is insufficient in the long term, because benchmarks (i) naturally end up contaminating training data, (ii) must extend to new tasks to provide a more complete picture, and (iii) must increase in difficulty to challenge more capable LLMs. In this work, we address these challenges and present AutoBaxBuilder, a framework that generates tasks and tests for code security benchmarking from scratch. We introduce a robust pipeline with fine-grained plausibility checks, leveraging the code understanding capabilities of LLMs to construct functionality tests and end-to-end security-probing exploits. To confirm the quality of the generated benchmark, we conduct both a qualitative analysis and perform quantitative experiments, comparing it against tasks constructed by human experts. We use AutoBaxBuilder to construct entirely new tasks and release them to the public as AutoBaxBench, together with a thorough evaluation of the security capabilities of LLMs on these tasks. We find that a new task can be generated in under 2 hours, costing less than USD 10.

</details>


### [14] [Casting a SPELL: Sentence Pairing Exploration for LLM Limitation-breaking](https://arxiv.org/abs/2512.21236)
*Yifan Huang,Xiaojun Jia,Wenbo Guo,Yuqiang Sun,Yihao Huang,Chong Wang,Yang Liu*

Main category: cs.CR

TL;DR: SPELL框架专门针对LLM生成恶意代码的安全对齐弱点进行测试，通过时间分配选择策略构建越狱提示，在GPT-4.1、Claude-3.5和Qwen2.5-Coder上分别达到83.75%、19.38%和68.12%的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: LLM辅助编程工具降低了软件开发门槛，但同时也可能被恶意行为者利用生成有害软件。现有越狱研究主要关注通用攻击场景，缺乏针对恶意代码生成这一特定目标的系统性评估。

Method: 提出SPELL测试框架，采用时间分配选择策略，从先验知识数据集中智能组合句子来系统构建越狱提示，平衡新颖攻击模式的探索和成功技术的利用。

Result: 在三个先进代码模型上评估：GPT-4.1攻击成功率83.75%，Claude-3.5为19.38%，Qwen2.5-Coder为68.12%。生成的提示在Cursor等实际开发工具中成功产生恶意代码，超过73%的输出被最先进检测系统确认为恶意。

Conclusion: 当前LLM实现存在显著安全漏洞，SPELL框架为改进代码生成应用中的AI安全对齐提供了有价值的见解。

Abstract: Large language models (LLMs) have revolutionized software development through AI-assisted coding tools, enabling developers with limited programming expertise to create sophisticated applications. However, this accessibility extends to malicious actors who may exploit these powerful tools to generate harmful software. Existing jailbreaking research primarily focuses on general attack scenarios against LLMs, with limited exploration of malicious code generation as a jailbreak target. To address this gap, we propose SPELL, a comprehensive testing framework specifically designed to evaluate the weakness of security alignment in malicious code generation. Our framework employs a time-division selection strategy that systematically constructs jailbreaking prompts by intelligently combining sentences from a prior knowledge dataset, balancing exploration of novel attack patterns with exploitation of successful techniques. Extensive evaluation across three advanced code models (GPT-4.1, Claude-3.5, and Qwen2.5-Coder) demonstrates SPELL's effectiveness, achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively across eight malicious code categories. The generated prompts successfully produce malicious code in real-world AI development tools such as Cursor, with outputs confirmed as malicious by state-of-the-art detection systems at rates exceeding 73%. These findings reveal significant security gaps in current LLM implementations and provide valuable insights for improving AI safety alignment in code generation applications.

</details>


### [15] [Industrial Ouroboros: Deep Lateral Movement via Living Off the Plant](https://arxiv.org/abs/2512.21248)
*Richard Derbyshire*

Main category: cs.CR

TL;DR: 论文提出首个基于PLC原生功能的横向移动技术"Living Off The Plant"(LOTP)，利用OT环境固有功能而非漏洞，实现隐蔽的跨设备移动和串行网络逃逸。


<details>
  <summary>Details</summary>
Motivation: 在OT环境中，特别是可编程逻辑控制器(PLC)之间，缺乏有效的横向移动方法。现有技术依赖复杂的漏洞链，既容易被检测又可能被修补，需要更隐蔽、基于原生功能的方法。

Method: 开发了PLC中心的横向移动技术，完全利用受害者环境的原生功能，不依赖漏洞。通过双宿主PLC实现从IP网络到传统串行网络的逃逸，利用常见的网络通信功能保持隐蔽性。

Result: 提出了首个OT特定的"Living Off The Plant"技术，能够在不依赖漏洞的情况下实现PLC间的横向移动，具有高度隐蔽性，难以被传统防御机制检测。

Conclusion: LOTP技术揭示了OT环境中的新风险，表明需要从根本上重新考虑传统的OT防御实践，因为基于原生功能的攻击难以被现有安全机制检测和阻止。

Abstract: Lateral movement is a tactic that adversaries employ most frequently in enterprise IT environments to traverse between assets. In operational technology (OT) environments, however, few methods exist for lateral movement between domain-specific devices, particularly programmable logic controllers (PLCs). Existing techniques often rely on complex chains of vulnerabilities, which are noisy and can be patched. This paper describes the first PLC-centric lateral movement technique that relies exclusively on the native functionality of the victim environment. This OT-specific form of `living off the land' is herein distinguished as `living off the plant' (LOTP). The described technique also facilitates escape from IP networks onto legacy serial networks via dual-homed PLCs. Furthermore, this technique is covert, leveraging common network communication functions that are challenging to detect. This serves as a reminder of the risks posed by LOTP techniques within OT, highlighting the need for a fundamental reconsideration of traditional OT defensive practices.

</details>


### [16] [CoTDeceptor:Adversarial Code Obfuscation Against CoT-Enhanced LLM Code Agents](https://arxiv.org/abs/2512.21250)
*Haoyang Li,Mingjin Li,Jinxin Zuo,Siqi Li,Xiao Li,Hao Wu,Yueming Lu,Xiaochuan He*

Main category: cs.CR

TL;DR: CoTDeceptor：首个针对CoT增强LLM检测器的对抗性代码混淆框架，通过多阶段混淆策略链有效破坏CoT驱动的检测逻辑，在真实软件供应链中构成威胁。


<details>
  <summary>Details</summary>
Motivation: 尽管CoT增强的LLM漏洞检测器被认为对混淆恶意代码具有更强鲁棒性，但研究发现其推理链和语义抽象过程存在可利用的系统性弱点，攻击者可借此隐蔽嵌入恶意逻辑，绕过代码审查，在软件供应链中传播后门组件。

Method: 提出CoTDeceptor框架，自主构建演化、难以逆向的多阶段混淆策略链，专门针对CoT增强的LLM检测器，破坏其基于推理链的检测逻辑。

Result: 实验使用安全企业提供的恶意代码，CoTDeceptor对最先进的LLM和漏洞检测代理实现稳定且可迁移的规避性能，在15个漏洞类别中成功绕过14个，而先前方法仅绕过2个。

Conclusion: 研究揭示了真实软件供应链中的潜在风险，强调需要更鲁棒和可解释的LLM驱动的安全分析系统。

Abstract: LLM-based code agents(e.g., ChatGPT Codex) are increasingly deployed as detector for code review and security auditing tasks. Although CoT-enhanced LLM vulnerability detectors are believed to provide improved robustness against obfuscated malicious code, we find that their reasoning chains and semantic abstraction processes exhibit exploitable systematic weaknesses.This allows attackers to covertly embed malicious logic, bypass code review, and propagate backdoored components throughout real-world software supply chains.To investigate this issue, we present CoTDeceptor, the first adversarial code obfuscation framework targeting CoT-enhanced LLM detectors. CoTDeceptor autonomously constructs evolving, hard-to-reverse multi-stage obfuscation strategy chains that effectively disrupt CoT-driven detection logic.We obtained malicious code provided by security enterprise, experimental results demonstrate that CoTDeceptor achieves stable and transferable evasion performance against state-of-the-art LLMs and vulnerability detection agents. CoTDeceptor bypasses 14 out of 15 vulnerability categories, compared to only 2 bypassed by prior methods. Our findings highlight potential risks in real-world software supply chains and underscore the need for more robust and interpretable LLM-powered security analysis systems.

</details>


### [17] [Uncertainty in security: managing cyber senescence](https://arxiv.org/abs/2512.21251)
*Martijn Dekker*

Main category: cs.CR

TL;DR: 作者提出"网络衰老"概念，指网络安全生态系统因控制措施不断累积、效果不确定而逐渐老化，最终可能导致系统性崩溃。


<details>
  <summary>Details</summary>
Motivation: 作者担忧网络安全生态系统正在逐渐老化，这种老化正在成为操作风险。问题不仅源于复杂性增加，更重要的是控制措施的累积及其效果的不确定性。作者将这种现象称为"网络衰老"。

Method: 作者从历史角度概述网络安全发展历程，分析导致当前担忧的原因。研究聚焦于网络安全中的不确定性角色，特别是控制措施的风险减少效果不确定性。

Result: 作者识别出网络安全中"废物"积累的问题——大量重叠的控制措施，其风险减少效果不确定。这种废物积累导致网络空间老化。

Conclusion: 除非开始修剪这些控制框架，否则废物积累将导致网络空间老化，最终可能导致系统崩溃。需要重新审视网络安全控制措施的有效性和必要性。

Abstract: My main worry, and the core of my research, is that our cybersecurity ecosystem is slowly but surely aging and getting old and that aging is becoming an operational risk. This is happening not only because of growing complexity, but more importantly because of accumulation of controls and measures whose effectiveness are uncertain. I introduce a new term for this aging phenomenon: cyber senescence. I will begin my lecture with a short historical overview in which I sketch a development over time that led to this worry for the future of cybersecurity. It is this worry that determined my research agenda and its central theme of the role of uncertainty in cybersecurity. My worry is that waste is accumulating in cyberspace. This waste consists of a multitude of overlapping controls whose risk reductions are uncertain. Unless we start pruning these control frameworks, this waste accumulation causes aging of cyberspace and could ultimately lead to a system collapse.

</details>
