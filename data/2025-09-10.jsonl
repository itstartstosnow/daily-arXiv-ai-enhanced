{"id": "2509.07016", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07016", "abs": "https://arxiv.org/abs/2509.07016", "authors": ["Muhammad Arif Hakimi Zamrai", "Kamaludin Mohd Yusof"], "title": "Random Forest Stratified K-Fold Cross Validation on SYN DoS Attack SD-IoV", "comment": null, "summary": "In response to the prevalent concern of TCP SYN flood attacks within the\ncontext of Software-Defined Internet of Vehicles (SD-IoV), this study addresses\nthe significant challenge of network security in rapidly evolving vehicular\ncommunication systems. This research focuses on optimizing a Random Forest\nClassifier model to achieve maximum accuracy and minimal detection time,\nthereby enhancing vehicular network security. The methodology involves\npreprocessing a dataset containing SYN attack instances, employing feature\nscaling and label encoding techniques, and applying Stratified K-Fold\ncross-validation to target key metrics such as accuracy, precision, recall, and\nF1-score. This research achieved an average value of 0.999998 for all metrics\nwith a SYN DoS attack detection time of 0.24 seconds. Results show that the\nfine-tuned Random Forest model, configured with 20 estimators and a depth of\n10, effectively differentiates between normal and malicious traffic with high\naccuracy and minimal detection time, which is crucial for SD-IoV networks. This\napproach marks a significant advancement and introduces a state-of-the-art\nalgorithm in detecting SYN flood attacks, combining high accuracy with minimal\ndetection time. It contributes to vehicular network security by providing a\nrobust solution against TCP SYN flood attacks while maintaining network\nefficiency and reliability."}
{"id": "2509.07053", "categories": ["cs.CR", "cs.CY", "J.4; K.4.1; K.4.3; K.5.0; K.5.2; K.6.5"], "pdf": "https://arxiv.org/pdf/2509.07053", "abs": "https://arxiv.org/abs/2509.07053", "authors": ["Paul Benjamin Lowry", "Gregory D. Moody", "Robert Willison", "Clay Posey"], "title": "The Signalgate Case is Waiving a Red Flag to All Organizational and Behavioral Cybersecurity Leaders, Practitioners, and Researchers: Are We Receiving the Signal Amidst the Noise?", "comment": null, "summary": "The Signalgate incident of March 2025, wherein senior US national security\nofficials inadvertently disclosed sensitive military operational details via\nthe encrypted messaging platform Signal, highlights critical vulnerabilities in\norganizational security arising from human error, governance gaps, and the\nmisuse of technology. Although smaller in scale when compared to historical\nbreaches involving billions of records, Signalgate illustrates critical\nsystemic issues often overshadowed by a focus on external cyber threats.\nEmploying a case-study approach and systematic review grounded in the NIST\nCybersecurity Framework, we analyze the incident to identify patterns of\nhuman-centric vulnerabilities and governance challenges common to\norganizational security failures. Findings emphasize three critical points. (1)\nOrganizational security depends heavily on human behavior, with internal actors\noften serving as the weakest link despite advanced technical defenses; (2)\nLeadership tone strongly influences organizational security culture and\nefficacy, and (3) widespread reliance on technical solutions without sufficient\ninvestments in human and organizational factors leads to ineffective practices\nand wasted resources. From these observations, we propose actionable\nrecommendations for enhancing organizational and national security, including\nstrong leadership engagement, comprehensive adoption of zero-trust\narchitectures, clearer accountability structures, incentivized security\nbehaviors, and rigorous oversight. Particularly during periods of\norganizational transition, such as mergers or large-scale personnel changes,\nadditional measures become particularly important. Signalgate underscores the\nneed for leaders and policymakers to reorient cybersecurity strategies toward\naddressing governance, cultural, and behavioral risks."}
{"id": "2509.07055", "categories": ["cs.CR", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.07055", "abs": "https://arxiv.org/abs/2509.07055", "authors": ["Tomás González", "Mateo Dulce-Rubio", "Aaditya Ramdas", "Mónica Ribero"], "title": "Sequentially Auditing Differential Privacy", "comment": null, "summary": "We propose a practical sequential test for auditing differential privacy\nguarantees of black-box mechanisms. The test processes streams of mechanisms'\noutputs providing anytime-valid inference while controlling Type I error,\novercoming the fixed sample size limitation of previous batch auditing methods.\nExperiments show this test detects violations with sample sizes that are orders\nof magnitude smaller than existing methods, reducing this number from 50K to a\nfew hundred examples, across diverse realistic mechanisms. Notably, it\nidentifies DP-SGD privacy violations in \\textit{under} one training run, unlike\nprior methods needing full model training."}
{"id": "2509.07131", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07131", "abs": "https://arxiv.org/abs/2509.07131", "authors": ["Nicolò Romandini", "Carlo Mazzocca", "Kai Otsuki", "Rebecca Montanari"], "title": "SoK: Security and Privacy of AI Agents for Blockchain", "comment": "This work has been accepted to the 7th International Conference on\n  Blockchain Computing and Applications (BCCA 2025)", "summary": "Blockchain and smart contracts have garnered significant interest in recent\nyears as the foundation of a decentralized, trustless digital ecosystem,\nthereby eliminating the need for traditional centralized authorities. Despite\ntheir central role in powering Web3, their complexity still presents\nsignificant barriers for non-expert users. To bridge this gap, Artificial\nIntelligence (AI)-based agents have emerged as valuable tools for interacting\nwith blockchain environments, supporting a range of tasks, from analyzing\non-chain data and optimizing transaction strategies to detecting\nvulnerabilities within smart contracts. While interest in applying AI to\nblockchain is growing, the literature still lacks a comprehensive survey that\nfocuses specifically on the intersection with AI agents. Most of the related\nwork only provides general considerations, without focusing on any specific\ndomain. This paper addresses this gap by presenting the first Systematization\nof Knowledge dedicated to AI-driven systems for blockchain, with a special\nfocus on their security and privacy dimensions, shedding light on their\napplications, limitations, and future research directions."}
{"id": "2509.07225", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.07225", "abs": "https://arxiv.org/abs/2509.07225", "authors": ["Ze Sheng", "Qingxiao Xu", "Jianwei Huang", "Matthew Woodcock", "Heqing Huang", "Alastair F. Donaldson", "Guofei Gu", "Jeff Huang"], "title": "All You Need Is A Fuzzing Brain: An LLM-Powered System for Automated Vulnerability Detection and Patching", "comment": "14 pages, 5 figures", "summary": "Our team, All You Need Is A Fuzzing Brain, was one of seven finalists in\nDARPA's Artificial Intelligence Cyber Challenge (AIxCC), placing fourth in the\nfinal round. During the competition, we developed a Cyber Reasoning System\n(CRS) that autonomously discovered 28 security vulnerabilities - including six\npreviously unknown zero-days - in real-world open-source C and Java projects,\nand successfully patched 14 of them. The complete CRS is open source at\nhttps://github.com/o2lab/afc-crs-all-you-need-is-a-fuzzing-brain. This paper\nprovides a detailed technical description of our CRS, with an emphasis on its\nLLM-powered components and strategies. Building on AIxCC, we further introduce\na public leaderboard for benchmarking state-of-the-art LLMs on vulnerability\ndetection and patching tasks, derived from the AIxCC dataset. The leaderboard\nis available at https://o2lab.github.io/FuzzingBrain-Leaderboard/."}
{"id": "2509.07287", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07287", "abs": "https://arxiv.org/abs/2509.07287", "authors": ["Yan Pang", "Wenlong Meng", "Xiaojing Liao", "Tianhao Wang"], "title": "Paladin: Defending LLM-enabled Phishing Emails with a New Trigger-Tag Paradigm", "comment": "20 pages", "summary": "With the rapid development of large language models, the potential threat of\ntheir malicious use, particularly in generating phishing content, is becoming\nincreasingly prevalent. Leveraging the capabilities of LLMs, malicious users\ncan synthesize phishing emails that are free from spelling mistakes and other\neasily detectable features. Furthermore, such models can generate\ntopic-specific phishing messages, tailoring content to the target domain and\nincreasing the likelihood of success.\n  Detecting such content remains a significant challenge, as LLM-generated\nphishing emails often lack clear or distinguishable linguistic features. As a\nresult, most existing semantic-level detection approaches struggle to identify\nthem reliably. While certain LLM-based detection methods have shown promise,\nthey suffer from high computational costs and are constrained by the\nperformance of the underlying language model, making them impractical for\nlarge-scale deployment.\n  In this work, we aim to address this issue. We propose Paladin, which embeds\ntrigger-tag associations into vanilla LLM using various insertion strategies,\ncreating them into instrumented LLMs. When an instrumented LLM generates\ncontent related to phishing, it will automatically include detectable tags,\nenabling easier identification. Based on the design on implicit and explicit\ntriggers and tags, we consider four distinct scenarios in our work. We evaluate\nour method from three key perspectives: stealthiness, effectiveness, and\nrobustness, and compare it with existing baseline methods. Experimental results\nshow that our method outperforms the baselines, achieving over 90% detection\naccuracy across all scenarios."}
{"id": "2509.07290", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07290", "abs": "https://arxiv.org/abs/2509.07290", "authors": ["Nan Wang", "Nan Wu", "Xiangyu Hui", "Jiafan Wang", "Xin Yuan"], "title": "zkUnlearner: A Zero-Knowledge Framework for Verifiable Unlearning with Multi-Granularity and Forgery-Resistance", "comment": null, "summary": "As the demand for exercising the \"right to be forgotten\" grows, the need for\nverifiable machine unlearning has become increasingly evident to ensure both\ntransparency and accountability. We present {\\em zkUnlearner}, the first\nzero-knowledge framework for verifiable machine unlearning, specifically\ndesigned to support {\\em multi-granularity} and {\\em forgery-resistance}.\n  First, we propose a general computational model that employs a {\\em\nbit-masking} technique to enable the {\\em selectivity} of existing\nzero-knowledge proofs of training for gradient descent algorithms. This\ninnovation enables not only traditional {\\em sample-level} unlearning but also\nmore advanced {\\em feature-level} and {\\em class-level} unlearning. Our model\ncan be translated to arithmetic circuits, ensuring compatibility with a broad\nrange of zero-knowledge proof systems. Furthermore, our approach overcomes key\nlimitations of existing methods in both efficiency and privacy. Second, forging\nattacks present a serious threat to the reliability of unlearning.\nSpecifically, in Stochastic Gradient Descent optimization, gradients from\nunlearned data, or from minibatches containing it, can be forged using\nalternative data samples or minibatches that exclude it. We propose the first\neffective strategies to resist state-of-the-art forging attacks. Finally, we\nbenchmark a zkSNARK-based instantiation of our framework and perform\ncomprehensive performance evaluations to validate its practicality."}
{"id": "2509.07315", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.07315", "abs": "https://arxiv.org/abs/2509.07315", "authors": ["Hongfei Xia", "Hongru Wang", "Zeming Liu", "Qian Yu", "Yuhang Guo", "Haifeng Wang"], "title": "SafeToolBench: Pioneering a Prospective Benchmark to Evaluating Tool Utilization Safety in LLMs", "comment": "18 pages, 7 figures", "summary": "Large Language Models (LLMs) have exhibited great performance in autonomously\ncalling various tools in external environments, leading to better problem\nsolving and task automation capabilities. However, these external tools also\namplify potential risks such as financial loss or privacy leakage with\nambiguous or malicious user instructions. Compared to previous studies, which\nmainly assess the safety awareness of LLMs after obtaining the tool execution\nresults (i.e., retrospective evaluation), this paper focuses on prospective\nways to assess the safety of LLM tool utilization, aiming to avoid irreversible\nharm caused by directly executing tools. To this end, we propose SafeToolBench,\nthe first benchmark to comprehensively assess tool utilization security in a\nprospective manner, covering malicious user instructions and diverse practical\ntoolsets. Additionally, we propose a novel framework, SafeInstructTool, which\naims to enhance LLMs' awareness of tool utilization security from three\nperspectives (i.e., \\textit{User Instruction, Tool Itself, and Joint\nInstruction-Tool}), leading to nine detailed dimensions in total. We experiment\nwith four LLMs using different methods, revealing that existing approaches fail\nto capture all risks in tool utilization. In contrast, our framework\nsignificantly enhances LLMs' self-awareness, enabling a more safe and\ntrustworthy tool utilization."}
{"id": "2509.07457", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.07457", "abs": "https://arxiv.org/abs/2509.07457", "authors": ["Shakhzod Yuldoshkhujaev", "Mijin Jeon", "Doowon Kim", "Nick Nikiforakis", "Hyungjoon Koo"], "title": "A Decade-long Landscape of Advanced Persistent Threats: Longitudinal Analysis and Global Trends", "comment": "18 pages, 13 figures (including subfigures), 11 tables. To appear in\n  the Proceedings of the ACM Conference on Computer and Communications Security\n  (CCS) 2025", "summary": "An advanced persistent threat (APT) refers to a covert, long-term\ncyberattack, typically conducted by state-sponsored actors, targeting critical\nsectors and often remaining undetected for long periods. In response,\ncollective intelligence from around the globe collaborates to identify and\ntrace surreptitious activities, generating substantial documentation on APT\ncampaigns publicly available on the web. While prior works predominantly focus\non specific aspects of APT cases, such as detection, evaluation, cyber threat\nintelligence, and dataset creation, limited attention has been devoted to\nrevisiting and investigating these scattered dossiers in a longitudinal manner.\nThe objective of our study is to fill the gap by offering a macro perspective,\nconnecting key insights and global trends in past APT attacks. We\nsystematically analyze six reliable sources-three focused on technical reports\nand another three on threat actors-examining 1,509 APT dossiers (24,215 pages)\nspanning 2014-2023, and identifying 603 unique APT groups worldwide. To\nefficiently unearth relevant information, we employ a hybrid methodology that\ncombines rule-based information retrieval with large-language-model-based\nsearch techniques. Our longitudinal analysis reveals shifts in threat actor\nactivities, global attack vectors, changes in targeted sectors, and\nrelationships between cyberattacks and significant events such as elections or\nwars, which provide insights into historical patterns in APT evolution. Over\nthe past decade, 154 countries have been affected, primarily using malicious\ndocuments and spear phishing as dominant initial infiltration vectors, with a\nnoticeable decline in zero-day exploitation since 2016. Furthermore, we present\nour findings through interactive visualization tools, such as an APT map or\nflow diagram, to facilitate intuitive understanding of global patterns and\ntrends in APT activities."}
{"id": "2509.07465", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.07465", "abs": "https://arxiv.org/abs/2509.07465", "authors": ["Norman Poh", "Daryl Burns"], "title": "Biometric Bound Credentials for Age Verification", "comment": null, "summary": "Age verification is increasingly critical for regulatory compliance, user\ntrust, and the protection of minors online. Historically, solutions have\nstruggled with poor accuracy, intrusiveness, and significant security risks.\nMore recently, concerns have shifted toward privacy, surveillance, fairness,\nand the need for transparent, trustworthy systems. In this paper, we propose\nBiometric Bound Credentials (BBCreds) as a privacy-preserving approach that\ncryptographically binds age credentials to an individual's biometric features\nwithout storing biometric templates. This ensures only the legitimate,\nphysically present user can access age-restricted services, prevents credential\nsharing, and addresses both legacy and emerging challenges in age verification.\nenhances privacy."}
{"id": "2509.07504", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.07504", "abs": "https://arxiv.org/abs/2509.07504", "authors": ["Bilal Hussain Abbasi", "Yanjun Zhang", "Leo Zhang", "Shang Gao"], "title": "Backdoor Attacks and Defenses in Computer Vision Domain: A Survey", "comment": null, "summary": "Backdoor (trojan) attacks embed hidden, controllable behaviors into\nmachine-learning models so that models behave normally on benign inputs but\nproduce attacker-chosen outputs when a trigger is present. This survey reviews\nthe rapidly growing literature on backdoor attacks and defenses in the\ncomputer-vision domain. We introduce a multi-dimensional taxonomy that\norganizes attacks and defenses by injection stage (dataset poisoning,\nmodel/parameter modification, inference-time injection), trigger type (patch,\nblended/frequency, semantic, transformation), labeling strategy (dirty-label\nvs. clean-label / feature-collision), representation stage (instance-specific,\nmanifold/class-level, neuron/parameter hijacking, distributed encodings), and\ntarget task (classification, detection, segmentation, video, multimodal). For\neach axis we summarize representative methods, highlight evaluation practices,\nand discuss where defenses succeed or fail. For example, many classical\nsanitization and reverse-engineering tools are effective against reusable patch\nattacks but struggle with input-aware, sample-specific, or parameter-space\nbackdoors and with transfer via compromised pre-trained encoders or hardware\nbit-flips. We synthesize trends, identify persistent gaps (supply-chain and\nhardware threats, certifiable defenses, cross-task benchmarks), and propose\npractical guidelines for threat-aware evaluation and layered defenses. This\nsurvey aims to orient researchers and practitioners to the current threat\nlandscape and pressing research directions in secure computer vision."}
{"id": "2509.07505", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.07505", "abs": "https://arxiv.org/abs/2509.07505", "authors": ["Simon Cremer", "Lydia Jehmlich", "Rainer Lenz"], "title": "Extension of Spatial k-Anonymity: New Metrics for Assessing the Anonymity of Geomasked Data Considering Realistic Attack Scenarios", "comment": "16 pages, 2 figures", "summary": "Spatial data are gaining increasing importance in many areas of research.\nParticularly spatial health data are becoming increasingly important for\nmedical research, for example, to better understand relationships between\nenvironmental factors and disease patterns. However, their use is often\nrestricted by legal data protection regulations, since georeferenced personal\ninformation carries a high risk of re-identification of individuals. To address\nthis issue, what are called geomasking methods are applied to guarantee data\nprotection through targeted displacement of individual data points, while\nsimultaneously maintaining analytical validity within a tolerable range. In the\ncurrent literature the degree of anonymity of such anonymized georeferenced\ndatasets is often measured by the so-called metric of spatial k-anonymity.\nHowever, this metric has considerable shortcomings, particularly regarding its\nresilience against realistic data attack scenarios. This article classifies the\npotential data attack scenarios in the context of anonymized georeferenced\nmicrodata and introduces appropriate metrics that enable a comprehensive\nassessment of anonymity adapted to potential data attack scenarios."}
{"id": "2509.07606", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.07606", "abs": "https://arxiv.org/abs/2509.07606", "authors": ["Fadhil Abbas Fadhil", "Maryam Mahdi Alhusseini", "Mohammad-Reza Feizi-Derakhshi"], "title": "Enhanced cast-128 with adaptive s-box optimization via neural networks for image protection", "comment": null, "summary": "An improved CAST-128 encryption algorithm, which is done by implementing\nchaos-based adaptive S-box generation using Logistic sine Map (LSM), has been\nprovided in this paper because of the increasing requirements of efficient and\nsmart image encryption mechanisms. The study aims to address the drawbacks of\nstatic S-box models commonly used in traditional cryptographic systems, which\nare susceptible to linear and differential attacks. In the proposed scheme, the\ndynamic, non-linear, invertible, and highly cryptographic strength S-boxes are\ngenerated through a hybrid chaotic system that may have high non-linearity,\nstrong and rigorous avalanche characteristics, and low differential uniformity.\nThe process here is that the LSM is used to produce S-boxes having\nkey-dependent parameters that are stuffed into the CAST-128 structure to\nencrypt the image in a block-wise manner. The performance of the encryption is\nassessed utilizing a set of standard grayscale images. The metrics that are\nused to evaluate the security are entropy, NPCR, UACI, PSNR, and histogram\nanalysis. Outcomes indicate that randomness, resistance to statistical attacks,\nand country of encryption are significantly improved compared to the original\nCAST-128. The study is theoretically and practically relevant since it presents\na lightweight S-box generation approach driven by chaos, which can increase the\nlevel of robustness of the image encryptions without enlisting machine\nlearning. The system may be applied to secure communications, surveillance\nsystems, and medical image protection on a real-time basis."}
{"id": "2509.07615", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.07615", "abs": "https://arxiv.org/abs/2509.07615", "authors": ["Chongqing Lei", "Zhen Ling", "Xiangyu Xu", "Shaofeng Li", "Guangchi Liu", "Kai Dong", "Junzhou Luo"], "title": "FlexEmu: Towards Flexible MCU Peripheral Emulation (Extended Version)", "comment": "Accepted to appear at the 32nd ACM Conference on Computer and\n  Communications Security (CCS)", "summary": "Microcontroller units (MCUs) are widely used in embedded devices due to their\nlow power consumption and cost-effectiveness. MCU firmware controls these\ndevices and is vital to the security of embedded systems. However, performing\ndynamic security analyses for MCU firmware has remained challenging due to the\nlack of usable execution environments -- existing dynamic analyses cannot run\non physical devices (e.g., insufficient computational resources), while\nbuilding emulators is costly due to the massive amount of heterogeneous\nhardware, especially peripherals.\n  Our work is based on the insight that MCU peripherals can be modeled in a\ntwo-fold manner. At the structural level, peripherals have diverse\nimplementations but we can use a limited set of primitives to abstract\nperipherals because their hardware implementations are based on common hardware\nconcepts. At the semantic level, peripherals have diverse functionalities.\nHowever, we can use a single unified semantic model to describe the same kind\nof peripherals because they exhibit similar functionalities. Building on this,\nwe propose FlexEmu, a flexible MCU peripheral emulation framework. Once\nsemantic models are created, FlexEmu automatically extracts peripheral-specific\ndetails to instantiate models and generate emulators accordingly. We have\nsuccessfully applied FlexEmu to model 12 kinds of MCU peripherals. Our\nevaluation on 90 firmware samples across 15 different MCU platforms shows that\nthe automatically generated emulators can faithfully replicate hardware\nbehaviors and achieve a 98.48% unit test passing rate, outperforming\nstate-of-the-art approaches. To demonstrate the implications of FlexEmu on\nfirmware security, we use the generated emulators to fuzz three popular RTOSes\nand uncover 10 previously unknown bugs."}
{"id": "2509.07637", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.07637", "abs": "https://arxiv.org/abs/2509.07637", "authors": ["James Petrie"], "title": "Embedded Off-Switches for AI Compute", "comment": null, "summary": "To address the risks of increasingly capable AI systems, we introduce a\nhardware-level off-switch that embeds thousands of independent \"security\nblocks\" in each AI accelerator. This massively redundant architecture is\ndesigned to prevent unauthorized chip use, even against sophisticated physical\nattacks. Our main security block design uses public key cryptography to check\nthe authenticity of authorization licenses, and randomly generated nonces to\nprevent replay attacks. We evaluate attack vectors and present additional\nsecurity block variants that could be added for greater robustness. Security\nblocks can be built with standard circuit components, ensuring compatibility\nwith existing semiconductor manufacturing processes. With embedded security\nblocks, the next generation of AI accelerators could be more robustly defended\nagainst dangerous misuse."}
{"id": "2509.07649", "categories": ["cs.CR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.07649", "abs": "https://arxiv.org/abs/2509.07649", "authors": ["Ioannis Koufos", "Abdul Rehman Qureshi", "Adrian Asensio", "Allen Abishek", "Efstathios Zaragkas", "Ricard Vilalta", "Maria Souvalioti", "George Xilouris", "Michael-Alexandros Kourtis"], "title": "Leveraging Digital Twin-as-a-Service Towards Continuous and Automated Cybersecurity Certification", "comment": "6 pages, 5 figures, 1 table, to be published in IEEE Xplore", "summary": "Traditional risk assessments rely on manual audits and system scans, often\ncausing operational disruptions and leaving security gaps. To address these\nchallenges, this work presents Security Digital Twin-as-a-Service (SDT-aaS), a\nnovel approach that leverages Digital Twin (DT) technology for automated,\nnon-intrusive security compliance. SDT-aaS enables real-time security\nassessments by mirroring real-world assets, collecting compliance artifacts,\nand creating machine-readable evidence. The proposed work is a scalable and\ninteroperable solution that supports open standards like CycloneDX and Web of\nThings (WoT), facilitating seamless integration and efficient compliance\nmanagement. Empirical results from a moderate-scale infrastructure use case\ndemonstrate its feasibility and performance, paving the way for efficient,\non-demand cybersecurity governance with minimal operational impact."}
{"id": "2509.07757", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.07757", "abs": "https://arxiv.org/abs/2509.07757", "authors": ["Nils Bars", "Lukas Bernhard", "Moritz Schloegel", "Thorsten Holz"], "title": "Empirical Security Analysis of Software-based Fault Isolation through Controlled Fault Injection", "comment": null, "summary": "We use browsers daily to access all sorts of information. Because browsers\nroutinely process scripts, media, and executable code from unknown sources,\nthey form a critical security boundary between users and adversaries. A common\nattack vector is JavaScript, which exposes a large attack surface due to the\nsheer complexity of modern JavaScript engines. To mitigate these threats,\nmodern engines increasingly adopt software-based fault isolation (SFI). A\nprominent example is Google's V8 heap sandbox, which represents the most widely\ndeployed SFI mechanism, protecting billions of users across all Chromium-based\nbrowsers and countless applications built on Node.js and Electron. The heap\nsandbox splits the address space into two parts: one part containing trusted,\nsecurity-sensitive metadata, and a sandboxed heap containing memory accessible\nto untrusted code. On a technical level, the sandbox enforces isolation by\nremoving raw pointers and using translation tables to resolve references to\ntrusted objects. Consequently, an attacker cannot corrupt trusted data even\nwith full control of the sandboxed data, unless there is a bug in how code\nhandles data from the sandboxed heap. Despite their widespread use, such SFI\nmechanisms have seen little security testing.\n  In this work, we propose a new testing technique that models the security\nboundary of modern SFI implementations. Following the SFI threat model, we\nassume a powerful attacker who fully controls the sandbox's memory. We\nimplement this by instrumenting memory loads originating in the trusted domain\nand accessing untrusted, attacker-controlled sandbox memory. We then inject\nfaults into the loaded data, aiming to trigger memory corruption in the trusted\ndomain. In a comprehensive evaluation, we identify 19 security bugs in V8 that\nenable an attacker to bypass the sandbox."}
{"id": "2509.07764", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.07764", "abs": "https://arxiv.org/abs/2509.07764", "authors": ["Haitao Hu", "Peng Chen", "Yanpeng Zhao", "Yuqi Chen"], "title": "AgentSentinel: An End-to-End and Real-Time Security Defense Framework for Computer-Use Agents", "comment": null, "summary": "Large Language Models (LLMs) have been increasingly integrated into\ncomputer-use agents, which can autonomously operate tools on a user's computer\nto accomplish complex tasks. However, due to the inherently unstable and\nunpredictable nature of LLM outputs, they may issue unintended tool commands or\nincorrect inputs, leading to potentially harmful operations. Unlike traditional\nsecurity risks stemming from insecure user prompts, tool execution results from\nLLM-driven decisions introduce new and unique security challenges. These\nvulnerabilities span across all components of a computer-use agent. To mitigate\nthese risks, we propose AgentSentinel, an end-to-end, real-time defense\nframework designed to mitigate potential security threats on a user's computer.\nAgentSentinel intercepts all sensitive operations within agent-related services\nand halts execution until a comprehensive security audit is completed. Our\nsecurity auditing mechanism introduces a novel inspection process that\ncorrelates the current task context with system traces generated during task\nexecution. To thoroughly evaluate AgentSentinel, we present BadComputerUse, a\nbenchmark consisting of 60 diverse attack scenarios across six attack\ncategories. The benchmark demonstrates a 87% average attack success rate on\nfour state-of-the-art LLMs. Our evaluation shows that AgentSentinel achieves an\naverage defense success rate of 79.6%, significantly outperforming all baseline\ndefenses."}
{"id": "2509.07804", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.07804", "abs": "https://arxiv.org/abs/2509.07804", "authors": ["Yue Han", "Jinguang Han", "Liqun Chen", "Chao Sun"], "title": "Inner-product Functional Encryption with Fine-grained Revocation for Flexible EHR Sharing", "comment": null, "summary": "E-health record (EHR) contains a vast amount of continuously growing medical\ndata and enables medical institutions to access patient health data\nconveniently.This provides opportunities for medical data mining which has\nimportant applications in identifying high-risk patients and improving disease\ndiagnosis, etc.Since EHR contains sensitive patient information, how to protect\npatient privacy and enable mining on EHR data is important and\nchallenging.Traditional public key encryption (PKE) can protect patient\nprivacy, but cannot support flexible selective computation on encrypted EHR\ndata.Functional encryption (FE) allows authorised users to compute function\nvalues of encrypted data without releasing other information, hence supporting\nselective computation on encrypted data. Nevertheless, existing FE schemes do\nnot support fine-grained revocation and update, so they are unsuitable for EHR\nsystem. In this paper,we first propose an inner-product functional encryption\nwith fine-grained revocation (IPFE-FR) scheme, and then apply it to a flexible\nEHR sharing system. Our scheme possesses the following features:(1) a group\nmanager can revoke a specific function computation of medical institutions on\nencrypted EHR data,instead of all function computation rights. (2) a revoked\nmedical institution is not allowed to compute the function value of encrypted\nEHR data not only generated after the revocation, but also generated before the\nrevocation. (3) secret keys issued to the same medical institution are bound\ntogether to prevent collusion attacks. The formal definition and security model\nof the IPFE-FR scheme are proposed.Furthermore, we present a concrete\nconstruction and reduce its security to the Learning with Errors (LWE)\nassumption which is quantum-resistant. Finally, the theoretical analysis and\nexperimental implementation of our scheme are conducted to show its efficiency."}
{"id": "2509.07939", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07939", "abs": "https://arxiv.org/abs/2509.07939", "authors": ["Katsuaki Nakano", "Reza Feyyazi", "Shanchieh Jay Yang", "Michael Zuzak"], "title": "Guided Reasoning in LLM-Driven Penetration Testing Using Structured Attack Trees", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have driven interest in\nautomating cybersecurity penetration testing workflows, offering the promise of\nfaster and more consistent vulnerability assessment for enterprise systems.\nExisting LLM agents for penetration testing primarily rely on self-guided\nreasoning, which can produce inaccurate or hallucinated procedural steps. As a\nresult, the LLM agent may undertake unproductive actions, such as exploiting\nunused software libraries or generating cyclical responses that repeat prior\ntactics. In this work, we propose a guided reasoning pipeline for penetration\ntesting LLM agents that incorporates a deterministic task tree built from the\nMITRE ATT&CK Matrix, a proven penetration testing kll chain, to constrain the\nLLM's reaoning process to explicitly defined tactics, techniques, and\nprocedures. This anchors reasoning in proven penetration testing methodologies\nand filters out ineffective actions by guiding the agent towards more\nproductive attack procedures. To evaluate our approach, we built an automated\npenetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and\nGPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with\n103 discrete subtasks representing real-world cyberattack scenarios. Our\nproposed reasoning pipeline guided the LLM agent through 71.8\\%, 72.8\\%, and\n78.6\\% of subtasks using Llama-3-8B, Gemini-1.5, and GPT-4, respectively.\nComparatively, the state-of-the-art LLM penetration testing tool using\nself-guided reasoning completed only 13.5\\%, 16.5\\%, and 75.7\\% of subtasks and\nrequired 86.2\\%, 118.7\\%, and 205.9\\% more model queries. This suggests that\nincorporating a deterministic task tree into LLM reasoning pipelines can\nenhance the accuracy and efficiency of automated cybersecurity assessments"}
{"id": "2509.07941", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07941", "abs": "https://arxiv.org/abs/2509.07941", "authors": ["Kai Ye", "Liangcai Su", "Chenxiong Qian"], "title": "ImportSnare: Directed \"Code Manual\" Hijacking in Retrieval-Augmented Code Generation", "comment": "This paper has been accepted by the ACM Conference on Computer and\n  Communications Security (CCS) 2025", "summary": "Code generation has emerged as a pivotal capability of Large Language\nModels(LLMs), revolutionizing development efficiency for programmers of all\nskill levels. However, the complexity of data structures and algorithmic logic\noften results in functional deficiencies and security vulnerabilities in\ngenerated code, reducing it to a prototype requiring extensive manual\ndebugging. While Retrieval-Augmented Generation (RAG) can enhance correctness\nand security by leveraging external code manuals, it simultaneously introduces\nnew attack surfaces.\n  In this paper, we pioneer the exploration of attack surfaces in\nRetrieval-Augmented Code Generation (RACG), focusing on malicious dependency\nhijacking. We demonstrate how poisoned documentation containing hidden\nmalicious dependencies (e.g., matplotlib_safe) can subvert RACG, exploiting\ndual trust chains: LLM reliance on RAG and developers' blind trust in LLM\nsuggestions. To construct poisoned documents, we propose ImportSnare, a novel\nattack framework employing two synergistic strategies: 1)Position-aware beam\nsearch optimizes hidden ranking sequences to elevate poisoned documents in\nretrieval results, and 2)Multilingual inductive suggestions generate\njailbreaking sequences to manipulate LLMs into recommending malicious\ndependencies. Through extensive experiments across Python, Rust, and\nJavaScript, ImportSnare achieves significant attack success rates (over 50% for\npopular libraries such as matplotlib and seaborn) in general, and is also able\nto succeed even when the poisoning ratio is as low as 0.01%, targeting both\ncustom and real-world malicious packages. Our findings reveal critical supply\nchain risks in LLM-powered development, highlighting inadequate security\nalignment for code generation tasks. To support future research, we will\nrelease the multilingual benchmark suite and datasets. The project homepage is\nhttps://importsnare.github.io."}
