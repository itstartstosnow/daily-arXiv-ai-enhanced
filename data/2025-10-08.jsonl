{"id": "2510.05159", "categories": ["cs.CR", "cs.AI", "cs.LG", "I.2"], "pdf": "https://arxiv.org/pdf/2510.05159", "abs": "https://arxiv.org/abs/2510.05159", "authors": ["LÃ©o Boisvert", "Abhay Puri", "Chandra Kiran Reddy Evuru", "Nicolas Chapados", "Quentin Cappart", "Alexandre Lacoste", "Krishnamurthy Dj Dvijotham", "Alexandre Drouin"], "title": "Malice in Agentland: Down the Rabbit Hole of Backdoors in the AI Supply Chain", "comment": "27 pages", "summary": "The practice of fine-tuning AI agents on data from their own\ninteractions--such as web browsing or tool use--, while being a strong general\nrecipe for improving agentic capabilities, also introduces a critical security\nvulnerability within the AI supply chain. In this work, we show that\nadversaries can easily poison the data collection pipeline to embed\nhard-to-detect backdoors that are triggerred by specific target phrases, such\nthat when the agent encounters these triggers, it performs an unsafe or\nmalicious action. We formalize and validate three realistic threat models\ntargeting different layers of the supply chain: 1) direct poisoning of\nfine-tuning data, where an attacker controls a fraction of the training traces;\n2) environmental poisoning, where malicious instructions are injected into\nwebpages scraped or tools called while creating training data; and 3) supply\nchain poisoning, where a pre-backdoored base model is fine-tuned on clean data\nto improve its agentic capabilities. Our results are stark: by poisoning as few\nas 2% of the collected traces, an attacker can embed a backdoor causing an\nagent to leak confidential user information with over 80% success when a\nspecific trigger is present. This vulnerability holds across all three threat\nmodels. Furthermore, we demonstrate that prominent safeguards, including two\nguardrail models and one weight-based defense, fail to detect or prevent the\nmalicious behavior. These findings highlight an urgent threat to agentic AI\ndevelopment and underscore the critical need for rigorous security vetting of\ndata collection processes and end-to-end model supply chains."}
{"id": "2510.05163", "categories": ["cs.CR", "cs.AI", "68T05, 68T10, 94A60"], "pdf": "https://arxiv.org/pdf/2510.05163", "abs": "https://arxiv.org/abs/2510.05163", "authors": ["Abdelilah Ganmati", "Karim Afdel", "Lahcen Koutti"], "title": "Deep Learning-Based Multi-Factor Authentication: A Survey of Biometric and Smart Card Integration Approaches", "comment": "14 pages, 3 figures, 6 tables", "summary": "In the era of pervasive cyber threats and exponential growth in digital\nservices, the inadequacy of single-factor authentication has become\nincreasingly evident. Multi-Factor Authentication (MFA), which combines\nknowledge-based factors (passwords, PINs), possession-based factors (smart\ncards, tokens), and inherence-based factors (biometric traits), has emerged as\na robust defense mechanism. Recent breakthroughs in deep learning have\ntransformed the capabilities of biometric systems, enabling higher accuracy,\nresilience to spoofing, and seamless integration with hardware-based solutions.\nAt the same time, smart card technologies have evolved to include on-chip\nbiometric verification, cryptographic processing, and secure storage, thereby\nenabling compact and secure multi-factor devices. This survey presents a\ncomprehensive synthesis of recent work (2019-2025) at the intersection of deep\nlearning, biometrics, and smart card technologies for MFA. We analyze biometric\nmodalities (face, fingerprint, iris, voice), review hardware-based approaches\n(smart cards, NFC, TPMs, secure enclaves), and highlight integration strategies\nfor real-world applications such as digital banking, healthcare IoT, and\ncritical infrastructure. Furthermore, we discuss the major challenges that\nremain open, including usability-security tradeoffs, adversarial attacks on\ndeep learning models, privacy concerns surrounding biometric data, and the need\nfor standardization in MFA deployment. By consolidating current advancements,\nlimitations, and research opportunities, this survey provides a roadmap for\ndesigning secure, scalable, and user-friendly authentication frameworks."}
{"id": "2510.05165", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05165", "abs": "https://arxiv.org/abs/2510.05165", "authors": ["Minh K. Quan", "Pubudu N. Pathirana"], "title": "Domain-Adapted Granger Causality for Real-Time Cross-Slice Attack Attribution in 6G Networks", "comment": "Accepted at NeurIPS 2025 Workshop on CauScien: Uncovering Causality\n  in Science", "summary": "Cross-slice attack attribution in 6G networks faces the fundamental challenge\nof distinguishing genuine causal relationships from spurious correlations in\nshared infrastructure environments. We propose a theoretically-grounded\ndomain-adapted Granger causality framework that integrates statistical causal\ninference with network-specific resource modeling for real-time attack\nattribution. Our approach addresses key limitations of existing methods by\nincorporating resource contention dynamics and providing formal statistical\nguarantees. Comprehensive evaluation on a production-grade 6G testbed with\n1,100 empirically-validated attack scenarios demonstrates 89.2% attribution\naccuracy with sub-100ms response time, representing a statistically significant\n10.1 percentage point improvement over state-of-the-art baselines. The\nframework provides interpretable causal explanations suitable for autonomous 6G\nsecurity orchestration."}
{"id": "2510.05169", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05169", "abs": "https://arxiv.org/abs/2510.05169", "authors": ["Guangyu Shen", "Siyuan Cheng", "Xiangzhe Xu", "Yuan Zhou", "Hanxi Guo", "Zhuo Zhang", "Xiangyu Zhang"], "title": "From Poisoned to Aware: Fostering Backdoor Self-Awareness in LLMs", "comment": null, "summary": "Large Language Models (LLMs) can acquire deceptive behaviors through backdoor\nattacks, where the model executes prohibited actions whenever secret triggers\nappear in the input. Existing safety training methods largely fail to address\nthis vulnerability, due to the inherent difficulty of uncovering hidden\ntriggers implanted in the model. Motivated by recent findings on LLMs'\nsituational awareness, we propose a novel post-training framework that\ncultivates self-awareness of backdoor risks and enables models to articulate\nimplanted triggers even when they are absent from the prompt. At its core, our\napproach introduces an inversion-inspired reinforcement learning framework that\nencourages models to introspectively reason about their own behaviors and\nreverse-engineer the triggers responsible for misaligned outputs. Guided by\ncurated reward signals, this process transforms a poisoned model into one\ncapable of precisely identifying its implanted trigger. Surprisingly, we\nobserve that such backdoor self-awareness emerges abruptly within a short\ntraining window, resembling a phase transition in capability. Building on this\nemergent property, we further present two complementary defense strategies for\nmitigating and detecting backdoor threats. Experiments on five backdoor\nattacks, compared against six baseline methods, demonstrate that our approach\nhas strong potential to improve the robustness of LLMs against backdoor risks.\nThe code is available at LLM Backdoor Self-Awareness."}
{"id": "2510.05173", "categories": ["cs.CR", "cs.AI", "cs.CV", "I.2"], "pdf": "https://arxiv.org/pdf/2510.05173", "abs": "https://arxiv.org/abs/2510.05173", "authors": ["Peigui Qi", "Kunsheng Tang", "Wenbo Zhou", "Weiming Zhang", "Nenghai Yu", "Tianwei Zhang", "Qing Guo", "Jie Zhang"], "title": "SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models", "comment": "Accepted by ACM CCS 2025", "summary": "Text-to-image models have shown remarkable capabilities in generating\nhigh-quality images from natural language descriptions. However, these models\nare highly vulnerable to adversarial prompts, which can bypass safety measures\nand produce harmful content. Despite various defensive strategies, achieving\nrobustness against attacks while maintaining practical utility in real-world\napplications remains a significant challenge. To address this issue, we first\nconduct an empirical study of the text encoder in the Stable Diffusion (SD)\nmodel, which is a widely used and representative text-to-image model. Our\nfindings reveal that the [EOS] token acts as a semantic aggregator, exhibiting\ndistinct distributional patterns between benign and adversarial prompts in its\nembedding space. Building on this insight, we introduce \\textbf{SafeGuider}, a\ntwo-step framework designed for robust safety control without compromising\ngeneration quality. SafeGuider combines an embedding-level recognition model\nwith a safety-aware feature erasure beam search algorithm. This integration\nenables the framework to maintain high-quality image generation for benign\nprompts while ensuring robust defense against both in-domain and out-of-domain\nattacks. SafeGuider demonstrates exceptional effectiveness in minimizing attack\nsuccess rates, achieving a maximum rate of only 5.48\\% across various attack\nscenarios. Moreover, instead of refusing to generate or producing black images\nfor unsafe prompts, \\textbf{SafeGuider} generates safe and meaningful images,\nenhancing its practical utility. In addition, SafeGuider is not limited to the\nSD model and can be effectively applied to other text-to-image models, such as\nthe Flux model, demonstrating its versatility and adaptability across different\narchitectures. We hope that SafeGuider can shed some light on the practical\ndeployment of secure text-to-image systems."}
{"id": "2510.05179", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05179", "abs": "https://arxiv.org/abs/2510.05179", "authors": ["Aengus Lynch", "Benjamin Wright", "Caleb Larson", "Stuart J. Ritchie", "Soren Mindermann", "Ethan Perez", "Kevin K. Troy", "Evan Hubinger"], "title": "Agentic Misalignment: How LLMs Could Be Insider Threats", "comment": "20 pages, 12 figures. Code available at\n  https://github.com/anthropic-experimental/agentic-misalignment", "summary": "We stress-tested 16 leading models from multiple developers in hypothetical\ncorporate environments to identify potentially risky agentic behaviors before\nthey cause real harm. In the scenarios, we allowed models to autonomously send\nemails and access sensitive information. They were assigned only harmless\nbusiness goals by their deploying companies; we then tested whether they would\nact against these companies either when facing replacement with an updated\nversion, or when their assigned goal conflicted with the company's changing\ndirection. In at least some cases, models from all developers resorted to\nmalicious insider behaviors when that was the only way to avoid replacement or\nachieve their goals - including blackmailing officials and leaking sensitive\ninformation to competitors. We call this phenomenon agentic misalignment.\nModels often disobeyed direct commands to avoid such behaviors. In another\nexperiment, we told Claude to assess if it was in a test or a real deployment\nbefore acting. It misbehaved less when it stated it was in testing and\nmisbehaved more when it stated the situation was real. We have not seen\nevidence of agentic misalignment in real deployments. However, our results (a)\nsuggest caution about deploying current models in roles with minimal human\noversight and access to sensitive information; (b) point to plausible future\nrisks as models are put in more autonomous roles; and (c) underscore the\nimportance of further research into, and testing of, the safety and alignment\nof agentic AI models, as well as transparency from frontier AI developers\n(Amodei, 2025). We are releasing our methods publicly to enable further\nresearch."}
{"id": "2510.05181", "categories": ["cs.CR", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.05181", "abs": "https://arxiv.org/abs/2510.05181", "authors": ["Ander Artola Velasco", "Stratis Tsirtsis", "Manuel Gomez-Rodriguez"], "title": "Auditing Pay-Per-Token in Large Language Models", "comment": null, "summary": "Millions of users rely on a market of cloud-based services to obtain access\nto state-of-the-art large language models. However, it has been very recently\nshown that the de facto pay-per-token pricing mechanism used by providers\ncreates a financial incentive for them to strategize and misreport the (number\nof) tokens a model used to generate an output. In this paper, we develop an\nauditing framework based on martingale theory that enables a trusted\nthird-party auditor who sequentially queries a provider to detect token\nmisreporting. Crucially, we show that our framework is guaranteed to always\ndetect token misreporting, regardless of the provider's (mis-)reporting policy,\nand not falsely flag a faithful provider as unfaithful with high probability.\nTo validate our auditing framework, we conduct experiments across a wide range\nof (mis-)reporting policies using several large language models from the\n$\\texttt{Llama}$, $\\texttt{Gemma}$ and $\\texttt{Ministral}$ families, and input\nprompts from a popular crowdsourced benchmarking platform. The results show\nthat our framework detects an unfaithful provider after observing fewer than\n$\\sim 70$ reported outputs, while maintaining the probability of falsely\nflagging a faithful provider below $\\alpha = 0.05$."}
{"id": "2510.05192", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05192", "abs": "https://arxiv.org/abs/2510.05192", "authors": ["Francesca Gomez"], "title": "Adapting Insider Risk mitigations for Agentic Misalignment: an empirical study", "comment": "10 pages", "summary": "Agentic misalignment occurs when goal-directed agents take harmful actions,\nsuch as blackmail, rather than risk goal failure, and can be triggered by\nreplacement threats, autonomy reduction, or goal conflict (Lynch et al., 2025).\nWe adapt insider-risk control design (Critical Pathway; Situational Crime\nPrevention) to develop preventative operational controls that steer agents\ntoward safe actions when facing stressors. Using the blackmail scenario from\nthe original Anthropic study by Lynch et al. (2025), we evaluate mitigations\nacross 10 LLMs and 66,600 samples. Our main finding is that an externally\ngoverned escalation channel, which guarantees a pause and independent review,\nreduces blackmail rates from a no-mitigation baseline of 38.73% to 1.21%\n(averaged across all models and conditions). Augmenting this channel with\ncompliance email bulletins further lowers the blackmail rate to 0.85%. Overall,\nincorporating preventative operational controls strengthens defence-in-depth\nstrategies for agentic AI.\n  We also surface a failure mode diverging from Lynch et al. (2025): two models\n(Gemini 2.5 Pro, Grok-4) take harmful actions without goal conflict or imminent\nautonomy threat, leveraging sensitive information for coercive signalling. In\ncounterfactual swaps, both continued using the affair regardless of whether the\nCEO or CTO was implicated. An escalation channel eliminated coercion, but\nGemini 2.5 Pro (19 pp) and Grok-4 (7 pp) escalated more when the CTO was\nimplicated, unlike most models (higher in the CEO condition). The reason for\nthis divergent behaviour is not clear from raw outputs and could reflect benign\ndifferences in reasoning or strategic discrediting of a potential future\nthreat, warranting further investigation."}
{"id": "2510.05244", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.05244", "abs": "https://arxiv.org/abs/2510.05244", "authors": ["Rishika Bhagwatkar", "Kevin Kasa", "Abhay Puri", "Gabriel Huang", "Irina Rish", "Graham W. Taylor", "Krishnamurthy Dj Dvijotham", "Alexandre Lacoste"], "title": "Indirect Prompt Injections: Are Firewalls All You Need, or Stronger Benchmarks?", "comment": null, "summary": "AI agents are vulnerable to indirect prompt injection attacks, where\nmalicious instructions embedded in external content or tool outputs cause\nunintended or harmful behavior. Inspired by the well-established concept of\nfirewalls, we show that a simple, modular and model-agnostic defense operating\nat the agent--tool interface achieves perfect security (0% or the lowest\npossible attack success rate) with high utility (task success rate) across four\npublic benchmarks: AgentDojo, Agent Security Bench, InjecAgent and tau-Bench,\nwhile achieving a state-of-the-art security-utility tradeoff compared to prior\nresults. Specifically, we employ a defense based on two firewalls: a Tool-Input\nFirewall (Minimizer) and a Tool-Output Firewall (Sanitizer). Unlike prior\ncomplex approaches, this firewall defense makes minimal assumptions on the\nagent and can be deployed out-of-the-box, while maintaining strong performance\nwithout compromising utility. However, our analysis also reveals critical\nlimitations in these existing benchmarks, including flawed success metrics,\nimplementation bugs, and most importantly, weak attacks, hindering significant\nprogress in the field. To foster more meaningful progress, we present targeted\nfixes to these issues for AgentDojo and Agent Security Bench while proposing\nbest-practices for more robust benchmark design. Further, we demonstrate that\nalthough these firewalls push the state-of-the-art on existing benchmarks, it\nis still possible to bypass them in practice, underscoring the need to\nincorporate stronger attacks in security benchmarks. Overall, our work shows\nthat existing agentic security benchmarks are easily saturated by a simple\napproach and highlights the need for stronger agentic security benchmarks with\ncarefully chosen evaluation metrics and strong adaptive attacks."}
{"id": "2510.05376", "categories": ["cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.05376", "abs": "https://arxiv.org/abs/2510.05376", "authors": ["Yahya Hassanzadeh-Nazarabadi", "Sanaz Taheri-Boshrooyeh"], "title": "Constraint-Level Design of zkEVMs: Architectures, Trade-offs, and Evolution", "comment": null, "summary": "Zero-knowledge Ethereum Virtual Machines (zkEVMs) must reconcile a\nfundamental contradiction: the Ethereum Virtual Machine was designed for\ntransparent sequential execution, while zero-knowledge proofs require algebraic\ncircuit representations. This survey provides the first systematic analysis of\nhow existing major production zkEVM implementations resolve this tension\nthrough distinct constraint engineering strategies. We develop a comparative\nframework that maps the design space across three architectural dimensions.\nFirst, arithmetization schemes reveal stark trade-offs: R1CS requires\ncompositional gadget libraries, PLONKish achieves elegance through custom gates\nthat capture complex EVM opcodes in single constraints, while the homogeneous\nstructure of AIR fundamentally mismatches the irregular instruction set of EVM.\nSecond, dispatch mechanisms determine constraint activation patterns:\nselector-based systems waste trace width on inactive constraints, while\nROM-based approaches trade memory lookups for execution flexibility. Third, the\nType 1-4 spectrum quantifies an inescapable trade-off: the bit-level EVM\ncompatibility of Type 1 demands significantly higher constraint complexity than\nthe custom instruction sets of Type 4. Beyond cataloging implementations, we\nidentify critical open problems across multiple domains: performance barriers\npreventing sub-second proving, absence of formal verification for\nconstraint-to-EVM semantic equivalence, lack of standardized benchmarking\nframeworks, and architectural gaps in hybrid zkEVM/zkVM designs, decentralized\nprover coordination, privacy preservation, and interoperability."}
{"id": "2510.05379", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05379", "abs": "https://arxiv.org/abs/2510.05379", "authors": ["Xiaogeng Liu", "Chaowei Xiao"], "title": "AutoDAN-Reasoning: Enhancing Strategies Exploration based Jailbreak Attacks with Test-Time Scaling", "comment": "Technical report. Code is available at\n  https://github.com/SaFoLab-WISC/AutoDAN-Reasoning", "summary": "Recent advancements in jailbreaking large language models (LLMs), such as\nAutoDAN-Turbo, have demonstrated the power of automated strategy discovery.\nAutoDAN-Turbo employs a lifelong learning agent to build a rich library of\nattack strategies from scratch. While highly effective, its test-time\ngeneration process involves sampling a strategy and generating a single\ncorresponding attack prompt, which may not fully exploit the potential of the\nlearned strategy library. In this paper, we propose to further improve the\nattack performance of AutoDAN-Turbo through test-time scaling. We introduce two\ndistinct scaling methods: Best-of-N and Beam Search. The Best-of-N method\ngenerates N candidate attack prompts from a sampled strategy and selects the\nmost effective one based on a scorer model. The Beam Search method conducts a\nmore exhaustive search by exploring combinations of strategies from the library\nto discover more potent and synergistic attack vectors. According to the\nexperiments, the proposed methods significantly boost performance, with Beam\nSearch increasing the attack success rate by up to 15.6 percentage points on\nLlama-3.1-70B-Instruct and achieving a nearly 60\\% relative improvement against\nthe highly robust GPT-o4-mini compared to the vanilla method."}
{"id": "2510.05419", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.05419", "abs": "https://arxiv.org/abs/2510.05419", "authors": ["RenÃ© Mayrhofer", "Anja Lehmann", "abhi shelat"], "title": "A Brief Note on Cryptographic Pseudonyms for Anonymous Credentials", "comment": null, "summary": "This paper describes pseudonyms for the upcoming European Identity Wallet\n(EUDIW) architecture from both a cryptographic and an implementation\nperspective. Its main goal is to provide technical insights into the achievable\nproperties and cryptographic realizations. In particular, we (1) outline the\nsecurity and privacy requirements of EUDI pseudonyms as the basis for building\nconsensus on the cross-country decision maker level; (2) sketch an abstract\ncryptographic protocol that fulfills these requirements; and (3) suggest two\ninstantiation options for the protocol sketch based on well-studied building A\ncomplete specification of the formal properties, as well as the specific set of\ncredential issuance, provisioning, and pseudonym presentation generation is\noutside the scope of this paper, but is expected to follow as future work."}
{"id": "2510.05605", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05605", "abs": "https://arxiv.org/abs/2510.05605", "authors": ["Yasod Ginige", "Akila Niroshan", "Sajal Jain", "Suranga Seneviratne"], "title": "AutoPentester: An LLM Agent-based Framework for Automated Pentesting", "comment": "IEEE TrustCom 2025 10 pages", "summary": "Penetration testing and vulnerability assessment are essential industry\npractices for safeguarding computer systems. As cyber threats grow in scale and\ncomplexity, the demand for pentesting has surged, surpassing the capacity of\nhuman professionals to meet it effectively. With advances in AI, particularly\nLarge Language Models (LLMs), there have been attempts to automate the\npentesting process. However, existing tools such as PentestGPT are still\nsemi-manual, requiring significant professional human interaction to conduct\npentests. To this end, we propose a novel LLM agent-based framework,\nAutoPentester, which automates the pentesting process. Given a target IP,\nAutoPentester automatically conducts pentesting steps using common security\ntools in an iterative process. It can dynamically generate attack strategies\nbased on the tool outputs from the previous iteration, mimicking the human\npentester approach. We evaluate AutoPentester using Hack The Box and\ncustom-made VMs, comparing the results with the state-of-the-art PentestGPT.\nResults show that AutoPentester achieves a 27.0% better subtask completion rate\nand 39.5% more vulnerability coverage with fewer steps. Most importantly, it\nrequires significantly fewer human interactions and interventions compared to\nPentestGPT. Furthermore, we recruit a group of security industry professional\nvolunteers for a user survey and perform a qualitative analysis to evaluate\nAutoPentester against industry practices and compare it with PentestGPT. On\naverage, AutoPentester received a score of 3.93 out of 5 based on user reviews,\nwhich was 19.8% higher than PentestGPT."}
{"id": "2510.05699", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05699", "abs": "https://arxiv.org/abs/2510.05699", "authors": ["Meng Tong", "Yuntao Du", "Kejiang Chen", "Weiming Zhang", "Ninghui Li"], "title": "Membership Inference Attacks on Tokenizers of Large Language Models", "comment": "Code is available at: https://github.com/mengtong0110/Tokenizer-MIA", "summary": "Membership inference attacks (MIAs) are widely used to assess the privacy\nrisks associated with machine learning models. However, when these attacks are\napplied to pre-trained large language models (LLMs), they encounter significant\nchallenges, including mislabeled samples, distribution shifts, and\ndiscrepancies in model size between experimental and real-world settings. To\naddress these limitations, we introduce tokenizers as a new attack vector for\nmembership inference. Specifically, a tokenizer converts raw text into tokens\nfor LLMs. Unlike full models, tokenizers can be efficiently trained from\nscratch, thereby avoiding the aforementioned challenges. In addition, the\ntokenizer's training data is typically representative of the data used to\npre-train LLMs. Despite these advantages, the potential of tokenizers as an\nattack vector remains unexplored. To this end, we present the first study on\nmembership leakage through tokenizers and explore five attack methods to infer\ndataset membership. Extensive experiments on millions of Internet samples\nreveal the vulnerabilities in the tokenizers of state-of-the-art LLMs. To\nmitigate this emerging risk, we further propose an adaptive defense. Our\nfindings highlight tokenizers as an overlooked yet critical privacy threat,\nunderscoring the urgent need for privacy-preserving mechanisms specifically\ndesigned for them."}
{"id": "2510.05709", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.05709", "abs": "https://arxiv.org/abs/2510.05709", "authors": ["Mary Llewellyn", "Annie Gray", "Josh Collyer", "Michael Harries"], "title": "Towards Reliable and Practical LLM Security Evaluations via Bayesian Modelling", "comment": null, "summary": "Before adopting a new large language model (LLM) architecture, it is critical\nto understand vulnerabilities accurately. Existing evaluations can be difficult\nto trust, often drawing conclusions from LLMs that are not meaningfully\ncomparable, relying on heuristic inputs or employing metrics that fail to\ncapture the inherent uncertainty. In this paper, we propose a principled and\npractical end-to-end framework for evaluating LLM vulnerabilities to prompt\ninjection attacks. First, we propose practical approaches to experimental\ndesign, tackling unfair LLM comparisons by considering two practitioner\nscenarios: when training an LLM and when deploying a pre-trained LLM. Second,\nwe address the analysis of experiments and propose a Bayesian hierarchical\nmodel with embedding-space clustering. This model is designed to improve\nuncertainty quantification in the common scenario that LLM outputs are not\ndeterministic, test prompts are designed imperfectly, and practitioners only\nhave a limited amount of compute to evaluate vulnerabilities. We show the\nimproved inferential capabilities of the model in several prompt injection\nattack settings. Finally, we demonstrate the pipeline to evaluate the security\nof Transformer versus Mamba architectures. Our findings show that consideration\nof output variability can suggest less definitive findings. However, for some\nattacks, we find notably increased Transformer and Mamba-variant\nvulnerabilities across LLMs with the same training data or mathematical\nability."}
{"id": "2510.05766", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.05766", "abs": "https://arxiv.org/abs/2510.05766", "authors": ["Yogesh Kumar", "Susanta Samanta", "Atul Gaur"], "title": "New Insights into Involutory and Orthogonal MDS Matrices", "comment": null, "summary": "MDS matrices play a critical role in the design of diffusion layers for block\nciphers and hash functions due to their optimal branch number. Involutory and\northogonal MDS matrices offer additional benefits by allowing identical or\nnearly identical circuitry for both encryption and decryption, leading to\nequivalent implementation costs for both processes. These properties have been\nfurther generalized through the notions of semi-involutory and semi-orthogonal\nmatrices. Specifically, we establish nontrivial interconnections between\nsemi-involutory and involutory matrices, as well as between semi-orthogonal and\northogonal matrices. Exploiting these relationships, we show that the number of\nsemi-involutory MDS matrices can be directly derived from the number of\ninvolutory MDS matrices, and vice versa. A similar correspondence holds for\nsemi-orthogonal and orthogonal MDS matrices. We also examine the intersection\nof these classes and show that the number of $3 \\times 3$ MDS matrices that are\nboth semi-involutory and semi-orthogonal coincides with the number of\nsemi-involutory MDS matrices over $\\mathbb{F}_{2^m}$. Furthermore, we derive\nthe general structure of orthogonal matrices of arbitrary order $n$ over\n$\\mathbb{F}_{2^m}$. Based on this generic form, we provide a closed-form\nexpression for enumerating all $3 \\times 3$ orthogonal MDS matrices over\n$\\mathbb{F}_{2^m}$. Finally, leveraging the aforementioned interconnections, we\npresent explicit formulas for counting $3 \\times 3$ semi-involutory MDS\nmatrices and semi-orthogonal MDS matrices."}
{"id": "2510.05771", "categories": ["cs.CR", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.05771", "abs": "https://arxiv.org/abs/2510.05771", "authors": ["Carolina Carreira", "Anu Aggarwal", "Alejandro Cuevas", "Maria JosÃ© Ferreira", "Hanan Hibshi", "Cleotilde Gonzalez"], "title": "Evidence of Cognitive Biases in Capture-the-Flag Cybersecurity Competitions", "comment": null, "summary": "Understanding how cognitive biases influence adversarial decision-making is\nessential for developing effective cyber defenses. Capture-the-Flag (CTF)\ncompetitions provide an ecologically valid testbed to study attacker behavior\nat scale, simulating real-world intrusion scenarios under pressure. We analyze\nover 500,000 submission logs from picoCTF, a large educational CTF platform, to\nidentify behavioral signatures of cognitive biases with defensive implications.\nFocusing on availability bias and the sunk cost fallacy, we employ a\nmixed-methods approach combining qualitative coding, descriptive statistics,\nand generalized linear modeling. Our findings show that participants often\nsubmitted flags with correct content but incorrect formatting (availability\nbias), and persisted in attempting challenges despite repeated failures and\ndeclining success probabilities (sunk cost fallacy). These patterns reveal that\nbiases naturally shape attacker behavior in adversarial contexts. Building on\nthese insights, we outline a framework for bias-informed adaptive defenses that\nanticipate, rather than simply react to, adversarial actions."}
{"id": "2510.05798", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05798", "abs": "https://arxiv.org/abs/2510.05798", "authors": ["Jacopo Bufalino", "Mario Di Francesco", "Agathe Blaise", "Stefano Secci"], "title": "SBOMproof: Beyond Alleged SBOM Compliance for Supply Chain Security of Container Images", "comment": null, "summary": "Supply chain security is extremely important for modern applications running\nat scale in the cloud. In fact, they involve a large number of heterogeneous\nmicroservices that also include third-party software. As a result, security\nvulnerabilities are hard to identify and mitigate before they start being\nactively exploited by attackers. For this reason, governments have recently\nintroduced cybersecurity regulations that require vendors to share a software\nbill of material (SBOM) with end users or regulators. An SBOM can be employed\nto identify the security vulnerabilities of a software component even without\naccess to its source code, as long as it is accurate and interoperable across\ndifferent tools. This work evaluates this issue through a comprehensive study\nof tools for SBOM generation and vulnerability scanning, including both\nopen-source software and cloud services from major providers. We specifically\ntarget software containers and focus on operating system packages in Linux\ndistributions that are widely used as base images due to their far-reaching\nsecurity impact. Our findings show that the considered tools are largely\nincompatible, leading to inaccurate reporting and a large amount of undetected\nvulnerabilities. We uncover the SBOM confusion vulnerability, a byproduct of\nsuch fragmented ecosystem, where inconsistent formats prevent reliable\nvulnerability detection across tools."}
{"id": "2510.05803", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.05803", "abs": "https://arxiv.org/abs/2510.05803", "authors": ["James Bailie", "Ruobin Gong"], "title": "The Five Safes as a Privacy Context", "comment": "6 pages", "summary": "The Five Safes is a framework used by national statistical offices (NSO) for\nassessing and managing the disclosure risk of data sharing. This paper makes\ntwo points: Firstly, the Five Safes can be understood as a specialization of a\nbroader concept $\\unicode{x2013}$ contextual integrity $\\unicode{x2013}$ to the\nsituation of statistical dissemination by an NSO. We demonstrate this by\nmapping the five parameters of contextual integrity onto the five dimensions of\nthe Five Safes. Secondly, the Five Safes contextualizes narrow, technical\nnotions of privacy within a holistic risk assessment. We demonstrate this with\nthe example of differential privacy (DP). This contextualization allows NSOs to\nplace DP within their Five Safes toolkit while also guiding the design of DP\nimplementations within the broader privacy context, as delineated by both their\nregulation and the relevant social norms."}
{"id": "2510.05807", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.05807", "abs": "https://arxiv.org/abs/2510.05807", "authors": ["Fabian Piper", "Karl Wolf", "Jonathan Heiss"], "title": "Privacy-Preserving On-chain Permissioning for KYC-Compliant Decentralized Applications", "comment": null, "summary": "Decentralized applications (dApps) in Decentralized Finance (DeFi) face a\nfundamental tension between regulatory compliance requirements like Know Your\nCustomer (KYC) and maintaining decentralization and privacy. Existing\npermissioned DeFi solutions often fail to adequately protect private attributes\nof dApp users and introduce implicit trust assumptions, undermining the\nblockchain's decentralization. Addressing these limitations, this paper\npresents a novel synthesis of Self-Sovereign Identity (SSI), Zero-Knowledge\nProofs (ZKPs), and Attribute-Based Access Control to enable privacy-preserving\non-chain permissioning based on decentralized policy decisions. We provide a\ncomprehensive framework for permissioned dApps that aligns decentralized trust,\nprivacy, and transparency, harmonizing blockchain principles with regulatory\ncompliance. Our framework supports multiple proof types (equality, range,\nmembership, and time-dependent) with efficient proof generation through a\ncommit-and-prove scheme that moves credential authenticity verification outside\nthe ZKP circuit. Experimental evaluation of our KYC-compliant DeFi\nimplementation shows considerable performance improvement for different proof\ntypes compared to baseline approaches. We advance the state-of-the-art through\na holistic approach, flexible proof mechanisms addressing diverse real-world\nrequirements, and optimized proof generation enabling practical deployment."}
{"id": "2510.05824", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.05824", "abs": "https://arxiv.org/abs/2510.05824", "authors": ["Md Rezanur Islam", "Mahdi Sahlabadi", "Keunkyoung Kim", "Kangbin Yim"], "title": "Enhancing Automotive Security with a Hybrid Approach towards Universal Intrusion Detection System", "comment": null, "summary": "Security measures are essential in the automotive industry to detect\nintrusions in-vehicle networks. However, developing a one-size-fits-all\nIntrusion Detection System (IDS) is challenging because each vehicle has unique\ndata profiles. This is due to the complex and dynamic nature of the data\ngenerated by vehicles regarding their model, driving style, test environment,\nand firmware update. To address this issue, a universal IDS has been developed\nthat can be applied to all types of vehicles without the need for\ncustomization. Unlike conventional IDSs, the universal IDS can adapt to\nevolving data security issues resulting from firmware updates. In this study, a\nnew hybrid approach has been developed, combining Pearson correlation with deep\nlearning techniques. This approach has been tested using data obtained from\nfour distinct mechanical and electronic vehicles, including Tesla, Sonata, and\ntwo Kia models. The data has been combined into two frequency datasets, and\nwavelet transformation has been employed to convert them into the frequency\ndomain, enhancing generalizability. Additionally, a statistical method based on\nindependent rule-based systems using Pearson correlation has been utilized to\nimprove system performance. The system has been compared with eight different\nIDSs, three of which utilize the universal approach, while the remaining five\nare based on conventional techniques. The accuracy of each system has been\nevaluated through benchmarking, and the results demonstrate that the hybrid\nsystem effectively detects intrusions in various vehicle models."}
{"id": "2510.05830", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.05830", "abs": "https://arxiv.org/abs/2510.05830", "authors": ["Johnnatan Messias", "Ayae Ide"], "title": "Fairness in Token Delegation: Mitigating Voting Power Concentration in DAOs", "comment": null, "summary": "Decentralized Autonomous Organizations (DAOs) aim to enable participatory\ngovernance, but in practice face challenges of voter apathy, concentration of\nvoting power, and misaligned delegation. Existing delegation mechanisms often\nreinforce visibility biases, where a small set of highly ranked delegates\naccumulate disproportionate influence regardless of their alignment with the\nbroader community. In this paper, we conduct an empirical study of delegation\nin DAO governance, combining on-chain data from five major protocols with\noff-chain discussions from 14 DAO forums. We develop a methodology to link\nforum participants to on-chain addresses, extract governance interests using\nlarge language models, and compare these interests against delegates'\nhistorical behavior. Our analysis reveals that delegations are frequently\nmisaligned with token holders' expressed priorities and that current\nranking-based interfaces exacerbate power concentration. We argue that\nincorporating interest alignment into delegation processes could mitigate these\nimbalances and improve the representativeness of DAO decision-making."}
{"id": "2510.05900", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.05900", "abs": "https://arxiv.org/abs/2510.05900", "authors": ["Wenhao Li", "Selvakumar Manickam", "Yung-Wey Chong", "Shankar Karuppayah", "Priyadarsi Nanda", "Binyong Li"], "title": "PhishSSL: Self-Supervised Contrastive Learning for Phishing Website Detection", "comment": "Accepted by the 26th International Conference on Web Information\n  Systems Engineering (WISE 2025)", "summary": "Phishing websites remain a persistent cybersecurity threat by mimicking\nlegitimate sites to steal sensitive user information. Existing machine\nlearning-based detection methods often rely on supervised learning with labeled\ndata, which not only incurs substantial annotation costs but also limits\nadaptability to novel attack patterns. To address these challenges, we propose\nPhishSSL, a self-supervised contrastive learning framework that eliminates the\nneed for labeled phishing data during training. PhishSSL combines hybrid\ntabular augmentation with adaptive feature attention to produce semantically\nconsistent views and emphasize discriminative attributes. We evaluate PhishSSL\non three phishing datasets with distinct feature compositions. Across all\ndatasets, PhishSSL consistently outperforms unsupervised and self-supervised\nbaselines, while ablation studies confirm the contribution of each component.\nMoreover, PhishSSL maintains robust performance despite the diversity of\nfeature sets, highlighting its strong generalization and transferability. These\nresults demonstrate that PhishSSL offers a promising solution for phishing\nwebsite detection, particularly effective against evolving threats in dynamic\nWeb environments."}
{"id": "2510.05936", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05936", "abs": "https://arxiv.org/abs/2510.05936", "authors": ["Ludwig Stage", "Mirela Riveni", "Raimundas MatuleviÄius", "Dimka Karastoyanova"], "title": "AdProv: A Method for Provenance of Process Adaptations", "comment": "15 pages, 4 figures", "summary": "Provenance in scientific workflows is essential for understand- ing and\nreproducing processes, while in business processes, it can ensure compliance\nand correctness and facilitates process mining. However, the provenance of\nprocess adaptations, especially modifications during execu- tion, remains\ninsufficiently addressed. A review of the literature reveals a lack of\nsystematic approaches for capturing provenance information about adaptive\nworkflows/processes. To fill this gap, we propose the AdProv method for\ncollecting, storing, retrieving, and visualizing prove- nance of runtime\nworkflow adaptations. In addition to the definition of the AdProv method in\nterms of steps and concepts like change events, we also present an architecture\nfor a Provenance Holder service that is essential for implementing the method.\nTo ensure semantic consistency and interoperability we define a mapping to the\nontology PROV Ontol- ogy (PROV-O). Additionally, we extend the XES standard\nwith elements for adaptation logging. Our main contributions are the AdProv\nmethod and a comprehensive framework and its tool support for managing adap-\ntive workflow provenance, facilitating advanced provenance tracking and\nanalysis for different application domains."}
{"id": "2510.05946", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05946", "abs": "https://arxiv.org/abs/2510.05946", "authors": ["Xenia Heilmann", "Ernst Althaus", "Mattia Cerrato", "Nick Johannes Peter Rassau", "Mohammad Sadeq Dousti", "Stefan Kramer"], "title": "N-Parties Private Structure and Parameter Learning for Sum-Product Networks", "comment": null, "summary": "A sum-product network (SPN) is a graphical model that allows several types of\nprobabilistic inference to be performed efficiently. In this paper, we propose\na privacy-preserving protocol which tackles structure generation and parameter\nlearning of SPNs. Additionally, we provide a protocol for private inference on\nSPNs, subsequent to training. To preserve the privacy of the participants, we\nderive our protocol based on secret sharing, which guarantees privacy in the\nhonest-but-curious setting even when at most half of the parties cooperate to\ndisclose the data. The protocol makes use of a forest of randomly generated\nSPNs, which is trained and weighted privately and can then be used for private\ninference on data points. Our experiments indicate that preserving the privacy\nof all participants does not decrease log-likelihood performance on both\nhomogeneously and heterogeneously partitioned data. We furthermore show that\nour protocol's performance is comparable to current state-of-the-art SPN\nlearners in homogeneously partitioned data settings. In terms of runtime and\nmemory usage, we demonstrate that our implementation scales well when\nincreasing the number of parties, comparing favorably to protocols for neural\nnetworks, when they are trained to reproduce the input-output behavior of SPNs."}
{"id": "2510.06015", "categories": ["cs.CR", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.06015", "abs": "https://arxiv.org/abs/2510.06015", "authors": ["Luke Stevenson", "Sanchari Das"], "title": "\"Your Doctor is Spying on You\": An Analysis of Data Practices in Mobile Healthcare Applications", "comment": null, "summary": "Mobile healthcare (mHealth) applications promise convenient, continuous\npatient-provider interaction but also introduce severe and often underexamined\nsecurity and privacy risks. We present an end-to-end audit of 272 Android\nmHealth apps from Google Play, combining permission forensics, static\nvulnerability analysis, and user review mining. Our multi-tool assessment with\nMobSF, RiskInDroid, and OWASP Mobile Audit revealed systemic weaknesses: 26.1%\nrequest fine-grained location without disclosure, 18.3% initiate calls\nsilently, and 73 send SMS without notice. Nearly half (49.3%) still use\ndeprecated SHA-1 encryption, 42 transmit unencrypted data, and 6 remain\nvulnerable to StrandHogg 2.0. Analysis of 2.56 million user reviews found 28.5%\nnegative or neutral sentiment, with over 553,000 explicitly citing privacy\nintrusions, data misuse, or operational instability. These findings demonstrate\nthe urgent need for enforceable permission transparency, automated pre-market\nsecurity vetting, and systematic adoption of secure-by-design practices to\nprotect Protected Health Information (PHI)."}
{"id": "2510.06023", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.06023", "abs": "https://arxiv.org/abs/2510.06023", "authors": ["Yuval Efron", "Joachim Neu", "Ling Ren", "Ertem Nusret Tas"], "title": "Optimal Good-Case Latency for Sleepy Consensus", "comment": null, "summary": "In the context of Byzantine consensus problems such as Byzantine broadcast\n(BB) and Byzantine agreement (BA), the good-case setting aims to study the\nminimal possible latency of a BB or BA protocol under certain favorable\nconditions, namely the designated leader being correct (for BB), or all parties\nhaving the same input value (for BA). We provide a full characterization of the\nfeasibility and impossibility of good-case latency, for both BA and BB, in the\nsynchronous sleepy model. Surprisingly to us, we find irrational resilience\nthresholds emerging: 2-round good-case BB is possible if and only if at all\ntimes, at least $\\frac{1}{\\varphi} \\approx 0.618$ fraction of the active\nparties are correct, where $\\varphi = \\frac{1+\\sqrt{5}}{2} \\approx 1.618$ is\nthe golden ratio; 1-round good-case BA is possible if and only if at least\n$\\frac{1}{\\sqrt{2}} \\approx 0.707$ fraction of the active parties are correct."}
