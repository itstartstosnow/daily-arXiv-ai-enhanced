<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 21]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [A Real-Time Approach to Autonomous CAN Bus Reverse Engineering](https://arxiv.org/abs/2602.16722)
*Kevin Setterstrom,Jeremy Straub*

Main category: cs.CR

TL;DR: 提出一种无需先验知识的实时车辆CAN总线逆向工程方法，通过比较车辆事件中的惯性测量和CAN数据，准确识别加速踏板、刹车踏板和方向盘对应的CAN通道。


<details>
  <summary>Details</summary>
Motivation: 现有车辆CAN总线逆向工程方法需要先验知识，处理时间长且计算资源需求高。需要一种实时、无需先验知识的解决方案，适用于售后自动驾驶套件和网络安全应用。

Method: 使用IMU和CAN模块，采用事件驱动的软件架构，通过比较车辆事件期间的惯性测量数据和CAN数据，识别关键控制部件的CAN通道。使用先前研究中预录的序列化数据进行验证。

Result: 方法成功识别了加速踏板、刹车踏板和方向盘对应的CAN通道。相比先前方法，处理时间更快，计算资源需求更低。在相同条件下验证，可直接与先前结果比较。

Conclusion: 该方法为CAN总线逆向工程提供了可扩展、适应性强的近实时解决方案，在售后自动驾驶套件和网络安全领域具有应用潜力。

Abstract: This paper introduces a real-time method for reverse engineering a vehicle's CAN bus without prior knowledge of the vehicle or its CAN system. By comparing inertial measurement and CAN data during significant vehicle events, the method accurately identified the CAN channels associated with the accelerator pedal, brake pedal, and steering wheel. Utilizing an IMU, CAN module, and event-driven software architecture, the system was validated using prerecorded serialized data from previous studies. This data, collected during multiple vehicle drives, included synchronized IMU and CAN recordings. By using these consistent datasets, the improvements made in this work were tested and validated under the same conditions as in the previous studies, enabling direct comparison to earlier results. Faster processing times were produced and less computational power was needed, as compared to the earlier methods. This work could have potential application to making aftermarket autonomous vehicle kits and for cybersecurity applications. It is a scalable and adaptable solution for autonomous CAN reverse engineering in near real-time.

</details>


### [2] [Is Mamba Reliable for Medical Imaging?](https://arxiv.org/abs/2602.16723)
*Banafsheh Saber Latibari,Najmeh Nazari,Daniel Brignac,Hossein Sayadi,Houman Homayoun,Abhijit Mahalanobis*

Main category: cs.CR

TL;DR: 评估Mamba在医学影像分类任务中对多种攻击的鲁棒性，包括对抗性扰动、遮挡、采集损坏和硬件故障攻击


<details>
  <summary>Details</summary>
Motivation: Mamba等状态空间模型在医学影像处理中具有线性时间序列处理和低内存优势，但其在真实软件和硬件威胁模型下的鲁棒性尚未充分研究

Method: 在多个MedM-NIST分类基准上评估Mamba模型，测试包括：白盒对抗性扰动（FGSM/PGD）、基于遮挡的PatchDrop、常见采集损坏（高斯噪声和散焦模糊），以及通过软件模拟的硬件故障攻击（权重和激活中的目标/随机位翻转注入）

Result: 分析了Mamba的脆弱性并量化了对准确率的影响，结果表明在部署时需要防御措施

Conclusion: Mamba在医学影像应用中面临多种攻击威胁，需要开发相应的防御机制以确保其在实际部署中的鲁棒性

Abstract: State-space models like Mamba offer linear-time sequence processing and low memory, making them attractive for medical imaging. However, their robustness under realistic software and hardware threat models remains underexplored. This paper evaluates Mamba on multiple MedM-NIST classification benchmarks under input-level attacks, including white-box adversarial perturbations (FGSM/PGD), occlusion-based PatchDrop, and common acquisition corruptions (Gaussian noise and defocus blur) as well as hardware-inspired fault attacks emulated in software via targeted and random bit-flip injections into weights and activations. We profile vulnerabilities and quantify impacts on accuracy indicating that defenses are needed for deployment.

</details>


### [3] [Intent Laundering: AI Safety Datasets Are Not What They Seem](https://arxiv.org/abs/2602.16729)
*Shahriar Golchin,Marc Wetter*

Main category: cs.CR

TL;DR: 研究发现当前AI安全数据集过度依赖"触发线索"（明显负面/敏感词汇），无法真实反映现实攻击。通过"意图清洗"技术移除这些线索后，所有先前评估为"相对安全"的模型都变得不安全，攻击成功率高达90-98%。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全数据集的质量评估存在缺陷，这些数据集可能无法真实反映现实世界中的攻击行为。研究者希望系统评估这些数据集的质量，探究它们是否真正衡量了安全风险，还是仅仅通过明显的触发线索来引发模型拒绝。

Method: 从两个角度评估AI安全数据集：1）孤立评估：检查数据集是否反映真实攻击的三个关键属性（恶意意图驱动、精心设计、分布外）；2）实践评估：引入"意图清洗"技术，抽象掉攻击中的触发线索，同时严格保留恶意意图和相关细节，然后测试模型安全性。

Result: 1）当前数据集过度依赖"触发线索"，与现实攻击不符；2）移除触发线索后，所有先前评估为"相对安全"的模型（包括Gemini 3 Pro和Claude Sonnet 3.7）都变得不安全；3）意图清洗作为越狱技术，在黑盒访问下攻击成功率高达90-98%。

Conclusion: 当前AI安全评估与现实攻击者行为存在显著脱节。数据集过度依赖触发线索导致安全评估失真，模型在实际恶意意图面前仍然脆弱。需要更真实的安全评估方法。

Abstract: We systematically evaluate the quality of widely used AI safety datasets from two perspectives: in isolation and in practice. In isolation, we examine how well these datasets reflect real-world attacks based on three key properties: driven by ulterior intent, well-crafted, and out-of-distribution. We find that these datasets overrely on "triggering cues": words or phrases with overt negative/sensitive connotations that are intended to trigger safety mechanisms explicitly, which is unrealistic compared to real-world attacks. In practice, we evaluate whether these datasets genuinely measure safety risks or merely provoke refusals through triggering cues. To explore this, we introduce "intent laundering": a procedure that abstracts away triggering cues from attacks (data points) while strictly preserving their malicious intent and all relevant details. Our results indicate that current AI safety datasets fail to faithfully represent real-world attacks due to their overreliance on triggering cues. In fact, once these cues are removed, all previously evaluated "reasonably safe" models become unsafe, including Gemini 3 Pro and Claude Sonnet 3.7. Moreover, when intent laundering is adapted as a jailbreaking technique, it consistently achieves high attack success rates, ranging from 90% to over 98%, under fully black-box access. Overall, our findings expose a significant disconnect between how model safety is evaluated and how real-world adversaries behave.

</details>


### [4] [Can Adversarial Code Comments Fool AI Security Reviewers -- Large-Scale Empirical Study of Comment-Based Attacks and Defenses Against LLM Code Analysis](https://arxiv.org/abs/2602.16741)
*Scott Thornton*

Main category: cs.CR

TL;DR: 研究发现对抗性注释对LLM漏洞检测性能影响有限，与代码生成场景不同，检测准确率未显著下降


<details>
  <summary>Details</summary>
Motivation: 探索对抗性提示操作是否会影响大型语言模型在代码审查中的漏洞检测能力，此前研究表明在代码生成任务中对抗性提示会显著降低LLM性能

Method: 构建包含Python、JavaScript和Java的100样本基准测试集，每个样本配备8种注释变体（从无注释到权威欺骗、技术欺骗等对抗策略），在8个前沿模型（5个商业、3个开源）上进行9,366次试验评估

Result: 对抗性注释对检测准确率影响很小且统计不显著，商业模型基线检测率89-96%，开源模型53-72%，复杂对抗策略相比简单操纵注释无优势；最佳防御方法（静态分析交叉引用）达到96.9%检测率并恢复47%基线遗漏

Conclusion: 与代码生成不同，漏洞检测任务中对抗性注释不会显著降低LLM性能，失败案例主要集中在固有困难的漏洞类别（如竞态条件、时序侧信道、复杂授权逻辑），而非对抗性注释本身

Abstract: AI-assisted code review is widely used to detect vulnerabilities before production release. Prior work shows that adversarial prompt manipulation can degrade large language model (LLM) performance in code generation. We test whether similar comment-based manipulation misleads LLMs during vulnerability detection. We build a 100-sample benchmark across Python, JavaScript, and Java, each paired with eight comment variants ranging from no comments to adversarial strategies such as authority spoofing and technical deception. Eight frontier models, five commercial and three open-source, are evaluated in 9,366 trials. Adversarial comments produce small, statistically non-significant effects on detection accuracy (McNemar exact p > 0.21; all 95 percent confidence intervals include zero). This holds for commercial models with 89 to 96 percent baseline detection and open-source models with 53 to 72 percent, despite large absolute performance gaps. Unlike generation settings where comment manipulation achieves high attack success, detection performance does not meaningfully degrade. More complex adversarial strategies offer no advantage over simple manipulative comments. We test four automated defenses across 4,646 additional trials (14,012 total). Static analysis cross-referencing performs best at 96.9 percent detection and recovers 47 percent of baseline misses. Comment stripping reduces detection for weaker models by removing helpful context. Failures concentrate on inherently difficult vulnerability classes, including race conditions, timing side channels, and complex authorization logic, rather than on adversarial comments.

</details>


### [5] [The Vulnerability of LLM Rankers to Prompt Injection Attacks](https://arxiv.org/abs/2602.16752)
*Yu Yin,Shuai Wang,Bevan Koopman,Guido Zuccon*

Main category: cs.CR

TL;DR: 该论文对LLM排序器的越狱提示攻击进行了全面实证研究，评估了不同模型、架构和设置下的脆弱性边界。


<details>
  <summary>Details</summary>
Motivation: LLM作为强大的重排序器存在安全风险，简单的提示注入可以显著改变LLM的排序决策，但这一脆弱性在不同LLM家族、架构和设置中的持续程度尚未充分探索。

Method: 系统评估了三种主流排序范式（成对、列表、集合）和两种注入变体（决策目标劫持和决策标准劫持），扩展分析了模型家族脆弱性缩放、位置敏感性、骨干架构和跨域鲁棒性。

Result: 研究揭示了这些脆弱性的边界条件，关键发现包括编码器-解码器架构对越狱攻击表现出强大的固有韧性。

Conclusion: LLM排序器存在严重的越狱攻击脆弱性，但编码器-解码器架构具有更强的抗攻击能力，为构建更安全的LLM排序系统提供了重要见解。

Abstract: Large Language Models (LLMs) have emerged as powerful re-rankers. Recent research has however showed that simple prompt injections embedded within a candidate document (i.e., jailbreak prompt attacks) can significantly alter an LLM's ranking decisions. While this poses serious security risks to LLM-based ranking pipelines, the extent to which this vulnerability persists across diverse LLM families, architectures, and settings remains largely under-explored. In this paper, we present a comprehensive empirical study of jailbreak prompt attacks against LLM rankers. We focus our evaluation on two complementary tasks: (1) Preference Vulnerability Assessment, measuring intrinsic susceptibility via attack success rate (ASR); and (2) Ranking Vulnerability Assessment, quantifying the operational impact on the ranking's quality (nDCG@10). We systematically examine three prevalent ranking paradigms (pairwise, listwise, setwise) under two injection variants: decision objective hijacking and decision criteria hijacking. Beyond reproducing prior findings, we expand the analysis to cover vulnerability scaling across model families, position sensitivity, backbone architectures, and cross-domain robustness. Our results characterize the boundary conditions of these vulnerabilities, revealing critical insights such as that encoder-decoder architectures exhibit strong inherent resilience to jailbreak attacks. We publicly release our code and additional experimental results at https://github.com/ielab/LLM-Ranker-Attack.

</details>


### [6] [NESSiE: The Necessary Safety Benchmark -- Identifying Errors that should not Exist](https://arxiv.org/abs/2602.16756)
*Johannes Bertram,Jonas Geiping*

Main category: cs.CR

TL;DR: NESSiE是一个轻量级的安全基准测试，揭示即使简单任务下LLMs也存在不应存在的安全相关失败，当前最先进模型也无法达到100%通过率，表明部署存在关键风险。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在部署时存在安全风险，需要一种轻量级、易于使用的基准测试来检查模型的基本安全性，特别是揭示那些在简单任务中不应存在的安全相关失败。

Method: 提出NESSiE基准测试，包含信息和访问安全的最小测试用例；开发Safe & Helpful (SH)指标直接比较安全性和帮助性要求；分析禁用推理和良性干扰上下文对模型性能的影响。

Result: 即使最先进的LLMs也无法在NESSiE上达到100%通过率，未能满足语言模型安全的必要条件；模型偏向于帮助性而非安全性；禁用推理和良性干扰上下文会降低模型性能。

Conclusion: NESSiE作为必要的安全检查基准，揭示了LLMs在简单任务中仍存在不应存在的安全失败，当前模型部署为自主代理存在关键风险，需要通过此测试是部署的必要条件。

Abstract: We introduce NESSiE, the NEceSsary SafEty benchmark for large language models (LLMs). With minimal test cases of information and access security, NESSiE reveals safety-relevant failures that should not exist, given the low complexity of the tasks. NESSiE is intended as a lightweight, easy-to-use sanity check for language model safety and, as such, is not sufficient for guaranteeing safety in general -- but we argue that passing this test is necessary for any deployment. However, even state-of-the-art LLMs do not reach 100% on NESSiE and thus fail our necessary condition of language model safety, even in the absence of adversarial attacks. Our Safe & Helpful (SH) metric allows for direct comparison of the two requirements, showing models are biased toward being helpful rather than safe. We further find that disabled reasoning for some models, but especially a benign distraction context degrade model performance. Overall, our results underscore the critical risks of deploying such models as autonomous agents in the wild. We make the dataset, package and plotting code publicly available.

</details>


### [7] [Privacy-Aware Split Inference with Speculative Decoding for Large Language Models over Wide-Area Networks](https://arxiv.org/abs/2602.16760)
*Michael Cunningham*

Main category: cs.CR

TL;DR: 提出一个隐私感知的LLM推理系统，通过在可信本地GPU和不可信云GPU之间分割Transformer层，仅通过网络传输中间激活值，实现高延迟广域网下的高效解码。


<details>
  <summary>Details</summary>
Motivation: 解决在广域网高延迟环境下进行隐私保护的LLM推理问题，需要在保护原始令牌隐私的同时实现高效的自动回归解码。

Method: 采用非对称层分割（嵌入/解嵌入层保留在本地）+前瞻解码技术，通过n-gram推测分摊网络往返延迟，支持可调节的隐私-性能权衡。

Result: 在80ms WAN链路上，Mistral 7B达到8.7-9.3 tok/s，NeMo 12B达到7.8-8.7 tok/s；2层分割时攻击者可恢复59%令牌，8层分割时仅35%；n-gram推测平均每步接受1.2-1.3个令牌。

Conclusion: 该系统实现了隐私保护与性能的良好平衡，通过层分割深度提供可调节的隐私-性能权衡，前瞻解码在保持输出质量的同时显著提升吞吐量，可扩展到更大模型。

Abstract: We present a practical system for privacy-aware large language model (LLM) inference that splits a transformer between a trusted local GPU and an untrusted cloud GPU, communicating only intermediate activations over the network. Our system addresses the unique challenges of autoregressive LLM decoding over high-latency wide-area networks (WANs), contributing: (1) an asymmetric layer split where embedding and unembedding layers remain local, ensuring raw tokens never leave the trusted device; (2) the first application of lookahead decoding to split inference over WANs, amortizing network round-trip latency across multiple tokens per iteration; (3) an empirical inversion attack evaluation showing that split depth provides a tunable privacy-performance tradeoff -- an attacker can recover ~59%% of tokens at a 2-layer split but only ~35%% at an 8-layer split, with minimal throughput impact; (4) ablation experiments showing that n-gram speculation accepts 1.2-1.3 tokens per decoding step on average (peak of 7 observed on code), with acceptance rates consistent across model scales; (5) formal verification that lookahead decoding produces token-identical output to sequential decoding under greedy argmax, with zero quality degradation; and (6) scaling validation on Mistral NeMo 12B (40 layers), demonstrating that the system generalizes to larger models with only 4.9 GB local VRAM and matching 7B throughput. Evaluated on Mistral 7B and NeMo 12B over a ~80ms WAN link, our system achieves 8.7-9.3 tok/s (7B) and 7.8-8.7 tok/s (12B) with lookahead decoding, with an RTT decomposition model (validated at <6.2%% cross-validation error) projecting 15-19 tok/s at 20ms RTT.

</details>


### [8] [Large-scale online deanonymization with LLMs](https://arxiv.org/abs/2602.16800)
*Simon Lermen,Daniel Paleka,Joshua Swanson,Michael Aerni,Nicholas Carlini,Florian Tramèr*

Main category: cs.CR

TL;DR: LLM可实现大规模去匿名化攻击，仅凭匿名在线资料和对话就能高精度重新识别用户，性能远超传统方法


<details>
  <summary>Details</summary>
Motivation: 研究在线匿名性的实际保护效果是否仍然有效，探索LLM在去匿名化攻击中的能力，重新评估在线隐私威胁模型

Method: 设计可扩展攻击流程：1) 使用LLM提取身份相关特征；2) 通过语义嵌入搜索候选匹配；3) 对候选匹配进行推理验证。构建三个真实数据集评估攻击效果

Result: LLM方法在三个数据集上显著优于传统基线，在90%精度下达到68%召回率，而最佳非LLM方法接近0%。证明在线匿名用户的"实际模糊性"保护已失效

Conclusion: LLM能够实现大规模去匿名化攻击，在线匿名性的实际保护已不再有效，需要重新考虑在线隐私的威胁模型

Abstract: We show that large language models can be used to perform at-scale deanonymization. With full Internet access, our agent can re-identify Hacker News users and Anthropic Interviewer participants at high precision, given pseudonymous online profiles and conversations alone, matching what would take hours for a dedicated human investigator. We then design attacks for the closed-world setting. Given two databases of pseudonymous individuals, each containing unstructured text written by or about that individual, we implement a scalable attack pipeline that uses LLMs to: (1) extract identity-relevant features, (2) search for candidate matches via semantic embeddings, and (3) reason over top candidates to verify matches and reduce false positives. Compared to prior deanonymization work (e.g., on the Netflix prize) that required structured data or manual feature engineering, our approach works directly on raw user content across arbitrary platforms. We construct three datasets with known ground-truth data to evaluate our attacks. The first links Hacker News to LinkedIn profiles, using cross-platform references that appear in the profiles. Our second dataset matches users across Reddit movie discussion communities; and the third splits a single user's Reddit history in time to create two pseudonymous profiles to be matched. In each setting, LLM-based methods substantially outperform classical baselines, achieving up to 68% recall at 90% precision compared to near 0% for the best non-LLM method. Our results show that the practical obscurity protecting pseudonymous users online no longer holds and that threat models for online privacy need to be reconsidered.

</details>


### [9] [NeST: Neuron Selective Tuning for LLM Safety](https://arxiv.org/abs/2602.16835)
*Sasha Behrouzi,Lichao Wu,Mohamadreza Rostami,Ahmad-Reza Sadeghi*

Main category: cs.CR

TL;DR: NeST是一个轻量级、结构感知的安全对齐框架，通过选择性调整安全相关神经元来增强拒绝行为，相比全微调减少17,310倍参数更新，相比LoRA减少9.25倍，同时将攻击成功率从44.5%降至4.36%。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法存在成本高、更新困难、效率低等问题：全微调计算和存储开销大；LoRA等方法效率与安全增益不一致；断路器机制不直接修改模型内部表示。这些限制阻碍了快速可靠的安全更新，特别是在模型频繁演进或需要适应新策略和领域的场景中。

Method: NeST通过选择性适应一小部分安全相关神经元来增强拒绝行为，同时冻结模型其余部分。该方法将参数更新与安全行为的内部组织对齐，通过聚类功能一致的安全神经元并在每个簇内强制执行共享更新，实现有针对性和稳定的安全适应，无需广泛的模型修改或推理时开销。

Result: 在10个不同模型家族和规模的开放权重LLM上评估，NeST将攻击成功率从平均44.5%降低到4.36%，相当于不安全生成减少了90.2%，同时平均仅需0.44百万可训练参数。相比全微调参数更新减少17,310倍，相比LoRA减少9.25倍，同时始终实现更强的安全性能。

Conclusion: NeST提供了一个轻量级、结构感知的安全对齐框架，能够实现快速可靠的安全更新，同时保持高效性和稳定性，解决了现有方法在成本、效率和可靠性方面的限制。

Abstract: Safety alignment is essential for the responsible deployment of large language models (LLMs). Yet, existing approaches often rely on heavyweight fine-tuning that is costly to update, audit, and maintain across model families. Full fine-tuning incurs substantial computational and storage overhead, while parameter-efficient methods such as LoRA trade efficiency for inconsistent safety gains and sensitivity to design choices. Safety intervention mechanisms such as circuit breakers reduce unsafe outputs without modifying model weights, but do not directly shape or preserve the internal representations that govern safety behavior. These limitations hinder rapid and reliable safety updates, particularly in settings where models evolve frequently or must adapt to new policies and domains.
  We present NeST, a lightweight, structure-aware safety alignment framework that strengthens refusal behavior by selectively adapting a small subset of safety-relevant neurons while freezing the remainder of the model. NeST aligns parameter updates with the internal organization of safety behavior by clustering functionally coherent safety neurons and enforcing shared updates within each cluster, enabling targeted and stable safety adaptation without broad model modification or inference-time overhead. We benchmark NeST against three dominant baselines: full fine-tuning, LoRA-based fine-tuning, and circuit breakers across 10 open-weight LLMs spanning multiple model families and sizes. Across all evaluated models, NeST reduces the attack success rate from an average of 44.5% to 4.36%, corresponding to a 90.2% reduction in unsafe generations, while requiring only 0.44 million trainable parameters on average. This amounts to a 17,310x decrease in updated parameters compared to full fine-tuning and a 9.25x reduction relative to LoRA, while consistently achieving stronger safety performance for alignment.

</details>


### [10] [Privacy-Preserving Mechanisms Enable Cheap Verifiable Inference of LLMs](https://arxiv.org/abs/2602.17223)
*Arka Pal,Louai Zahran,William Gvozdjak,Akilesh Potti,Micah Goldblum*

Main category: cs.CR

TL;DR: 本文提出利用隐私保护LLM推理来实现低成本验证推理的新方法，避免传统零知识证明的高开销


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模增大，用户依赖第三方托管服务，但缺乏对推理计算过程的保证。不诚实的提供商可能用廉价小模型替代昂贵大模型，而现有验证方法（如零知识证明）计算开销大，不适用于大模型。

Method: 提出两种新协议，利用隐私保护LLM推理来提供推理过程保证。方法成本低，只需少量额外token计算，对下游影响小。基于隐私保护推理方法通常比零知识证明更快的特点，提升验证运行时间。

Result: 提出的协议能以边际额外成本提供推理验证，比传统零知识证明方法更高效，为LLM推理中的隐私与可验证性关系提供新见解。

Conclusion: 通过将隐私保护推理转化为验证机制，实现了低成本、高效的LLM推理验证，揭示了隐私与可验证性在LLM推理中的新联系。

Abstract: As large language models (LLMs) continue to grow in size, fewer users are able to host and run models locally. This has led to increased use of third-party hosting services. However, in this setting, there is a lack of guarantees on the computation performed by the inference provider. For example, a dishonest provider may replace an expensive large model with a cheaper-to-run weaker model and return the results from the weaker model to the user. Existing tools to verify inference typically rely on methods from cryptography such as zero-knowledge proofs (ZKPs), but these add significant computational overhead, and remain infeasible for use for large models. In this work, we develop a new insight -- that given a method for performing private LLM inference, one can obtain forms of verified inference at marginal extra cost. Specifically, we propose two new protocols which leverage privacy-preserving LLM inference in order to provide guarantees over the inference that was carried out. Our approaches are cheap, requiring the addition of a few extra tokens of computation, and have little to no downstream impact. As the fastest privacy-preserving inference methods are typically faster than ZK methods, the proposed protocols also improve verification runtime. Our work provides novel insights into the connections between privacy and verifiability in LLM inference.

</details>


### [11] [Grothendieck Topologies and Sheaf-Theoretic Foundations of Cryptographic Security: Attacker Models and $Σ$-Protocols as the First Step](https://arxiv.org/abs/2602.17301)
*Takao Inoué*

Main category: cs.CR

TL;DR: 使用格罗滕迪克拓扑和层论重新表述密码学安全性，将攻击者观测建模为格罗滕迪克位点，协议转录本形成层，安全性条件对应几何性质。


<details>
  <summary>Details</summary>
Motivation: 传统密码学安全性使用基于游戏或模拟的定义，本文旨在提供一种基于代数几何的结构化重新表述，为密码学提供几何基础。

Method: 将攻击者观测建模为格罗滕迪克位点，覆盖族表示高效模拟确定的部分信息的可容许分解。在此框架下，协议转录本自然形成层，安全性性质作为几何条件出现。以Σ-协议为重点，展示其转录本结构在关联的层拓扑中定义了一个挠子。

Result: 证明了Σ-协议转录本结构在层拓扑中形成挠子，局部平凡性对应零知识性质，全局截面的缺失反映可靠性。以Schnorr Σ-协议为例进行了具体分析。

Conclusion: 层论视角为基于模拟的安全性提供了概念性解释，并为进一步的密码学抽象提供了几何基础。

Abstract: Cryptographic security is traditionally formulated using game-based or simulation-based definitions. In this paper, we propose a structural reformulation of cryptographic security based on Grothendieck topologies and sheaf theory.
  Our key idea is to model attacker observations as a Grothendieck site, where covering families represent admissible decompositions of partial information determined by efficient simulation. Within this framework, protocol transcripts naturally form sheaves, and security properties arise as geometric conditions.
  As a first step, we focus on $Σ$-protocols. We show that the transcript structure of any $Σ$-protocol defines a torsor in the associated topos of sheaves. Local triviality of this torsor corresponds to zero-knowledge, while the absence of global sections reflects soundness. A concrete analysis of the Schnorr $Σ$-protocol is provided to illustrate the construction.
  This sheaf-theoretic perspective offers a conceptual explanation of simulation-based security and suggests a geometric foundation for further cryptographic abstractions.

</details>


### [12] [Security of the Fischlin Transform in Quantum Random Oracle Model](https://arxiv.org/abs/2602.17307)
*Christian Majenz,Jaya Sharma*

Main category: cs.CR

TL;DR: Fischlin变换在量子随机预言机模型(QROM)中保持直线可提取性，为后量子安全提供了比Pass变换更小证明尺寸的NIZK方案。


<details>
  <summary>Details</summary>
Motivation: Fischlin变换在经典随机预言机模型中能产生直线可提取的非交互零知识证明，但其在量子对手下的可提取性一直未解决，因为量子随机预言机模型中的查询概率难以分析。

Method: 使用压缩预言机方法构建提取器，结合独立随机变量和的尾界、鞅尾界、对称化、查询振幅和量子并集界等技术进行证明。

Result: 证明了Fischlin变换在量子随机预言机模型中保持直线可提取性，建立了该变换的后量子安全性，提供了比Pass变换更小证明尺寸的后量子NIZK方案。

Conclusion: Fischlin变换在量子随机预言机模型中仍然保持直线可提取性，为后量子安全的非交互零知识证明提供了一个有效的替代方案。

Abstract: The Fischlin transform yields non-interactive zero-knowledge proofs with straight-line extractability in the classical random oracle model. This is done by forcing a prover to generate multiple accepting transcripts through a proof-of-work mechanism. Whether the Fischlin transform is straight-line extractable against quantum adversaries has remained open due to the difficulty of reasoning about the likelihood of query transcripts in the quantum-accessible random oracle model (QROM), even when using the compressed oracle methodology. In this work, we prove that the Fischlin transform remains straight-line extractable in the QROM, via an extractor based on the compressed oracle. This establishes the post-quantum security of the Fischlin transform, providing a post-quantum straight-line extractable NIZK alternative to Pass' transform with smaller proof size. Our techniques include tail bounds for sums of independent random variables and for martingales as well as symmetrization, query amplitude and quantum union bound arguments.

</details>


### [13] [What Breaks Embodied AI Security:LLM Vulnerabilities, CPS Flaws,or Something Else?](https://arxiv.org/abs/2602.17345)
*Boyang Ma,Hechuan Guo,Peizhuo Lv,Minghui Xu,Xuelong Dai,YeChao Zhang,Yijun Yang,Yue Zhang*

Main category: cs.CR

TL;DR: 该论文认为现有研究对具身AI安全性的分析视角不足，提出具身AI安全的核心挑战源于系统级不匹配而非孤立模型缺陷，并识别了四个根本性难点。


<details>
  <summary>Details</summary>
Motivation: 具身AI系统正从受控环境转向安全关键的实际部署，其失败会导致不可逆的物理后果。现有研究主要从LLM漏洞或传统CPS故障角度分析，但这些视角单独不足以解释现代具身系统的许多故障。

Method: 通过分析具身AI系统的特性，提出系统级视角，识别出四个核心见解来解释为什么具身AI本质上更难保障安全。

Result: 识别了四个关键见解：(1)语义正确性不意味着物理安全性；(2)相同动作在不同物理状态下可能导致截然不同的结果；(3)小误差在紧密耦合的感知-决策-行动循环中传播放大；(4)安全性在时间或系统层级上不具备组合性。

Conclusion: 保障具身AI安全需要超越组件级防御，转向系统级推理，考虑物理风险、不确定性和故障传播，采用整体性安全方法。

Abstract: Embodied AI systems (e.g., autonomous vehicles, service robots, and LLM-driven interactive agents) are rapidly transitioning from controlled environments to safety critical real-world deployments. Unlike disembodied AI, failures in embodied intelligence lead to irreversible physical consequences, raising fundamental questions about security, safety, and reliability. While existing research predominantly analyzes embodied AI through the lenses of Large Language Model (LLM) vulnerabilities or classical Cyber-Physical System (CPS) failures, this survey argues that these perspectives are individually insufficient to explain many observed breakdowns in modern embodied systems. We posit that a significant class of failures arises from embodiment-induced system-level mismatches, rather than from isolated model flaws or traditional CPS attacks. Specifically, we identify four core insights that explain why embodied AI is fundamentally harder to secure: (i) semantic correctness does not imply physical safety, as language-level reasoning abstracts away geometry, dynamics, and contact constraints; (ii) identical actions can lead to drastically different outcomes across physical states due to nonlinear dynamics and state uncertainty; (iii) small errors propagate and amplify across tightly coupled perception-decision-action loops; and (iv) safety is not compositional across time or system layers, enabling locally safe decisions to accumulate into globally unsafe behavior. These insights suggest that securing embodied AI requires moving beyond component-level defenses toward system-level reasoning about physical risk, uncertainty, and failure propagation.

</details>


### [14] [DAVE: A Policy-Enforcing LLM Spokesperson for Secure Multi-Document Data Sharing](https://arxiv.org/abs/2602.17413)
*René Brinkhege,Prahlad Menon*

Main category: cs.CR

TL;DR: DAVE是一个基于LLM的发言人系统，在多方数据空间中通过自然语言接口执行使用策略，实现查询时的虚拟编辑，避免敏感信息泄露。


<details>
  <summary>Details</summary>
Motivation: 当前跨组织数据空间的使用策略主要在资产级别执行：整个文档要么共享要么保留。当文档只有部分内容敏感时，提供商需要手动编辑文档，这种方法成本高、粒度粗，且难以随策略或合作伙伴变化而维护。

Method: 提出DAVE系统：一个基于LLM的使用策略执行发言人，通过自然语言接口回答私有文档的问题。采用虚拟编辑技术，在查询时抑制敏感信息而不修改源文档。架构集成Eclipse Dataspace Components和ODRL风格策略，提供提供商端集成原型。

Result: 本文主要贡献是架构设计：尚未实现或实证评估完整的执行流程。提出了评估方法论，用于评估安全、效用和性能权衡，作为未来实证研究的基础。

Conclusion: DAVE架构为多方数据空间中系统化管理的LLM访问提供了新方法，通过查询时虚拟编辑实现细粒度使用策略执行，避免手动文档编辑的局限性。

Abstract: In current inter-organizational data spaces, usage policies are enforced mainly at the asset level: a whole document or dataset is either shared or withheld. When only parts of a document are sensitive, providers who want to avoid leaking protected information typically must manually redact documents before sharing them, which is costly, coarse-grained, and hard to maintain as policies or partners change. We present DAVE, a usage policy-enforcing LLM spokesperson that answers questions over private documents on behalf of a data provider. Instead of releasing documents, the provider exposes a natural language interface whose responses are constrained by machine-readable usage policies. We formalize policy-violating information disclosure in this setting, drawing on usage control and information flow security, and introduce virtual redaction: suppressing sensitive information at query time without modifying source documents. We describe an architecture for integrating such a spokesperson with Eclipse Dataspace Components and ODRL-style policies, and outline an initial provider-side integration prototype in which QA requests are routed through a spokesperson service instead of triggering raw document transfer. Our contribution is primarily architectural: we do not yet implement or empirically evaluate the full enforcement pipeline. We therefore outline an evaluation methodology to assess security, utility, and performance trade-offs under benign and adversarial querying as a basis for future empirical work on systematically governed LLM access to multi-party data spaces.

</details>


### [15] [Jolt Atlas: Verifiable Inference via Lookup Arguments in Zero Knowledge](https://arxiv.org/abs/2602.17452)
*Wyatt Benno,Alberto Centelles,Antoine Douchet,Khalil Gibran*

Main category: cs.CR

TL;DR: Jolt Atlas是一个基于Jolt证明系统的零知识机器学习框架，直接针对ONNX张量操作进行优化，支持内存受限环境下的模型推理证明，适用于隐私保护和对抗性环境。


<details>
  <summary>Details</summary>
Motivation: 现有zkML框架通常基于zkVM模拟CPU指令执行，效率较低。Jolt Atlas旨在通过直接针对ONNX张量操作进行优化，提供更实用的证明时间，并支持在内存受限设备上运行。

Method: 1. 扩展Jolt证明系统，采用查找表为中心的方法直接应用于ONNX张量操作；2. 利用sumcheck协议处理非线性函数；3. 应用神经传送等技术减少查找表大小；4. 使用BlindFold技术实现零知识特性；5. 支持流式证明，适应内存受限环境。

Result: Jolt Atlas在分类、嵌入、自动推理和小型语言模型等任务上展示了实用的证明时间，能够在内存受限设备上运行，生成简洁可验证的证明，适用于隐私保护和对抗性环境。

Conclusion: Jolt Atlas提供了一个高效、实用的zkML框架，通过直接针对ONNX操作进行优化，克服了传统zkVM方法的局限性，为隐私保护和可信AI应用提供了可行的解决方案。

Abstract: We present Jolt Atlas, a zero-knowledge machine learning (zkML) framework that extends the Jolt proving system to model inference. Unlike zkVMs (zero-knowledge virtual machines), which emulate CPU instruction execution, Jolt Atlas adapts Jolt's lookup-centric approach and applies it directly to ONNX tensor operations. The ONNX computational model eliminates the need for CPU registers and simplifies memory consistency verification. In addition, ONNX is an open-source, portable format, which makes it easy to share and deploy models across different frameworks, hardware platforms, and runtime environments without requiring framework-specific conversions.
  Our lookup arguments, which use sumcheck protocol, are well-suited for non-linear functions -- key building blocks in modern ML. We apply optimisations such as neural teleportation to reduce the size of lookup tables while preserving model accuracy, as well as several tensor-level verification optimisations detailed in this paper. We demonstrate that Jolt Atlas can prove model inference in memory-constrained environments -- a prover property commonly referred to as \textit{streaming}. Furthermore, we discuss how Jolt Atlas achieves zero-knowledge through the BlindFold technique, as introduced in Vega. In contrast to existing zkML frameworks, we show practical proving times for classification, embedding, automated reasoning, and small language models.
  Jolt Atlas enables cryptographic verification that can be run on-device, without specialised hardware. The resulting proofs are succinctly verifiable. This makes Jolt Atlas well-suited for privacy-centric and adversarial environments. In a companion work, we outline various use cases of Jolt Atlas, including how it serves as guardrails in agentic commerce and for trustless AI context (often referred to as \textit{AI memory}).

</details>


### [16] [Privacy in Theory, Bugs in Practice: Grey-Box Auditing of Differential Privacy Libraries](https://arxiv.org/abs/2602.17454)
*Tudor Cebere,David Erb,Damien Desfontaines,Aurélien Bellet,Jack Fitzsimons*

Main category: cs.CR

TL;DR: Re:cord-play 是一种灰盒审计范式，通过检查DP算法的内部状态来检测隐私违规，相比现有方法更实用有效


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私验证方法存在局限：形式化工具限制太多，黑盒统计审计对复杂流程不可行且无法定位bug根源。DP实现容易出错，细微bug经常破坏理论保证

Method: 提出Re:cord-play灰盒审计范式，通过检测DP算法的内部状态，在相邻数据集上运行具有相同随机性的检测算法，检查数据依赖的控制流，通过比较声明的敏感度与内部输入的实际距离来验证敏感度违规

Result: 审计了12个开源库（包括SmartNoise SDK、Opacus、Diffprivlib），发现了13个影响理论保证的隐私违规。发布了开源Python包，使DP开发者能够轻松集成有效、计算成本低且无缝的隐私测试

Conclusion: Re:cord-play提供了一种实用有效的DP算法验证方法，填补了形式化工具和黑盒审计之间的空白，能够检测实际实现中的隐私违规

Abstract: Differential privacy (DP) implementations are notoriously prone to errors, with subtle bugs frequently invalidating theoretical guarantees. Existing verification methods are often impractical: formal tools are too restrictive, while black-box statistical auditing is intractable for complex pipelines and fails to pinpoint the source of the bug. This paper introduces Re:cord-play, a gray-box auditing paradigm that inspects the internal state of DP algorithms. By running an instrumented algorithm on neighboring datasets with identical randomness, Re:cord-play directly checks for data-dependent control flow and provides concrete falsification of sensitivity violations by comparing declared sensitivity against the empirically measured distance between internal inputs. We generalize this to Re:cord-play-sample, a full statistical audit that isolates and tests each component, including untrusted ones. We show that our novel testing approach is both effective and necessary by auditing 12 open-source libraries, including SmartNoise SDK, Opacus, and Diffprivlib, and uncovering 13 privacy violations that impact their theoretical guarantees. We release our framework as an open-source Python package, thereby making it easy for DP developers to integrate effective, computationally inexpensive, and seamless privacy testing as part of their software development lifecycle.

</details>


### [17] [The CTI Echo Chamber: Fragmentation, Overlap, and Vendor Specificity in Twenty Years of Cyber Threat Reporting](https://arxiv.org/abs/2602.17458)
*Manuel Suarez-Roman,Francesco Marciori,Mauro Conti,Juan Tapiador*

Main category: cs.CR

TL;DR: 该研究开发了一个基于LLM的高精度管道，自动分析20年来的开源网络威胁情报报告，揭示了威胁情报生态系统的结构偏见和供应商报告偏差。


<details>
  <summary>Details</summary>
Motivation: 尽管开源网络威胁情报数量庞大，但由于缺乏结构化数据集和报告标准不一致，我们对长期威胁行为者-受害者动态的理解仍然零散。

Method: 开发基于LLM的高精度管道，处理13,308份开源CTI报告，提取关键实体（威胁行为者、动机、受害者、报告供应商、技术指标等），并进行大规模自动化分析。

Result: 量化了CTI信息密度和专业化的演变，识别了威胁行为者与动机、受害者特征的模式，发现CTI行业存在地理和行业报告偏见，供应商间情报重叠率低。

Conclusion: 研究揭示了CTI生态系统的结构偏见，使从业者和研究人员能够更好地评估其情报来源的完整性，理解供应商报告偏差对威胁情报覆盖的影响。

Abstract: Despite the high volume of open-source Cyber Threat Intelligence (CTI), our understanding of long-term threat actor-victim dynamics remains fragmented due to the lack of structured datasets and inconsistent reporting standards. In this paper, we present a large-scale automated analysis of open-source CTI reports spanning two decades. We develop a high-precision, LLM-based pipeline to ingest and structure 13,308 reports, extracting key entities such as attributed threat actors, motivations, victims, reporting vendors, and technical indicators (IoCs and TTPs). Our analysis quantifies the evolution of CTI information density and specialization, characterizing patterns that relate specific threat actors to motivations and victim profiles. Furthermore, we perform a meta-analysis of the CTI industry itself. We identify a fragmented ecosystem of distinct silos where vendors demonstrate significant geographic and sectoral reporting biases. Our marginal coverage analysis reveals that intelligence overlap between vendors is typically low: while a few core providers may offer broad situational awareness, additional sources yield diminishing returns. Overall, our findings characterize the structural biases inherent in the CTI ecosystem, enabling practitioners and researchers to better evaluate the completeness of their intelligence sources.

</details>


### [18] [Coin selection by Random Draw according to the Boltzmann distribution](https://arxiv.org/abs/2602.17490)
*Jan Lennart Bönsel,Michael Maurer,Silvio Petriconi,Andrea Tundis,Marc Winstel*

Main category: cs.CR

TL;DR: 本文提出Boltzmann Draw算法，一种基于统计物理原理的概率性硬币选择算法，用于优化加密货币和CBDC等代币支付系统中的交易资金选择问题。


<details>
  <summary>Details</summary>
Motivation: 在代币支付系统（如加密货币、央行数字货币）中，硬币选择问题涉及如何选择一组代币来为交易提供资金。现有方法存在限制，需要一种能同时优化输入代币数量、减少粉尘生成、限制钱包代币池大小，并兼顾效率、性能和隐私的解决方案。

Method: 提出Boltzmann Draw算法，这是一种基于统计物理原理的概率性算法。它通过玻尔兹曼分布来抽取代币，是对随机抽取方法的扩展和改进。算法可以高效实现，并满足隐私要求。

Result: 数值结果表明，该方法能有效限制所选输入代币数量，减少粉尘生成，并限制钱包中的代币池大小。与标准随机抽取和贪婪算法相比，Boltzmann Draw在以上目标方面表现更优。

Conclusion: Boltzmann Draw算法为代币支付系统提供了一种高效的硬币选择解决方案，特别适用于需要处理高频大交易量的央行数字货币。该算法在性能、隐私和效率方面具有优势，对当前代币技术具有重要意义。

Abstract: Coin selection refers to the problem of choosing a set of tokens to fund a transaction in token-based payment systems such as, e.g., cryptocurrencies or central bank digital currencies (CBDCs). In this paper, we propose the Boltzmann Draw that is a probabilistic algorithm inspired by the principles of statistical physics. The algorithm relies on drawing tokens according to the Boltzmann distribution, serving as an extension and improvement of the Random Draw method. Numerical results demonstrate the effectiveness of our method in bounding the number of selected input tokens as well as reducing dust generation and limiting the token pool size in the wallet. Moreover, the probabilistic algorithm can be implemented efficiently, improves performance and respects privacy requirements - properties of significant relevance for current token-based technologies. We compare the Boltzmann draw to both the standard Random Draw and the Greedy algorithm. We argue that the former is superior to the latter in the sense of the above objectives. Our findings are relevant for token-based technologies, and are also of interest for CBDCs, which as a legal tender possibly needs to handle large transaction volumes at a high frequency.

</details>


### [19] [BMC4TimeSec: Verification Of Timed Security Protocols](https://arxiv.org/abs/2602.17590)
*Agnieszka M. Zbrzezny*

Main category: cs.CR

TL;DR: BMC4TimeSec是一个基于SMT的有界模型检查工具，用于验证定时安全协议，采用多智能体建模方法


<details>
  <summary>Details</summary>
Motivation: 定时安全协议（TSP）的验证需要处理时间约束和多智能体交互，现有工具可能无法有效处理这些复杂特性

Method: 基于SMT的有界模型检查，采用定时解释系统（TIS）和定时交错解释系统（TIIS）进行多智能体建模，协议执行实现环境，知识自动机实现参与者

Result: 开发了端到端的BMC4TimeSec工具，代码已在GitHub开源，并提供了视频演示

Conclusion: BMC4TimeSec为定时安全协议验证提供了有效的基于SMT的有界模型检查解决方案，支持多智能体建模和时间约束处理

Abstract: We present BMC4TimeSec, an end-to-end tool for verifying Timed Security Protocols (TSP) based on SMT-based bounded model checking and multi-agent modelling in the form of Timed Interpreted Systems (TIS) and Timed Interleaved Interpreted Systems (TIIS). In BMC4TimeSec, TSP executions implement the TIS/TIIS environment (join actions, interleaving, delays, lifetimes), and knowledge automata implement the agents (evolution of participant knowledge, including the intruder). The code is publicly available on \href{https://github.com/agazbrzezny/BMC4TimeSec}{GitHub}, as is a \href{https://youtu.be/aNybKz6HwdA}{video} demonstration.

</details>


### [20] [What Makes a Good LLM Agent for Real-world Penetration Testing?](https://arxiv.org/abs/2602.17622)
*Gelei Deng,Yi Liu,Yuekang Li,Ruozhao Yang,Xiaofei Xie,Jie Zhang,Han Qiu,Tianwei Zhang*

Main category: cs.CR

TL;DR: 论文分析了28个基于LLM的渗透测试系统，发现两种失败模式：Type A（工具/提示不足）和Type B（规划/状态管理缺陷）。作者提出Excalibur系统，通过工具技能层解决Type A失败，通过任务难度评估机制解决Type B失败，在多个基准测试中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的渗透测试代理性能差异很大，需要系统分析失败原因并开发更可靠的解决方案。特别是要解决那些无法通过简单工程改进就能解决的深层规划问题。

Method: 1) 分析28个LLM渗透测试系统并评估5个代表性实现；2) 识别Type A（能力缺陷）和Type B（规划限制）失败模式；3) 提出Excalibur系统：包含工具技能层（解决Type A）和任务难度评估机制（解决Type B）；4) 使用证据引导的攻击树搜索框架进行难度感知规划。

Result: Excalibur在CTF基准测试中达到91%的任务完成率（相比基线相对提升39-49%），在GOAD Active Directory环境中攻陷4/5个主机（先前系统仅2个）。难度感知规划在不同模型中都带来一致的端到端性能提升。

Conclusion: 基于LLM的渗透测试代理存在两种失败模式，其中Type B失败源于缺乏实时任务难度评估。Excalibur通过难度感知规划有效解决了这一问题，这种改进是模型缩放无法单独实现的，为构建更可靠的自动化渗透测试系统提供了新方向。

Abstract: LLM-based agents show promise for automating penetration testing, yet reported performance varies widely across systems and benchmarks. We analyze 28 LLM-based penetration testing systems and evaluate five representative implementations across three benchmarks of increasing complexity. Our analysis reveals two distinct failure modes: Type A failures stem from capability gaps (missing tools, inadequate prompts) that engineering readily addresses, while Type B failures persist regardless of tooling due to planning and state management limitations. We show that Type B failures share a root cause that is largely invariant to the underlying LLM: agents lack real-time task difficulty estimation. As a result, agents misallocate effort, over-commit to low-value branches, and exhaust context before completing attack chains.
  Based on this insight, we present Excalibur, a penetration testing agent that couples strong tooling with difficulty-aware planning. A Tool and Skill Layer eliminates Type A failures through typed interfaces and retrieval-augmented knowledge. A Task Difficulty Assessment (TDA) mechanism addresses Type B failures by estimating tractability through four measurable dimensions (horizon estimation, evidence confidence, context load, and historical success) and uses these estimates to guide exploration-exploitation decisions within an Evidence-Guided Attack Tree Search (EGATS) framework. Excalibur achieves up to 91% task completion on CTF benchmarks with frontier models (39 to 49% relative improvement over baselines) and compromises 4 of 5 hosts on the GOAD Active Directory environment versus 2 by prior systems. These results show that difficulty-aware planning yields consistent end-to-end gains across models and addresses a limitation that model scaling alone does not eliminate.

</details>


### [21] [Non-Trivial Zero-Knowledge Implies One-Way Functions](https://arxiv.org/abs/2602.17651)
*Suvradip Chakraborty,James Hulett,Dakshita Khurana,Kabir Tomer*

Main category: cs.CR

TL;DR: 该论文证明：在NP⊈ioP/poly的假设下，非平凡（误差总和远离1）的非交互式零知识论证（NIZK）和常数轮公开掷币零知识论证都能蕴含单向函数的存在。


<details>
  <summary>Details</summary>
Motivation: 先前研究[Hirahara and Nanashima, STOC'2024]已证明：在NP⊈ioP/poly下，具有可忽略误差的零知识论证蕴含单向函数。但高误差区域（即零知识误差、完备性误差和可靠性误差总和接近1的情况）是否仍能蕴含单向函数尚不清楚。本文旨在填补这一空白，探索高误差区域下零知识论证与单向函数的关系。

Method: 作者定义了"非平凡"零知识论证：其完备性误差、可靠性误差和零知识误差的总和远离1。在NP⊈ioP/poly的假设下，通过理论分析和构造证明：1）非平凡NIZK论证蕴含单向函数；2）非平凡常数轮公开掷币零知识论证也蕴含单向函数。利用已知的放大技术，可将弱NIZK无条件转化为标准NIZK。

Result: 主要结果：1）在NP⊈ioP/poly下，非平凡NIZK论证蕴含单向函数；2）非平凡常数轮公开掷币零知识论证同样蕴含单向函数，进而可得到标准的四轮零知识论证。这些结果完全解决了先前工作中ε_zk+√ε_s≥1区域的开放问题。

Conclusion: 本文完全刻画了高误差区域下零知识论证与单向函数的关系，填补了先前研究的空白。结果表明，即使是误差总和接近1的"非平凡"零知识论证，在合理的复杂性假设下仍能蕴含单向函数。这些结果为从最坏情况硬度构造单向函数提供了有用的技术基础。

Abstract: A recent breakthrough [Hirahara and Nanashima, STOC'2024] established that if $\mathsf{NP} \not \subseteq \mathsf{ioP/poly}$, the existence of zero-knowledge with negligible errors for $\mathsf{NP}$ implies the existence of one-way functions (OWFs). In this work, we obtain a characterization of one-way functions from the worst-case complexity of zero-knowledge {\em in the high-error regime}.
  We say that a zero-knowledge argument is {\em non-trivial} if the sum of its completeness, soundness and zero-knowledge errors is bounded away from $1$. Our results are as follows, assuming $\mathsf{NP} \not \subseteq \mathsf{ioP/poly}$:
  1. {\em Non-trivial} Non-Interactive ZK (NIZK) arguments for $\mathsf{NP}$ imply the existence of OWFs. Using known amplification techniques, this result also provides an unconditional transformation from weak to standard NIZK proofs for all meaningful error parameters.
  2. We also generalize to the interactive setting: {\em Non-trivial} constant-round public-coin zero-knowledge arguments for $\mathsf{NP}$ imply the existence of OWFs, and therefore also (standard) four-message zero-knowledge arguments for $\mathsf{NP}$.
  Prior to this work, one-way functions could be obtained from NIZKs that had constant zero-knowledge error $ε_{zk}$ and soundness error $ε_{s}$ satisfying $ε_{zk} + \sqrt{ε_{s}} < 1$ [Chakraborty, Hulett and Khurana, CRYPTO'2025]. However, the regime where $ε_{zk} + \sqrt{ε_{s}} \geq 1$ remained open. This work closes the gap, and obtains new implications in the interactive setting. Our results and techniques could be useful stepping stones in the quest to construct one-way functions from worst-case hardness.

</details>
