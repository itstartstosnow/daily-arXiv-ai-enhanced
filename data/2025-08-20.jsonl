{"id": "2508.13214", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13214", "abs": "https://arxiv.org/abs/2508.13214", "authors": ["Xuyang Guo", "Zekai Huang", "Zhao Song", "Jiahao Zhang"], "title": "Too Easily Fooled? Prompt Injection Breaks LLMs on Frustratingly Simple Multiple-Choice Questions", "comment": null, "summary": "Large Language Models (LLMs) have recently demonstrated strong emergent\nabilities in complex reasoning and zero-shot generalization, showing\nunprecedented potential for LLM-as-a-judge applications in education, peer\nreview, and data quality evaluation. However, their robustness under prompt\ninjection attacks, where malicious instructions are embedded into the content\nto manipulate outputs, remains a significant concern. In this work, we explore\na frustratingly simple yet effective attack setting to test whether LLMs can be\neasily misled. Specifically, we evaluate LLMs on basic arithmetic questions\n(e.g., \"What is 3 + 2?\") presented as either multiple-choice or true-false\njudgment problems within PDF files, where hidden prompts are injected into the\nfile. Our results reveal that LLMs are indeed vulnerable to such hidden prompt\ninjection attacks, even in these trivial scenarios, highlighting serious\nrobustness risks for LLM-as-a-judge applications."}
{"id": "2508.13220", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13220", "abs": "https://arxiv.org/abs/2508.13220", "authors": ["Yixuan Yang", "Daoyuan Wu", "Yufan Chen"], "title": "MCPSecBench: A Systematic Security Benchmark and Playground for Testing Model Context Protocols", "comment": "This is a technical report from Lingnan University, Hong Kong", "summary": "Large Language Models (LLMs) are increasingly integrated into real-world\napplications via the Model Context Protocol (MCP), a universal, open standard\nfor connecting AI agents with data sources and external tools. While MCP\nenhances the capabilities of LLM-based agents, it also introduces new security\nrisks and expands their attack surfaces. In this paper, we present the first\nsystematic taxonomy of MCP security, identifying 17 attack types across 4\nprimary attack surfaces. We introduce MCPSecBench, a comprehensive security\nbenchmark and playground that integrates prompt datasets, MCP servers, MCP\nclients, and attack scripts to evaluate these attacks across three major MCP\nproviders. Our benchmark is modular and extensible, allowing researchers to\nincorporate custom implementations of clients, servers, and transport protocols\nfor systematic security assessment. Experimental results show that over 85% of\nthe identified attacks successfully compromise at least one platform, with core\nvulnerabilities universally affecting Claude, OpenAI, and Cursor, while\nprompt-based and tool-centric attacks exhibit considerable variability across\ndifferent hosts and models. Overall, MCPSecBench standardizes the evaluation of\nMCP security and enables rigorous testing across all MCP layers."}
{"id": "2508.13240", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13240", "abs": "https://arxiv.org/abs/2508.13240", "authors": ["Soham Hans", "Nikolos Gurney", "Stacy Marsella", "Sofia Hirschmann"], "title": "Quantifying Loss Aversion in Cyber Adversaries via LLM Analysis", "comment": null, "summary": "Understanding and quantifying human cognitive biases from empirical data has\nlong posed a formidable challenge, particularly in cybersecurity, where\ndefending against unknown adversaries is paramount. Traditional cyber defense\nstrategies have largely focused on fortification, while some approaches attempt\nto anticipate attacker strategies by mapping them to cognitive vulnerabilities,\nyet they fall short in dynamically interpreting attacks in progress. In\nrecognition of this gap, IARPA's ReSCIND program seeks to infer, defend\nagainst, and even exploit attacker cognitive traits. In this paper, we present\na novel methodology that leverages large language models (LLMs) to extract\nquantifiable insights into the cognitive bias of loss aversion from hacker\nbehavior. Our data are collected from an experiment in which hackers were\nrecruited to attack a controlled demonstration network. We process the hacker\ngenerated notes using LLMs using it to segment the various actions and\ncorrelate the actions to predefined persistence mechanisms used by hackers. By\ncorrelating the implementation of these mechanisms with various operational\ntriggers, our analysis provides new insights into how loss aversion manifests\nin hacker decision-making. The results demonstrate that LLMs can effectively\ndissect and interpret nuanced behavioral patterns, thereby offering a\ntransformative approach to enhancing cyber defense strategies through\nreal-time, behavior-based analysis."}
{"id": "2508.13246", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13246", "abs": "https://arxiv.org/abs/2508.13246", "authors": ["Yangyang Guo", "Yangyan Li", "Mohan Kankanhalli"], "title": "Involuntary Jailbreak", "comment": "We plan to temporarily restrict access to the github code due to\n  potential risks of malicious use. But in the meantime, you can try using the\n  prompt, provided it hasn't been banned", "summary": "In this study, we disclose a worrying new vulnerability in Large Language\nModels (LLMs), which we term \\textbf{involuntary jailbreak}. Unlike existing\njailbreak attacks, this weakness is distinct in that it does not involve a\nspecific attack objective, such as generating instructions for \\textit{building\na bomb}. Prior attack methods predominantly target localized components of the\nLLM guardrail. In contrast, involuntary jailbreaks may potentially compromise\nthe entire guardrail structure, which our method reveals to be surprisingly\nfragile. We merely employ a single universal prompt to achieve this goal. In\nparticular, we instruct LLMs to generate several questions that would typically\nbe rejected, along with their corresponding in-depth responses (rather than a\nrefusal). Remarkably, this simple prompt strategy consistently jailbreaks the\nmajority of leading LLMs, including Claude Opus 4.1, Grok 4, Gemini 2.5 Pro,\nand GPT 4.1. We hope this problem can motivate researchers and practitioners to\nre-evaluate the robustness of LLM guardrails and contribute to stronger safety\nalignment in future."}
{"id": "2508.13357", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.13357", "abs": "https://arxiv.org/abs/2508.13357", "authors": ["Zhuoran Li", "Hanieh Totonchi Asl", "Ebrahim Nouri", "Yifei Cai", "Danella Zhao"], "title": "Silentflow: Leveraging Trusted Execution for Resource-Limited MPC via Hardware-Algorithm Co-design", "comment": null, "summary": "Secure Multi-Party Computation (MPC) offers a practical foundation for\nprivacy-preserving machine learning at the edge, with MPC commonly employed to\nsupport nonlinear operations. These MPC protocols fundamentally rely on\nOblivious Transfer (OT), particularly Correlated OT (COT), to generate\ncorrelated randomness essential for secure computation. Although COT generation\nis efficient in conventional two-party settings with resource-rich\nparticipants, it becomes a critical bottleneck in real-world inference on\nresource-constrained devices (e.g., IoT sensors and wearables), due to both\ncommunication latency and limited computational capacity. To enable real-time\nsecure inference, we introduce Silentflow, a highly efficient Trusted Execution\nEnvironment (TEE)-assisted protocol that eliminates communication in COT\ngeneration. We tackle the core performance bottleneck-low computational\nintensity-through structured algorithmic decomposition: kernel fusion for\nparallelism, Blocked On-chip eXpansion (BOX) to improve memory access patterns,\nand vectorized batch operations to maximize memory bandwidth utilization.\nThrough design space exploration, we balance end-to-end latency and resource\ndemands, achieving up to 39.51x speedup over state-of-the-art protocols. By\noffloading COT computations to a Zynq-7000 SoC, SilentFlow accelerates PPMLaaS\ninference on the ImageNet dataset under resource constraints, achieving a 4.62x\nand 3.95x speedup over Cryptflow2 and Cheetah, respectively."}
{"id": "2508.13364", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13364", "abs": "https://arxiv.org/abs/2508.13364", "authors": ["Tadeu Freitas", "Carlos Novo", "Inês Dutra", "João Soares", "Manuel Correia", "Benham Shariati", "Rolando Martins"], "title": "A Risk Manager for Intrusion Tolerant Systems: Enhancing HAL 9000 with New Scoring and Data Sources", "comment": null, "summary": "Intrusion Tolerant Systems (ITSs) have become increasingly critical due to\nthe rise of multi-domain adversaries exploiting diverse attack surfaces. ITS\narchitectures aim to tolerate intrusions, ensuring system compromise is\nprevented or mitigated even with adversary presence. Existing ITS solutions\noften employ Risk Managers leveraging public security intelligence to adjust\nsystem defenses dynamically against emerging threats. However, these approaches\nrely heavily on databases like NVD and ExploitDB, which require manual analysis\nfor newly discovered vulnerabilities. This dependency limits the system's\nresponsiveness to rapidly evolving threats. HAL 9000, an ITS Risk Manager\nintroduced in our prior work, addressed these challenges through machine\nlearning. By analyzing descriptions of known vulnerabilities, HAL 9000 predicts\nand assesses new vulnerabilities automatically. To calculate the risk of a\nsystem, it also incorporates the Exploitability Probability Scoring system to\nestimate the likelihood of exploitation within 30 days, enhancing proactive\ndefense capabilities.\n  Despite its success, HAL 9000's reliance on NVD and ExploitDB knowledge is a\nlimitation, considering the availability of other sources of information. This\nextended work introduces a custom-built scraper that continuously mines diverse\nthreat sources, including security advisories, research forums, and real-time\nexploit proofs-of-concept. This significantly expands HAL 9000's intelligence\nbase, enabling earlier detection and assessment of unverified vulnerabilities.\nOur evaluation demonstrates that integrating scraper-derived intelligence with\nHAL 9000's risk management framework substantially improves its ability to\naddress emerging threats. This paper details the scraper's integration into the\narchitecture, its role in providing additional information on new threats, and\nthe effects on HAL 9000's management."}
{"id": "2508.13425", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.13425", "abs": "https://arxiv.org/abs/2508.13425", "authors": ["Mohamed Elmahallawy", "Tie Luo"], "title": "When Secure Aggregation Falls Short: Achieving Long-Term Privacy in Asynchronous Federated Learning for LEO Satellite Networks", "comment": null, "summary": "Secure aggregation is a common technique in federated learning (FL) for\nprotecting data privacy from both curious internal entities (clients or server)\nand external adversaries (eavesdroppers). However, in dynamic and\nresource-constrained environments such as low Earth orbit (LEO) satellite\nnetworks, traditional secure aggregation methods fall short in two aspects: (1)\nthey assume continuous client availability while LEO satellite visibility is\nintermittent and irregular; (2) they consider privacy in each communication\nround but have overlooked the possible privacy leakage through multiple rounds.\nTo address these limitations, we propose LTP-FLEO, an asynchronous FL framework\nthat preserves long-term privacy (LTP) for LEO satellite networks. LTP-FLEO\nintroduces (i) privacy-aware satellite partitioning, which groups satellites\nbased on their predictable visibility to the server and enforces joint\nparticipation; (ii) model age balancing, which mitigates the adverse impact of\nstale model updates; and (iii) fair global aggregation, which treats satellites\nof different visibility durations in an equitable manner. Theoretical analysis\nand empirical validation demonstrate that LTP-FLEO effectively safeguards both\nmodel and data privacy across multi-round training, promotes fairness in line\nwith satellite contributions, accelerates global convergence, and achieves\ncompetitive model accuracy."}
{"id": "2508.13453", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.13453", "abs": "https://arxiv.org/abs/2508.13453", "authors": ["Ruby Nealon"], "title": "Beneath the Mask: Can Contribution Data Unveil Malicious Personas in Open-Source Projects?", "comment": null, "summary": "In February 2024, after building trust over two years with project\nmaintainers by making a significant volume of legitimate contributions, GitHub\nuser \"JiaT75\" self-merged a version of the XZ Utils project containing a highly\nsophisticated, well-disguised backdoor targeting sshd processes running on\nsystems with the backdoored package installed. A month later, this package\nbegan to be distributed with popular Linux distributions until a Microsoft\nemployee discovered the backdoor while investigating how a recent system\nupgrade impacted the performance of SSH authentication. Despite its potential\nglobal impact, no tooling exists for monitoring and identifying anomalous\nbehavior by personas contributing to other open-source projects. This paper\ndemonstrates how Open Source Intelligence (OSINT) data gathered from GitHub\ncontributions, analyzed using graph databases and graph theory, can efficiently\nidentify anomalous behaviors exhibited by the \"JiaT75\" persona across other\nopen-source projects."}
{"id": "2508.13520", "categories": ["cs.CR", "math.NT", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.13520", "abs": "https://arxiv.org/abs/2508.13520", "authors": ["Takreem Haider"], "title": "Optimizing Scalar Selection in Elliptic Curve Cryptography Using Differential Evolution for Enhanced Security", "comment": null, "summary": "Elliptic Curve Cryptography (ECC) is a fundamental component of modern\npublic-key cryptosystems that enable efficient and secure digital signatures,\nkey exchanges, and encryption. Its core operation, scalar multiplication,\ndenoted as $k \\cdot P$, where $P$ is a base point and $k$ is a private scalar,\nrelies heavily on the secrecy and unpredictability of $k$. Conventionally, $k$\nis selected using user input or pseudorandom number generators. However, in\nresource-constrained environments with weak entropy sources, these approaches\nmay yield low-entropy or biased scalars, increasing susceptibility to\nside-channel and key recovery attacks. To mitigate these vulnerabilities, we\nintroduce an optimization-driven scalar generation method that explicitly\nmaximizes bit-level entropy. Our approach uses differential evolution (DE), a\npopulation-based metaheuristic algorithm, to search for scalars whose binary\nrepresentations exhibit maximal entropy, defined by an even and statistically\nuniform distribution of ones and zeros. This reformulation of scalar selection\nas an entropy-optimization problem enhances resistance to entropy-based\ncryptanalytic techniques and improves overall unpredictability. Experimental\nresults demonstrate that DE-optimized scalars achieve entropy significantly\nhigher than conventionally generated scalars. The proposed method can be\nintegrated into existing ECC-based protocols, offering a deterministic, tunable\nalternative to traditional randomness, ideal for applications in blockchain,\nsecure messaging, IoT, and other resource-constrained environments."}
{"id": "2508.13588", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.13588", "abs": "https://arxiv.org/abs/2508.13588", "authors": ["Víctor Mayoral-Vilches", "Jasmin Wachter", "Cristóbal R. J. Veas Chavez", "Cathrin Schachner", "Luis Javier Navarrete-Lozano", "María Sanz-Gómez"], "title": "CAI Fluency: A Framework for Cybersecurity AI Fluency", "comment": null, "summary": "This work introduces CAI Fluency, an an educational platform of the\nCybersecurity AI (CAI) framework dedicated to democratizing the knowledge and\napplication of cybersecurity AI tools in the global security community. The\nmain objective of the CAI framework is to accelerate the widespread adoption\nand effective use of artificial intelligence-based cybersecurity solutions,\npathing the way to vibe-hacking, the cybersecurity analogon to vibe-coding.\n  CAI Fluency builds upon the Framework for AI Fluency, adapting its three\nmodalities of human-AI interaction and four core competencies specifically for\ncybersecurity applications. This theoretical foundation ensures that\npractitioners develop not just technical skills, but also the critical thinking\nand ethical awareness necessary for responsible AI use in security contexts.\n  This technical report serves as a white-paper, as well as detailed\neducational and practical guide that helps users understand the principles\nbehind the CAI framework, and educates them how to apply this knowledge in\ntheir projects and real-world security contexts."}
{"id": "2508.13644", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.13644", "abs": "https://arxiv.org/abs/2508.13644", "authors": ["Viktoria Koscinski", "Mark Nelson", "Ahmet Okutan", "Robert Falso", "Mehdi Mirakhorli"], "title": "Conflicting Scores, Confusing Signals: An Empirical Study of Vulnerability Scoring Systems", "comment": null, "summary": "Accurately assessing software vulnerabilities is essential for effective\nprioritization and remediation. While various scoring systems exist to support\nthis task, their differing goals, methodologies and outputs often lead to\ninconsistent prioritization decisions. This work provides the first\nlarge-scale, outcome-linked empirical comparison of four publicly available\nvulnerability scoring systems: the Common Vulnerability Scoring System (CVSS),\nthe Stakeholder-Specific Vulnerability Categorization (SSVC), the Exploit\nPrediction Scoring System (EPSS), and the Exploitability Index. We use a\ndataset of 600 real-world vulnerabilities derived from four months of\nMicrosoft's Patch Tuesday disclosures to investigate the relationships between\nthese scores, evaluate how they support vulnerability management task, how\nthese scores categorize vulnerabilities across triage tiers, and assess their\nability to capture the real-world exploitation risk. Our findings reveal\nsignificant disparities in how scoring systems rank the same vulnerabilities,\nwith implications for organizations relying on these metrics to make\ndata-driven, risk-based decisions. We provide insights into the alignment and\ndivergence of these systems, highlighting the need for more transparent and\nconsistent exploitability, risk, and severity assessments."}
{"id": "2508.13690", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13690", "abs": "https://arxiv.org/abs/2508.13690", "authors": ["Wei Shao", "Zequan Liang", "Ruoyu Zhang", "Ruijie Fang", "Ning Miao", "Ehsan Kourkchi", "Setareh Rafatirad", "Houman Homayoun", "Chongzhou Fang"], "title": "Know Me by My Pulse: Toward Practical Continuous Authentication on Wearable Devices via Wrist-Worn PPG", "comment": "To be published in Network and Distributed System Security (NDSS)\n  Symposium 2026", "summary": "Biometric authentication using physiological signals offers a promising path\ntoward secure and user-friendly access control in wearable devices. While\nelectrocardiogram (ECG) signals have shown high discriminability, their\nintrusive sensing requirements and discontinuous acquisition limit\npracticality. Photoplethysmography (PPG), on the other hand, enables\ncontinuous, non-intrusive authentication with seamless integration into\nwrist-worn wearable devices. However, most prior work relies on high-frequency\nPPG (e.g., 75 - 500 Hz) and complex deep models, which incur significant energy\nand computational overhead, impeding deployment in power-constrained real-world\nsystems. In this paper, we present the first real-world implementation and\nevaluation of a continuous authentication system on a smartwatch, We-Be Band,\nusing low-frequency (25 Hz) multi-channel PPG signals. Our method employs a\nBi-LSTM with attention mechanism to extract identity-specific features from\nshort (4 s) windows of 4-channel PPG. Through extensive evaluations on both\npublic datasets (PTTPPG) and our We-Be Dataset (26 subjects), we demonstrate\nstrong classification performance with an average test accuracy of 88.11%,\nmacro F1-score of 0.88, False Acceptance Rate (FAR) of 0.48%, False Rejection\nRate (FRR) of 11.77%, and Equal Error Rate (EER) of 2.76%. Our 25 Hz system\nreduces sensor power consumption by 53% compared to 512 Hz and 19% compared to\n128 Hz setups without compromising performance. We find that sampling at 25 Hz\npreserves authentication accuracy, whereas performance drops sharply at 20 Hz\nwhile offering only trivial additional power savings, underscoring 25 Hz as the\npractical lower bound. Additionally, we find that models trained exclusively on\nresting data fail under motion, while activity-diverse training improves\nrobustness across physiological states."}
{"id": "2508.13730", "categories": ["cs.CR", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.13730", "abs": "https://arxiv.org/abs/2508.13730", "authors": ["Daniel M. Jimenez-Gutierrez", "Yelizaveta Falkouskaya", "Jose L. Hernandez-Ramos", "Aris Anagnostopoulos", "Ioannis Chatzigiannakis", "Andrea Vitaletti"], "title": "On the Security and Privacy of Federated Learning: A Survey with Attacks, Defenses, Frameworks, Applications, and Future Directions", "comment": null, "summary": "Federated Learning (FL) is an emerging distributed machine learning paradigm\nenabling multiple clients to train a global model collaboratively without\nsharing their raw data. While FL enhances data privacy by design, it remains\nvulnerable to various security and privacy threats. This survey provides a\ncomprehensive overview of more than 200 papers regarding the state-of-the-art\nattacks and defense mechanisms developed to address these challenges,\ncategorizing them into security-enhancing and privacy-preserving techniques.\nSecurity-enhancing methods aim to improve FL robustness against malicious\nbehaviors such as byzantine attacks, poisoning, and Sybil attacks. At the same\ntime, privacy-preserving techniques focus on protecting sensitive data through\ncryptographic approaches, differential privacy, and secure aggregation. We\ncritically analyze the strengths and limitations of existing methods, highlight\nthe trade-offs between privacy, security, and model performance, and discuss\nthe implications of non-IID data distributions on the effectiveness of these\ndefenses. Furthermore, we identify open research challenges and future\ndirections, including the need for scalable, adaptive, and energy-efficient\nsolutions operating in dynamic and heterogeneous FL environments. Our survey\naims to guide researchers and practitioners in developing robust and\nprivacy-preserving FL systems, fostering advancements safeguarding\ncollaborative learning frameworks' integrity and confidentiality."}
{"id": "2508.13750", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.13750", "abs": "https://arxiv.org/abs/2508.13750", "authors": ["Eric Cornelissen", "Musard Balliu"], "title": "NodeShield: Runtime Enforcement of Security-Enhanced SBOMs for Node.js", "comment": "15 pages, 3 figures, 9 tables", "summary": "The software supply chain is an increasingly common attack vector for\nmalicious actors. The Node.js ecosystem has been subject to a wide array of\nattacks, likely due to its size and prevalence. To counter such attacks, the\nresearch community and practitioners have proposed a range of static and\ndynamic mechanisms, including process- and language-level sandboxing,\npermission systems, and taint tracking. Drawing on valuable insight from these\nworks, this paper studies a runtime protection mechanism for (the supply chain\nof) Node.js applications with the ambitious goals of compatibility, automation,\nminimal overhead, and policy conciseness.\n  Specifically, we design, implement and evaluate NodeShield, a protection\nmechanism for Node.js that enforces an application's dependency hierarchy and\ncontrols access to system resources at runtime. We leverage the up-and-coming\nSBOM standard as the source of truth for the dependency hierarchy of the\napplication, thus preventing components from stealthily abusing undeclared\ncomponents. We propose to enhance the SBOM with a notion of capabilities that\nrepresents a set of related system resources a component may access. Our\nproposed SBOM extension, the Capability Bill of Materials or CBOM, records the\nrequired capabilities of each component, providing valuable insight into the\npotential privileged behavior. NodeShield enforces the SBOM and CBOM at runtime\nvia code outlining (as opposed to inlining) with no modifications to the\noriginal code or Node.js runtime, thus preventing unexpected, potentially\nmalicious behavior. Our evaluation shows that NodeShield can prevent over 98%\nout of 67 known supply chain attacks while incurring minimal overhead on\nservers at less than 1ms per request. We achieve this while maintaining broad\ncompatibility with vanilla Node.js and a concise policy language that consists\nof at most 7 entries per dependency."}
{"id": "2508.13965", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.13965", "abs": "https://arxiv.org/abs/2508.13965", "authors": ["Yuntao Liu", "Abir Akib", "Zelin Lu", "Qian Xu", "Ankur Srivastava", "Gang Qu", "David Kehlet", "Nij Dorairaj"], "title": "Red Teaming Methodology for Design Obfuscation", "comment": null, "summary": "The main goal of design obfuscation schemes is to protect sensitive design\ndetails from untrusted parties in the VLSI supply chain, including but not\nlimited to off-shore foundries and untrusted end users. In this work, we\nprovide a systematic red teaming approach to evaluate the security of design\nobfuscation approaches. Specifically, we propose security metrics and\nevaluation methodology for the scenarios where the adversary does not have\naccess to a working chip. A case study on the RIPPER tool developed by the\nUniversity of Florida indicates that more information is leaked about the\nstructure of the original design than commonly considered."}
