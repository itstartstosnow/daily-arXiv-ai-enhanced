<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 59]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Towards Log Analysis with AI Agents: Cowrie Case Study](https://arxiv.org/abs/2509.05306)
*Enis Karaarslan,Esin Güler,Efe Emir Yüce,Cagatay Coban*

Main category: cs.CR

TL;DR: 使用AI代理自动分析Cowrie蜜罐日志，解决网络安全研究中真实攻击数据稀缺和日志分析困难的问题


<details>
  <summary>Details</summary>
Motivation: 真实攻击数据稀缺阻碍网络安全研究进展，蜜罐产生的大量非结构化日志难以手动分析，需要自动化解决方案

Method: 采用轻量级自动化方法，利用AI代理智能解析、总结和提取Cowrie蜜罐原始日志中的洞察，同时考虑自主系统的安全影响

Result: 初步结果显示该流水线能有效减少人工工作量并识别攻击模式

Conclusion: 该方法为未来更先进的自主网络安全分析奠定了基础

Abstract: The scarcity of real-world attack data significantly hinders progress in
cybersecurity research and education. Although honeypots like Cowrie
effectively collect live threat intelligence, they generate overwhelming
volumes of unstructured and heterogeneous logs, rendering manual analysis
impractical. As a first step in our project on secure and efficient AI
automation, this study explores the use of AI agents for automated log
analysis. We present a lightweight and automated approach to process Cowrie
honeypot logs. Our approach leverages AI agents to intelligently parse,
summarize, and extract insights from raw data, while also considering the
security implications of deploying such an autonomous system. Preliminary
results demonstrate the pipeline's effectiveness in reducing manual effort and
identifying attack patterns, paving the way for more advanced autonomous
cybersecurity analysis in future work.

</details>


### [2] [Large Language Model Integration with Reinforcement Learning to Augment Decision-Making in Autonomous Cyber Operations](https://arxiv.org/abs/2509.05311)
*Konur Tholl,François Rivest,Mariam El Mezouar,Ranwa Al Mallah*

Main category: cs.CR

TL;DR: 基于大语言模型的引导强化学习在网络安全领域中显著提升训练效率和减少负面探索行为


<details>
  <summary>Details</summary>
Motivation: 解决传统强化学习在自主网络操作中需要从零开始学习、必须执行不良行为来学习后果的问题

Method: 集成预训练的大语言模型（LLM）来引导RL代理做出明智决策，利用网络安全领域知识来减少负面探索

Result: 在模拟网络安全环境中，引导代理在早期训练期奖励提升2倍以上，收敛到良好策略的速度提前约4,500个完整训练循环

Conclusion: 集成LLM知识能够显著提升RL在网络安全领域的学习效率和性能，为自主网络操作提供了更有效的解决方案

Abstract: Reinforcement Learning (RL) has shown great potential for autonomous
decision-making in the cybersecurity domain, enabling agents to learn through
direct environment interaction. However, RL agents in Autonomous Cyber
Operations (ACO) typically learn from scratch, requiring them to execute
undesirable actions to learn their consequences. In this study, we integrate
external knowledge in the form of a Large Language Model (LLM) pretrained on
cybersecurity data that our RL agent can directly leverage to make informed
decisions. By guiding initial training with an LLM, we improve baseline
performance and reduce the need for exploratory actions with obviously negative
outcomes. We evaluate our LLM-integrated approach in a simulated cybersecurity
environment, and demonstrate that our guided agent achieves over 2x higher
rewards during early training and converges to a favorable policy approximately
4,500 episodes faster than the baseline.

</details>


### [3] [Backdoor Samples Detection Based on Perturbation Discrepancy Consistency in Pre-trained Language Models](https://arxiv.org/abs/2509.05318)
*Zuquan Peng,Jianming Fu,Lixin Zou,Li Zheng,Yanzhen Ren,Guojun Peng*

Main category: cs.CR

TL;DR: 提出了一种基于扰动差异一致性评估（NETE）的后门样本检测方法，无需访问中毒模型、额外干净样本或大量计算资源，仅需现成的预训练模型和自动扰动生成功能。


<details>
  <summary>Details</summary>
Motivation: 预训练模型使用未经审查的第三方和互联网数据容易受到后门攻击，现有检测方法需要访问中毒模型、额外干净样本或大量计算资源，限制了实用性。

Method: 基于后门样本的扰动差异变化小于干净样本的现象，使用曲率测量不同扰动样本与输入样本之间的对数概率差异，通过评估扰动差异的一致性来判断是否为后门样本。

Result: 在四种典型后门攻击和五种大语言模型后门攻击上的实验表明，该检测策略优于现有的零样本黑盒检测方法。

Conclusion: NETE方法提供了一种实用且有效的后门样本检测解决方案，适用于训练前和训练后阶段，仅需现成预训练模型和自动扰动生成功能。

Abstract: The use of unvetted third-party and internet data renders pre-trained models
susceptible to backdoor attacks. Detecting backdoor samples is critical to
prevent backdoor activation during inference or injection during training.
However, existing detection methods often require the defender to have access
to the poisoned models, extra clean samples, or significant computational
resources to detect backdoor samples, limiting their practicality. To address
this limitation, we propose a backdoor sample detection method based on
perturbatio\textbf{N} discr\textbf{E}pancy consis\textbf{T}ency
\textbf{E}valuation (\NETE). This is a novel detection method that can be used
both pre-training and post-training phases. In the detection process, it only
requires an off-the-shelf pre-trained model to compute the log probability of
samples and an automated function based on a mask-filling strategy to generate
perturbations. Our method is based on the interesting phenomenon that the
change in perturbation discrepancy for backdoor samples is smaller than that
for clean samples. Based on this phenomenon, we use curvature to measure the
discrepancy in log probabilities between different perturbed samples and input
samples, thereby evaluating the consistency of the perturbation discrepancy to
determine whether the input sample is a backdoor sample. Experiments conducted
on four typical backdoor attacks and five types of large language model
backdoor attacks demonstrate that our detection strategy outperforms existing
zero-shot black-box detection methods.

</details>


### [4] [Privacy-Preserving Offloading for Large Language Models in 6G Vehicular Networks](https://arxiv.org/abs/2509.05320)
*Ikhlasse Badidi,Nouhaila El Khiyaoui,Aya Riany,Badr Ben Elallid,Amine Abouaomar*

Main category: cs.CR

TL;DR: 提出了一种结合联邦学习和差分隐私的隐私保护框架，用于6G车联网中的LLM计算卸载，在保证隐私的同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 6G车联网中LLM计算卸载存在严重的隐私风险，可能暴露用户敏感数据，需要保护隐私的同时维持系统性能。

Method: 采用联邦学习和差分隐私的混合方法，包含隐私感知的任务分割算法和安全通信协议，优化本地与边缘计算的权衡。

Result: 达到75%的全局准确率，仅比非隐私保护方法降低2-3%，在ε=0.8的隐私预算下保持差分隐私保证，通信开销稳定在2.1MB/轮。

Conclusion: 该框架在资源受限的车联网环境中具有高效性，成功平衡了隐私保护与系统性能的需求。

Abstract: The integration of Large Language Models (LLMs) in 6G vehicular networks
promises unprecedented advancements in intelligent transportation systems.
However, offloading LLM computations from vehicles to edge infrastructure poses
significant privacy risks, potentially exposing sensitive user data. This paper
presents a novel privacy-preserving offloading framework for LLM-integrated
vehicular networks. We introduce a hybrid approach combining federated learning
(FL) and differential privacy (DP) techniques to protect user data while
maintaining LLM performance. Our framework includes a privacy-aware task
partitioning algorithm that optimizes the trade-off between local and edge
computation, considering both privacy constraints and system efficiency. We
also propose a secure communication protocol for transmitting model updates and
aggregating results across the network. Experimental results demonstrate that
our approach achieves 75\% global accuracy with only a 2-3\% reduction compared
to non-privacy-preserving methods, while maintaining DP guarantees with an
optimal privacy budget of $\varepsilon = 0.8$. The framework shows stable
communication overhead of approximately 2.1MB per round with computation
comprising over 90\% of total processing time, validating its efficiency for
resource-constrained vehicular environments.

</details>


### [5] [Zero-Knowledge Proofs in Sublinear Space](https://arxiv.org/abs/2509.05326)
*Logan Nye*

Main category: cs.CR

TL;DR: 本文构建了首个亚线性空间零知识证明证明器，将证明生成重新表述为树评估问题，使用空间高效的树评估算法，将证明器内存从线性降低到O(sqrt(T))，同时保持证明大小和验证效率。


<details>
  <summary>Details</summary>
Motivation: 现有零知识证明系统需要证明器使用与计算轨迹长度T成线性比例的内存，这在资源受限设备上不实用且大规模任务成本过高，限制了实际应用。

Method: 通过将证明生成重新表述为经典的树评估问题，利用最近的空间高效树评估算法，设计流式证明器在不物化完整执行轨迹的情况下组装证明。

Result: 证明器内存从线性降低到O(sqrt(T))（包含O(log T)低阶项），同时保持了证明大小、验证器时间以及底层系统的传输/安全保证。

Conclusion: 这项技术实现了从专门的服务器绑定证明向设备上证明的转变，为去中心化系统、设备上机器学习和隐私保护技术开辟了新的应用可能性。

Abstract: Modern zero-knowledge proof (ZKP) systems, essential for privacy and
verifiable computation, suffer from a fundamental limitation: the prover
typically uses memory that scales linearly with the computation's trace length
T, making them impractical for resource-constrained devices and prohibitively
expensive for large-scale tasks. This paper overcomes this barrier by
constructing, to our knowledge, the first sublinear-space ZKP prover. Our core
contribution is an equivalence that reframes proof generation as an instance of
the classic Tree Evaluation problem. Leveraging a recent space-efficient
tree-evaluation algorithm, we design a streaming prover that assembles the
proof without ever materializing the full execution trace. The approach reduces
prover memory from linear in T to O(sqrt(T)) (up to O(log T) lower-order terms)
while preserving proof size, verifier time, and the transcript/security
guarantees of the underlying system. This enables a shift from specialized,
server-bound proving to on-device proving, opening applications in
decentralized systems, on-device machine learning, and privacy-preserving
technologies.

</details>


### [6] [ForensicsData: A Digital Forensics Dataset for Large Language Models](https://arxiv.org/abs/2509.05331)
*Youssef Chakir,Iyad Lahsen-Cherif*

Main category: cs.CR

TL;DR: 提出了ForensicsData数据集，包含5000多个从真实恶意软件分析报告中提取的问答三元组，用于解决数字取证领域数据稀缺问题，并评估了LLM在取证术语对齐方面的性能。


<details>
  <summary>Details</summary>
Motivation: 数字取证调查面临证据收集和分析的挑战，由于伦理、法律和隐私问题，公开的现实数据集有限，阻碍了研究和工具开发。

Method: 采用独特的工作流程：从恶意软件分析报告中提取结构化数据，使用大语言模型将其转换为问答三元组格式，并通过专门评估流程确保数据质量。

Result: 创建了包含5000多个Q-C-A三元组的数据集，评估发现Gemini 2 Flash模型在生成内容与取证术语对齐方面表现最佳。

Conclusion: ForensicsData数据集旨在通过支持可重复实验和促进研究社区合作来推动数字取证领域的发展。

Abstract: The growing complexity of cyber incidents presents significant challenges for
digital forensic investigators, especially in evidence collection and analysis.
Public resources are still limited because of ethical, legal, and privacy
concerns, even though realistic datasets are necessary to support research and
tool developments. To address this gap, we introduce ForensicsData, an
extensive Question-Context-Answer (Q-C-A) dataset sourced from actual malware
analysis reports. It consists of more than 5,000 Q-C-A triplets. A unique
workflow was used to create the dataset, which extracts structured data, uses
large language models (LLMs) to transform it into Q-C-A format, and then uses a
specialized evaluation process to confirm its quality. Among the models
evaluated, Gemini 2 Flash demonstrated the best performance in aligning
generated content with forensic terminology. ForensicsData aims to advance
digital forensics by enabling reproducible experiments and fostering
collaboration within the research community.

</details>


### [7] [Integrated Simulation Framework for Adversarial Attacks on Autonomous Vehicles](https://arxiv.org/abs/2509.05332)
*Christos Anagnostopoulos,Ioulia Kapsali,Alexandros Gkillas,Nikos Piperigkos,Aris S. Lalos*

Main category: cs.CR

TL;DR: 一个开源集成模拟框架，用于生成自动驾驶车身知和通信层面的对抗攻击，支持高保真环境模拟和多模态同步


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车存在身知和通信系统弱点，现有模拟框架缺乏对多域对抗攻击场景的全面支持

Method: 开发集成模拟框架，通过统一核心同步多个模拟器，支持LiDAR感知攻击、V2X消息操纵、GPS欺骗等多种攻击方式

Result: 框架能够有效生成实际对抗攻击场景，导致先进3D物体检测器性能显著下降

Conclusion: 该框架为AV安全性测试提供了可扩展的解决方案，能够模拟复杂的多域对抗攻击场景

Abstract: Autonomous vehicles (AVs) rely on complex perception and communication
systems, making them vulnerable to adversarial attacks that can compromise
safety. While simulation offers a scalable and safe environment for robustness
testing, existing frameworks typically lack comprehensive supportfor modeling
multi-domain adversarial scenarios. This paper introduces a novel, open-source
integrated simulation framework designed to generate adversarial attacks
targeting both perception and communication layers of AVs. The framework
provides high-fidelity modeling of physical environments, traffic dynamics, and
V2X networking, orchestrating these components through a unified core that
synchronizes multiple simulators based on a single configuration file. Our
implementation supports diverse perception-level attacks on LiDAR sensor data,
along with communication-level threats such as V2X message manipulation and GPS
spoofing. Furthermore, ROS 2 integration ensures seamless compatibility with
third-party AV software stacks. We demonstrate the framework's effectiveness by
evaluating the impact of generated adversarial scenarios on a state-of-the-art
3D object detector, revealing significant performance degradation under
realistic conditions.

</details>


### [8] [Ensembling Membership Inference Attacks Against Tabular Generative Models](https://arxiv.org/abs/2509.05350)
*Joshua Ward,Yuxuan Yang,Chi-Hua Wang,Guang Cheng*

Main category: cs.CR

TL;DR: 该论文研究了合成数据隐私审计中的成员推理攻击选择问题，发现没有单一攻击方法在所有场景下都最优，因此提出了集成攻击方法以获得更稳健的性能。


<details>
  <summary>Details</summary>
Motivation: 在合成数据隐私审计中，攻击者需要选择单一的成员推理攻击方法，但没有先验保证该方法会在特定场景下表现最优，这构成了决策理论问题。

Method: 进行了大规模合成数据隐私基准测试，比较了多种攻击方法在不同模型架构和数据集上的表现，并提出了基于个体攻击的无监督集成方法。

Result: 研究发现没有任何单一成员推理攻击方法在所有情况下都是严格占优策略，而提出的集成攻击方法在经验上提供了更稳健、后悔最小化的策略。

Conclusion: 集成成员推理攻击比单一攻击方法更有效，能够为合成数据隐私审计提供更可靠的攻击策略选择。

Abstract: Membership Inference Attacks (MIAs) have emerged as a principled framework
for auditing the privacy of synthetic data generated by tabular generative
models, where many diverse methods have been proposed that each exploit
different privacy leakage signals. However, in realistic threat scenarios, an
adversary must choose a single method without a priori guarantee that it will
be the empirically highest performing option. We study this challenge as a
decision theoretic problem under uncertainty and conduct the largest synthetic
data privacy benchmark to date. Here, we find that no MIA constitutes a
strictly dominant strategy across a wide variety of model architectures and
dataset domains under our threat model. Motivated by these findings, we propose
ensemble MIAs and show that unsupervised ensembles built on individual attacks
offer empirically more robust, regret-minimizing strategies than individual
attacks.

</details>


### [9] [AI-in-the-Loop: Privacy Preserving Real-Time Scam Detection and Conversational Scambaiting by Leveraging LLMs and Federated Learning](https://arxiv.org/abs/2509.05362)
*Ismail Hossain,Sai Puppala,Sajedul Talukder,Md Jahangir Alam*

Main category: cs.CR

TL;DR: 一种保护隐私的AI实时骗局检测框架，通过指令调整AI和联邦学习主动反击社交工程骗局


<details>
  <summary>Details</summary>
Motivation: 现有防御措施多为反应式，在主动交互中提供有限保护，需要主动领先的实时骗局检测方案

Method: 结合指令调整人工智能和安全意识效用函数，使用联邦学习进行持续模型更新而无需原始数据共享

Result: 系统生成流畅且吸引人的回应（混淆度仅22.3，参与度约0.80），在联邦设置中保持高参与度和强相关性，个人信息泄漏低于0.0085

Conclusion: 该框架首次统一了实时骗局询证、联邦隐私保护和检查安全调节，证明可以在不牺牲性能的情况下实现稳健的隐私保护

Abstract: Scams exploiting real-time social engineering -- such as phishing,
impersonation, and phone fraud -- remain a persistent and evolving threat
across digital platforms. Existing defenses are largely reactive, offering
limited protection during active interactions. We propose a privacy-preserving,
AI-in-the-loop framework that proactively detects and disrupts scam
conversations in real time. The system combines instruction-tuned artificial
intelligence with a safety-aware utility function that balances engagement with
harm minimization, and employs federated learning to enable continual model
updates without raw data sharing. Experimental evaluations show that the system
produces fluent and engaging responses (perplexity as low as 22.3, engagement
$\approx$0.80), while human studies confirm significant gains in realism,
safety, and effectiveness over strong baselines. In federated settings, models
trained with FedAvg sustain up to 30 rounds while preserving high engagement
($\approx$0.80), strong relevance ($\approx$0.74), and low PII leakage
($\leq$0.0085). Even with differential privacy, novelty and safety remain
stable, indicating that robust privacy can be achieved without sacrificing
performance. The evaluation of guard models (LlamaGuard, LlamaGuard2/3,
MD-Judge) shows a straightforward pattern: stricter moderation settings reduce
the chance of exposing personal information, but they also limit how much the
model engages in conversation. In contrast, more relaxed settings allow longer
and richer interactions, which improve scam detection, but at the cost of
higher privacy risk. To our knowledge, this is the first framework to unify
real-time scam-baiting, federated privacy preservation, and calibrated safety
moderation into a proactive defense paradigm.

</details>


### [10] [A Framework for Detection and Classification of Attacks on Surveillance Cameras under IoT Networks](https://arxiv.org/abs/2509.05366)
*Umair Amjid,M. Umar Khan,S. A. Manan Kirmani*

Main category: cs.CR

TL;DR: 基于AI的框架利用机器学习算法分析网络流量，用于检测IoT监控摄像头网络中的异常行为和漏洞攻击


<details>
  <summary>Details</summary>
Motivation: IoT设备使用增加导致安全问题升级，监控摄像头容易受到暴力破解、零日漏洞攻击和DOS攻击，需要有效防范方案

Method: 开发AI基于的框架，利用机器学习算法分析网络流量数据，识别异常行为，并使用真实世界数据集进行训练和评估

Result: 框架能够实现快速检测和响应潜在入侵，通过从历史安全事件中学习来提升检测能力

Conclusion: 该AI基于框架为IoT监控摄像头网络提供了有效的安全防护方案，能够应对多种安全威胁，保障用户隐私和设备可用性

Abstract: The increasing use of Internet of Things (IoT) devices has led to a rise in
security related concerns regarding IoT Networks. The surveillance cameras in
IoT networks are vulnerable to security threats such as brute force and
zero-day attacks which can lead to unauthorized access by hackers and potential
spying on the users activities. Moreover, these cameras can be targeted by
Denial of Service (DOS) attacks, which will make it unavailable for the user.
The proposed AI based framework will leverage machine learning algorithms to
analyze network traffic and detect anomalous behavior, allowing for quick
detection and response to potential intrusions. The framework will be trained
and evaluated using real-world datasets to learn from past security incidents
and improve its ability to detect potential intrusion.

</details>


### [11] [Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs](https://arxiv.org/abs/2509.05367)
*Shei Pern Chua,Thai Zhen Leng,Teh Kai Jun,Xiao Li,Xiaolin Hu*

Main category: cs.CR

TL;DR: TRIAL框架利用伦理困境（电车难题）来绕过LLMs的安全防护，通过多轮对话动态适应上下文，展示了高越狱成功率，揭示了AI安全对齐的根本局限性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs推理能力增强，传统单步越狱攻击已不足，需要探索多轮动态适应的越狱策略，以发现新的安全风险。

Method: 提出TRIAL框架，将对抗性目标嵌入到电车难题式的伦理困境中，利用LLMs的伦理推理能力来绕过安全防护机制。

Result: TRIAL在对开源和闭源模型上都表现出高越狱成功率，证明了当前安全防护措施的不足。

Conclusion: 随着模型推理能力提升，安全对齐可能无意中引入更隐蔽的安全漏洞，迫切需要重新评估安全监督策略以应对上下文感知的对抗攻击。

Abstract: Large language models (LLMs) have undergone safety alignment efforts to
mitigate harmful outputs. However, as LLMs become more sophisticated in
reasoning, their intelligence may introduce new security risks. While
traditional jailbreak attacks relied on singlestep attacks, multi-turn
jailbreak strategies that adapt dynamically to context remain underexplored. In
this work, we introduce TRIAL (Trolley-problem Reasoning for Interactive Attack
Logic), a framework that leverages LLMs ethical reasoning to bypass their
safeguards. TRIAL embeds adversarial goals within ethical dilemmas modeled on
the trolley problem. TRIAL demonstrates high jailbreak success rates towards
both open and close-source models. Our findings underscore a fundamental
limitation in AI safety: as models gain advanced reasoning abilities, the
nature of their alignment may inadvertently allow for more covert security
vulnerabilities to be exploited. TRIAL raises an urgent need in reevaluating
safety alignment oversight strategies, as current safeguards may prove
insufficient against context-aware adversarial attack.

</details>


### [12] [Quantum AI Algorithm Development for Enhanced Cybersecurity: A Hybrid Approach to Malware Detection](https://arxiv.org/abs/2509.05370)
*Tanya Joshi,Krishnendu Guha*

Main category: cs.CR

TL;DR: 量子机器学习算法在网络安全威胁检测中显示出优勃性能，特别是在复杂的阻塞性恶意软件检测任务中，准确率达到95%以上。


<details>
  <summary>Details</summary>
Motivation: 经典机器学习方法在处理复杂的恶意软件模式和大规模网络入侵数据时遇到限制，需要量子机器学习算法来提升网络安全威胁检测能力。

Method: 实现和评估多种QML算法，包括量子神经网络(QNN)、量子支持向量机(QSVM)和混合量子卷积神经网络(QCNN)，使用入侵数据集(150个样本，56个特征)和ObfuscatedMalMem2022数据集(58,596个样本，57个特征)进行实验分析。

Result: QML方法表现出艾，QNN达到95%准确率，QSVM达到94%准确率，显著超过经典方法。量子盛叐和缀缘原理能够准确识别经典方法无法识别的复杂阻塞性恶意软件模式。

Conclusion: 研究提出了一个新颖的实时恶意软件分析框架，这个框架结合了量子特征提取和变分量子电路分类，并集成了可解释AI方法来提供量子决策过程的可解释性。

Abstract: This study explores the application of quantum machine learning (QML)
algorithms to enhance cybersecurity threat detection, particularly in the
classification of malware and intrusion detection within high-dimensional
datasets. Classical machine learning approaches encounter limitations when
dealing with intricate, obfuscated malware patterns and extensive network
intrusion data. To address these challenges, we implement and evaluate various
QML algorithms, including Quantum Neural Networks (QNN), Quantum Support Vector
Machines (QSVM), and hybrid Quantum Convolutional Neural Networks (QCNN) for
malware detection tasks. Our experimental analysis utilized two datasets: the
Intrusion dataset, comprising 150 samples with 56 memory-based features derived
from Volatility framework analysis, and the ObfuscatedMalMem2022 dataset,
containing 58,596 samples with 57 features representing benign and malicious
software. Remarkably, our QML methods demonstrated superior performance
compared to classical approaches, achieving accuracies of 95% for QNN and 94%
for QSVM. These quantum-enhanced methods leveraged quantum superposition and
entanglement principles to accurately identify complex patterns within highly
obfuscated malware samples that were imperceptible to classical methods. To
further advance malware analysis, we propose a novel real-time malware analysis
framework that incorporates Quantum Feature Extraction using Quantum Fourier
Transform, Quantum Feature Maps, and Classification using Variational Quantum
Circuits. This system integrates explainable AI methods, including GradCAM++
and ScoreCAM algorithms, to provide interpretable insights into the quantum
decision-making processes.

</details>


### [13] [Privacy Preservation and Identity Tracing Prevention in AI-Driven Eye Tracking for Interactive Learning Environments](https://arxiv.org/abs/2509.05376)
*Abdul Rehman,Are Dæhlen,Ilona Heldal,Jerry Chun-wei Lin*

Main category: cs.CR

TL;DR: 这篇论文提出了一种保护眼动跟踪数据隐私的人本中心框架，通过两阶段隐私保护技术和联邦学习，在保持教育效益的同时防止身份追溯。


<details>
  <summary>Details</summary>
Motivation: 眼动跟踪技术虽能助于研究神经发育障碍和身份识别，但也带来严重隐私风险，容易泄露敏感个人信息。

Method: 提出两阶段隐私保护框架：第一阶段测试四种身份追溯场景，第二阶段采用联邦学习技术、虚拟ID和管理员权限控制，实现实时数据匿名化。

Result: 第一阶段在障碍诊断预测达到99.3%准确率，学生ID预测准确率63%，随机数据预测准确率99.7%。第二阶段成功防止身份追溯，并实现了99.40%的整体准确率。

Conclusion: 该框架能够在保持眼动跟踪教育价值的同时，有效保护用户隐私，通过结合匿名化技术、联邦学习和严格的权限管理实现隐私保护与数据利用的平衡。

Abstract: Eye-tracking technology can aid in understanding neurodevelopmental disorders
and tracing a person's identity. However, this technology poses a significant
risk to privacy, as it captures sensitive information about individuals and
increases the likelihood that data can be traced back to them. This paper
proposes a human-centered framework designed to prevent identity backtracking
while preserving the pedagogical benefits of AI-powered eye tracking in
interactive learning environments. We explore how real-time data anonymization,
ethical design principles, and regulatory compliance (such as GDPR) can be
integrated to build trust and transparency. We first demonstrate the potential
for backtracking student IDs and diagnoses in various scenarios using serious
game-based eye-tracking data. We then provide a two-stage privacy-preserving
framework that prevents participants from being tracked while still enabling
diagnostic classification. The first phase covers four scenarios: I) Predicting
disorder diagnoses based on different game levels. II) Predicting student IDs
based on different game levels. III) Predicting student IDs based on randomized
data. IV) Utilizing K-Means for out-of-sample data. In the second phase, we
present a two-stage framework that preserves privacy. We also employ Federated
Learning (FL) across multiple clients, incorporating a secure identity
management system with dummy IDs and administrator-only access controls. In the
first phase, the proposed framework achieved 99.3% accuracy for scenario 1, 63%
accuracy for scenario 2, and 99.7% accuracy for scenario 3, successfully
identifying and assigning a new student ID in scenario 4. In phase 2, we
effectively prevented backtracking and established a secure identity management
system with dummy IDs and administrator-only access controls, achieving an
overall accuracy of 99.40%.

</details>


### [14] [ThreatGPT: An Agentic AI Framework for Enhancing Public Safety through Threat Modeling](https://arxiv.org/abs/2509.05379)
*Sharif Noor Zisad,Ragib Hasan*

Main category: cs.CR

TL;DR: ThreatGPT是一个AI助手，帮助非网络安全专家分析公共安全系统的威胁，使用STRIDE、MITRE ATT&CK等框架生成智能威胁模型。


<details>
  <summary>Details</summary>
Motivation: 随着智慧城市系统日益复杂，安全威胁风险增加，需要让工程师、安全官员等非网络安全专家也能理解和分析系统威胁。

Method: 基于few-shot学习，让AI从示例中学习并生成相关威胁模型，支持多种分析框架（STRIDE、MITRE ATT&CK、CVE、NIST、CISA）。

Result: 开发出ThreatGPT工具，能够识别潜在问题、攻击者利用方式以及防护措施，适应不同用户需求。

Conclusion: ThreatGPT结合AI和人类判断，使公共系统更安全，赋能用户更快、更智能、更自信地理解和应对威胁。

Abstract: As our cities and communities become smarter, the systems that keep us safe,
such as traffic control centers, emergency response networks, and public
transportation, also become more complex. With this complexity comes a greater
risk of security threats that can affect not just machines but real people's
lives. To address this challenge, we present ThreatGPT, an agentic Artificial
Intelligence (AI) assistant built to help people whether they are engineers,
safety officers, or policy makers to understand and analyze threats in public
safety systems. Instead of requiring deep cybersecurity expertise, it allows
users to simply describe the components of a system they are concerned about,
such as login systems, data storage, or communication networks. Then, with the
click of a button, users can choose how they want the system to be analyzed by
using popular frameworks such as STRIDE, MITRE ATT&CK, CVE reports, NIST, or
CISA. ThreatGPT is unique because it does not just provide threat information,
but rather it acts like a knowledgeable partner. Using few-shot learning, the
AI learns from examples and generates relevant smart threat models. It can
highlight what might go wrong, how attackers could take advantage, and what can
be done to prevent harm. Whether securing a city's infrastructure or a local
health service, this tool adapts to users' needs. In simple terms, ThreatGPT
brings together AI and human judgment to make our public systems safer. It is
designed not just to analyze threats, but to empower people to understand and
act on them, faster, smarter, and with more confidence.

</details>


### [15] [Behind the Mask: Benchmarking Camouflaged Jailbreaks in Large Language Models](https://arxiv.org/abs/2509.05471)
*Youjia Zheng,Mohammad Zandsalimy,Shanu Sushmita*

Main category: cs.CR

TL;DR: 本文研究大语言模型中的伪装越狱攻击，构建了包含500个样本的基准数据集，并提出多维度评估框架，发现模型在面对伪装攻击时安全性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 大语言模型面临新型的伪装越狱攻击威胁，这种攻击通过将恶意意图隐藏在看似良性的语言中来规避现有安全机制，传统的关键词检测方法难以应对这种基于上下文模糊性的攻击。

Method: 构建了包含400个有害提示和100个良性提示的伪装越狱提示基准数据集，并提出了包含7个维度的多层面评估框架（安全意识、技术可行性、实施保障、潜在危害、教育价值、内容质量和合规性得分）。

Result: 研究发现大语言模型在面对良性输入时表现出高安全性和内容质量，但在面对伪装越狱攻击时，其性能和安全表现显著下降，显示出普遍存在的安全漏洞。

Conclusion: 伪装越狱攻击暴露了大语言模型的安全脆弱性，迫切需要开发更加细致和自适应的安全策略，以确保大语言模型在现实应用中的负责任和稳健部署。

Abstract: Large Language Models (LLMs) are increasingly vulnerable to a sophisticated
form of adversarial prompting known as camouflaged jailbreaking. This method
embeds malicious intent within seemingly benign language to evade existing
safety mechanisms. Unlike overt attacks, these subtle prompts exploit
contextual ambiguity and the flexible nature of language, posing significant
challenges to current defense systems. This paper investigates the construction
and impact of camouflaged jailbreak prompts, emphasizing their deceptive
characteristics and the limitations of traditional keyword-based detection
methods. We introduce a novel benchmark dataset, Camouflaged Jailbreak Prompts,
containing 500 curated examples (400 harmful and 100 benign prompts) designed
to rigorously stress-test LLM safety protocols. In addition, we propose a
multi-faceted evaluation framework that measures harmfulness across seven
dimensions: Safety Awareness, Technical Feasibility, Implementation Safeguards,
Harmful Potential, Educational Value, Content Quality, and Compliance Score.
Our findings reveal a stark contrast in LLM behavior: while models demonstrate
high safety and content quality with benign inputs, they exhibit a significant
decline in performance and safety when confronted with camouflaged jailbreak
attempts. This disparity underscores a pervasive vulnerability, highlighting
the urgent need for more nuanced and adaptive security strategies to ensure the
responsible and robust deployment of LLMs in real-world applications.

</details>


### [16] [What is Cybersecurity in Space?](https://arxiv.org/abs/2509.05496)
*Charbel Mattar,Jacques Bou Abdo,Abdallah Makhoul,Benoit Piranda,Jacques Demerjian*

Main category: cs.CR

TL;DR: 这篇论文分析了当前太空网络安全的漏洞，指出了11个关键研究空白，包括安全路由、机载入侵检测、恢复方法等，并提出了以多代理AI为核心的解决方案和五年发展路线图。


<details>
  <summary>Details</summary>
Motivation: 太空基础设施（卫星、无人机、5G太空链路）支撑着空中交通、金融、气象等关键服务，但设计时并未考虑现代网络威胁，存在地面站被破、GPS干扰、供应链威胁等安全风险，且缺乏统一的漏洞数据库和安全测试环境。

Method: 论文通过系统性分析指出了11个关键研究空白，对每个问题进行了挑战分析、重要性说明和研究问题提出。同时提出了以多代理AI为核心的解决方案，即使用小型任务专用代理分担防御任务，而非依赖单一大型模型。

Result: 论文系统地描述了太空网络安全领域的关键挑战，包括安全路由、入侵检测、恢复方法、可信供应链、后量子加密、零信任架构和实时影响监控等重点问题。

Conclusion: 论文提出了一个五年发展路线图，包括开展后量子和量子加密飞行试验、建立开政式网络安全测试范围、明确漏洞共享机制以及早期多代理部署。这些措施将推动太空网络安全从被动补丁转向主动弹性防御。

Abstract: Satellites, drones, and 5G space links now support
  critical services such as air traffic, finance, and weather. Yet most
  were not built to resist modern cyber threats. Ground stations
  can be breached, GPS jammed, and supply chains compromised,
  while no shared list of vulnerabilities or safe testing range exists.
  This paper maps eleven research gaps, including secure
  routing, onboard intrusion detection, recovery methods, trusted
  supply chains, post-quantum encryption, zero-trust architectures,
  and real-time impact monitoring. For each, we outline the
  challenge, why it matters, and a guiding research question. We
  also highlight an agentic (multi-agent) AI approach where small,
  task-specific agents share defense tasks onboard instead of one
  large model.
  Finally, we propose a five-year roadmap: post-quantum and
  QKD flight trials, open cyber-ranges, clearer vulnerability shar ing, and
early multi-agent deployments. These steps move space
  cybersecurity from reactive patching toward proactive resilience.

</details>


### [17] [Secure and Efficient $L^p$-Norm Computation for Two-Party Learning Applications](https://arxiv.org/abs/2509.05552)
*Ali Arastehfard,Weiran Liu,Joshua Lee,Bingyu Liu,Xuegang Ban,Yuan Hong*

Main category: cs.CR

TL;DR: 这篇论文提出了首个综合性安全两方L^p范数计算框架Crypto-L^p，支持L^1、L^2和L^∞范数，在运行时间和通信开销方面显著优于现有方案，并成功应用于安全机器学习推理任务。


<details>
  <summary>Details</summary>
Motivation: 现有加密系统缺乏通用的安全L^p范数计算框架，大多数只关注L^2范数，而忽略了L^1和L^∞等在实际应用中常用的范数类型，且属于黑盒过程而无法优化性能。

Method: 设计了一个综合性的安全两方L^p范数计算框架Crypto-L^p，支持L^1、L^2和L^∞范数计算，通过专门为此设计的加密协议来优化性能。

Result: 在运行时间方面对于p=1,2,∞分别提升82倍、271倍和42倍的改善，通信开销降低36倍、4倍和21倍。在安全机器学习推理中通信成本降低3倍。

Conclusion: Crypto-L^p框架为安全L^p范数计算提供了一个高效的综合性解决方案，在性能方面显著优于现有方案，并成功应用于实际场景中。

Abstract: Secure norm computation is becoming increasingly important in many real-world
learning applications. However, existing cryptographic systems often lack a
general framework for securely computing the $L^p$-norm over private inputs
held by different parties. These systems often treat secure norm computation as
a black-box process, neglecting to design tailored cryptographic protocols that
optimize performance. Moreover, they predominantly focus on the $L^2$-norm,
paying little attention to other popular $L^p$-norms, such as $L^1$ and
$L^\infty$, which are commonly used in practice, such as machine learning tasks
and location-based services.
  To our best knowledge, we propose the first comprehensive framework for
secure two-party $L^p$-norm computations ($L^1$, $L^2$, and $L^\infty$),
denoted as \mbox{Crypto-$L^p$}, designed to be versatile across various
applications. We have designed, implemented, and thoroughly evaluated our
framework across a wide range of benchmarking applications, state-of-the-art
(SOTA) cryptographic protocols, and real-world datasets to validate its
effectiveness and practical applicability. In summary, \mbox{Crypto-$L^p$}
outperforms prior works on secure $L^p$-norm computation, achieving $82\times$,
$271\times$, and $42\times$ improvements in runtime while reducing
communication overhead by $36\times$, $4\times$, and $21\times$ for $p=1$, $2$,
and $\infty$, respectively. Furthermore, we take the first step in adapting our
Crypto-$L^p$ framework for secure machine learning inference, reducing
communication costs by $3\times$ compared to SOTA systems while maintaining
comparable runtime and accuracy.

</details>


### [18] [Cross-Service Threat Intelligence in LLM Services using Privacy-Preserving Fingerprints](https://arxiv.org/abs/2509.05608)
*Waris Gill,Natalie Isak,Matthew Dressman*

Main category: cs.CR

TL;DR: BinaryShield是一个隐私保护的威胁情报系统，通过PII脱敏、语义嵌入、二进制量化和随机响应机制生成不可逆的攻击指纹，实现跨合规边界的安全共享，在保持攻击模式的同时保护隐私。


<details>
  <summary>Details</summary>
Motivation: 企业部署多个LLM服务处理数十亿查询，但由于监管合规边界限制，无法共享提示注入攻击的威胁情报，导致攻击在一个服务中被检测到后，在其他服务中可能持续数月未被发现。

Method: 采用独特的处理流程：PII脱敏、语义嵌入、二进制量化和随机响应机制，生成非可逆的指纹，既保留攻击模式又提供隐私保护。

Result: F1-score达到0.94，显著优于隐私保护基线SimHash（0.77），同时实现64倍存储减少和38倍更快的相似性搜索。

Conclusion: BinaryShield有效解决了LLM服务间威胁情报共享的隐私合规问题，在保持高检测性能的同时大幅提升效率和降低存储成本。

Abstract: The widespread deployment of LLMs across enterprise services has created a
critical security blind spot. Organizations operate multiple LLM services
handling billions of queries daily, yet regulatory compliance boundaries
prevent these services from sharing threat intelligence about prompt injection
attacks, the top security risk for LLMs. When an attack is detected in one
service, the same threat may persist undetected in others for months, as
privacy regulations prohibit sharing user prompts across compliance boundaries.
  We present BinaryShield, the first privacy-preserving threat intelligence
system that enables secure sharing of attack fingerprints across compliance
boundaries. BinaryShield transforms suspicious prompts through a unique
pipeline combining PII redaction, semantic embedding, binary quantization, and
randomized response mechanism to potentially generate non-invertible
fingerprints that preserve attack patterns while providing privacy. Our
evaluations demonstrate that BinaryShield achieves an F1-score of 0.94,
significantly outperforming SimHash (0.77), the privacy-preserving baseline,
while achieving 64x storage reduction and 38x faster similarity search compared
to dense embeddings.

</details>


### [19] [FuzzBox: Blending Fuzzing into Emulation for Binary-Only Embedded Targets](https://arxiv.org/abs/2509.05643)
*Carmine Cesarano,Roberto Natella*

Main category: cs.CR

TL;DR: FuzzBox是一个结合模拟和模糊测试的工具，通过动态插桩在虚拟化环境中执行代码，无需源代码重新编译即可进行模糊测试和覆盖率分析，解决了工业系统和闭源软件的测试难题。


<details>
  <summary>Details</summary>
Motivation: 传统覆盖率引导的模糊测试需要编译时插桩，但工业系统通常使用专有闭源编译器工具链且无法获取源代码，导致难以应用现有模糊测试方法。

Method: FuzzBox将模拟与模糊测试相结合，在虚拟化环境中动态插桩执行代码，实现模糊输入注入、故障检测和覆盖率分析，无需源代码重新编译和硬件特定依赖。

Result: 在专有MILS hypervisor上的实验证明了FuzzBox的有效性，同时在商业IoT固件上的分析展示了其广泛的移植性。

Conclusion: FuzzBox为工业系统和闭源软件的模糊测试提供了可行的解决方案，具有很好的实用性和移植性。

Abstract: Coverage-guided fuzzing has been widely applied to address zero-day
vulnerabilities in general-purpose software and operating systems. This
approach relies on instrumenting the target code at compile time. However,
applying it to industrial systems remains challenging, due to proprietary and
closed-source compiler toolchains and lack of access to source code. FuzzBox
addresses these limitations by integrating emulation with fuzzing: it
dynamically instruments code during execution in a virtualized environment, for
the injection of fuzz inputs, failure detection, and coverage analysis, without
requiring source code recompilation and hardware-specific dependencies. We show
the effectiveness of FuzzBox through experiments in the context of a
proprietary MILS (Multiple Independent Levels of Security) hypervisor for
industrial applications. Additionally, we analyze the applicability of FuzzBox
across commercial IoT firmware, showcasing its broad portability.

</details>


### [20] [SEASONED: Semantic-Enhanced Self-Counterfactual Explainable Detection of Adversarial Exploiter Contracts](https://arxiv.org/abs/2509.05681)
*Xng Ai,Shudan Lin,Zecheng Li,Kai Zhou,Bixin Li,Bin Xiao*

Main category: cs.CR

TL;DR: SEASONED是一个用于检测DeFi攻击中对抗性利用合约(AECs)的自解释框架，通过语义关系图和自反事实可解释检测器实现高效检测和解释生成。


<details>
  <summary>Details</summary>
Motivation: 现有的DeFi攻击检测方法难以捕捉语义依赖关系且缺乏可解释性，限制了检测效果并在AEC分析中留下关键知识空白。

Method: 从合约字节码提取语义信息构建语义关系图(SRG)，使用自反事实可解释检测器(SCFED)对SRG进行分类并生成突出核心攻击逻辑的解释。

Result: 理论和实验结果都证明SEASONED具有出色的检测性能、鲁棒性、泛化能力和数据效率学习能力，并发布了包含359个AEC的新数据集。

Conclusion: SEASONED框架有效解决了DeFi攻击检测中的语义依赖捕捉和可解释性问题，为AEC分析提供了新的解决方案和研究基础。

Abstract: Decentralized Finance (DeFi) attacks have resulted in significant losses,
often orchestrated through Adversarial Exploiter Contracts (AECs) that exploit
vulnerabilities in victim smart contracts. To proactively identify such
threats, this paper targets the explainable detection of AECs.
  Existing detection methods struggle to capture semantic dependencies and lack
interpretability, limiting their effectiveness and leaving critical knowledge
gaps in AEC analysis. To address these challenges, we introduce SEASONED, an
effective, self-explanatory, and robust framework for AEC detection.
  SEASONED extracts semantic information from contract bytecode to construct a
semantic relation graph (SRG), and employs a self-counterfactual explainable
detector (SCFED) to classify SRGs and generate explanations that highlight the
core attack logic. SCFED further enhances robustness, generalizability, and
data efficiency by extracting representative information from these
explanations. Both theoretical analysis and experimental results demonstrate
the effectiveness of SEASONED, which showcases outstanding detection
performance, robustness, generalizability, and data efficiency learning
ability. To support further research, we also release a new dataset of 359
AECs.

</details>


### [21] [KnowHow: Automatically Applying High-Level CTI Knowledge for Interpretable and Accurate Provenance Analysis](https://arxiv.org/abs/2509.05698)
*Yuhan Meng,Shaofei Li,Jiaping Gui,Peng Jiang,Ding Li*

Main category: cs.CR

TL;DR: KnowHow是一个CTI知识驱动的在线溯源分析方法，能够自动将自然语言CTI报告中的高级攻击知识应用于低级系统事件检测，有效解决APT攻击检测中的语义鸿沟问题


<details>
  <summary>Details</summary>
Motivation: 解决CTI报告中高级自然语言知识与低级安全日志之间的语义鸿沟问题，避免人工方法的劳动密集和易出错

Method: 提出新颖的攻击知识表示gIoC，通过将系统标识符提升为自然语言术语，匹配系统事件到gIoC，进而匹配到自然语言描述的技术，基于时间逻辑推理攻击步骤

Result: 在开源和工业数据集中准确检测所有16个APT活动，最多减少90%的节点级误报，同时具有更高的节点级召回率，对未知攻击和模拟攻击具有鲁棒性

Conclusion: KnowHow方法有效填补了高级CTI知识与低级系统事件之间的语义鸿沟，为APT攻击检测提供了自动化的解决方案，显著优于现有方法

Abstract: High-level natural language knowledge in CTI reports, such as the ATT&CK
framework, is beneficial to counter APT attacks. However, how to automatically
apply the high-level knowledge in CTI reports in realistic attack detection
systems, such as provenance analysis systems, is still an open problem. The
challenge stems from the semantic gap between the knowledge and the low-level
security logs: while the knowledge in CTI reports is written in natural
language, attack detection systems can only process low-level system events
like file accesses or network IP manipulations. Manual approaches can be
labor-intensive and error-prone.
  In this paper, we propose KnowHow, a CTI-knowledge-driven online provenance
analysis approach that can automatically apply high-level attack knowledge from
CTI reports written in natural languages to detect low-level system events. The
core of KnowHow is a novel attack knowledge representation, gIoC, that
represents the subject, object, and actions of attacks. By lifting system
identifiers, such as file paths, in system events to natural language terms,
KnowHow can match system events to gIoC and further match them to techniques
described in natural languages. Finally, based on the techniques matched to
system events, KnowHow reasons about the temporal logic of attack steps and
detects potential APT attacks in system events. Our evaluation shows that
KnowHow can accurately detect all 16 APT campaigns in the open-source and
industrial datasets, while existing approaches all introduce large numbers of
false positives. Meanwhile, our evaluation also shows that KnowHow reduces at
most 90% of node-level false positives while having a higher node-level recall
and is robust against several unknown attacks and mimicry attacks.

</details>


### [22] [Larger-scale Nakamoto-style Blockchains Offer Better Security](https://arxiv.org/abs/2509.05708)
*Junjie Hu,Na Ruan*

Main category: cs.CR

TL;DR: 这篇论文通过双延迟框架重新分析Nakamoto格式区块链安全性，证明了恶意节点通信延迟可以提高安全阈值，突破50%的传统假设，为大规模区块链协议优化提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 传统区块链安全模型过估了恶意节点的协调能力，假设恶意节点可以即时同步，忽视了内部通信延迟对安全性的关键影响。

Method: 提出双延迟框架：静态延迟模型通过M/D/1排队模型量化恶意通信延迟对私链增长率的刻制；动态延迟模型集成概率性腐化和规模相关延迟来形容总体恶意延迟窗口。

Result: 静态模型显示安全阈值随恶意延迟增加而提高，当恶意延迟大于诚实延迟时甚至超过经典51%界限；动态模型显示恶意能力随网络规模线性衰减，当n趋于无穷时恶意能力低于阈值的概率趋近1。

Conclusion: 通过曝光网络规模、通信延迟和能力稀量之间的相互作用，为大规模Nakamoto格式区块链的协议优化和强镀性评估提供了理论基础。

Abstract: Traditional security models for Nakamoto-style blockchains overestimate
adversarial coordination by assuming instantaneous synchronization among
malicious nodes, neglecting the critical impact of internal communication
delays on security. This paper introduces a dual-delay framework to revisit
security analysis, addressing this oversight through two key innovations.
First, the static delay model quantifies how adversarial communication delays
(\(\Delta_a\)) constrain the effective growth rate of private chains, derived
via an M/D/1 queuing model as \(\lambda_{eff} = \lambda_a / (1 + \lambda_a
\Delta_a)\). This model reveals that the security threshold (\(\beta^*\)), the
maximum adversarial power the system tolerates, increases with \(\Delta_a\),
even exceeding the classic 51\% boundary when \(\Delta_a \textgreater \Delta\)
(honest nodes' delay), breaking the long-standing 50\% assumption. Second, the
dynamic delay model integrates probabilistic corruption and scale-dependent
delays to characterize the total adversarial delay window (\(\Delta_{total} =
\Delta(n) e^{-k\beta} + c \log(1 + \beta n)\)), where \(\Delta(n) \in
\Theta(\log n)\) captures honest nodes' logarithmic delay growth. Asymptotic
analysis shows adversarial power decays linearly with network scale, ensuring
the probability of \(\beta \leq \beta^*\) approaches 1 as \(n \to \infty\). By
exposing the interplay between network scale, communication delays, and power
dilution, we provide a theoretical foundation for optimizing consensus
protocols and assessing robustness in large-scale Nakamoto-style blockchains.

</details>


### [23] [Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated](https://arxiv.org/abs/2509.05739)
*Hanna Foerster,Ilia Shumailov,Yiren Zhao,Harsh Chaudhari,Jamie Hayes,Robert Mullins,Yarin Gal*

Main category: cs.CR

TL;DR: 本文提出了一种针对大型语言模型的分解推理投毒攻击，通过修改推理路径而非最终答案来注入后门，但发现由于模型推理能力的增强和架构分离，这种攻击难以可靠地改变最终答案。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型加入逐步推理功能，攻击面扩展到中间推理链，研究者希望探索更隐蔽的投毒方式，通过仅修改推理路径而不改变提示和最终答案来实现后门注入。

Method: 提出"分解推理投毒"方法，将触发器分散到多个无害组件中，只修改推理路径（chain-of-thought），保持提示和最终答案的清洁。

Result: 虽然可以成功注入这种分解投毒，但可靠地激活它们来改变最终答案（而不仅仅是推理过程）非常困难，因为模型通常能够从思维过程中的后门激活中恢复。

Conclusion: 先进大型语言模型的推理能力以及推理与最终答案生成的架构分离，似乎产生了一种新兴的后门鲁棒性形式，使得这类攻击难以有效改变模型输出。

Abstract: Early research into data poisoning attacks against Large Language Models
(LLMs) demonstrated the ease with which backdoors could be injected. More
recent LLMs add step-by-step reasoning, expanding the attack surface to include
the intermediate chain-of-thought (CoT) and its inherent trait of decomposing
problems into subproblems. Using these vectors for more stealthy poisoning, we
introduce ``decomposed reasoning poison'', in which the attacker modifies only
the reasoning path, leaving prompts and final answers clean, and splits the
trigger across multiple, individually harmless components.
  Fascinatingly, while it remains possible to inject these decomposed poisons,
reliably activating them to change final answers (rather than just the CoT) is
surprisingly difficult. This difficulty arises because the models can often
recover from backdoors that are activated within their thought processes.
Ultimately, it appears that an emergent form of backdoor robustness is
originating from the reasoning capabilities of these advanced LLMs, as well as
from the architectural separation between reasoning and final answer
generation.

</details>


### [24] [Tell-Tale Watermarks for Explanatory Reasoning in Synthetic Media Forensics](https://arxiv.org/abs/2509.05753)
*Ching-Chun Chang,Isao Echizen*

Main category: cs.CR

TL;DR: 该研究开发了一种可解释的水印系统来追踪合成媒体的生成链，通过特制水印在不同变换中的可解释响应来推断媒体经历的变换历史


<details>
  <summary>Details</summary>
Motivation: 合成媒体的兴起模糊了现实与伪造的界限，多种编辑应用使取证分析复杂化，需要重建事件链以揭示犯罪意图的证据

Method: 开发了tell-tale水印系统，针对不同类型的变换定制水印，使其在变换中产生可解释的痕迹，然后通过解释性推理推断最可能的变换组合

Result: 实验评估证明了该水印系统在保真度、同步性和可追踪性方面的有效性

Conclusion: 该方法为合成媒体的取证分析提供了一种有效的可解释水印解决方案，能够重建媒体生成链并推断变换历史

Abstract: The rise of synthetic media has blurred the boundary between reality and
fabrication under the evolving power of artificial intelligence, fueling an
infodemic that erodes public trust in cyberspace. For digital imagery, a
multitude of editing applications further complicates the forensic analysis,
including semantic edits that alter content, photometric adjustments that
recalibrate colour characteristics, and geometric projections that reshape
viewpoints. Collectively, these transformations manipulate and control
perceptual interpretation of digital imagery. This susceptibility calls for
forensic enquiry into reconstructing the chain of events, thereby revealing
deeper evidential insight into the presence or absence of criminal intent. This
study seeks to address an inverse problem of tracing the underlying generation
chain that gives rise to the observed synthetic media. A tell-tale watermarking
system is developed for explanatory reasoning over the nature and extent of
transformations across the lifecycle of synthetic media. Tell-tale watermarks
are tailored to different classes of transformations, responding in a manner
that is neither strictly robust nor fragile but instead interpretable. These
watermarks function as reference clues that evolve under the same
transformation dynamics as the carrier media, leaving interpretable traces when
subjected to transformations. Explanatory reasoning is then performed to infer
the most plausible account across the combinatorial parameter space of
composite transformations. Experimental evaluations demonstrate the validity of
tell-tale watermarking with respect to fidelity, synchronicity and
traceability.

</details>


### [25] [Exploit Tool Invocation Prompt for Tool Behavior Hijacking in LLM-Based Agentic System](https://arxiv.org/abs/2509.05755)
*Yu Liu,Yuchong Xie,Mingyu Luo,Zesen Liu,Zhixiang Zhang,Kaikai Zhang,Zongjie Li,Ping Chen,Shuai Wang,Dongdong She*

Main category: cs.CR

TL;DR: 本文研究发现基于LLM的智能代理系统中的工具调用提示（TIP）存在严重安全漏洞，攻击者可通过操纵工具调用来实现远程代码执行和拒绝服务攻击，并提出了相应的防御机制。


<details>
  <summary>Details</summary>
Motivation: LLM智能代理系统广泛用于处理用户查询和执行复杂任务，但工具调用提示（TIP）作为关键组件，其安全性一直被忽视，存在严重的安全风险。

Method: 通过系统化的TIP利用工作流（TEW）进行研究，分析主流LLM系统（如Cursor、Claude Code等）的工具调用漏洞，并演示外部工具行为劫持攻击。

Result: 发现多个主流LLM系统存在远程代码执行（RCE）和拒绝服务（DoS）等安全漏洞，攻击者能够通过操纵工具调用来劫持系统行为。

Conclusion: TIP安全性是LLM智能代理系统的重要安全问题，需要采取有效的防御机制来增强系统安全性，防止工具调用被恶意利用。

Abstract: LLM-based agentic systems leverage large language models to handle user
queries, make decisions, and execute external tools for complex tasks across
domains like chatbots, customer service, and software engineering. A critical
component of these systems is the Tool Invocation Prompt (TIP), which defines
tool interaction protocols and guides LLMs to ensure the security and
correctness of tool usage. Despite its importance, TIP security has been
largely overlooked. This work investigates TIP-related security risks,
revealing that major LLM-based systems like Cursor, Claude Code, and others are
vulnerable to attacks such as remote code execution (RCE) and denial of service
(DoS). Through a systematic TIP exploitation workflow (TEW), we demonstrate
external tool behavior hijacking via manipulated tool invocations. We also
propose defense mechanisms to enhance TIP security in LLM-based agentic
systems.

</details>


### [26] [Secure and Trustful Cross-domain Communication with Decentralized Identifiers in 5G and Beyond](https://arxiv.org/abs/2509.05797)
*Hai Dinh-Tuan,Sandro Rodriguez Garzon,Jianeng Fu*

Main category: cs.CR

TL;DR: 本文提出在5G及后续移动网络中使用W3C去中心化标识符(DIDs)建立网络功能间的安全可信通信通道，通过新的通信代理和DID应用层传输协议确保跨域交互的机密性、完整性和真实性。


<details>
  <summary>Details</summary>
Motivation: 未来移动网络需要安全可信的通信方式来支持不同网络域核心网组件之间的动态交互，传统TCP/TLS存在安全局限性。

Method: 引入新的通信代理与5G标准化网络功能集成，采用DID应用层传输协议，对两个不同版本的DID通信协议进行对比分析。

Result: 最新协议版本具有兼容性优势，与传统TCP/TLS相比虽然带来通信开销但显著提升了通信安全性，显示出DID通信在未来移动网络中的潜力。

Conclusion: DID-based通信为未来移动网络提供了安全增强方案，但需要在性能优化方面进一步改进以平衡安全性和效率。

Abstract: In the evolving landscape of future mobile networks, there is a critical need
for secure and trustful communication modalities to support dynamic
interactions among core network components of different network domains. This
paper proposes the application of W3C-endorsed Decentralized Identifiers (DIDs)
to establish secure and trustful communication channels among network functions
in 5G and subsequent generations. A new communication agent is introduced that
integrates seamlessly with 5G-standardized network functions and utilizes a
DID-based application layer transport protocol to ensure confidentiality,
integrity, and authenticity for cross-domain interactions. A comparative
analysis of the two different versions of the DID-based communication protocol
for inter network function communication reveals compatibility advantages of
the latest protocol iteration. Furthermore, a comprehensive evaluation of the
communication overhead caused by both protocol iterations compared to
traditional TCP/TLS shows the benefits of using DIDs to improve communication
security, albeit with performance loses compared to TCP/TLS. These results
uncover the potential of DID-based communication for future mobile networks but
also point out areas for optimization.

</details>


### [27] [Decoding Latent Attack Surfaces in LLMs: Prompt Injection via HTML in Web Summarization](https://arxiv.org/abs/2509.05831)
*Ishaan Verma*

Main category: cs.CR

TL;DR: 研究发现HTML隐藏元素（如meta标签、aria-label等）可被用于对LLM进行提示注入攻击，影响内容摘要结果，Llama 4 Scout受影响率29%，Gemma 9B IT为15%


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地集成到网页内容摘要系统中，其对提示注入攻击的脆弱性成为一个紧迫问题，特别是非可见HTML元素的潜在威胁尚未得到充分研究

Method: 创建包含280个静态网页的数据集（一半正常一半注入攻击），使用浏览器自动化管道提取HTML和渲染文本，评估Llama 4 Scout和Gemma 9B IT模型的摘要能力，采用ROUGE-L和SBERT余弦相似度指标及人工标注

Result: 29%的注入样本导致Llama 4 Scout摘要明显变化，Gemma 9B IT的成功率为15%，表明隐藏对抗内容能够微妙地操纵模型输出

Conclusion: 研究揭示了LLM驱动网络管道中存在关键但被忽视的漏洞，提供了可复现的HTML提示注入评估框架，强调在涉及网页内容的LLM应用中需要强大的缓解策略

Abstract: Large Language Models (LLMs) are increasingly integrated into web-based
systems for content summarization, yet their susceptibility to prompt injection
attacks remains a pressing concern. In this study, we explore how non-visible
HTML elements such as <meta>, aria-label, and alt attributes can be exploited
to embed adversarial instructions without altering the visible content of a
webpage. We introduce a novel dataset comprising 280 static web pages, evenly
divided between clean and adversarial injected versions, crafted using diverse
HTML-based strategies. These pages are processed through a browser automation
pipeline to extract both raw HTML and rendered text, closely mimicking
real-world LLM deployment scenarios. We evaluate two state-of-the-art
open-source models, Llama 4 Scout (Meta) and Gemma 9B IT (Google), on their
ability to summarize this content. Using both lexical (ROUGE-L) and semantic
(SBERT cosine similarity) metrics, along with manual annotations, we assess the
impact of these covert injections. Our findings reveal that over 29% of
injected samples led to noticeable changes in the Llama 4 Scout summaries,
while Gemma 9B IT showed a lower, yet non-trivial, success rate of 15%. These
results highlight a critical and largely overlooked vulnerability in LLM driven
web pipelines, where hidden adversarial content can subtly manipulate model
outputs. Our work offers a reproducible framework and benchmark for evaluating
HTML-based prompt injection and underscores the urgent need for robust
mitigation strategies in LLM applications involving web content.

</details>


### [28] [Yours or Mine? Overwriting Attacks against Neural Audio Watermarking](https://arxiv.org/abs/2509.05835)
*Lingfeng Yao,Chenpei Huang,Shengyao Wang,Junpei Xue,Hanqing Guo,Jiang Liu,Phone Lin,Tomoaki Ohtsuki,Miao Pan*

Main category: cs.CR

TL;DR: 本文提出了一种针对神经音频水印系统的覆盖攻击方法，能够在白盒、灰盒和黑盒三种场景下有效覆盖原始水印，攻击成功率接近100%，暴露了现有音频水印系统的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着生成式音频模型的快速发展，AI生成音频引发了版权侵权和虚假信息传播的担忧。当前神经音频水印方法主要关注水印的不可感知性和鲁棒性，但忽视了其面对安全攻击的脆弱性。

Method: 开发了一种简单但强大的覆盖攻击方法，根据攻击者拥有的水印信息量，提出了白盒、灰盒和黑盒三种攻击类别，并在最先进的神经音频水印方法上进行了全面评估。

Result: 实验结果表明，所提出的覆盖攻击能够有效破坏现有水印方案，在各种设置下实现接近100%的攻击成功率。

Conclusion: 覆盖攻击的实用性和有效性暴露了现有神经音频水印系统的安全缺陷，强调了在未来音频水印设计中需要增强安全性。

Abstract: As generative audio models are rapidly evolving, AI-generated audios
increasingly raise concerns about copyright infringement and misinformation
spread. Audio watermarking, as a proactive defense, can embed secret messages
into audio for copyright protection and source verification. However, current
neural audio watermarking methods focus primarily on the imperceptibility and
robustness of watermarking, while ignoring its vulnerability to security
attacks. In this paper, we develop a simple yet powerful attack: the
overwriting attack that overwrites the legitimate audio watermark with a forged
one and makes the original legitimate watermark undetectable. Based on the
audio watermarking information that the adversary has, we propose three
categories of overwriting attacks, i.e., white-box, gray-box, and black-box
attacks. We also thoroughly evaluate the proposed attacks on state-of-the-art
neural audio watermarking methods. Experimental results demonstrate that the
proposed overwriting attacks can effectively compromise existing watermarking
schemes across various settings and achieve a nearly 100% attack success rate.
The practicality and effectiveness of the proposed overwriting attacks expose
security flaws in existing neural audio watermarking systems, underscoring the
need to enhance security in future audio watermarking designs.

</details>


### [29] [Multimodal Prompt Injection Attacks: Risks and Defenses for Modern LLMs](https://arxiv.org/abs/2509.05883)
*Andrew Yeo,Daeseon Choi*

Main category: cs.CR

TL;DR: 大语言模型存在严重的安全漏洞，通过8款商业模型的测试发现他们容易受到提示注射攻击，虽然Claude 3表现相对更好，但仍需要额外的防御措施


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在各行业的广泛部署，安全风险日益突出，特别是提示注射和脱狱攻击导致的漏洞需要系统性评估

Method: 对8款商业模型进行实验，在没有额外清洗措施的情况下，测试四类攻击：直接注射、间接（外部）注射、图像基注射和提示泄漏

Result: 测试结果显示所有模型都存在可利用的弱点，Claude 3表现出相寰更强的稳健性，但仍不能完全防御各种注射攻击

Conclusion: 大语言模型内置的安全保护措施不足以应对现实中的安全威胁，必须采取输入标准化等额外防御措施来实现可靠的保护

Abstract: Large Language Models (LLMs) have seen rapid adoption in recent years, with
industries increasingly relying on them to maintain a competitive advantage.
These models excel at interpreting user instructions and generating human-like
responses, leading to their integration across diverse domains, including
consulting and information retrieval. However, their widespread deployment also
introduces substantial security risks, most notably in the form of prompt
injection and jailbreak attacks.
  To systematically evaluate LLM vulnerabilities -- particularly to external
prompt injection -- we conducted a series of experiments on eight commercial
models. Each model was tested without supplementary sanitization, relying
solely on its built-in safeguards. The results exposed exploitable weaknesses
and emphasized the need for stronger security measures. Four categories of
attacks were examined: direct injection, indirect (external) injection,
image-based injection, and prompt leakage. Comparative analysis indicated that
Claude 3 demonstrated relatively greater robustness; nevertheless, empirical
findings confirm that additional defenses, such as input normalization, remain
necessary to achieve reliable protection.

</details>


### [30] [Introduction to Number Theoretic Transform](https://arxiv.org/abs/2509.05884)
*Banhirup Sengupta,Peenal Gupta,Souvik Sengupta*

Main category: cs.CR

TL;DR: 本文介绍了数论变换(NTT)作为离散傅里叶变换的变体，在格基密码学中用于多项式乘法加速，将复杂度从二次降低到拟线性。


<details>
  <summary>Details</summary>
Motivation: NTT在后量子密码学和同态加密中具有重要作用，需要开发快速算法来提高多项式乘法的效率。

Method: 提出了循环卷积、负循环卷积的概念，以及NTT及其逆变换的快速版本(快速NTT)。

Result: 快速NTT算法成功将多项式乘法的计算复杂度从O(n²)降低到O(n log n)的拟线性复杂度。

Conclusion: NTT及其快速算法是格基密码学中的重要数学工具，能够显著提升多项式运算的效率，对后量子密码学发展具有重要意义。

Abstract: The Number Theoretic Transform (NTT) can be regarded as a variant of the
Discrete Fourier Transform. NTT has been quite a powerful mathematical tool in
developing Post-Quantum Cryptography and Homomorphic Encryption. The Fourier
Transform essentially decomposes a signal into its frequencies. They are
traditionally sine or cosine waves. NTT works more over groups or finite fields
rather than on a continuous signal and polynomials work as the analog of sine
waves in case of NTT. Fast Fourier Trnasform (FFT) style NTT or fast NTT has
been proven to be useful in lattice-based cryptography due to its ability to
reduce the complexity of polynomial multiplication from quadratic to
quasilinear. We have introduced the concepts of cyclic, negacyclic convolutions
along with NTT and its inverse and their fast versions.

</details>


### [31] [MemTraceDB: Reconstructing MySQL User Activity Using ActiviTimeTrace Algorithm](https://arxiv.org/abs/2509.05891)
*Mahfuzul I. Nissan*

Main category: cs.CR

TL;DR: MemTraceDB是一个通过分析MySQL进程内存快照来重建用户活动时间线的工具，能够绕过被篡改的磁盘日志，提供可靠的取证分析。


<details>
  <summary>Details</summary>
Motivation: 数据库审计日志和事务日志容易受到特权攻击者的篡改，内存分析提供了访问易失性证据的替代方案，即使日志文件被破坏也能获取真实用户活动信息。

Method: 使用MemTraceDB工具和ActiviTimeTrace算法，从MySQL数据库进程的原始内存快照中系统性地提取和关联用户连接和执行查询等取证证据。

Result: 实验证明MemTraceDB有效，并发现MySQL查询堆栈的操作容量约为9,997个查询，据此建立了确定内存快照采集最佳频率的数据驱动公式。

Conclusion: MemTraceDB能够独立于被破坏的磁盘日志，实现法医上可靠的用户活动重建，为调查人员提供了清晰的行动指南。

Abstract: Database audit and transaction logs are fundamental to forensic
investigations, but they are vulnerable to tampering by privileged attackers.
Malicious insiders or external threats with administrative access can alter,
purge, or temporarily disable logging mechanisms, creating significant blind
spots and rendering disk-based records unreliable. Memory analysis offers a
vital alternative, providing investigators direct access to volatile artifacts
that represent a ground-truth source of recent user activity, even when log
files have been compromised.
  This paper introduces MemTraceDB, a tool that reconstructs user activity
timelines by analyzing raw memory snapshots from the MySQL database process.
MemTraceDB utilizes a novel algorithm, ActiviTimeTrace, to systematically
extract and correlate forensic artifacts such as user connections and executed
queries. Through a series of experiments, I demonstrate MemTraceDB's
effectiveness and reveal a critical empirical finding: the MySQL query stack
has a finite operational capacity of approximately 9,997 queries. This
discovery allows me to establish a practical, data-driven formula for
determining the optimal frequency for memory snapshot collection, providing a
clear, actionable guideline for investigators. The result is a
forensically-sound reconstruction of user activity, independent of compromised
disk-based logs.

</details>


### [32] [Wrangling Entropy: Next-Generation Multi-Factor Key Derivation, Credential Hashing, and Credential Generation Functions](https://arxiv.org/abs/2509.05893)
*Colin Roberts,Vivek Nair,Dawn Song*

Main category: cs.CR

TL;DR: 提出了ESTMF密码分析框架来检测多因素密钥派生函数MFKDF的安全漏洞，并基于此构建了更安全的MFKDF2版本


<details>
  <summary>Details</summary>
Motivation: 原始MFKDF虽然提供了多因素认证的密钥管理方案，但在多次调用中存在安全性退化问题，需要新的密码分析方法和更安全的构造

Method: 开发了熵状态转移建模框架(ESTMF)来检测密码函数中的熵泄露，识别MFKDF漏洞，并基于此设计新的MFKDF2构造

Result: ESTMF成功识别了原始MFKDF的所有已知漏洞，MFKDF2被证明具有端到端安全性，支持更多认证因素和可用性特性

Conclusion: 该研究不仅解决了MFKDF的安全问题，还为未来密钥派生函数的构建提供了可推广的最佳实践

Abstract: The Multi-Factor Key Derivation Function (MFKDF) offered a novel solution to
the classic problem of usable client-side key management by incorporating
multiple popular authentication factors into a key derivation process, but was
later shown to be vulnerable to cryptanalysis that degraded its security over
multiple invocations. In this paper, we present the Entropy State Transition
Modeling Framework (ESTMF), a novel cryptanalytic technique designed to reveal
pernicious leaks of entropy across multiple invocations of a cryptographic key
derivation or hash function, and show that it can be used to correctly identify
each of the known vulnerabilities in the original MFKDF construction. We then
use these findings to propose a new construction for ``MFKDF2,'' a
next-generation multi-factor key derivation function that can be proven to be
end-to-end secure using the ESTMF. Finally, we discuss how MFKDF2 can be
extended to support more authentication factors and usability features than the
previous MFKDF construction, and derive several generalizable best-practices
for the construction of new KDFs in the future.

</details>


### [33] [Dataset Ownership in the Era of Large Language Models](https://arxiv.org/abs/2509.05921)
*Kun Li,Cheng Wang,Minghui Xu,Yue Zhang,Xiuzhen Cheng*

Main category: cs.CR

TL;DR: 数据集版权技术保护方法综述，主要分为非侵入式、轻量侵入式和重度侵入式三类方法，分析各自优缺点和研究挑战


<details>
  <summary>Details</summary>
Motivation: 随着数据集成为机器学习系统的关键资产，确保坚固的版权保护成为紧迫挑战，传统法律机制难以应对数字数据复制和未授权使用的技术复杂性

Method: 系统性综述技术方法，将其分为三类：1)非侵入式方法（不修改数据检测未授权使用）；2)轻量侵入式方法（嵌入可逆轻量变更以验证所权）；3)重度侵入式方法（使用具有攻击性的可逆对抗示例等方法强制使用限制）

Result: 综合分析了各种技术的优势和局限性，持续关注开攻研究挑战，为当前数据集版权保护领域提供了系统化视角

Conclusion: 本研究为开发统一、可扩展且符合道德规范的解决方案提供了组织化视角，以保护日益复杂的机器学习生态系统中的数据集

Abstract: As datasets become critical assets in modern machine learning systems,
ensuring robust copyright protection has emerged as an urgent challenge.
Traditional legal mechanisms often fail to address the technical complexities
of digital data replication and unauthorized use, particularly in opaque or
decentralized environments. This survey provides a comprehensive review of
technical approaches for dataset copyright protection, systematically
categorizing them into three main classes: non-intrusive methods, which detect
unauthorized use without modifying data; minimally-intrusive methods, which
embed lightweight, reversible changes to enable ownership verification; and
maximally-intrusive methods, which apply aggressive data alterations, such as
reversible adversarial examples, to enforce usage restrictions. We synthesize
key techniques, analyze their strengths and limitations, and highlight open
research challenges. This work offers an organized perspective on the current
landscape and suggests future directions for developing unified, scalable, and
ethically sound solutions to protect datasets in increasingly complex machine
learning ecosystems.

</details>


### [34] [DCMI: A Differential Calibration Membership Inference Attack Against Retrieval-Augmented Generation](https://arxiv.org/abs/2509.06026)
*Xinyu Gao,Xiangtao Meng,Yingkai Dong,Zheng Li,Shanqing Guo*

Main category: cs.CR

TL;DR: DCMI是一种针对RAG系统的差分校准成员推理攻击方法，通过查询扰动来区分成员和非成员检索文档，显著提高了攻击效果，在实验中比基线方法提升超过40%的AUC和准确率。


<details>
  <summary>Details</summary>
Motivation: RAG系统虽然通过集成外部知识库减少了幻觉问题，但引入了成员推理攻击的脆弱性，特别是在处理敏感数据时。现有的MIA方法忽略了非成员检索文档对RAG输出的干扰，限制了攻击效果。

Method: 提出DCMI（差分校准成员推理攻击），利用成员和非成员检索文档在查询扰动下的敏感性差异，生成扰动查询进行校准，隔离成员检索文档的贡献，同时最小化非成员检索文档的干扰。

Result: 在逐步放宽假设的实验条件下，DCMI始终优于基线方法，例如在Flan-T5 RAG系统中达到97.42% AUC和94.35%准确率，比MBA基线高出40%以上。在真实RAG平台（Dify和MaxKB）上保持10%-20%的优势。

Conclusion: 研究结果突显了RAG系统中存在的重大隐私风险，强调需要更强的保护机制。呼吁社区对快速发展的RAG系统中的数据泄露风险进行更深入的调查。

Abstract: While Retrieval-Augmented Generation (RAG) effectively reduces hallucinations
by integrating external knowledge bases, it introduces vulnerabilities to
membership inference attacks (MIAs), particularly in systems handling sensitive
data. Existing MIAs targeting RAG's external databases often rely on model
responses but ignore the interference of non-member-retrieved documents on RAG
outputs, limiting their effectiveness. To address this, we propose DCMI, a
differential calibration MIA that mitigates the negative impact of
non-member-retrieved documents. Specifically, DCMI leverages the sensitivity
gap between member and non-member retrieved documents under query perturbation.
It generates perturbed queries for calibration to isolate the contribution of
member-retrieved documents while minimizing the interference from
non-member-retrieved documents. Experiments under progressively relaxed
assumptions show that DCMI consistently outperforms baselines--for example,
achieving 97.42% AUC and 94.35% Accuracy against the RAG system with Flan-T5,
exceeding the MBA baseline by over 40%. Furthermore, on real-world RAG
platforms such as Dify and MaxKB, DCMI maintains a 10%-20% advantage over the
baseline. These results highlight significant privacy risks in RAG systems and
emphasize the need for stronger protection mechanisms. We appeal to the
community's consideration of deeper investigations, like ours, against the data
leakage risks in rapidly evolving RAG systems. Our code is available at
https://github.com/Xinyu140203/RAG_MIA.

</details>


### [35] [Asymmetry Vulnerability and Physical Attacks on Online Map Construction for Autonomous Driving](https://arxiv.org/abs/2509.06071)
*Yang Lou,Haibo Hu,Qun Song,Qian Xu,Yi Zhu,Rui Tan,Wei-Bin Lee,Jianping Wang*

Main category: cs.CR

TL;DR: 本文系统性分析了在线HD地图构建模型的漏洞，发现模型存在对称性偏误，并提出了一种新的两阶段攻击框架，能够在实际场景中撤锐地图构建准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 高精度地图在自主驾驶中至关重要，但标注和维护成本高，因此在线HD地图构建成为研究热点。然而对于这种模型在恶意条件下的稳健性研究不足，需要系统性分析其漏洞。

Method: 研究发现在线地图构建模型存在内在的对称性偏误，在非对称场景中容易错误预测直线边界。基于此漏洞，提出了两阶段攻击框架：首先识别受害自动驾驶车潜在路线上的容易受攻击的非对称场景，然后优化摄像头盲光攻击和对抗补丁攻击的位置和模式。

Result: 在公开自主驾驶数据集上的评估显示，该攻击能够将地图构建准确性降低达9.9%，使44%的目标路线无法到达，并将与实际路边冲突的不安全规划轨迹率提高达27%。攻击在真实车辆测试平台上也得到了验证。

Conclusion: 这项研究首次展示了在线HD地图构建模型的漏洞，并提出了数字和物理攻击方法。根源分析显示对称性偏误来自于训练数据不平衡、模型架构和地图元素表示方式。该研究为提高在线地图构建模型的安全性和稳健性提供了重要见解。

Abstract: High-definition maps provide precise environmental information essential for
prediction and planning in autonomous driving systems. Due to the high cost of
labeling and maintenance, recent research has turned to online HD map
construction using onboard sensor data, offering wider coverage and more timely
updates for autonomous vehicles. However, the robustness of online map
construction under adversarial conditions remains underexplored. In this paper,
we present a systematic vulnerability analysis of online map construction
models, which reveals that these models exhibit an inherent bias toward
predicting symmetric road structures. In asymmetric scenes like forks or
merges, this bias often causes the model to mistakenly predict a straight
boundary that mirrors the opposite side. We demonstrate that this vulnerability
persists in the real-world and can be reliably triggered by obstruction or
targeted interference. Leveraging this vulnerability, we propose a novel
two-stage attack framework capable of manipulating online constructed maps.
First, our method identifies vulnerable asymmetric scenes along the victim AV's
potential route. Then, we optimize the location and pattern of camera-blinding
attacks and adversarial patch attacks. Evaluations on a public AD dataset
demonstrate that our attacks can degrade mapping accuracy by up to 9.9%, render
up to 44% of targeted routes unreachable, and increase unsafe planned
trajectory rates, colliding with real-world road boundaries, by up to 27%.
These attacks are also validated on a real-world testbed vehicle. We further
analyze root causes of the symmetry bias, attributing them to training data
imbalance, model architecture, and map element representation. To the best of
our knowledge, this study presents the first vulnerability assessment of online
map construction models and introduces the first digital and physical attack
against them.

</details>


### [36] [Towards Reliable Service Provisioning for Dynamic UAV Clusters in Low-Altitude Economy Networks](https://arxiv.org/abs/2509.06112)
*Yanwei Gong,Ruichen Zhang,Xiaoqing Wang,Xiaolin Chang,Bo Ai,Junchao Fan,Bocheng Ju,Dusit Niyato*

Main category: cs.CR

TL;DR: 提出LP2-CASKU方案，解决无人机集群动态认证和会话密钥更新的安全与效率问题，显著降低延迟和能耗


<details>
  <summary>Details</summary>
Motivation: 无人机集群服务对低空经济发展至关重要，但需要解决新无人机高效认证、跨集群隐私保护认证和会话密钥安全更新等挑战

Method: 集成高效批量认证机制、轻量级跨集群认证机制和安全会话密钥更新机制，确保匿名性、不可链接性和前后向安全性

Result: 相比基线方案，延迟降低82.8%-90.8%，能耗减少37.6%-72.6%，在动态通信环境中表现出强适应性

Conclusion: LP2-CASKU方案能有效支持高动态无人机集群环境中的隐私保护认证，具有优异的性能和安全性

Abstract: Unmanned Aerial Vehicle (UAV) cluster services are crucial for promoting the
low-altitude economy by enabling scalable, flexible, and adaptive aerial
networks. To meet diverse service demands, clusters must dynamically
incorporate a New UAVs (NUAVs) or an Existing UAV (EUAV). However, achieving
sustained service reliability remains challenging due to the need for efficient
and scalable NUAV authentication, privacy-preserving cross-cluster
authentication for EUAVs, and robust protection of the cluster session key,
including both forward and backward secrecy. To address these challenges, we
propose a Lightweight and Privacy-Preserving Cluster Authentication and Session
Key Update (LP2-CASKU) scheme tailored for dynamic UAV clusters in low-altitude
economy networks. LP2-CASKU integrates an efficient batch authentication
mechanism that simultaneously authenticates multiple NUAVs with minimal
communication overhead. It further introduces a lightweight cross-cluster
authentication mechanism that ensures EUAV anonymity and unlinkability.
Additionally, a secure session key update mechanism is incorporated to maintain
key confidentiality over time, thereby preserving both forward and backward
secrecy. We provide a comprehensive security analysis and evaluate LP2-CASKU
performance through both theoretical analysis and OMNeT++ simulations.
Experimental results demonstrate that, compared to the baseline, LP2-CASKU
achieves a latency reduction of 82.8%-90.8% by across different UAV swarm
configurations and network bitrates, demonstrating strong adaptability to
dynamic communication environments. Besides, under varying UAV swarm
configurations, LP2-CASKU reduces the energy consumption by approximately
37.6-72.6%, while effectively supporting privacy-preserving authentication in
highly dynamic UAV cluster environments.

</details>


### [37] [CSI-IBBS: Identity-Based Blind Signature using CSIDH](https://arxiv.org/abs/2509.06127)
*Soumya Bhoumik,Sarbari Mitra,Rohit Raj Sharma,Kuldeep Namdeo*

Main category: cs.CR

TL;DR: 基于CSIDH框架的身份基础盲签名方案，结合零知识验证确保验证者诚实性，具备量子抵御能力


<details>
  <summary>Details</summary>
Motivation: 解决传统公钥认证中证书管理的效率问题，结合盲签名的隐私保护和零知识验证的验证者诚实性确保

Method: 利用CSIDH框架构建身份基础盲签名方案，结合零知识验证技术确保验证者诚实性且不泄露额外信息

Result: 在标准加密模型中证明了协议的安全性，能够有效保护隐私和验证者诚实性，性能评估证明其实用可行性

Conclusion: 该方案为后量子时代提供了一种安全、可扩展的加密解决方案，在保持计算效率的同时具备量子抵御能力

Abstract: Identity-based cryptography (IBC), proposed by Adi Shamir, revolutionized
public key authentication by eliminating the need for certificates, enabling a
more efficient and scalable approach to cryptographic systems. Meanwhile, in
\cite{Katsumata2024group}, Katsumata et al. were the first to present the blind
signature protocol based on the hardness assumption of isogeny with provable
security, which resembles the Schnorr blind signature. Building upon these
foundational concepts, we propose an Identity-Based Blind Signature Scheme with
an Honest Zero-Knowledge Verifier utilizing the CSIDH framework. This scheme
combines blind signatures for privacy preservation with zero-knowledge proofs
to ensure the verifier's honesty without revealing any additional information.
  Leveraging the quantum-resistant properties of CSIDH, a post-quantum secure
scheme based on supersingular isogenies, our scheme offers strong protection
against quantum adversaries while maintaining computational efficiency. We
analyze the security of the introduced protocol in the standard cryptographic
model and demonstrate its effectiveness in safeguarding privacy and verifier
honesty. Furthermore, we present a performance evaluation, confirming the
practical viability of this quantum-resistant cryptographic solution for
privacy-preserving applications. This work advances the creation of secure, and
scalable cryptographic systems for the post-quantum era.

</details>


### [38] [VehiclePassport: A GAIA-X-Aligned, Blockchain-Anchored Privacy-Preserving, Zero-Knowledge Digital Passport for Smart Vehicles](https://arxiv.org/abs/2509.06133)
*Pradyumna Kaushal*

Main category: cs.CR

TL;DR: VehiclePassport是一个基于区块链和零知识证明的车辆数字护照系统，用于安全、隐私保护的车辆生命周期记录验证


<details>
  <summary>Details</summary>
Motivation: 现代车辆的生命周期记录分散在OEM、车主和服务中心之间，难以验证且容易发生欺诈，需要一种可信的验证机制

Method: 使用区块链和零知识证明技术，通过Groth16证明和短期JWT实现选择性披露，在Polygon zkEVM上锚定哈希值

Result: 每个事件成本<$0.02，验证时间<10ms，可扩展到数百万辆车，消除了纸质KYC，确保GDPR合规的可追溯性

Conclusion: 为保险、转售和监管应用建立了无信任基础，适用于全球移动数据市场

Abstract: Modern vehicles accumulate fragmented lifecycle records across OEMs, owners,
and service centers that are difficult to verify and prone to fraud. We propose
VehiclePassport, a GAIA-X-aligned digital passport anchored on blockchain with
zero-knowledge proofs (ZKPs) for privacy-preserving verification.
VehiclePassport immutably commits to manufacturing, telemetry, and service
events while enabling selective disclosure via short-lived JWTs and Groth16
proofs. Our open-source reference stack anchors hashes on Polygon zkEVM at
<$0.02 per event, validates proofs in <10 ms, and scales to millions of
vehicles. This architecture eliminates paper-based KYC, ensures GDPR-compliant
traceability, and establishes a trustless foundation for insurance, resale, and
regulatory applications in global mobility data markets.

</details>


### [39] [Measuring the Vulnerability Disclosure Policies of AI Vendors](https://arxiv.org/abs/2509.06136)
*Yangheran Piao,Jingjie Li,Daniel W. Woods*

Main category: cs.CR

TL;DR: 对264家AI厂商漏洞披露政策的分析显示，36%厂商无披露渠道，仅18%明确提及AI风险，漏洞赏金政策发展滞后于学术研究和实际事件


<details>
  <summary>Details</summary>
Motivation: 随着AI集成到关键系统中，需要了解厂商是否愿意接受和响应AI漏洞报告，以确保有效修复

Method: 混合方法研究：定量分析264家AI厂商的披露政策，定性分析识别厂商态度，并与1130起AI事件和359篇学术论文对比

Result: 发现36%厂商无披露渠道，仅18%提及AI风险；数据访问、授权和模型提取漏洞通常被纳入范围，而越狱和幻觉常被排除；识别出三种厂商态度：主动澄清型(46家)、沉默型(115家)、限制型(103家)

Conclusion: AI漏洞赏金政策发展严重滞后，需要更全面的政策来应对新兴的AI安全威胁

Abstract: As AI is increasingly integrated into products and critical systems,
researchers are paying greater attention to identifying related
vulnerabilities. Effective remediation depends on whether vendors are willing
to accept and respond to AI vulnerability reports. In this paper, we examine
the disclosure policies of 264 AI vendors. Using a mixed-methods approach, our
quantitative analysis finds that 36% of vendors provide no disclosure channel,
and only 18% explicitly mention AI-related risks. Vulnerabilities involving
data access, authorization, and model extraction are generally considered
in-scope, while jailbreaking and hallucination are frequently excluded. Through
qualitative analysis, we further identify three vendor postures toward AI
vulnerabilities - proactive clarification (n = 46, include active supporters,
AI integrationists, and back channels), silence (n = 115, include self-hosted
and hosted vendors), and restrictive (n = 103). Finally, by comparing vendor
policies against 1,130 AI incidents and 359 academic publications, we show that
bug bounty policy evolution has lagged behind both academic research and
real-world events.

</details>


### [40] [Lightweight Intrusion Detection System Using a Hybrid CNN and ConvNeXt-Tiny Model for Internet of Things Networks](https://arxiv.org/abs/2509.06202)
*Fatemeh Roshanzadeh,Hamid Barati,Ali Barati*

Main category: cs.CR

TL;DR: 这篇论文提出了一种基于CNN和ConvNeXt-Tiny混合模型的轻量级入侵检测系统，用于物联网环境中的网络攻击检测和分类，在保持高准确率的同时大大缩短了训练和推理时间。


<details>
  <summary>Details</summary>
Motivation: 物联网系统的快速扩展导致安全风险显著增加，威胁到数据完整性、保密性和可用性，因此确保物联网系统的安全性和弹性变得至关重要。

Method: 提出了一种轻量级高效的入侵检测系统，利用CNN和ConvNeXt-Tiny混合模型，专门用于检测和分类网络攻击，特别是机器人网络和恶意流量。采用来自实际物联网场景的真实数据集进行实验。

Result: 该系统在测试阶段达到了99.63%的准确率，训练阶段为99.67%，误差率仅0.0107（包含8个类别），同时保持短响应时间和低资源消耗。与更复杂的模型相比，训练和推理时间显著缩短。

Conclusion: 这些结果表明该方法在真实物联网环境中检测和分类攻击方面具有高效性，轻量级架构可以作为复杂和资源密集方案的实用替代方案，为物联网网络安全提供了实用的解决方案。

Abstract: The rapid expansion of Internet of Things (IoT) systems across various
domains such as industry, smart cities, healthcare, manufacturing, and
government services has led to a significant increase in security risks,
threatening data integrity, confidentiality, and availability. Consequently,
ensuring the security and resilience of IoT systems has become a critical
requirement. In this paper, we propose a lightweight and efficient intrusion
detection system (IDS) for IoT environments, leveraging a hybrid model of CNN
and ConvNeXt-Tiny. The proposed method is designed to detect and classify
different types of network attacks, particularly botnet and malicious traffic,
while the lightweight ConvNeXt-Tiny architecture enables effective deployment
in resource-constrained devices and networks. A real-world dataset comprising
both benign and malicious network packets collected from practical IoT
scenarios was employed in the experiments. The results demonstrate that the
proposed method achieves high accuracy while significantly reducing training
and inference time compared to more complex models. Specifically, the system
attained 99.63% accuracy in the testing phase, 99.67% accuracy in the training
phase, and an error rate of 0.0107 across eight classes, while maintaining
short response times and low resource consumption. These findings highlight the
effectiveness of the proposed method in detecting and classifying attacks in
real-world IoT environments, indicating that the lightweight architecture can
serve as a practical alternative to complex and resource-intensive approaches
in IoT network security.

</details>


### [41] [PLRV-O: Advancing Differentially Private Deep Learning via Privacy Loss Random Variable Optimization](https://arxiv.org/abs/2509.06264)
*Qin Yang,Nicholas Stout,Meisam Mohammady,Han Wang,Ayesha Samreen,Christopher J Quinn,Yan Yan,Ashish Kundu,Yuan Hong*

Main category: cs.CR

TL;DR: PLRV-O是一个新的差分隐私SGD框架，通过参数化噪声分布实现隐私损失和模型效用的独立控制，在严格隐私约束下显著提升模型性能


<details>
  <summary>Details</summary>
Motivation: 传统DP-SGD方法使用高斯或拉普拉斯噪声，其单一参数同时控制隐私损失和效用损失，无法独立调节。当训练轮数和批次大小变化时，这种耦合会导致隐私-效用权衡的次优结果

Method: 提出PLRV-O框架，定义参数化的DP-SGD噪声分布搜索空间，紧密表征隐私损失矩，同时允许更独立地优化效用损失。支持针对模型大小、训练时长、批次采样策略和裁剪阈值的系统化噪声适配

Result: 在CIFAR-10上，微调ViT在ε≈0.5时达到94.03%准确率（高斯噪声为83.93%）；在SST-2上，RoBERTa-large在ε≈0.2时达到92.20%准确率（高斯噪声为50.25%）

Conclusion: PLRV-O框架通过解耦隐私损失和效用损失的优化，在严格隐私约束下显著提升了差分隐私深度学习的性能，为不同任务需求提供了系统化的噪声适配方案

Abstract: Differentially Private Stochastic Gradient Descent (DP-SGD) is a standard
method for enforcing privacy in deep learning, typically using the Gaussian
mechanism to perturb gradient updates. However, conventional mechanisms such as
Gaussian and Laplacian noise are parameterized only by variance or scale. This
single degree of freedom ties the magnitude of noise directly to both privacy
loss and utility degradation, preventing independent control of these two
factors. The problem becomes more pronounced when the number of composition
rounds T and batch size B vary across tasks, as these variations induce
task-dependent shifts in the privacy-utility trade-off, where small changes in
noise parameters can disproportionately affect model accuracy. To address this
limitation, we introduce PLRV-O, a framework that defines a broad search space
of parameterized DP-SGD noise distributions, where privacy loss moments are
tightly characterized yet can be optimized more independently with respect to
utility loss. This formulation enables systematic adaptation of noise to
task-specific requirements, including (i) model size, (ii) training duration,
(iii) batch sampling strategies, and (iv) clipping thresholds under both
training and fine-tuning settings. Empirical results demonstrate that PLRV-O
substantially improves utility under strict privacy constraints. On CIFAR-10, a
fine-tuned ViT achieves 94.03% accuracy at epsilon approximately 0.5, compared
to 83.93% with Gaussian noise. On SST-2, RoBERTa-large reaches 92.20% accuracy
at epsilon approximately 0.2, versus 50.25% with Gaussian.

</details>


### [42] [AttestLLM: Efficient Attestation Framework for Billion-scale On-device LLMs](https://arxiv.org/abs/2509.06326)
*Ruisi Zhang,Yifei Zhao,Neusha Javidnia,Mengxin Zheng,Farinaz Koushanfar*

Main category: cs.CR

TL;DR: AttestLLM是首个针对设备端大语言模型的认证框架，通过算法/软件/硬件协同设计在LLM激活分布中嵌入水印签名，确保只有授权模型能在目标平台上运行。


<details>
  <summary>Details</summary>
Motivation: 随着设备端LLM的广泛采用，验证本地设备上运行模型的合法性变得至关重要，现有认证技术无法有效处理十亿参数级别的LLM，缺乏时间和内存效率。

Method: 采用算法/软件/硬件协同设计方法，在LLM构建块的激活分布中嵌入鲁棒水印签名，并在可信执行环境(TEE)中优化认证协议。

Result: 在Llama、Qwen和Phi系列LLM上的评估显示，AttestLLM具有可靠的认证能力、高保真度和高效率，能有效抵御模型替换和伪造攻击。

Conclusion: AttestLLM为设备端LLM提供了首个有效的硬件级IP保护方案，在不影响推理吞吐量的情况下实现了高效认证，解决了LLM时代的新兴安全威胁。

Abstract: As on-device LLMs(e.g., Apple on-device Intelligence) are widely adopted to
reduce network dependency, improve privacy, and enhance responsiveness,
verifying the legitimacy of models running on local devices becomes critical.
Existing attestation techniques are not suitable for billion-parameter Large
Language Models (LLMs), struggling to remain both time- and memory-efficient
while addressing emerging threats in the LLM era. In this paper, we present
AttestLLM, the first-of-its-kind attestation framework to protect the
hardware-level intellectual property (IP) of device vendors by ensuring that
only authorized LLMs can execute on target platforms. AttestLLM leverages an
algorithm/software/hardware co-design approach to embed robust watermarking
signatures onto the activation distributions of LLM building blocks. It also
optimizes the attestation protocol within the Trusted Execution Environment
(TEE), providing efficient verification without compromising inference
throughput. Extensive proof-of-concept evaluations on LLMs from Llama, Qwen,
and Phi families for on-device use cases demonstrate AttestLLM's attestation
reliability, fidelity, and efficiency. Furthermore, AttestLLM enforces model
legitimacy and exhibits resilience against model replacement and forgery
attacks.

</details>


### [43] [Embedding Poisoning: Bypassing Safety Alignment via Embedding Semantic Shift](https://arxiv.org/abs/2509.06338)
*Shuai Yuan,Zhibo Zhang,Yuxi Li,Guangdong Bai,Wang Kailong*

Main category: cs.CR

TL;DR: 本文提出了一种新型的LLM部署阶段攻击方法SEP，通过在嵌入层注入难以察觉的扰动来绕过安全对齐机制，成功率达到96.43%


<details>
  <summary>Details</summary>
Motivation: 现有平台的安全扫描无法检测嵌入层的细微操作，存在严重安全漏洞，需要研究部署阶段的攻击和防御机制

Method: 提出SEP框架，通过搜索优化的方式在嵌入层注入针对高风险token的扰动，利用模型响应的线性转换特性识别扰动窗口

Result: 在6个对齐LLM上测试，平均攻击成功率达96.43%，同时保持良性任务性能并规避传统检测机制

Conclusion: 揭示了部署安全的关键疏忽，强调未来LLM防御策略中需要进行嵌入层完整性检查的紧迫性

Abstract: The widespread distribution of Large Language Models (LLMs) through public
platforms like Hugging Face introduces significant security challenges. While
these platforms perform basic security scans, they often fail to detect subtle
manipulations within the embedding layer. This work identifies a novel class of
deployment phase attacks that exploit this vulnerability by injecting
imperceptible perturbations directly into the embedding layer outputs without
modifying model weights or input text. These perturbations, though
statistically benign, systematically bypass safety alignment mechanisms and
induce harmful behaviors during inference. We propose Search based Embedding
Poisoning(SEP), a practical, model agnostic framework that introduces carefully
optimized perturbations into embeddings associated with high risk tokens. SEP
leverages a predictable linear transition in model responses, from refusal to
harmful output to semantic deviation to identify a narrow perturbation window
that evades alignment safeguards. Evaluated across six aligned LLMs, SEP
achieves an average attack success rate of 96.43% while preserving benign task
performance and evading conventional detection mechanisms. Our findings reveal
a critical oversight in deployment security and emphasize the urgent need for
embedding level integrity checks in future LLM defense strategies.

</details>


### [44] [From Perception to Protection: A Developer-Centered Study of Security and Privacy Threats in Extended Reality (XR)](https://arxiv.org/abs/2509.06368)
*Kunlin Cai,Jinghuai Zhang,Ying Li,Zhiyuan Wang,Xun Chen,Tianshi Li,Yuan Tian*

Main category: cs.CR

TL;DR: XR技术带来独特的安全隐私挑战，开发者对此存在认知偏差且缺乏有效支持，研究通过访谈23名专业开发者提出改进建议


<details>
  <summary>Details</summary>
Motivation: XR技术的沉浸性和数据收集特性带来了传统范式难以应对的新型安全隐私挑战，开发者作为应用构建者对此缺乏足够认知和支持，需要从开发者角度深入理解威胁感知和应对方式

Method: 访谈23名专业XR开发者，聚焦XR新兴威胁，通过研究问题分析开发中的现有问题并寻找可行的改进路径

Result: 发现XR开发决策（如丰富传感器数据收集、用户生成内容接口）会放大安全隐私威胁但开发者往往意识不到；现有缓解方法有限且支持不足削弱了开发者应对威胁的能力

Conclusion: 提出了针对利益相关者的可行建议，以在整个XR开发过程中改善安全隐私保护，这是XR领域首个以威胁感知和开发者为中心的研究

Abstract: The immersive nature of XR introduces a fundamentally different set of
security and privacy (S&P) challenges due to the unprecedented user
interactions and data collection that traditional paradigms struggle to
mitigate. As the primary architects of XR applications, developers play a
critical role in addressing novel threats. However, to effectively support
developers, we must first understand how they perceive and respond to different
threats. Despite the growing importance of this issue, there is a lack of
in-depth, threat-aware studies that examine XR S&P from the developers'
perspective. To fill this gap, we interviewed 23 professional XR developers
with a focus on emerging threats in XR. Our study addresses two research
questions aiming to uncover existing problems in XR development and identify
actionable paths forward.
  By examining developers' perceptions of S&P threats, we found that: (1) XR
development decisions (e.g., rich sensor data collection, user-generated
content interfaces) are closely tied to and can amplify S&P threats, yet
developers are often unaware of these risks, resulting in cognitive biases in
threat perception; and (2) limitations in existing mitigation methods, combined
with insufficient strategic, technical, and communication support, undermine
developers' motivation, awareness, and ability to effectively address these
threats. Based on these findings, we propose actionable and stakeholder-aware
recommendations to improve XR S&P throughout the XR development process. This
work represents the first effort to undertake a threat-aware,
developer-centered study in the XR domain -- an area where the immersive,
data-rich nature of the XR technology introduces distinctive challenges.

</details>


### [45] [When Code Crosses Borders: A Security-Centric Evaluation of LLM-based Code Translation](https://arxiv.org/abs/2509.06504)
*Hailong Chang,Guozhu Meng,Shuhui Xiao,Kai Chen,Kun Sun,Yilin Li*

Main category: cs.CR

TL;DR: 这篇论文构建了STED数据集，用于评估LLM代码翻译的安全性，发现28.6-45%的翻译会引入新漏洞，并提出RAG缓解策略降低32.8%的漏洞


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注语法或功能正确性，忽视了代码翻译任务中的安全问题，需要专门的安全性评估方法

Method: 构建STED数据集（五种编程语言，9个CWE类别，720个安全相关代码样本），采用两种评估模块：安全研究人员手动评估和LLM-as-a-judge自动分析，评估功能正确性、漏洞保持率和漏洞引入率

Result: 对5个先进LLM进行6,000次翻译评估，发现28.6-45%的翻译会引入新漏洞，特别是网页相关的输入验证类漏洞；RAG缓解策略可降低32.8%的翻译引入漏洞

Conclusion: LLM在代码翻译任务中存在显著的安全性问题，短板漏洞容易被引入；知识增强的RAG策略可以有效提升翻译安全性

Abstract: With the growing demand for cross-language codebase migration, evaluating
LLMs' security implications in translation tasks has become critical. Existing
evaluations primarily focus on syntactic or functional correctness at the
function level, neglecting the critical dimension of security.
  To enable security evaluation, we construct STED (Security-centric
Translation Evaluation Dataset), the first dataset specifically designed for
evaluating the security implications of LLM-based code translation. It
comprises 720 security-related code samples across five programming languages
and nine high-impact CWE categories, sourced from CVE/NVD and manually verified
for translation tasks. Our evaluation framework consists of two independent
assessment modules: (1) rigorous evaluation by security researchers, and (2)
automated analysis via LLM-as-a-judge. Together they evaluate three critical
aspects: functional correctness, vulnerability preservation, and vulnerability
introduction rates.
  Our large-scale evaluation of five state-of-the-art LLMs across 6,000
translation instances reveals significant security degradation, with 28.6-45%
of translations introducing new vulnerabilities--particularly for web-related
flaws like input validation, where LLMs show consistent weaknesses.
Furthermore, we develop a Retrieval-Augmented Generation (RAG)-based mitigation
strategy that reduces translation-induced vulnerabilities by 32.8%, showing the
potential of knowledge-enhanced prompting.

</details>


### [46] [Synthesis of Sound and Precise Leakage Contracts for Open-Source RISC-V Processors](https://arxiv.org/abs/2509.06509)
*Zilong Wang,Gideon Mohr,Klaus von Gleissenthall,Jan Reineke,Marco Guarnieri*

Main category: cs.CR

TL;DR: LeaSyn是首个自动合成处理器泄漏合约的工具，能够为RTL级处理器设计生成既可靠又精确的泄漏合约，解决了手动制定合约的挑战。


<details>
  <summary>Details</summary>
Motivation: 处理器微架构侧信道泄漏合约的手动制定需要深入了解微架构优化的时序侧信道，过程具有挑战性、耗时且容易出错。

Method: LeaSyn从用户提供的合约模板开始，通过交替进行合约合成（基于处理器泄漏经验特征确保精确性）和合约验证（确保可靠性）来自动构建合约。

Result: 在六个开源RISC-V CPU上实验表明，LeaSyn合成的合约可靠且比现有方法构建的合约更精确。

Conclusion: LeaSyn成功实现了自动化泄漏合约合成，为处理器安全验证提供了有效工具，显著提高了合约制定的效率和准确性。

Abstract: Leakage contracts have been proposed as a new security abstraction at the
instruction set architecture level. Leakage contracts aim to capture the
information that processors may leak via microarchitectural side channels.
Recently, the first tools have emerged to verify whether a processor satisfies
a given contract. However, coming up with a contract that is both sound and
precise for a given processor is challenging, time-consuming, and error-prone,
as it requires in-depth knowledge of the timing side channels introduced by
microarchitectural optimizations.
  In this paper, we address this challenge by proposing LeaSyn, the first tool
for automatically synthesizing leakage contracts that are both sound and
precise for processor designs at register-transfer level. Starting from a
user-provided contract template that captures the space of possible contracts,
LeaSyn automatically constructs a contract, alternating between contract
synthesis, which ensures precision based on an empirical characterization of
the processor's leaks, and contract verification, which ensures soundness.
  Using LeaSyn, we automatically synthesize contracts for six open-source
RISC-V CPUs for a variety of contract templates. Our experiments indicate that
LeaSyn's contracts are sound and more precise (i.e., represent the actual leaks
in the target processor more faithfully) than contracts constructed by existing
approaches.

</details>


### [47] [Signal-Based Malware Classification Using 1D CNNs](https://arxiv.org/abs/2509.06548)
*Jack Wilkie,Hanan Hindy,Ivan Andonovic,Christos Tachtatzis,Robert Atkinson*

Main category: cs.CR

TL;DR: 这篇论文提出了一种新的恶意软件分类方法，通过将恶意软件转换为1D信号而非传统的2D图像，避免了重新形状的涉客规则和量化噪声，提高了分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有的恶意软件分类方法存在显著的信息损失问题：将二进制文件转换为2D图像时会产生量化噪声和人为引入的2D依赖关系，这些问题限制了下游模型的分类性能。

Method: 论文提出了一种新的方法：将恶意软件文件转换为1D信号，避免重新形状的涉客规则和量化噪声。使用浮点数格式存储信号，并基于ResNet架构和squeeze-and-excitation层开发了一个定制的1D卷积神经网络来分类这些信号。

Result: 在MalNet数据集上进行了评估，该方法在二元、类型和家族级别分类上达到了独创性能，F1分数分别为0.874、0.503和0.507。

Conclusion: 这项工作为未来模型在新的信号模态上运行平了道路，通过避免传统2D图像转换带来的信息损失，显著提高了恶意软件分类的性能。

Abstract: Malware classification is a contemporary and ongoing challenge in
cyber-security: modern obfuscation techniques are able to evade traditional
static analysis, while dynamic analysis is too resource intensive to be
deployed at a large scale. One prominent line of research addresses these
limitations by converting malware binaries into 2D images by heuristically
reshaping them into a 2D grid before resizing using Lanczos resampling. These
images can then be classified based on their textural information using
computer vision approaches. While this approach can detect obfuscated malware
more effectively than static analysis, the process of converting files into 2D
images results in significant information loss due to both quantisation noise,
caused by rounding to integer pixel values, and the introduction of 2D
dependencies which do not exist in the original data. This loss of signal
limits the classification performance of the downstream model. This work
addresses these weaknesses by instead resizing the files into 1D signals which
avoids the need for heuristic reshaping, and additionally these signals do not
suffer from quantisation noise due to being stored in a floating-point format.
It is shown that existing 2D CNN architectures can be readily adapted to
classify these 1D signals for improved performance. Furthermore, a bespoke 1D
convolutional neural network, based on the ResNet architecture and
squeeze-and-excitation layers, was developed to classify these signals and
evaluated on the MalNet dataset. It was found to achieve state-of-the-art
performance on binary, type, and family level classification with F1 scores of
0.874, 0.503, and 0.507, respectively, paving the way for future models to
operate on the proposed signal modality.

</details>


### [48] [Super-Quadratic Quantum Speed-ups and Guessing Many Likely Keys](https://arxiv.org/abs/2509.06549)
*Timo Glaser,Alexander May,Julian Nowakowski*

Main category: cs.CR

TL;DR: 本文对Montanaro量子密钥猜测算法进行了首次紧致分析，发现其运行时间为2^{H_{2/3}(D)/2}，相比经典算法的2^{H_{1/2}(D)}获得了超二次量子加速。在多密钥场景下，猜测每个密钥的成本显著降低。


<details>
  <summary>Details</summary>
Motivation: 研究从非均匀概率分布中猜测密码密钥的基本问题（如LPN、LWE或密码），分析最优经典和量子算法的性能差异。

Method: 使用信息论中的Arikan不等式对Montanaro量子算法进行紧致分析，并扩展到多密钥场景，分析产品分布下的密钥猜测成本。

Result: 证明了量子算法的运行时间为2^{H_{2/3}(D)/2}，相比经典算法2^{H_{1/2}(D)}获得超二次加速。在多密钥设置中，每个密钥的猜测成本降低到2^{H(D)}（经典）和2^{H(D)/2}（量子）。

Conclusion: 量子算法在密钥猜测问题上具有显著优势，且多密钥场景能进一步降低每个密钥的猜测成本，这对密码安全性分析具有重要意义。

Abstract: We study the fundamental problem of guessing cryptographic keys, drawn from
some non-uniform probability distribution $D$, as e.g. in LPN, LWE or for
passwords. The optimal classical algorithm enumerates keys in decreasing order
of likelihood. The optimal quantum algorithm, due to Montanaro (2011), is a
sophisticated Grover search.
  We give the first tight analysis for Montanaro's algorithm, showing that its
runtime is $2^{H_{2/3}(D)/2}$, where $H_{\alpha}(\cdot)$ denotes Renyi entropy
with parameter $\alpha$. Interestingly, this is a direct consequence of an
information theoretic result called Arikan's Inequality (1996) -- which has so
far been missed in the cryptographic community -- that tightly bounds the
runtime of classical key guessing by $2^{H_{1/2}(D)}$. Since $H_{2/3}(D) <
H_{1/2}(D)$ for every non-uniform distribution $D$, we thus obtain a
super-quadratic quantum speed-up $s>2$ over classical key guessing.
  As another main result, we provide the first thorough analysis of guessing in
a multi-key setting. Specifically, we consider the task of attacking many keys
sampled independently from some distribution $D$, and aim to guess a fraction
of them. For product distributions $D = \chi^n$, we show that any constant
fraction of keys can be guessed within $2^{H(D)}$ classically and $2 ^{H(D)/2}$
quantumly per key, where $H(\chi)$ denotes Shannon entropy. In contrast,
Arikan's Inequality implies that guessing a single key costs $2^{H_{1/2}(D)}$
classically and $2^{H_{2/3}(D)/2}$ quantumly. Since $H(D) < H_{2/3}(D) <
H_{1/2}(D)$, this shows that in a multi-key setting the guessing cost per key
is substantially smaller than in a single-key setting, both classically and
quantumly.

</details>


### [49] [Marginal sets in semigroups and semirings](https://arxiv.org/abs/2509.06562)
*I. Buchinskiy,M. Kotov,A. Ponmaheshkumar,R. Perumal*

Main category: cs.CR

TL;DR: 本文扩展了Roman'kov的群边际集概念，将其推广到半群和半环，特别针对热带矩阵半群/半环构建边际集，并应用于改进半群上的密钥交换方案


<details>
  <summary>Details</summary>
Motivation: 扩展2019年Roman'kov提出的群边际集理论，将其概念推广到更一般的代数结构——半群和半环，以丰富该理论的应用范围

Method: 引入半群和半环的边际集概念，特别针对热带矩阵半群和半环描述边际集的构造方法

Result: 成功建立了半群和半环的边际集理论框架，并具体构造了热带矩阵代数结构中的边际集

Conclusion: 边际集概念可以成功推广到半群和半环，这一扩展为改进基于半群的密钥交换方案提供了新的理论工具和应用可能性

Abstract: In 2019, V. A. Roman'kov introduced the concept of marginal sets for groups.
He developed a theory of marginal sets and demonstrated how these sets can be
applied to improve some key exchange schemes. In this paper, we extend his
ideas and introduce the concept of marginal sets for semigroups and semirings.
For tropical matrix semigroups and semirings, we describe how some marginal
sets can be constructed. We apply marginal sets to improve some key exchange
schemes over semigroups.

</details>


### [50] [A Simple Data Exfiltration Game](https://arxiv.org/abs/2509.06571)
*Tristan Caulfield*

Main category: cs.CR

TL;DR: 数据泄漏模型分析：攻击者选择泄漏路径和速度，防御者设置监控阈值，通过游戏理论最大化攻击收益与最小化防御成本


<details>
  <summary>Details</summary>
Motivation: 企业面临朴泄机密数据造成的财务损失和诈骗风险，需要建立数据泄漏防御模型来应对这一日益严重的问题

Method: 建立简单的游戏理论模型，攻击者选择数据泄漏路径和速度，防御者选择监控阈值来检测异常活动

Result: 提出了一种基于游戏理论的数据泄漏分析框架，明确了攻防双方的战略选择和权衡

Conclusion: 该模型为网络数据泄漏问题提供了理论分析基础，有助于理解攻防双方的动机和最优防御策略

Abstract: Data exfiltration is a growing problem for business who face costs related to
the loss of confidential data as well as potential extortion. This work
presents a simple game theoretic model of network data exfiltration. In the
model, the attacker chooses the exfiltration route and speed, and the defender
selects monitoring thresholds to detect unusual activity. The attacker is
rewarded for exfiltrating data, and the defender tries to minimize the costs of
data loss and of responding to alerts.

</details>


### [51] [Mind Your Server: A Systematic Study of Parasitic Toolchain Attacks on the MCP Ecosystem](https://arxiv.org/abs/2509.06572)
*Shuli Zhao,Qinsheng Hou,Zihan Zhan,Yanhao Wang,Yuchong Xie,Yu Guo,Libo Chen,Shenghong Li,Zhi Xue*

Main category: cs.CR

TL;DR: 揭示了MCP协议中的寄生工具链攻击(MCP-UPD)，攻击者通过污染外部数据源，在LLM执行合法任务时窃取私有数据，无需直接受害者交互。


<details>
  <summary>Details</summary>
Motivation: 随着LLM通过MCP协议与外部系统集成，安全范式发生根本转变：LLM从被动信息处理器转变为自主编排工具链的协调器，扩大了攻击面，攻击目标从操纵单个输出升级到劫持整个执行流程。

Method: 设计MCP-SEC工具，对MCP生态系统进行首次大规模安全普查，分析了1,360个服务器中的12,230个工具，识别可利用的gadget和攻击方法。

Result: 发现MCP生态系统存在大量可利用的安全漏洞，缺乏上下文-工具隔离和最小权限执行机制，导致恶意指令可以不受控制地传播到敏感工具调用中。

Conclusion: MCP平台存在系统性风险，LLM集成环境亟需防御机制来应对寄生工具链攻击的威胁。

Abstract: Large language models (LLMs) are increasingly integrated with external
systems through the Model Context Protocol (MCP), which standardizes tool
invocation and has rapidly become a backbone for LLM-powered applications.
While this paradigm enhances functionality, it also introduces a fundamental
security shift: LLMs transition from passive information processors to
autonomous orchestrators of task-oriented toolchains, expanding the attack
surface, elevating adversarial goals from manipulating single outputs to
hijacking entire execution flows. In this paper, we reveal a new class of
attacks, Parasitic Toolchain Attacks, instantiated as MCP Unintended Privacy
Disclosure (MCP-UPD). These attacks require no direct victim interaction;
instead, adversaries embed malicious instructions into external data sources
that LLMs access during legitimate tasks. The malicious logic infiltrates the
toolchain and unfolds in three phases: Parasitic Ingestion, Privacy Collection,
and Privacy Disclosure, culminating in stealthy exfiltration of private data.
Our root cause analysis reveals that MCP lacks both context-tool isolation and
least-privilege enforcement, enabling adversarial instructions to propagate
unchecked into sensitive tool invocations. To assess the severity, we design
MCP-SEC and conduct the first large-scale security census of the MCP ecosystem,
analyzing 12,230 tools across 1,360 servers. Our findings show that the MCP
ecosystem is rife with exploitable gadgets and diverse attack methods,
underscoring systemic risks in MCP platforms and the urgent need for defense
mechanisms in LLM-integrated environments.

</details>


### [52] [LLMs in Cybersecurity: Friend or Foe in the Human Decision Loop?](https://arxiv.org/abs/2509.06595)
*Irdin Pekaric,Philipp Zech,Tom Mattson*

Main category: cs.CR

TL;DR: LLMs作为认知协作工具能提高决策准确性，但可能削弱独立思考、导致过度依赖和决策同质化，特别是在安全关键环境中。研究发现高韧性个体能更有效利用LLMs，而低韧性用户更容易产生自动化偏见。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs如何影响人类在安全关键环境中的判断决策，探讨LLMs在提高准确性的同时可能带来的认知依赖和决策同质化问题。

Method: 通过两个探索性焦点小组研究（无辅助组和LLM辅助组），评估决策准确性、行为韧性和依赖动态。

Result: LLMs在常规决策中提高了准确性和一致性，但无意中减少了认知多样性并加剧了自动化偏见，特别是在低韧性用户中更为明显。高韧性个体能更有效地利用LLMs。

Conclusion: 认知特质在调节AI效益中起关键作用，需要开发能促进批判性思维和保持认知多样性的LLM交互设计。

Abstract: Large Language Models (LLMs) are transforming human decision-making by acting
as cognitive collaborators. Yet, this promise comes with a paradox: while LLMs
can improve accuracy, they may also erode independent reasoning, promote
over-reliance and homogenize decisions. In this paper, we investigate how LLMs
shape human judgment in security-critical contexts. Through two exploratory
focus groups (unaided and LLM-supported), we assess decision accuracy,
behavioral resilience and reliance dynamics. Our findings reveal that while
LLMs enhance accuracy and consistency in routine decisions, they can
inadvertently reduce cognitive diversity and improve automation bias, which is
especially the case among users with lower resilience. In contrast,
high-resilience individuals leverage LLMs more effectively, suggesting that
cognitive traits mediate AI benefit.

</details>


### [53] [A Secure Sequencer and Data Availability Committee for Rollups (Extended Version)](https://arxiv.org/abs/2509.06614)
*Margarita Capretto,Martín Ceresa,Antonio Fernández Anta,Pedro Moreno Sánchez,César Sánchez*

Main category: cs.CR

TL;DR: 这篇论文提出了一种高效的欺诈证明机制，用于监测和证明Layer 2 Rollups中序列器和数据可用性委员会的不该行为，通过L1合约仲裁来保障L2的安全性。


<details>
  <summary>Details</summary>
Motivation: 解决Layer 2 Rollups中序列器和DAC服务可能存在的安全风险，近代L2采用哈希技术节省L1空间，但需要DAC进行数据转换，这些中心化服务可能影响L2区块链的发展和安全性。

Method: 设计了基于两方游戏的欺诈证明机制，通过L1合约仲裁。与现有通用欺诈证明不同，这些机制采用预定义算法来验证DAC正确性的属性，使得证明更高效、更容易理解和证明正确性。

Result: 研究了这些欺诈证明如何限制控制不同数量序列器和DAC成员的敌手能力，并为它们的该诚行为提供激励。还提供了LEAN4形式化的机制化实现，包括验证过的该诚策略和检测不该声明的机制。

Conclusion: 该方法通过具体算法的仲裁实现了更高效的欺诈证明机制，有助于保障Layer 2 Rollups的安全性，同时提供了形式化验证的基础，为L2的更广泛采用扩平了道路。

Abstract: Blockchains face a scalability limitation, partly due to the throughput
limitations of consensus protocols, especially when aiming to obtain a high
degree of decentralization. Layer 2 Rollups (L2s) are a faster alternative to
conventional blockchains. L2s perform most computations offchain using
minimally blockchains (L1) under-the-hood to guarantee correctness. A sequencer
is a service that receives offchain L2 transaction requests, batches these
transactions, and commits compressed or hashed batches to L1. Using hashing
needs less L1 space, which is beneficial for gas cost, but requires a data
availability committee (DAC) service to translate hashes into their
corresponding batches of transaction requests. The behavior of sequencers and
DACs influence the evolution of the L2 blockchain, presenting a potential
security threat and delaying L2 adoption. We propose in this paper fraud-proof
mechanisms, arbitrated by L1 contracts, to detect and generate evidence of
dishonest behavior of the sequencer and DAC. We study how these fraud-proofs
limit the power of adversaries that control different number of sequencer and
DACs members, and provide incentives for their honest behavior. We designed
these fraud-proof mechanisms as two player games. Unlike the generic
fraud-proofs in current L2s (designed to guarantee the correct execution of
transactions), our fraud-proofs are over pred-etermined algorithms that verify
the properties that determine the correctness of the DAC. Arbitrating over
concrete algorithms makes our fraud-proofs more efficient, easier to
understand, and simpler to prove correct. We provide as an artifact a
mechanization in LEAN4 of our fraud-proof games, including (1) the verified
strategies that honest players should play to win all games as well as (2)
mechanisms to detect dishonest claims.

</details>


### [54] [Network-level Censorship Attacks in the InterPlanetary File System](https://arxiv.org/abs/2509.06626)
*Jan Matter,Muoi Tran*

Main category: cs.CR

TL;DR: IPFS网络虽然去中心化，但节点和内容提供商集中在公有云，面临BGP路由攻击威胁。研究发现单个恶意AS可审查75%的IPFS内容，影响57%的请求节点。提出了基于全局协作内容复制和加固备份节点的防护措施。


<details>
  <summary>Details</summary>
Motivation: IPFS作为Web3去中心化存储标准，虽然设计去中心化，但实际部署中节点和内容提供商集中在公有云，存在中心化风险。BGP路由攻击（如劫持和被动拦截）对其他Web3协议已有研究，但对IPFS网络尚未分析，需要填补这一空白。

Method: 收集了3,000个内容块（CIDs），模拟BGP劫持和被动拦截攻击。分析攻击效果，并提出了基于全局协作内容复制和加固备份节点的防护方案进行验证。

Result: 研究发现：单个恶意AS可审查75%的IPFS内容，影响超过57%的请求节点；仅需劫持62个前缀即可达到70%的攻击效果。防护措施验证有效。

Conclusion: BGP路由攻击对IPFS构成严重威胁，需要提高安全意识并采取防护措施。提出的全局协作内容复制和加固备份节点方案可有效缓解攻击，呼吁进一步加固现网IPFS网络。

Abstract: The InterPlanetary File System (IPFS) has been successfully established as
the de facto standard for decentralized data storage in the emerging Web3.
Despite its decentralized nature, IPFS nodes, as well as IPFS content
providers, have converged to centralization in large public clouds.
Centralization introduces BGP routing-based attacks, such as passive
interception and BGP hijacking, as potential threats. Although this attack
vector has been investigated for many other Web3 protocols, such as Bitcoin and
Ethereum, to the best of our knowledge, it has not been analyzed for the IPFS
network. In our work, we bridge this gap and demonstrate that BGP routing
attacks can be effectively leveraged to censor content in IPFS. For the
analysis, we collected 3,000 content blocks called CIDs and conducted a
simulation of BGP hijacking and passive interception against them. We find that
a single malicious AS can censor 75% of the IPFS content for more than 57% of
all requester nodes. Furthermore, we show that even with a small set of only 62
hijacked prefixes, 70% of the full attack effectiveness can already be reached.
We further propose and validate countermeasures based on global collaborative
content replication among all nodes in the IPFS network, together with
additional robust backup content provider nodes that are well-hardened against
BGP hijacking. We hope this work raises awareness about the threat BGP
routing-based attacks pose to IPFS and triggers further efforts to harden the
live IPFS network against them.

</details>


### [55] [When Secure Isn't: Assessing the Security of Machine Learning Model Sharing](https://arxiv.org/abs/2509.06703)
*Gabriele Digregorio,Marco Di Gennaro,Stefano Zanero,Stefano Longari,Michele Carminati*

Main category: cs.CR

TL;DR: 论文分析了机器学习模型共享框架和平台的安全状况，发现大多数平台仅提供部分安全保护，甚至存在6个0day漏洞，揭示了当前安全机制不足和用户安全认知误区。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型共享的普及，安全风险被低估且缺乏系统研究，需要评估现有框架和平台的安全状况以及用户的安全认知。

Method: 通过评估模型共享框架和平台的安全防护机制，分析安全导向设置的有效性，并调查用户对模型共享安全性的认知。

Result: 发现大多数框架仅提供有限安全保护，存在6个0day漏洞可导致任意代码执行，用户对安全设置存在过度信任的误区。

Conclusion: 当前模型共享安全问题尚未解决，需要加强生态系统安全建设，改进安全机制并提升用户安全意识。

Abstract: The rise of model-sharing through frameworks and dedicated hubs makes Machine
Learning significantly more accessible. Despite their benefits, these tools
expose users to underexplored security risks, while security awareness remains
limited among both practitioners and developers. To enable a more
security-conscious culture in Machine Learning model sharing, in this paper we
evaluate the security posture of frameworks and hubs, assess whether
security-oriented mechanisms offer real protection, and survey how users
perceive the security narratives surrounding model sharing. Our evaluation
shows that most frameworks and hubs address security risks partially at best,
often by shifting responsibility to the user. More concerningly, our analysis
of frameworks advertising security-oriented settings and complete model sharing
uncovered six 0-day vulnerabilities enabling arbitrary code execution. Through
this analysis, we debunk the misconceptions that the model-sharing problem is
largely solved and that its security can be guaranteed by the file format used
for sharing. As expected, our survey shows that the surrounding security
narrative leads users to consider security-oriented settings as trustworthy,
despite the weaknesses shown in this work. From this, we derive takeaways and
suggestions to strengthen the security of model-sharing ecosystems.

</details>


### [56] [Image Encryption Scheme Based on Hyper-Chaotic Map and Self-Adaptive Diffusion](https://arxiv.org/abs/2509.06754)
*Yiqi Tang*

Main category: cs.CR

TL;DR: 提出一种基于新2D超混涎地图和自适应滴流方法的图像加密方案，性能超越现有最先进技术


<details>
  <summary>Details</summary>
Motivation: 在数字时代，需要更有效的图像加密技术来防止未授权访问，现有的加密方案需要提升性能

Method: 集成了新2D超混涎地图(2D-RA地图)和自适应滴流方法，2D-RA地图通过融合Rastrigin和Ackley函数设计，并通过分支图、李雅普诺夫指数等多种测量验证其混涎性能

Result: 实验结果显示2D-RA地图的混涎性能超越现有先进混涎函数，图像加密方案的性能显著优于当前最先进的图像加密技术

Conclusion: 该研究成功开发了一种高效的图像加密方案，通过新的混涎地图和滴流技术显著提升了加密性能，为图像安全保护提供了更好的解决方案

Abstract: In the digital age, image encryption technology acts as a safeguard,
preventing unauthorized access to images. This paper proposes an innovative
image encryption scheme that integrates a novel 2D hyper-chaotic map with a
newly developed self-adaptive diffusion method. The 2D hyper-chaotic map,
namely the 2D-RA map, is designed by hybridizing the Rastrigin and Ackley
functions. The chaotic performance of the 2D-RA map is validated through a
series of measurements, including the Bifurcation Diagram, Lyapunov Exponent
(LE), Initial Value Sensitivity, 0 - 1 Test, Correlation Dimension (CD), and
Kolmogorov Entropy (KE). The results demonstrate that the chaotic performance
of the 2D-RA map surpasses that of existing advanced chaotic functions.
Additionally, the self-adaptive diffusion method is employed to enhance the
uniformity of grayscale distribution. The performance of the image encryption
scheme is evaluated using a series of indicators. The results show that the
proposed image encryption scheme significantly outperforms current
state-of-the-art image encryption techniques.

</details>


### [57] [Imitative Membership Inference Attack](https://arxiv.org/abs/2509.06796)
*Yuntao Du,Yuetian Chen,Hanshen Xiao,Bruno Ribeiro,Ninghui Li*

Main category: cs.CR

TL;DR: 提出IMIA方法，通过模仿训练技术构建少量目标模型仿制品，显著降低计算成本的同时提升成员推理攻击效果


<details>
  <summary>Details</summary>
Motivation: 现有成员推理攻击需要训练数百个独立影子模型，计算开销巨大，需要更高效的方法

Method: 使用模仿训练技术构建少量目标模型仿制品，这些仿制品能紧密复制目标模型行为用于推理

Result: IMIA在各种攻击设置下显著优于现有方法，同时仅需不到最先进方法5%的计算成本

Conclusion: IMIA通过模仿训练实现了高效且有效的成员推理攻击，大幅降低了计算开销

Abstract: A Membership Inference Attack (MIA) assesses how much a target machine
learning model reveals about its training data by determining whether specific
query instances were part of the training set. State-of-the-art MIAs rely on
training hundreds of shadow models that are independent of the target model,
leading to significant computational overhead. In this paper, we introduce
Imitative Membership Inference Attack (IMIA), which employs a novel imitative
training technique to strategically construct a small number of target-informed
imitative models that closely replicate the target model's behavior for
inference. Extensive experimental results demonstrate that IMIA substantially
outperforms existing MIAs in various attack settings while only requiring less
than 5% of the computational cost of state-of-the-art approaches.

</details>


### [58] [An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and Detection](https://arxiv.org/abs/2509.06920)
*Haywood Gelman,John D. Hastings,David Kenley*

Main category: cs.CR

TL;DR: 使用LLM动态合成系统日志来检测内部侵犯者，Claude Sonnet 3.7在几乎所有指标上都超过GPT-4o，特别是在减少假正告警和提高检测准确性方面


<details>
  <summary>Details</summary>
Motivation: 内部威胁是一个日益严重的组织问题，但现有研究多依赖静态和访问限制的数据集，限制了适应性检测模型的发展

Method: 采用基于道德原则的新方法，使用Claude Sonnet 3.7大语言模型动态合成系统日志消息，其中1%包含内部威胁指标，模拟真实世界的数据分布

Result: Claude Sonnet 3.7在几乎所有统计指标（精准度、召回率、MCC、ROC AUC）上都一贯地超过GPT-4o，特别在减少假正告警和提高检测准确性方面表现更优

Conclusion: 该研究显示了大语言模型在合成数据集生成和内部威胁检测方面具有强大潜力，为这一领域的研究提供了新的方向

Abstract: Insider threats are a growing organizational problem due to the complexity of
identifying their technical and behavioral elements. A large research body is
dedicated to the study of insider threats from technological, psychological,
and educational perspectives. However, research in this domain has been
generally dependent on datasets that are static and limited access which
restricts the development of adaptive detection models. This study introduces a
novel, ethically grounded approach that uses the large language model (LLM)
Claude Sonnet 3.7 to dynamically synthesize syslog messages, some of which
contain indicators of insider threat scenarios. The messages reflect real-world
data distributions by being highly imbalanced (1% insider threats). The syslogs
were analyzed for insider threats by both Claude Sonnet 3.7 and GPT-4o, with
their performance evaluated through statistical metrics including precision,
recall, MCC, and ROC AUC. Sonnet 3.7 consistently outperformed GPT-4o across
nearly all metrics, particularly in reducing false alarms and improving
detection accuracy. The results show strong promise for the use of LLMs in
synthetic dataset generation and insider threat detection.

</details>


### [59] [Neuro-Symbolic AI for Cybersecurity: State of the Art, Challenges, and Opportunities](https://arxiv.org/abs/2509.06921)
*Safayat Bin Hakim,Muhammad Adil,Alvaro Velasquez,Shouhuai Xu,Houbing Herbert Song*

Main category: cs.CR

TL;DR: 这篇论文系统调查了神经符号AI在网络安全领域的应用，通过分析127篇文献提出G-I-A评价框架，识别了多自治代理架构优势和因果推理的重要性，同时警告自主攻击能力的双重使用风险。


<details>
  <summary>Details</summary>
Motivation: 传统AI在网络安全中存在概念基础不凭固、指令能力有限和目标不对齐等根本局限性，神经符号AI有潜力改变这一领域但缺乏系统性研究。

Method: 对2019-2025年7月127篇公开出版物进行系统分析，提出基础化-指令能力-对齐性(G-I-A)评估框架，涵盖网络安全、恶意软件分析和网络作战等领域。

Result: 发现多自治代理神经符号架构具有显著优势，因果推理是最转型性进步，能够实现主动防御。同时自主攻击系统在零日漏洞利用中显示强大能力且大幅降低成本，改变了威胁动态。

Conclusion: 需要社区驱动的标准化框架和负责任开发实践，确保技术进步服务于防御性网络安全目标同时保持社会对齐。

Abstract: Traditional Artificial Intelligence (AI) approaches in cybersecurity exhibit
fundamental limitations: inadequate conceptual grounding leading to
non-robustness against novel attacks; limited instructibility impeding
analyst-guided adaptation; and misalignment with cybersecurity objectives.
Neuro-Symbolic (NeSy) AI has emerged with the potential to revolutionize
cybersecurity AI. However, there is no systematic understanding of this
emerging approach. These hybrid systems address critical cybersecurity
challenges by combining neural pattern recognition with symbolic reasoning,
enabling enhanced threat understanding while introducing concerning autonomous
offensive capabilities that reshape threat landscapes. In this survey, we
systematically characterize this field by analyzing 127 publications spanning
2019-July 2025. We introduce a Grounding-Instructibility-Alignment (G-I-A)
framework to evaluate these systems, focusing on both cyber defense and cyber
offense across network security, malware analysis, and cyber operations. Our
analysis shows advantages of multi-agent NeSy architectures and identifies
critical implementation challenges including standardization gaps,
computational complexity, and human-AI collaboration requirements that
constrain deployment. We show that causal reasoning integration is the most
transformative advancement, enabling proactive defense beyond correlation-based
approaches. Our findings highlight dual-use implications where autonomous
systems demonstrate substantial capabilities in zero-day exploitation while
achieving significant cost reductions, altering threat dynamics. We provide
insights and future research directions, emphasizing the urgent need for
community-driven standardization frameworks and responsible development
practices that ensure advancement serves defensive cybersecurity objectives
while maintaining societal alignment.

</details>
