<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 15]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [APoW: Auditable Proof-of-Work Against Block Withholding Attacks](https://arxiv.org/abs/2601.02496)
*Sergio Demian Lerner*

Main category: cs.CR

TL;DR: APoW是一种新型工作量证明机制，通过可审计的非空间重扫描，使矿工能够验证其他矿工的工作，从而检测区块扣留攻击并支持去中心化矿池。


<details>
  <summary>Details</summary>
Motivation: 传统PoW机制中，矿池无法有效验证矿工是否诚实工作，导致区块扣留攻击难以检测。现有解决方案需要可信硬件或第三方，限制了去中心化矿池的发展。

Method: 基于Hashcash式随机数搜索，设计可审计的PoW方案。矿工可以概率性地证明在先前挖矿轮次中搜索了特定非空间区域，同时为新区块或矿池份额执行生产性工作。

Result: APoW实现了对矿工工作的可追溯审计，能够概率性检测区块扣留攻击，无需可信硬件或第三方。该方案保持了传统PoW的基本特性，同时增加了专门针对矿池挖矿的审计层。

Conclusion: APoW支持去中心化矿池设计，其中工作归属可验证且扣留激励大幅降低。虽然完整部署需要共识规则变更，但即使没有共识变更，也可作为矿池级审计机制实用。

Abstract: We introduce APoW, a novel proof-of-work (PoW) construction inspired by Hashcash-style nonce searching, which enables the auditing of other miners' work through accountable re-scanning of the nonce space. The proposed scheme allows a miner to probabilistically attest to having searched specified regions of the nonce space in earlier mining rounds, while concurrently earning rewards for performing productive work for a new block or pool share. This capability enables miners belonging to a mining pools to audit another miner's claimed effort retroactively, thereby allowing the probabilistic detection of block withholding attacks (BWAs) without requiring trusted hardware or trusted third parties. As a consequence, the construction supports the design of decentralized mining pools in which work attribution is verifiable and withholding incentives are substantially reduced. The scheme preserves the fundamental properties of conventional PoW, including public verifiability and difficulty adjustment, while adding an orthogonal auditability layer tailored to pool-based mining. Finally, while a full deployment of APoW in Bitcoin would require a consensus rule change and minor modifications to mining ASICs, the construction remains practically useful even without consensus changes, for instance, as a pool-level auditing mechanism that enables verifiable pay-for-auditing using existing pool reserves.

</details>


### [2] [Coordinated Multi-Domain Deception: A Stackelberg Game Approach](https://arxiv.org/abs/2601.02596)
*Md Abu Sayed,Asif Rahman,Ahmed Hemida,Christopher Kiekintveld,Charles Kamhoua*

Main category: cs.CR

TL;DR: 该论文提出了一种协调网络和物理系统防御的欺骗策略，通过Stackelberg博弈框架建模攻防交互，利用CVSS漏洞评分和NVD数据指导欺骗部署，使用网络和物理副本干扰攻击者侦察，证明多层协调欺骗优于单层和基线策略。


<details>
  <summary>Details</summary>
Motivation: 当前网络物理系统面临复杂攻击威胁，需要更有效的防御机制。传统防御策略往往被动应对，缺乏主动欺骗和协调机制。攻击者能够利用系统漏洞进行侦察和攻击，因此需要开发能够主动误导攻击者、增强防御效果的协同欺骗策略。

Method: 采用Stackelberg博弈框架建模攻防双方的战略交互。利用CVSS漏洞评分系统的利用概率和NVD真实漏洞数据指导欺骗部署。在网络和物理层面创建副本系统来干扰攻击者侦察。提出基于CVE的效用函数识别最关键漏洞，并比较多层协调欺骗与单层策略的效果。

Result: 研究表明，协调的多层欺骗策略在两种CVSS版本下均能显著提高防御者效用，优于单层欺骗策略和基线防御方法。基于CVE的效用函数能够有效识别关键漏洞，指导防御资源的最优分配。

Conclusion: 通过协调网络和物理系统的欺骗策略，结合博弈论框架和真实漏洞数据，可以显著增强网络物理系统的防御能力。多层协调欺骗为复杂系统安全提供了有效的主动防御方案，未来可进一步扩展到更广泛的攻击场景和系统类型。

Abstract: This paper explores coordinated deception strategies by synchronizing defenses across coupled cyber and physical systems to mislead attackers and strengthen defense mechanisms. We introduce a Stackelberg game framework to model the strategic interaction between defenders and attackers, where the defender leverages CVSS-based exploit probabilities and real-world vulnerability data from the National Vulnerability Database (NVD) to guide the deployment of deception. Cyber and physical replicas are used to disrupt attacker reconnaissance and enhance defensive effectiveness. We propose a CVE-based utility function to identify the most critical vulnerabilities and demonstrate that coordinated multilayer deception outperforms single-layer and baseline strategies in improving defender utility across both CVSS versions.

</details>


### [3] [SWaRL: Safeguard Code Watermarking via Reinforcement Learning](https://arxiv.org/abs/2601.02602)
*Neusha Javidnia,Ruisi Zhang,Ashish Kundu,Farinaz Koushanfar*

Main category: cs.CR

TL;DR: SWaRL是一个基于强化学习的代码水印框架，通过编译器反馈确保功能正确性，使用LoRA实现水印可迁移性，在保持代码功能的同时提高水印检测准确率。


<details>
  <summary>Details</summary>
Motivation: 保护代码LLM所有者的知识产权，现有方法依赖手动规则或概率操纵，容易导致编译错误，需要更鲁棒且保真的水印方案。

Method: 采用强化学习协同训练框架，利用编译器反馈确保功能正确性，联合训练机密验证器作为奖励信号保持水印可检测性，使用LoRA进行微调实现水印信息跨模型更新可迁移。

Result: 相比现有方法获得更高的水印检测准确率，完全保持水印代码功能，LoRA签名嵌入引导基础模型以水印特定方式生成和解决代码，计算开销小，对重构和对抗转换攻击具有强韧性。

Conclusion: SWaRL是一个鲁棒且保真的水印框架，有效保护代码LLM知识产权，通过强化学习和编译器反馈解决了现有方法的局限性，实现了功能正确性和水印可检测性的平衡。

Abstract: We present SWaRL, a robust and fidelity-preserving watermarking framework designed to protect the intellectual property of code LLM owners by embedding unique and verifiable signatures in the generated output. Existing approaches rely on manually crafted transformation rules to preserve watermarked code functionality or manipulate token-generation probabilities at inference time, which are prone to compilation errors. To address these challenges, SWaRL employs a reinforcement learning-based co-training framework that uses compiler feedback for functional correctness and a jointly trained confidential verifier as a reward signal to maintain watermark detectability. Furthermore, SWaRL employs low-rank adaptation (LoRA) during fine-tuning, allowing the learned watermark information to be transferable across model updates. Extensive experiments show that SWaRL achieves higher watermark detection accuracy compared to prior methods while fully maintaining watermarked code functionality. The LoRA-based signature embedding steers the base model to generate and solve code in a watermark-specific manner without significant computational overhead. Moreover, SWaRL exhibits strong resilience against refactoring and adversarial transformation attacks.

</details>


### [4] [LAsset: An LLM-assisted Security Asset Identification Framework for System-on-Chip (SoC) Verification](https://arxiv.org/abs/2601.02624)
*Md Ajoad Hasan,Dipayan Saha,Khan Thamid Hasan,Nashmin Alam,Azim Uddin,Sujan Kumar Saha,Mark Tehranipoor,Farimah Farahmandi*

Main category: cs.CR

TL;DR: LAsset是一个利用大语言模型自动从硬件设计规范和RTL描述中识别安全资产的框架，显著减少人工开销并提高可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现代SoC和IP设计日益复杂，使得安全验证变得困难。传统方法依赖安全专家手动识别安全资产，耗时耗力且需要专业知识，需要自动化解决方案来提高效率和可扩展性。

Method: LAsset框架利用大语言模型，通过结构和语义分析识别模块内的一级和二级资产，并推导模块间关系，系统性地描述设计层面的安全依赖关系。

Result: 实验结果显示，该框架在SoC设计中达到90%的召回率，在IP设计中达到93%的召回率，显著减少了人工开销。

Conclusion: LAsset通过自动化资产识别，为安全硬件开发提供了可扩展的路径，显著提高了安全验证的效率和准确性。

Abstract: The growing complexity of modern system-on-chip (SoC) and IP designs is making security assurance difficult day by day. One of the fundamental steps in the pre-silicon security verification of a hardware design is the identification of security assets, as it substantially influences downstream security verification tasks, such as threat modeling, security property generation, and vulnerability detection. Traditionally, assets are determined manually by security experts, requiring significant time and expertise. To address this challenge, we present LAsset, a novel automated framework that leverages large language models (LLMs) to identify security assets from both hardware design specifications and register-transfer level (RTL) descriptions. The framework performs structural and semantic analysis to identify intra-module primary and secondary assets and derives inter-module relationships to systematically characterize security dependencies at the design level. Experimental results show that the proposed framework achieves high classification accuracy, reaching up to 90% recall rate in SoC design, and 93% recall rate in IP designs. This automation in asset identification significantly reduces manual overhead and supports a scalable path forward for secure hardware development.

</details>


### [5] [Adversarial Contrastive Learning for LLM Quantization Attacks](https://arxiv.org/abs/2601.02680)
*Dinghong Song,Zhiwei Xu,Hai Wan,Xibin Zhao,Pengfei Su,Dong Li*

Main category: cs.CR

TL;DR: 提出对抗对比学习（ACL）方法，通过最大化良性响应与有害响应概率差距，实现高效量化攻击，显著提升攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 模型量化对在资源受限硬件上部署大语言模型至关重要，但近期研究发现全精度良性LLM在量化后可能表现出恶意行为，存在严重安全风险。

Method: 提出对抗对比学习（ACL），基于梯度的量化攻击方法，使用三元组对比损失作为攻击目标，结合投影梯度下降两阶段分布式微调策略确保稳定高效优化。

Result: ACL在过度拒绝攻击中达到86.00%成功率，越狱攻击97.69%，广告注入攻击92.40%，分别比最先进方法提升44.67%、18.84%和50.80%。

Conclusion: ACL方法在量化攻击方面表现出卓越效果，显著超越现有方法，揭示了LLM量化过程中的安全漏洞需要更深入研究和防御机制。

Abstract: Model quantization is critical for deploying large language models (LLMs) on resource-constrained hardware, yet recent work has revealed severe security risks that benign LLMs in full precision may exhibit malicious behaviors after quantization. In this paper, we propose Adversarial Contrastive Learning (ACL), a novel gradient-based quantization attack that achieves superior attack effectiveness by explicitly maximizing the gap between benign and harmful responses probabilities. ACL formulates the attack objective as a triplet-based contrastive loss, and integrates it with a projected gradient descent two-stage distributed fine-tuning strategy to ensure stable and efficient optimization. Extensive experiments demonstrate ACL's remarkable effectiveness, achieving attack success rates of 86.00% for over-refusal, 97.69% for jailbreak, and 92.40% for advertisement injection, substantially outperforming state-of-the-art methods by up to 44.67%, 18.84%, and 50.80%, respectively.

</details>


### [6] [Privacy-Preserving AI-Enabled Decentralized Learning and Employment Records System](https://arxiv.org/abs/2601.02720)
*Yuqiao Xu,Mina Namazi,Sahith Reddy Jalapally,Osama Zafar,Youngjin Yoo,Erman Ayday*

Main category: cs.CR

TL;DR: 提出一个隐私保护的AI赋能去中心化学习与就业记录系统，通过可信执行环境自动生成可验证技能证书，实现安全的教育就业凭证管理和隐私保护的技能提取。


<details>
  <summary>Details</summary>
Motivation: 现有的区块链学习与就业记录系统虽然使用可验证证书，但缺乏自动技能证书生成能力，且无法整合非结构化学习证据，需要更智能、隐私保护的解决方案。

Method: 采用去中心化架构，在可信执行环境内运行自然语言处理管道，分析正式记录和非正式学习成果，生成可验证的自发行技能证书，所有验证和职位匹配都在enclave内完成。

Result: NLP组件在样本数据上评估，遵循验证的Syllabus-to-O*NET方法，重复运行稳定性测试显示顶级技能排名方差<5%，系统提供不可伪造的派生证书并保持敏感信息机密性。

Conclusion: 该系统支持安全的教育就业凭证管理、强大的成绩单验证，以及在去中心化框架内实现自动化的隐私保护技能提取，减少筛选偏见机会。

Abstract: Learning and Employment Record (LER) systems are emerging as critical infrastructure for securely compiling and sharing educational and work achievements. Existing blockchain-based platforms leverage verifiable credentials but typically lack automated skill-credential generation and the ability to incorporate unstructured evidence of learning. In this paper,a privacy-preserving, AI-enabled decentralized LER system is proposed to address these gaps. Digitally signed transcripts from educational institutions are accepted, and verifiable self-issued skill credentials are derived inside a trusted execution environment (TEE) by a natural language processing pipeline that analyzes formal records (e.g., transcripts, syllabi) and informal artifacts. All verification and job-skill matching are performed inside the enclave with selective disclosure, so raw credentials and private keys remain enclave-confined. Job matching relies solely on attested skill vectors and is invariant to non-skill resume fields, thereby reducing opportunities for screening bias.The NLP component was evaluated on sample learner data; the mapping follows the validated Syllabus-to-O*NET methodology,and a stability test across repeated runs observed <5% variance in top-ranked skills. Formal security statements and proof sketches are provided showing that derived credentials are unforgeable and that sensitive information remains confidential. The proposed system thus supports secure education and employment credentialing, robust transcript verification,and automated, privacy-preserving skill extraction within a decentralized framework.

</details>


### [7] [SastBench: A Benchmark for Testing Agentic SAST Triage](https://arxiv.org/abs/2601.02941)
*Jake Feiglin,Guy Dar*

Main category: cs.CR

TL;DR: SAST工具产生大量误报需要人工筛选，现有基准无法模拟真实SAST发现分布。作者提出SastBench基准，结合真实CVE作为真阳性与过滤的SAST工具发现作为近似假阳性，用于评估SAST分类代理。


<details>
  <summary>Details</summary>
Motivation: SAST工具在网络安全防御中广泛应用，但会产生大量误报，需要昂贵的人工筛选。虽然LLM驱动的代理在自动化网络安全任务方面有潜力，但现有基准无法模拟真实世界的SAST发现分布。

Method: 提出SastBench基准，结合真实CVE作为真阳性与过滤的SAST工具发现作为近似假阳性。该基准采用代理无关的设计，用于评估SAST分类代理的性能。

Result: 在基准上评估了不同代理的性能，提供了比较分析、数据集的详细分析，并讨论了未来发展的影响。

Conclusion: SastBench为评估SAST分类代理提供了一个更真实的基准，有助于推动自动化网络安全任务的发展。

Abstract: SAST (Static Application Security Testing) tools are among the most widely used techniques in defensive cybersecurity, employed by commercial and non-commercial organizations to identify potential vulnerabilities in software. Despite their great utility, they generate numerous false positives, requiring costly manual filtering (aka triage). While LLM-powered agents show promise for automating cybersecurity tasks, existing benchmarks fail to emulate real-world SAST finding distributions. We introduce SastBench, a benchmark for evaluating SAST triage agents that combines real CVEs as true positives with filtered SAST tool findings as approximate false positives. SastBench features an agent-agnostic design. We evaluate different agents on the benchmark and present a comparative analysis of their performance, provide a detailed analysis of the dataset, and discuss the implications for future development.

</details>


### [8] [Quality Degradation Attack in Synthetic Data](https://arxiv.org/abs/2601.02947)
*Qinyi Liu,Dong Liu,Farhad Vadiee,Mohammad Khalil,Pedro P. Vergara Barrios*

Main category: cs.CR

TL;DR: 该研究揭示了合成数据生成中的质量降级攻击风险，攻击者通过操纵真实数据或生成过程，即使微小扰动也能显著降低合成数据质量，暴露了SDG流程的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据生成研究主要关注接收方发起的隐私攻击，但忽略了数据所有者、提供者或入侵者可能发起的质量降级攻击，需要建立更全面的安全框架。

Method: 形式化了相应的威胁模型，实证评估了真实数据的针对性操纵（如标签翻转和基于特征重要性的干预）对生成合成数据质量的影响。

Result: 结果显示，即使微小扰动也能显著降低下游预测性能并增加统计差异，暴露了合成数据生成流程中的脆弱性。

Conclusion: 研究强调需要在隐私保护之外，集成完整性验证和鲁棒性机制，以确保合成数据共享框架的可靠性和可信度。

Abstract: Synthetic Data Generation (SDG) can be used to facilitate privacy-preserving data sharing. However, most existing research focuses on privacy attacks where the adversary is the recipient of the released synthetic data and attempts to infer sensitive information from it. This study investigates quality degradation attacks initiated by adversaries who possess access to the real dataset or control over the generation process, such as the data owner, the synthetic data provider, or potential intruders. We formalize a corresponding threat model and empirically evaluate the effectiveness of targeted manipulations of real data (e.g., label flipping and feature-importance-based interventions) on the quality of generated synthetic data. The results show that even small perturbations can substantially reduce downstream predictive performance and increase statistical divergence, exposing vulnerabilities within SDG pipelines. This study highlights the need to integrate integrity verification and robustness mechanisms, alongside privacy protection, to ensure the reliability and trustworthiness of synthetic data sharing frameworks.

</details>


### [9] [Exploring Blockchain Interoperability: Frameworks, Use Cases, and Future Challenges](https://arxiv.org/abs/2601.02949)
*Stanly Wilson,Kwabena Adu-Duodu,Yinhao Li,Ellis Solaiman,Omer Rana,Rajiv Ranjan*

Main category: cs.CR

TL;DR: 论文探讨区块链互操作性解决方案，分析连接异构区块链的平台，并通过案例研究展示互操作性的重要性和优势，同时指出需要解决的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着更多应用迁移到区块链，产生了大量数据且应用日益复杂，不同区块链间的信息共享成为必要。早期区块链无法与其他链共享信息，因此需要研究开发互操作性解决方案。

Method: 讨论提供互操作性解决方案的区块链平台，重点关注它们连接异构区块链的能力。通过案例研究场景说明互操作性的重要性和优势。

Result: 分析了几种区块链互操作性平台，展示了它们如何连接异构区块链，并通过案例研究证明了互操作性解决方案的重要价值。

Conclusion: 区块链互操作性是解决不同链间信息共享的关键，虽然已有一些解决方案，但仍有一些问题需要在互操作性领域解决。

Abstract: Trust between entities in any scenario without a trusted third party is very difficult, and trust is exactly what blockchain aims to bring into the digital world with its basic features. Many applications are moving to blockchain adoption, enabling users to work in a trustworthy manner. The early generations of blockchain have a problem; they cannot share information with other blockchains. As more and more entities move their applications to the blockchain, they generate large volumes of data, and as applications have become more complex, sharing information between different blockchains has become a necessity. This has led to the research and development of interoperable solutions allowing blockchains to connect together. This paper discusses a few blockchain platforms that provide interoperable solutions, emphasising their ability to connect heterogeneous blockchains. It also discusses a case study scenario to illustrate the importance and benefits of using interoperable solutions. We also present a few topics that need to be solved in the realm of interoperability.

</details>


### [10] [Developing and Evaluating Lightweight Cryptographic Algorithms for Secure Embedded Systems in IoT Devices](https://arxiv.org/abs/2601.02981)
*Brahim Khalil Sedraoui,Abdelmadjid Benmachiche,Amina Makhlouf*

Main category: cs.CR

TL;DR: 本文研究物联网设备中的轻量级加密算法，比较PRESENT、SPECK和SIMON等算法的性能，并提出基于Feistel网络架构的新算法，在FPGA平台上验证其安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 物联网设备（如RFID标签、传感器、嵌入式系统）资源有限，传统加密算法计算复杂度和能耗过高，不适合这些平台，需要设计专门的轻量级加密解决方案。

Method: 1. 比较现有轻量级加密算法（PRESENT、SPECK、SIMON）的性能指标（吞吐量、内存使用、能耗）；2. 设计基于Feistel网络架构的新型轻量级算法；3. 在FPGA平台上进行硬件实现和测试；4. 分析算法抵抗差分和线性密码分析的安全性。

Result: 研究证明轻量级加密算法在物联网环境中是有效的安全策略，能够在保持性能的同时提供足够的安全性。提出的新算法在FPGA实现中表现出良好的性能和安全特性。

Conclusion: 轻量级加密是解决物联网等资源受限环境中安全问题的可行方案，基于Feistel架构的设计在安全性和效率之间取得了良好平衡，适合嵌入式系统应用。

Abstract: The high rate of development of Internet of Things (IoT) devices has brought to attention new challenges in the area of data security, especially within the resource-limited realm of RFID tags, sensors, and embedded systems. Traditional cryptographic implementations can be of inappropriate computational complexity and energy usage and hence are not suitable on these platforms. This paper examines the design, implementation, and testing of lightweight cryptographic algorithms that have been specifically designed to be used in secure embedded systems. A comparison of some of the state-of-the-art lightweight encryption algorithms, that is PRESENT, SPECK, and SIMON, focuses on the main performance indicators, i.e., throughput, use of memory, and energy utilization. The study presents novel lightweight algorithms that are founded upon the Feistel-network architecture and their safety under cryptanalytic attacks, e.g., differential and linear cryptanalysis. The proposed solutions are proven through hardware implementation on the FPGA platform. The results have shown that lightweight cryptography is an effective strategy that could be used to establish security and maintain performance in the IoT and other resource-limited settings.

</details>


### [11] [Selfish Mining in Multi-Attacker Scenarios: An Empirical Evaluation of Nakamoto, Fruitchain, and Strongchain](https://arxiv.org/abs/2601.02984)
*Martin Perešíni,Tomáš Hladký,Jakub Kubík,Ivan Homoliak*

Main category: cs.CR

TL;DR: 提出一个随机模拟框架，分析多攻击者自私挖矿在不同共识协议中的影响，验证已知阈值并发现新阈值。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单个自私矿工在特定协议中的攻击，对多个攻击者的研究有限。需要深入理解不同共识协议中自私挖矿攻击的安全性，特别是那些旨在缓解自私挖矿的协议。

Method: 开发随机模拟框架，建模PoW Nakamoto共识（作为基线）以及两种旨在缓解自私挖矿的协议：Fruitchain和Strongchain。该框架支持分析多个自私矿工在不同协议中的行为。

Result: 验证了文献中报告的阈值，并为2个及以上攻击者发现了多个新阈值。框架源代码已公开，可供研究人员评估新协议并与现有协议进行交叉比较。

Conclusion: 该随机模拟框架为分析多攻击者自私挖矿提供了有效工具，有助于深入理解不同共识协议的安全性，并为未来协议设计提供参考。

Abstract: The aim of this work is to enhance blockchain security by deepening the understanding of selfish mining attacks in various consensus protocols, especially the ones that have the potential to mitigate selfish mining. Previous research was mainly focused on a particular protocol with a single selfish miner, while only limited studies have been conducted on two or more attackers. To address this gap, we proposed a stochastic simulation framework that enables analysis of selfish mining with multiple attackers across various consensus protocols. We created the model of Proof-of-Work (PoW) Nakamoto consensus (serving as the baseline) as well as models of two additional consensus protocols designed to mitigate selfish mining: Fruitchain and Strongchain. Using our framework, thresholds reported in the literature were verified, and several novel thresholds were discovered for 2 and more attackers. We made the source code of our framework available, enabling researchers to evaluate any newly added protocol with one or more selfish miners and cross-compare it with already modeled protocols.

</details>


### [12] [JPU: Bridging Jailbreak Defense and Unlearning via On-Policy Path Rectification](https://arxiv.org/abs/2601.03005)
*Xi Wang,Songlei Jian,Shasha Li,Xiaopeng Li,Zhaoye Li,Bin Ji,Baosheng Wang,Jie Yu*

Main category: cs.CR

TL;DR: JPU是一种针对LLM越狱攻击的新型防御方法，通过动态挖掘对抗样本来识别和修正动态越狱路径，显著提升模型安全性同时保持实用性。


<details>
  <summary>Details</summary>
Motivation: 尽管进行了广泛的安全对齐，大型语言模型仍然容易受到越狱攻击。现有的机器遗忘防御方法虽然能擦除特定有害参数，但对多样化的越狱攻击仍然脆弱。研究发现失败机制源于越狱攻击主要激活中间层的未擦除参数，这些参数通过动态越狱路径重新组装成被禁止的输出。

Method: 提出Jailbreak Path Unlearning (JPU)方法，通过动态挖掘在线对抗样本来暴露漏洞并识别越狱路径，然后将这些动态越狱路径修正到安全锚点上。这是首个能够修正动态越狱路径的防御方法。

Result: 大量实验表明，JPU显著增强了模型对动态越狱攻击的抵抗能力，同时保持了模型的实用性。相比现有方法，JPU在防御效果上有显著提升。

Conclusion: JPU通过识别和修正动态越狱路径，填补了现有遗忘防御方法的根本性缺陷，为LLM安全防御提供了新的有效解决方案，能够在保持模型实用性的同时显著提升对越狱攻击的抵抗力。

Abstract: Despite extensive safety alignment, Large Language Models (LLMs) often fail against jailbreak attacks. While machine unlearning has emerged as a promising defense by erasing specific harmful parameters, current methods remain vulnerable to diverse jailbreaks. We first conduct an empirical study and discover that this failure mechanism is caused by jailbreaks primarily activating non-erased parameters in the intermediate layers. Further, by probing the underlying mechanism through which these circumvented parameters reassemble into the prohibited output, we verify the persistent existence of dynamic $\textbf{jailbreak paths}$ and show that the inability to rectify them constitutes the fundamental gap in existing unlearning defenses. To bridge this gap, we propose $\textbf{J}$ailbreak $\textbf{P}$ath $\textbf{U}$nlearning (JPU), which is the first to rectify dynamic jailbreak paths towards safety anchors by dynamically mining on-policy adversarial samples to expose vulnerabilities and identify jailbreak paths. Extensive experiments demonstrate that JPU significantly enhances jailbreak resistance against dynamic attacks while preserving the model's utility.

</details>


### [13] [LLMs, You Can Evaluate It! Design of Multi-perspective Report Evaluation for Security Operation Centers](https://arxiv.org/abs/2601.03013)
*Hiroyuki Okada,Tatsumi Oba,Naoto Yanai*

Main category: cs.CR

TL;DR: 论文提出MESSALA框架，利用LLM生成SOC安全事件分析报告，通过分析师检查表和新技术使评估结果最接近资深分析师水平


<details>
  <summary>Details</summary>
Motivation: SOC通常需要生成安全事件分析报告，未来LLM可能用于此任务。理解资深分析师如何评估报告及其反馈有助于改进SOC中的报告生成

Method: 1) 通过文献综述和SOC从业者用户研究构建分析师检查表；2) 设计MESSALA框架，引入粒度化指南和多视角评估两项新技术

Result: MESSALA的评估结果最接近资深SOC从业者，相比现有LLM方法效果最好。定性分析显示MESSALA能提供改进报告所需的可操作建议

Conclusion: MESSALA框架能有效利用LLM生成SOC分析报告，其评估结果接近资深分析师水平，并能提供有价值的改进反馈

Abstract: Security operation centers (SOCs) often produce analysis reports on security incidents, and large language models (LLMs) will likely be used for this task in the near future. We postulate that a better understanding of how veteran analysts evaluate reports, including their feedback, can help produce analysis reports in SOCs. In this paper, we aim to leverage LLMs for analysis reports. To this end, we first construct a Analyst-wise checklist to reflect SOC practitioners' opinions for analysis report evaluation through literature review and user study with SOC practitioners. Next, we design a novel LLM-based conceptual framework, named MESSALA, by further introducing two new techniques, granularization guideline and multi-perspective evaluation. MESSALA can maximize report evaluation and provide feedback on veteran SOC practitioners' perceptions. When we conduct extensive experiments with MESSALA, the evaluation results by MESSALA are the closest to those of veteran SOC practitioners compared with the existing LLM-based methods. We then show two key insights. We also conduct qualitative analysis with MESSALA, and then identify that MESSALA can provide actionable items that are necessary for improving analysis reports.

</details>


### [14] [FlexProofs: A Vector Commitment with Flexible Linear Time for Computing All Proofs](https://arxiv.org/abs/2601.03031)
*Jing Liu,Liang Feng Zhang*

Main category: cs.CR

TL;DR: FlexProofs是一种新型向量承诺方案，具有最优证明生成时间O(N)和灵活的批量参数b，可直接与zkSNARKs兼容，比现有方案快6倍。


<details>
  <summary>Details</summary>
Motivation: 现有向量承诺方案在生成所有个体开放证明时效率不足，特别是与zkSNARKs的兼容性有限。需要一种既能高效生成证明，又能直接与零知识证明系统集成的方案。

Method: 提出FlexProofs向量承诺方案，引入灵活的批量参数b来优化证明生成时间。作为关键构建块，设计了首个支持批量开放的多指数功能承诺方案。

Result: 当N=2^16且b=log^2 N时，FlexProofs比现有最佳方案HydraProofs快6倍。方案可直接与zkSNARKs集成，支持可验证秘密共享和可验证鲁棒聚合等应用。

Conclusion: FlexProofs通过灵活的批量参数实现了高效的向量承诺证明生成，并与zkSNARKs直接兼容，为实际应用提供了实用的密码学原语。

Abstract: In this paper, we introduce FlexProofs, a new vector commitment (VC) scheme that achieves two key properties: (1) the prover can generate all individual opening proofs for a vector of size $N$ in optimal time ${\cal O}(N)$, and there is a flexible batch size parameter $b$ that can be increased to further reduce the time to generate all proofs; and (2) the scheme is directly compatible with a family of zkSNARKs that encode their input as a multi-linear polynomial. As a critical building block, we propose the first functional commitment (FC) scheme for multi-exponentiations with batch opening. Compared with HydraProofs, the only existing VC scheme that computes all proofs in optimal time ${\cal O}(N)$ and is directly compatible with zkSNARKs, FlexProofs may speed up the process of generating all proofs, if the parameter $b$ is properly chosen. Our experiments show that for $N=2^{16}$ and $b=\log^2 N$, FlexProofs can be $6\times$ faster than HydraProofs. Moreover, when combined with suitable zkSNARKs, FlexProofs enable practical applications such as verifiable secret sharing and verifiable robust aggregation.

</details>


### [15] [SLIM: Stealthy Low-Coverage Black-Box Watermarking via Latent-Space Confusion Zones](https://arxiv.org/abs/2601.03242)
*Hengyu Wu,Yang Cao*

Main category: cs.CR

TL;DR: SLIM是一个用于大语言模型训练数据溯源验证的水印框架，通过创建潜在空间混淆区实现超低覆盖率下的黑盒验证，解决了现有方法在单个用户贡献极少数据时的实用性限制。


<details>
  <summary>Details</summary>
Motivation: 训练数据是LLM开发中的关键资产，需要水印技术进行使用验证。现有方法在低覆盖率场景下（单个数据所有者仅贡献训练语料的极小部分）无法同时保持隐蔽性、验证可行性和鲁棒性，限制了实际应用。

Method: SLIM框架利用LLM内在特性，通过训练模型将语义相似的前缀映射到不同的后续内容，创建潜在空间混淆区。这表现为局部生成不稳定性，可通过假设检验可靠检测，实现黑盒访问下的数据溯源验证。

Result: 实验表明SLIM具备超低覆盖率能力（可验证极小比例的训练数据）、强大的黑盒验证性能、良好的可扩展性，同时保持了隐蔽性和模型实用性。

Conclusion: SLIM为现代LLM流水线中的训练数据保护提供了鲁棒解决方案，解决了低覆盖率场景下的数据水印难题，实现了实用的数据溯源验证。

Abstract: Training data is a critical and often proprietary asset in Large Language Model (LLM) development, motivating the use of data watermarking to embed model-transferable signals for usage verification. We identify low coverage as a vital yet largely overlooked requirement for practicality, as individual data owners typically contribute only a minute fraction of massive training corpora. Prior methods fail to maintain stealthiness, verification feasibility, or robustness when only one or a few sequences can be modified. To address these limitations, we introduce SLIM, a framework enabling per-user data provenance verification under strict black-box access. SLIM leverages intrinsic LLM properties to induce a Latent-Space Confusion Zone by training the model to map semantically similar prefixes to divergent continuations. This manifests as localized generation instability, which can be reliably detected via hypothesis testing. Experiments demonstrate that SLIM achieves ultra-low coverage capability, strong black-box verification performance, and great scalability while preserving both stealthiness and model utility, offering a robust solution for protecting training data in modern LLM pipelines.

</details>
