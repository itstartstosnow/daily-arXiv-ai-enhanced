{"id": "2509.05306", "categories": ["cs.CR", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.05306", "abs": "https://arxiv.org/abs/2509.05306", "authors": ["Enis Karaarslan", "Esin Güler", "Efe Emir Yüce", "Cagatay Coban"], "title": "Towards Log Analysis with AI Agents: Cowrie Case Study", "comment": null, "summary": "The scarcity of real-world attack data significantly hinders progress in\ncybersecurity research and education. Although honeypots like Cowrie\neffectively collect live threat intelligence, they generate overwhelming\nvolumes of unstructured and heterogeneous logs, rendering manual analysis\nimpractical. As a first step in our project on secure and efficient AI\nautomation, this study explores the use of AI agents for automated log\nanalysis. We present a lightweight and automated approach to process Cowrie\nhoneypot logs. Our approach leverages AI agents to intelligently parse,\nsummarize, and extract insights from raw data, while also considering the\nsecurity implications of deploying such an autonomous system. Preliminary\nresults demonstrate the pipeline's effectiveness in reducing manual effort and\nidentifying attack patterns, paving the way for more advanced autonomous\ncybersecurity analysis in future work."}
{"id": "2509.05311", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05311", "abs": "https://arxiv.org/abs/2509.05311", "authors": ["Konur Tholl", "François Rivest", "Mariam El Mezouar", "Ranwa Al Mallah"], "title": "Large Language Model Integration with Reinforcement Learning to Augment Decision-Making in Autonomous Cyber Operations", "comment": null, "summary": "Reinforcement Learning (RL) has shown great potential for autonomous\ndecision-making in the cybersecurity domain, enabling agents to learn through\ndirect environment interaction. However, RL agents in Autonomous Cyber\nOperations (ACO) typically learn from scratch, requiring them to execute\nundesirable actions to learn their consequences. In this study, we integrate\nexternal knowledge in the form of a Large Language Model (LLM) pretrained on\ncybersecurity data that our RL agent can directly leverage to make informed\ndecisions. By guiding initial training with an LLM, we improve baseline\nperformance and reduce the need for exploratory actions with obviously negative\noutcomes. We evaluate our LLM-integrated approach in a simulated cybersecurity\nenvironment, and demonstrate that our guided agent achieves over 2x higher\nrewards during early training and converges to a favorable policy approximately\n4,500 episodes faster than the baseline."}
{"id": "2509.05318", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05318", "abs": "https://arxiv.org/abs/2509.05318", "authors": ["Zuquan Peng", "Jianming Fu", "Lixin Zou", "Li Zheng", "Yanzhen Ren", "Guojun Peng"], "title": "Backdoor Samples Detection Based on Perturbation Discrepancy Consistency in Pre-trained Language Models", "comment": "13 pages, 9 figures, 8 tables, journal", "summary": "The use of unvetted third-party and internet data renders pre-trained models\nsusceptible to backdoor attacks. Detecting backdoor samples is critical to\nprevent backdoor activation during inference or injection during training.\nHowever, existing detection methods often require the defender to have access\nto the poisoned models, extra clean samples, or significant computational\nresources to detect backdoor samples, limiting their practicality. To address\nthis limitation, we propose a backdoor sample detection method based on\nperturbatio\\textbf{N} discr\\textbf{E}pancy consis\\textbf{T}ency\n\\textbf{E}valuation (\\NETE). This is a novel detection method that can be used\nboth pre-training and post-training phases. In the detection process, it only\nrequires an off-the-shelf pre-trained model to compute the log probability of\nsamples and an automated function based on a mask-filling strategy to generate\nperturbations. Our method is based on the interesting phenomenon that the\nchange in perturbation discrepancy for backdoor samples is smaller than that\nfor clean samples. Based on this phenomenon, we use curvature to measure the\ndiscrepancy in log probabilities between different perturbed samples and input\nsamples, thereby evaluating the consistency of the perturbation discrepancy to\ndetermine whether the input sample is a backdoor sample. Experiments conducted\non four typical backdoor attacks and five types of large language model\nbackdoor attacks demonstrate that our detection strategy outperforms existing\nzero-shot black-box detection methods."}
{"id": "2509.05320", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05320", "abs": "https://arxiv.org/abs/2509.05320", "authors": ["Ikhlasse Badidi", "Nouhaila El Khiyaoui", "Aya Riany", "Badr Ben Elallid", "Amine Abouaomar"], "title": "Privacy-Preserving Offloading for Large Language Models in 6G Vehicular Networks", "comment": "7 pages, 6 figures, 1 algorithm, 5 equations", "summary": "The integration of Large Language Models (LLMs) in 6G vehicular networks\npromises unprecedented advancements in intelligent transportation systems.\nHowever, offloading LLM computations from vehicles to edge infrastructure poses\nsignificant privacy risks, potentially exposing sensitive user data. This paper\npresents a novel privacy-preserving offloading framework for LLM-integrated\nvehicular networks. We introduce a hybrid approach combining federated learning\n(FL) and differential privacy (DP) techniques to protect user data while\nmaintaining LLM performance. Our framework includes a privacy-aware task\npartitioning algorithm that optimizes the trade-off between local and edge\ncomputation, considering both privacy constraints and system efficiency. We\nalso propose a secure communication protocol for transmitting model updates and\naggregating results across the network. Experimental results demonstrate that\nour approach achieves 75\\% global accuracy with only a 2-3\\% reduction compared\nto non-privacy-preserving methods, while maintaining DP guarantees with an\noptimal privacy budget of $\\varepsilon = 0.8$. The framework shows stable\ncommunication overhead of approximately 2.1MB per round with computation\ncomprising over 90\\% of total processing time, validating its efficiency for\nresource-constrained vehicular environments."}
{"id": "2509.05326", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05326", "abs": "https://arxiv.org/abs/2509.05326", "authors": ["Logan Nye"], "title": "Zero-Knowledge Proofs in Sublinear Space", "comment": "21 pages", "summary": "Modern zero-knowledge proof (ZKP) systems, essential for privacy and\nverifiable computation, suffer from a fundamental limitation: the prover\ntypically uses memory that scales linearly with the computation's trace length\nT, making them impractical for resource-constrained devices and prohibitively\nexpensive for large-scale tasks. This paper overcomes this barrier by\nconstructing, to our knowledge, the first sublinear-space ZKP prover. Our core\ncontribution is an equivalence that reframes proof generation as an instance of\nthe classic Tree Evaluation problem. Leveraging a recent space-efficient\ntree-evaluation algorithm, we design a streaming prover that assembles the\nproof without ever materializing the full execution trace. The approach reduces\nprover memory from linear in T to O(sqrt(T)) (up to O(log T) lower-order terms)\nwhile preserving proof size, verifier time, and the transcript/security\nguarantees of the underlying system. This enables a shift from specialized,\nserver-bound proving to on-device proving, opening applications in\ndecentralized systems, on-device machine learning, and privacy-preserving\ntechnologies."}
{"id": "2509.05331", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05331", "abs": "https://arxiv.org/abs/2509.05331", "authors": ["Youssef Chakir", "Iyad Lahsen-Cherif"], "title": "ForensicsData: A Digital Forensics Dataset for Large Language Models", "comment": "Accepted to WiMob 2025 (21st International Conference on Wireless and\n  Mobile Computing, Networking and Communications), Marrakesh, Morocco, Oct\n  20-22, 2025. 6 pages, 5 figures, 5 tables. IEEEtran conference format", "summary": "The growing complexity of cyber incidents presents significant challenges for\ndigital forensic investigators, especially in evidence collection and analysis.\nPublic resources are still limited because of ethical, legal, and privacy\nconcerns, even though realistic datasets are necessary to support research and\ntool developments. To address this gap, we introduce ForensicsData, an\nextensive Question-Context-Answer (Q-C-A) dataset sourced from actual malware\nanalysis reports. It consists of more than 5,000 Q-C-A triplets. A unique\nworkflow was used to create the dataset, which extracts structured data, uses\nlarge language models (LLMs) to transform it into Q-C-A format, and then uses a\nspecialized evaluation process to confirm its quality. Among the models\nevaluated, Gemini 2 Flash demonstrated the best performance in aligning\ngenerated content with forensic terminology. ForensicsData aims to advance\ndigital forensics by enabling reproducible experiments and fostering\ncollaboration within the research community."}
{"id": "2509.05332", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05332", "abs": "https://arxiv.org/abs/2509.05332", "authors": ["Christos Anagnostopoulos", "Ioulia Kapsali", "Alexandros Gkillas", "Nikos Piperigkos", "Aris S. Lalos"], "title": "Integrated Simulation Framework for Adversarial Attacks on Autonomous Vehicles", "comment": "6 pages, 2 figures", "summary": "Autonomous vehicles (AVs) rely on complex perception and communication\nsystems, making them vulnerable to adversarial attacks that can compromise\nsafety. While simulation offers a scalable and safe environment for robustness\ntesting, existing frameworks typically lack comprehensive supportfor modeling\nmulti-domain adversarial scenarios. This paper introduces a novel, open-source\nintegrated simulation framework designed to generate adversarial attacks\ntargeting both perception and communication layers of AVs. The framework\nprovides high-fidelity modeling of physical environments, traffic dynamics, and\nV2X networking, orchestrating these components through a unified core that\nsynchronizes multiple simulators based on a single configuration file. Our\nimplementation supports diverse perception-level attacks on LiDAR sensor data,\nalong with communication-level threats such as V2X message manipulation and GPS\nspoofing. Furthermore, ROS 2 integration ensures seamless compatibility with\nthird-party AV software stacks. We demonstrate the framework's effectiveness by\nevaluating the impact of generated adversarial scenarios on a state-of-the-art\n3D object detector, revealing significant performance degradation under\nrealistic conditions."}
{"id": "2509.05350", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05350", "abs": "https://arxiv.org/abs/2509.05350", "authors": ["Joshua Ward", "Yuxuan Yang", "Chi-Hua Wang", "Guang Cheng"], "title": "Ensembling Membership Inference Attacks Against Tabular Generative Models", "comment": null, "summary": "Membership Inference Attacks (MIAs) have emerged as a principled framework\nfor auditing the privacy of synthetic data generated by tabular generative\nmodels, where many diverse methods have been proposed that each exploit\ndifferent privacy leakage signals. However, in realistic threat scenarios, an\nadversary must choose a single method without a priori guarantee that it will\nbe the empirically highest performing option. We study this challenge as a\ndecision theoretic problem under uncertainty and conduct the largest synthetic\ndata privacy benchmark to date. Here, we find that no MIA constitutes a\nstrictly dominant strategy across a wide variety of model architectures and\ndataset domains under our threat model. Motivated by these findings, we propose\nensemble MIAs and show that unsupervised ensembles built on individual attacks\noffer empirically more robust, regret-minimizing strategies than individual\nattacks."}
{"id": "2509.05362", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2509.05362", "abs": "https://arxiv.org/abs/2509.05362", "authors": ["Ismail Hossain", "Sai Puppala", "Sajedul Talukder", "Md Jahangir Alam"], "title": "AI-in-the-Loop: Privacy Preserving Real-Time Scam Detection and Conversational Scambaiting by Leveraging LLMs and Federated Learning", "comment": "This paper got accepted in 26th Privacy Enhancing Technologies\n  Symposium (PETS 2026). We uploaded it into ArXiv as pre-print", "summary": "Scams exploiting real-time social engineering -- such as phishing,\nimpersonation, and phone fraud -- remain a persistent and evolving threat\nacross digital platforms. Existing defenses are largely reactive, offering\nlimited protection during active interactions. We propose a privacy-preserving,\nAI-in-the-loop framework that proactively detects and disrupts scam\nconversations in real time. The system combines instruction-tuned artificial\nintelligence with a safety-aware utility function that balances engagement with\nharm minimization, and employs federated learning to enable continual model\nupdates without raw data sharing. Experimental evaluations show that the system\nproduces fluent and engaging responses (perplexity as low as 22.3, engagement\n$\\approx$0.80), while human studies confirm significant gains in realism,\nsafety, and effectiveness over strong baselines. In federated settings, models\ntrained with FedAvg sustain up to 30 rounds while preserving high engagement\n($\\approx$0.80), strong relevance ($\\approx$0.74), and low PII leakage\n($\\leq$0.0085). Even with differential privacy, novelty and safety remain\nstable, indicating that robust privacy can be achieved without sacrificing\nperformance. The evaluation of guard models (LlamaGuard, LlamaGuard2/3,\nMD-Judge) shows a straightforward pattern: stricter moderation settings reduce\nthe chance of exposing personal information, but they also limit how much the\nmodel engages in conversation. In contrast, more relaxed settings allow longer\nand richer interactions, which improve scam detection, but at the cost of\nhigher privacy risk. To our knowledge, this is the first framework to unify\nreal-time scam-baiting, federated privacy preservation, and calibrated safety\nmoderation into a proactive defense paradigm."}
{"id": "2509.05366", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.05366", "abs": "https://arxiv.org/abs/2509.05366", "authors": ["Umair Amjid", "M. Umar Khan", "S. A. Manan Kirmani"], "title": "A Framework for Detection and Classification of Attacks on Surveillance Cameras under IoT Networks", "comment": null, "summary": "The increasing use of Internet of Things (IoT) devices has led to a rise in\nsecurity related concerns regarding IoT Networks. The surveillance cameras in\nIoT networks are vulnerable to security threats such as brute force and\nzero-day attacks which can lead to unauthorized access by hackers and potential\nspying on the users activities. Moreover, these cameras can be targeted by\nDenial of Service (DOS) attacks, which will make it unavailable for the user.\nThe proposed AI based framework will leverage machine learning algorithms to\nanalyze network traffic and detect anomalous behavior, allowing for quick\ndetection and response to potential intrusions. The framework will be trained\nand evaluated using real-world datasets to learn from past security incidents\nand improve its ability to detect potential intrusion."}
{"id": "2509.05367", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05367", "abs": "https://arxiv.org/abs/2509.05367", "authors": ["Shei Pern Chua", "Thai Zhen Leng", "Teh Kai Jun", "Xiao Li", "Xiaolin Hu"], "title": "Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs", "comment": null, "summary": "Large language models (LLMs) have undergone safety alignment efforts to\nmitigate harmful outputs. However, as LLMs become more sophisticated in\nreasoning, their intelligence may introduce new security risks. While\ntraditional jailbreak attacks relied on singlestep attacks, multi-turn\njailbreak strategies that adapt dynamically to context remain underexplored. In\nthis work, we introduce TRIAL (Trolley-problem Reasoning for Interactive Attack\nLogic), a framework that leverages LLMs ethical reasoning to bypass their\nsafeguards. TRIAL embeds adversarial goals within ethical dilemmas modeled on\nthe trolley problem. TRIAL demonstrates high jailbreak success rates towards\nboth open and close-source models. Our findings underscore a fundamental\nlimitation in AI safety: as models gain advanced reasoning abilities, the\nnature of their alignment may inadvertently allow for more covert security\nvulnerabilities to be exploited. TRIAL raises an urgent need in reevaluating\nsafety alignment oversight strategies, as current safeguards may prove\ninsufficient against context-aware adversarial attack."}
{"id": "2509.05370", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.05370", "abs": "https://arxiv.org/abs/2509.05370", "authors": ["Tanya Joshi", "Krishnendu Guha"], "title": "Quantum AI Algorithm Development for Enhanced Cybersecurity: A Hybrid Approach to Malware Detection", "comment": "10 pages", "summary": "This study explores the application of quantum machine learning (QML)\nalgorithms to enhance cybersecurity threat detection, particularly in the\nclassification of malware and intrusion detection within high-dimensional\ndatasets. Classical machine learning approaches encounter limitations when\ndealing with intricate, obfuscated malware patterns and extensive network\nintrusion data. To address these challenges, we implement and evaluate various\nQML algorithms, including Quantum Neural Networks (QNN), Quantum Support Vector\nMachines (QSVM), and hybrid Quantum Convolutional Neural Networks (QCNN) for\nmalware detection tasks. Our experimental analysis utilized two datasets: the\nIntrusion dataset, comprising 150 samples with 56 memory-based features derived\nfrom Volatility framework analysis, and the ObfuscatedMalMem2022 dataset,\ncontaining 58,596 samples with 57 features representing benign and malicious\nsoftware. Remarkably, our QML methods demonstrated superior performance\ncompared to classical approaches, achieving accuracies of 95% for QNN and 94%\nfor QSVM. These quantum-enhanced methods leveraged quantum superposition and\nentanglement principles to accurately identify complex patterns within highly\nobfuscated malware samples that were imperceptible to classical methods. To\nfurther advance malware analysis, we propose a novel real-time malware analysis\nframework that incorporates Quantum Feature Extraction using Quantum Fourier\nTransform, Quantum Feature Maps, and Classification using Variational Quantum\nCircuits. This system integrates explainable AI methods, including GradCAM++\nand ScoreCAM algorithms, to provide interpretable insights into the quantum\ndecision-making processes."}
{"id": "2509.05376", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05376", "abs": "https://arxiv.org/abs/2509.05376", "authors": ["Abdul Rehman", "Are Dæhlen", "Ilona Heldal", "Jerry Chun-wei Lin"], "title": "Privacy Preservation and Identity Tracing Prevention in AI-Driven Eye Tracking for Interactive Learning Environments", "comment": null, "summary": "Eye-tracking technology can aid in understanding neurodevelopmental disorders\nand tracing a person's identity. However, this technology poses a significant\nrisk to privacy, as it captures sensitive information about individuals and\nincreases the likelihood that data can be traced back to them. This paper\nproposes a human-centered framework designed to prevent identity backtracking\nwhile preserving the pedagogical benefits of AI-powered eye tracking in\ninteractive learning environments. We explore how real-time data anonymization,\nethical design principles, and regulatory compliance (such as GDPR) can be\nintegrated to build trust and transparency. We first demonstrate the potential\nfor backtracking student IDs and diagnoses in various scenarios using serious\ngame-based eye-tracking data. We then provide a two-stage privacy-preserving\nframework that prevents participants from being tracked while still enabling\ndiagnostic classification. The first phase covers four scenarios: I) Predicting\ndisorder diagnoses based on different game levels. II) Predicting student IDs\nbased on different game levels. III) Predicting student IDs based on randomized\ndata. IV) Utilizing K-Means for out-of-sample data. In the second phase, we\npresent a two-stage framework that preserves privacy. We also employ Federated\nLearning (FL) across multiple clients, incorporating a secure identity\nmanagement system with dummy IDs and administrator-only access controls. In the\nfirst phase, the proposed framework achieved 99.3% accuracy for scenario 1, 63%\naccuracy for scenario 2, and 99.7% accuracy for scenario 3, successfully\nidentifying and assigning a new student ID in scenario 4. In phase 2, we\neffectively prevented backtracking and established a secure identity management\nsystem with dummy IDs and administrator-only access controls, achieving an\noverall accuracy of 99.40%."}
{"id": "2509.05379", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05379", "abs": "https://arxiv.org/abs/2509.05379", "authors": ["Sharif Noor Zisad", "Ragib Hasan"], "title": "ThreatGPT: An Agentic AI Framework for Enhancing Public Safety through Threat Modeling", "comment": null, "summary": "As our cities and communities become smarter, the systems that keep us safe,\nsuch as traffic control centers, emergency response networks, and public\ntransportation, also become more complex. With this complexity comes a greater\nrisk of security threats that can affect not just machines but real people's\nlives. To address this challenge, we present ThreatGPT, an agentic Artificial\nIntelligence (AI) assistant built to help people whether they are engineers,\nsafety officers, or policy makers to understand and analyze threats in public\nsafety systems. Instead of requiring deep cybersecurity expertise, it allows\nusers to simply describe the components of a system they are concerned about,\nsuch as login systems, data storage, or communication networks. Then, with the\nclick of a button, users can choose how they want the system to be analyzed by\nusing popular frameworks such as STRIDE, MITRE ATT&CK, CVE reports, NIST, or\nCISA. ThreatGPT is unique because it does not just provide threat information,\nbut rather it acts like a knowledgeable partner. Using few-shot learning, the\nAI learns from examples and generates relevant smart threat models. It can\nhighlight what might go wrong, how attackers could take advantage, and what can\nbe done to prevent harm. Whether securing a city's infrastructure or a local\nhealth service, this tool adapts to users' needs. In simple terms, ThreatGPT\nbrings together AI and human judgment to make our public systems safer. It is\ndesigned not just to analyze threats, but to empower people to understand and\nact on them, faster, smarter, and with more confidence."}
{"id": "2509.05471", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05471", "abs": "https://arxiv.org/abs/2509.05471", "authors": ["Youjia Zheng", "Mohammad Zandsalimy", "Shanu Sushmita"], "title": "Behind the Mask: Benchmarking Camouflaged Jailbreaks in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) are increasingly vulnerable to a sophisticated\nform of adversarial prompting known as camouflaged jailbreaking. This method\nembeds malicious intent within seemingly benign language to evade existing\nsafety mechanisms. Unlike overt attacks, these subtle prompts exploit\ncontextual ambiguity and the flexible nature of language, posing significant\nchallenges to current defense systems. This paper investigates the construction\nand impact of camouflaged jailbreak prompts, emphasizing their deceptive\ncharacteristics and the limitations of traditional keyword-based detection\nmethods. We introduce a novel benchmark dataset, Camouflaged Jailbreak Prompts,\ncontaining 500 curated examples (400 harmful and 100 benign prompts) designed\nto rigorously stress-test LLM safety protocols. In addition, we propose a\nmulti-faceted evaluation framework that measures harmfulness across seven\ndimensions: Safety Awareness, Technical Feasibility, Implementation Safeguards,\nHarmful Potential, Educational Value, Content Quality, and Compliance Score.\nOur findings reveal a stark contrast in LLM behavior: while models demonstrate\nhigh safety and content quality with benign inputs, they exhibit a significant\ndecline in performance and safety when confronted with camouflaged jailbreak\nattempts. This disparity underscores a pervasive vulnerability, highlighting\nthe urgent need for more nuanced and adaptive security strategies to ensure the\nresponsible and robust deployment of LLMs in real-world applications."}
{"id": "2509.05496", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.05496", "abs": "https://arxiv.org/abs/2509.05496", "authors": ["Charbel Mattar", "Jacques Bou Abdo", "Abdallah Makhoul", "Benoit Piranda", "Jacques Demerjian"], "title": "What is Cybersecurity in Space?", "comment": null, "summary": "Satellites, drones, and 5G space links now support\n  critical services such as air traffic, finance, and weather. Yet most\n  were not built to resist modern cyber threats. Ground stations\n  can be breached, GPS jammed, and supply chains compromised,\n  while no shared list of vulnerabilities or safe testing range exists.\n  This paper maps eleven research gaps, including secure\n  routing, onboard intrusion detection, recovery methods, trusted\n  supply chains, post-quantum encryption, zero-trust architectures,\n  and real-time impact monitoring. For each, we outline the\n  challenge, why it matters, and a guiding research question. We\n  also highlight an agentic (multi-agent) AI approach where small,\n  task-specific agents share defense tasks onboard instead of one\n  large model.\n  Finally, we propose a five-year roadmap: post-quantum and\n  QKD flight trials, open cyber-ranges, clearer vulnerability shar ing, and\nearly multi-agent deployments. These steps move space\n  cybersecurity from reactive patching toward proactive resilience."}
{"id": "2509.05552", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.05552", "abs": "https://arxiv.org/abs/2509.05552", "authors": ["Ali Arastehfard", "Weiran Liu", "Joshua Lee", "Bingyu Liu", "Xuegang Ban", "Yuan Hong"], "title": "Secure and Efficient $L^p$-Norm Computation for Two-Party Learning Applications", "comment": null, "summary": "Secure norm computation is becoming increasingly important in many real-world\nlearning applications. However, existing cryptographic systems often lack a\ngeneral framework for securely computing the $L^p$-norm over private inputs\nheld by different parties. These systems often treat secure norm computation as\na black-box process, neglecting to design tailored cryptographic protocols that\noptimize performance. Moreover, they predominantly focus on the $L^2$-norm,\npaying little attention to other popular $L^p$-norms, such as $L^1$ and\n$L^\\infty$, which are commonly used in practice, such as machine learning tasks\nand location-based services.\n  To our best knowledge, we propose the first comprehensive framework for\nsecure two-party $L^p$-norm computations ($L^1$, $L^2$, and $L^\\infty$),\ndenoted as \\mbox{Crypto-$L^p$}, designed to be versatile across various\napplications. We have designed, implemented, and thoroughly evaluated our\nframework across a wide range of benchmarking applications, state-of-the-art\n(SOTA) cryptographic protocols, and real-world datasets to validate its\neffectiveness and practical applicability. In summary, \\mbox{Crypto-$L^p$}\noutperforms prior works on secure $L^p$-norm computation, achieving $82\\times$,\n$271\\times$, and $42\\times$ improvements in runtime while reducing\ncommunication overhead by $36\\times$, $4\\times$, and $21\\times$ for $p=1$, $2$,\nand $\\infty$, respectively. Furthermore, we take the first step in adapting our\nCrypto-$L^p$ framework for secure machine learning inference, reducing\ncommunication costs by $3\\times$ compared to SOTA systems while maintaining\ncomparable runtime and accuracy."}
{"id": "2509.05608", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05608", "abs": "https://arxiv.org/abs/2509.05608", "authors": ["Waris Gill", "Natalie Isak", "Matthew Dressman"], "title": "Cross-Service Threat Intelligence in LLM Services using Privacy-Preserving Fingerprints", "comment": null, "summary": "The widespread deployment of LLMs across enterprise services has created a\ncritical security blind spot. Organizations operate multiple LLM services\nhandling billions of queries daily, yet regulatory compliance boundaries\nprevent these services from sharing threat intelligence about prompt injection\nattacks, the top security risk for LLMs. When an attack is detected in one\nservice, the same threat may persist undetected in others for months, as\nprivacy regulations prohibit sharing user prompts across compliance boundaries.\n  We present BinaryShield, the first privacy-preserving threat intelligence\nsystem that enables secure sharing of attack fingerprints across compliance\nboundaries. BinaryShield transforms suspicious prompts through a unique\npipeline combining PII redaction, semantic embedding, binary quantization, and\nrandomized response mechanism to potentially generate non-invertible\nfingerprints that preserve attack patterns while providing privacy. Our\nevaluations demonstrate that BinaryShield achieves an F1-score of 0.94,\nsignificantly outperforming SimHash (0.77), the privacy-preserving baseline,\nwhile achieving 64x storage reduction and 38x faster similarity search compared\nto dense embeddings."}
{"id": "2509.05643", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.05643", "abs": "https://arxiv.org/abs/2509.05643", "authors": ["Carmine Cesarano", "Roberto Natella"], "title": "FuzzBox: Blending Fuzzing into Emulation for Binary-Only Embedded Targets", "comment": null, "summary": "Coverage-guided fuzzing has been widely applied to address zero-day\nvulnerabilities in general-purpose software and operating systems. This\napproach relies on instrumenting the target code at compile time. However,\napplying it to industrial systems remains challenging, due to proprietary and\nclosed-source compiler toolchains and lack of access to source code. FuzzBox\naddresses these limitations by integrating emulation with fuzzing: it\ndynamically instruments code during execution in a virtualized environment, for\nthe injection of fuzz inputs, failure detection, and coverage analysis, without\nrequiring source code recompilation and hardware-specific dependencies. We show\nthe effectiveness of FuzzBox through experiments in the context of a\nproprietary MILS (Multiple Independent Levels of Security) hypervisor for\nindustrial applications. Additionally, we analyze the applicability of FuzzBox\nacross commercial IoT firmware, showcasing its broad portability."}
{"id": "2509.05681", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05681", "abs": "https://arxiv.org/abs/2509.05681", "authors": ["Xng Ai", "Shudan Lin", "Zecheng Li", "Kai Zhou", "Bixin Li", "Bin Xiao"], "title": "SEASONED: Semantic-Enhanced Self-Counterfactual Explainable Detection of Adversarial Exploiter Contracts", "comment": null, "summary": "Decentralized Finance (DeFi) attacks have resulted in significant losses,\noften orchestrated through Adversarial Exploiter Contracts (AECs) that exploit\nvulnerabilities in victim smart contracts. To proactively identify such\nthreats, this paper targets the explainable detection of AECs.\n  Existing detection methods struggle to capture semantic dependencies and lack\ninterpretability, limiting their effectiveness and leaving critical knowledge\ngaps in AEC analysis. To address these challenges, we introduce SEASONED, an\neffective, self-explanatory, and robust framework for AEC detection.\n  SEASONED extracts semantic information from contract bytecode to construct a\nsemantic relation graph (SRG), and employs a self-counterfactual explainable\ndetector (SCFED) to classify SRGs and generate explanations that highlight the\ncore attack logic. SCFED further enhances robustness, generalizability, and\ndata efficiency by extracting representative information from these\nexplanations. Both theoretical analysis and experimental results demonstrate\nthe effectiveness of SEASONED, which showcases outstanding detection\nperformance, robustness, generalizability, and data efficiency learning\nability. To support further research, we also release a new dataset of 359\nAECs."}
{"id": "2509.05698", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.05698", "abs": "https://arxiv.org/abs/2509.05698", "authors": ["Yuhan Meng", "Shaofei Li", "Jiaping Gui", "Peng Jiang", "Ding Li"], "title": "KnowHow: Automatically Applying High-Level CTI Knowledge for Interpretable and Accurate Provenance Analysis", "comment": "Accepted by NDSS 2026", "summary": "High-level natural language knowledge in CTI reports, such as the ATT&CK\nframework, is beneficial to counter APT attacks. However, how to automatically\napply the high-level knowledge in CTI reports in realistic attack detection\nsystems, such as provenance analysis systems, is still an open problem. The\nchallenge stems from the semantic gap between the knowledge and the low-level\nsecurity logs: while the knowledge in CTI reports is written in natural\nlanguage, attack detection systems can only process low-level system events\nlike file accesses or network IP manipulations. Manual approaches can be\nlabor-intensive and error-prone.\n  In this paper, we propose KnowHow, a CTI-knowledge-driven online provenance\nanalysis approach that can automatically apply high-level attack knowledge from\nCTI reports written in natural languages to detect low-level system events. The\ncore of KnowHow is a novel attack knowledge representation, gIoC, that\nrepresents the subject, object, and actions of attacks. By lifting system\nidentifiers, such as file paths, in system events to natural language terms,\nKnowHow can match system events to gIoC and further match them to techniques\ndescribed in natural languages. Finally, based on the techniques matched to\nsystem events, KnowHow reasons about the temporal logic of attack steps and\ndetects potential APT attacks in system events. Our evaluation shows that\nKnowHow can accurately detect all 16 APT campaigns in the open-source and\nindustrial datasets, while existing approaches all introduce large numbers of\nfalse positives. Meanwhile, our evaluation also shows that KnowHow reduces at\nmost 90% of node-level false positives while having a higher node-level recall\nand is robust against several unknown attacks and mimicry attacks."}
{"id": "2509.05708", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.05708", "abs": "https://arxiv.org/abs/2509.05708", "authors": ["Junjie Hu", "Na Ruan"], "title": "Larger-scale Nakamoto-style Blockchains Offer Better Security", "comment": "22 pages, 2 figures", "summary": "Traditional security models for Nakamoto-style blockchains overestimate\nadversarial coordination by assuming instantaneous synchronization among\nmalicious nodes, neglecting the critical impact of internal communication\ndelays on security. This paper introduces a dual-delay framework to revisit\nsecurity analysis, addressing this oversight through two key innovations.\nFirst, the static delay model quantifies how adversarial communication delays\n(\\(\\Delta_a\\)) constrain the effective growth rate of private chains, derived\nvia an M/D/1 queuing model as \\(\\lambda_{eff} = \\lambda_a / (1 + \\lambda_a\n\\Delta_a)\\). This model reveals that the security threshold (\\(\\beta^*\\)), the\nmaximum adversarial power the system tolerates, increases with \\(\\Delta_a\\),\neven exceeding the classic 51\\% boundary when \\(\\Delta_a \\textgreater \\Delta\\)\n(honest nodes' delay), breaking the long-standing 50\\% assumption. Second, the\ndynamic delay model integrates probabilistic corruption and scale-dependent\ndelays to characterize the total adversarial delay window (\\(\\Delta_{total} =\n\\Delta(n) e^{-k\\beta} + c \\log(1 + \\beta n)\\)), where \\(\\Delta(n) \\in\n\\Theta(\\log n)\\) captures honest nodes' logarithmic delay growth. Asymptotic\nanalysis shows adversarial power decays linearly with network scale, ensuring\nthe probability of \\(\\beta \\leq \\beta^*\\) approaches 1 as \\(n \\to \\infty\\). By\nexposing the interplay between network scale, communication delays, and power\ndilution, we provide a theoretical foundation for optimizing consensus\nprotocols and assessing robustness in large-scale Nakamoto-style blockchains."}
{"id": "2509.05739", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05739", "abs": "https://arxiv.org/abs/2509.05739", "authors": ["Hanna Foerster", "Ilia Shumailov", "Yiren Zhao", "Harsh Chaudhari", "Jamie Hayes", "Robert Mullins", "Yarin Gal"], "title": "Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated", "comment": null, "summary": "Early research into data poisoning attacks against Large Language Models\n(LLMs) demonstrated the ease with which backdoors could be injected. More\nrecent LLMs add step-by-step reasoning, expanding the attack surface to include\nthe intermediate chain-of-thought (CoT) and its inherent trait of decomposing\nproblems into subproblems. Using these vectors for more stealthy poisoning, we\nintroduce ``decomposed reasoning poison'', in which the attacker modifies only\nthe reasoning path, leaving prompts and final answers clean, and splits the\ntrigger across multiple, individually harmless components.\n  Fascinatingly, while it remains possible to inject these decomposed poisons,\nreliably activating them to change final answers (rather than just the CoT) is\nsurprisingly difficult. This difficulty arises because the models can often\nrecover from backdoors that are activated within their thought processes.\nUltimately, it appears that an emergent form of backdoor robustness is\noriginating from the reasoning capabilities of these advanced LLMs, as well as\nfrom the architectural separation between reasoning and final answer\ngeneration."}
{"id": "2509.05753", "categories": ["cs.CR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05753", "abs": "https://arxiv.org/abs/2509.05753", "authors": ["Ching-Chun Chang", "Isao Echizen"], "title": "Tell-Tale Watermarks for Explanatory Reasoning in Synthetic Media Forensics", "comment": null, "summary": "The rise of synthetic media has blurred the boundary between reality and\nfabrication under the evolving power of artificial intelligence, fueling an\ninfodemic that erodes public trust in cyberspace. For digital imagery, a\nmultitude of editing applications further complicates the forensic analysis,\nincluding semantic edits that alter content, photometric adjustments that\nrecalibrate colour characteristics, and geometric projections that reshape\nviewpoints. Collectively, these transformations manipulate and control\nperceptual interpretation of digital imagery. This susceptibility calls for\nforensic enquiry into reconstructing the chain of events, thereby revealing\ndeeper evidential insight into the presence or absence of criminal intent. This\nstudy seeks to address an inverse problem of tracing the underlying generation\nchain that gives rise to the observed synthetic media. A tell-tale watermarking\nsystem is developed for explanatory reasoning over the nature and extent of\ntransformations across the lifecycle of synthetic media. Tell-tale watermarks\nare tailored to different classes of transformations, responding in a manner\nthat is neither strictly robust nor fragile but instead interpretable. These\nwatermarks function as reference clues that evolve under the same\ntransformation dynamics as the carrier media, leaving interpretable traces when\nsubjected to transformations. Explanatory reasoning is then performed to infer\nthe most plausible account across the combinatorial parameter space of\ncomposite transformations. Experimental evaluations demonstrate the validity of\ntell-tale watermarking with respect to fidelity, synchronicity and\ntraceability."}
{"id": "2509.05755", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05755", "abs": "https://arxiv.org/abs/2509.05755", "authors": ["Yu Liu", "Yuchong Xie", "Mingyu Luo", "Zesen Liu", "Zhixiang Zhang", "Kaikai Zhang", "Zongjie Li", "Ping Chen", "Shuai Wang", "Dongdong She"], "title": "Exploit Tool Invocation Prompt for Tool Behavior Hijacking in LLM-Based Agentic System", "comment": null, "summary": "LLM-based agentic systems leverage large language models to handle user\nqueries, make decisions, and execute external tools for complex tasks across\ndomains like chatbots, customer service, and software engineering. A critical\ncomponent of these systems is the Tool Invocation Prompt (TIP), which defines\ntool interaction protocols and guides LLMs to ensure the security and\ncorrectness of tool usage. Despite its importance, TIP security has been\nlargely overlooked. This work investigates TIP-related security risks,\nrevealing that major LLM-based systems like Cursor, Claude Code, and others are\nvulnerable to attacks such as remote code execution (RCE) and denial of service\n(DoS). Through a systematic TIP exploitation workflow (TEW), we demonstrate\nexternal tool behavior hijacking via manipulated tool invocations. We also\npropose defense mechanisms to enhance TIP security in LLM-based agentic\nsystems."}
{"id": "2509.05797", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.05797", "abs": "https://arxiv.org/abs/2509.05797", "authors": ["Hai Dinh-Tuan", "Sandro Rodriguez Garzon", "Jianeng Fu"], "title": "Secure and Trustful Cross-domain Communication with Decentralized Identifiers in 5G and Beyond", "comment": null, "summary": "In the evolving landscape of future mobile networks, there is a critical need\nfor secure and trustful communication modalities to support dynamic\ninteractions among core network components of different network domains. This\npaper proposes the application of W3C-endorsed Decentralized Identifiers (DIDs)\nto establish secure and trustful communication channels among network functions\nin 5G and subsequent generations. A new communication agent is introduced that\nintegrates seamlessly with 5G-standardized network functions and utilizes a\nDID-based application layer transport protocol to ensure confidentiality,\nintegrity, and authenticity for cross-domain interactions. A comparative\nanalysis of the two different versions of the DID-based communication protocol\nfor inter network function communication reveals compatibility advantages of\nthe latest protocol iteration. Furthermore, a comprehensive evaluation of the\ncommunication overhead caused by both protocol iterations compared to\ntraditional TCP/TLS shows the benefits of using DIDs to improve communication\nsecurity, albeit with performance loses compared to TCP/TLS. These results\nuncover the potential of DID-based communication for future mobile networks but\nalso point out areas for optimization."}
{"id": "2509.05831", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05831", "abs": "https://arxiv.org/abs/2509.05831", "authors": ["Ishaan Verma"], "title": "Decoding Latent Attack Surfaces in LLMs: Prompt Injection via HTML in Web Summarization", "comment": null, "summary": "Large Language Models (LLMs) are increasingly integrated into web-based\nsystems for content summarization, yet their susceptibility to prompt injection\nattacks remains a pressing concern. In this study, we explore how non-visible\nHTML elements such as <meta>, aria-label, and alt attributes can be exploited\nto embed adversarial instructions without altering the visible content of a\nwebpage. We introduce a novel dataset comprising 280 static web pages, evenly\ndivided between clean and adversarial injected versions, crafted using diverse\nHTML-based strategies. These pages are processed through a browser automation\npipeline to extract both raw HTML and rendered text, closely mimicking\nreal-world LLM deployment scenarios. We evaluate two state-of-the-art\nopen-source models, Llama 4 Scout (Meta) and Gemma 9B IT (Google), on their\nability to summarize this content. Using both lexical (ROUGE-L) and semantic\n(SBERT cosine similarity) metrics, along with manual annotations, we assess the\nimpact of these covert injections. Our findings reveal that over 29% of\ninjected samples led to noticeable changes in the Llama 4 Scout summaries,\nwhile Gemma 9B IT showed a lower, yet non-trivial, success rate of 15%. These\nresults highlight a critical and largely overlooked vulnerability in LLM driven\nweb pipelines, where hidden adversarial content can subtly manipulate model\noutputs. Our work offers a reproducible framework and benchmark for evaluating\nHTML-based prompt injection and underscores the urgent need for robust\nmitigation strategies in LLM applications involving web content."}
{"id": "2509.05835", "categories": ["cs.CR", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.05835", "abs": "https://arxiv.org/abs/2509.05835", "authors": ["Lingfeng Yao", "Chenpei Huang", "Shengyao Wang", "Junpei Xue", "Hanqing Guo", "Jiang Liu", "Phone Lin", "Tomoaki Ohtsuki", "Miao Pan"], "title": "Yours or Mine? Overwriting Attacks against Neural Audio Watermarking", "comment": null, "summary": "As generative audio models are rapidly evolving, AI-generated audios\nincreasingly raise concerns about copyright infringement and misinformation\nspread. Audio watermarking, as a proactive defense, can embed secret messages\ninto audio for copyright protection and source verification. However, current\nneural audio watermarking methods focus primarily on the imperceptibility and\nrobustness of watermarking, while ignoring its vulnerability to security\nattacks. In this paper, we develop a simple yet powerful attack: the\noverwriting attack that overwrites the legitimate audio watermark with a forged\none and makes the original legitimate watermark undetectable. Based on the\naudio watermarking information that the adversary has, we propose three\ncategories of overwriting attacks, i.e., white-box, gray-box, and black-box\nattacks. We also thoroughly evaluate the proposed attacks on state-of-the-art\nneural audio watermarking methods. Experimental results demonstrate that the\nproposed overwriting attacks can effectively compromise existing watermarking\nschemes across various settings and achieve a nearly 100% attack success rate.\nThe practicality and effectiveness of the proposed overwriting attacks expose\nsecurity flaws in existing neural audio watermarking systems, underscoring the\nneed to enhance security in future audio watermarking designs."}
{"id": "2509.05883", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05883", "abs": "https://arxiv.org/abs/2509.05883", "authors": ["Andrew Yeo", "Daeseon Choi"], "title": "Multimodal Prompt Injection Attacks: Risks and Defenses for Modern LLMs", "comment": "8 pages, 4 figures, 2 tables", "summary": "Large Language Models (LLMs) have seen rapid adoption in recent years, with\nindustries increasingly relying on them to maintain a competitive advantage.\nThese models excel at interpreting user instructions and generating human-like\nresponses, leading to their integration across diverse domains, including\nconsulting and information retrieval. However, their widespread deployment also\nintroduces substantial security risks, most notably in the form of prompt\ninjection and jailbreak attacks.\n  To systematically evaluate LLM vulnerabilities -- particularly to external\nprompt injection -- we conducted a series of experiments on eight commercial\nmodels. Each model was tested without supplementary sanitization, relying\nsolely on its built-in safeguards. The results exposed exploitable weaknesses\nand emphasized the need for stronger security measures. Four categories of\nattacks were examined: direct injection, indirect (external) injection,\nimage-based injection, and prompt leakage. Comparative analysis indicated that\nClaude 3 demonstrated relatively greater robustness; nevertheless, empirical\nfindings confirm that additional defenses, such as input normalization, remain\nnecessary to achieve reliable protection."}
{"id": "2509.05884", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.05884", "abs": "https://arxiv.org/abs/2509.05884", "authors": ["Banhirup Sengupta", "Peenal Gupta", "Souvik Sengupta"], "title": "Introduction to Number Theoretic Transform", "comment": null, "summary": "The Number Theoretic Transform (NTT) can be regarded as a variant of the\nDiscrete Fourier Transform. NTT has been quite a powerful mathematical tool in\ndeveloping Post-Quantum Cryptography and Homomorphic Encryption. The Fourier\nTransform essentially decomposes a signal into its frequencies. They are\ntraditionally sine or cosine waves. NTT works more over groups or finite fields\nrather than on a continuous signal and polynomials work as the analog of sine\nwaves in case of NTT. Fast Fourier Trnasform (FFT) style NTT or fast NTT has\nbeen proven to be useful in lattice-based cryptography due to its ability to\nreduce the complexity of polynomial multiplication from quadratic to\nquasilinear. We have introduced the concepts of cyclic, negacyclic convolutions\nalong with NTT and its inverse and their fast versions."}
{"id": "2509.05891", "categories": ["cs.CR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.05891", "abs": "https://arxiv.org/abs/2509.05891", "authors": ["Mahfuzul I. Nissan"], "title": "MemTraceDB: Reconstructing MySQL User Activity Using ActiviTimeTrace Algorithm", "comment": null, "summary": "Database audit and transaction logs are fundamental to forensic\ninvestigations, but they are vulnerable to tampering by privileged attackers.\nMalicious insiders or external threats with administrative access can alter,\npurge, or temporarily disable logging mechanisms, creating significant blind\nspots and rendering disk-based records unreliable. Memory analysis offers a\nvital alternative, providing investigators direct access to volatile artifacts\nthat represent a ground-truth source of recent user activity, even when log\nfiles have been compromised.\n  This paper introduces MemTraceDB, a tool that reconstructs user activity\ntimelines by analyzing raw memory snapshots from the MySQL database process.\nMemTraceDB utilizes a novel algorithm, ActiviTimeTrace, to systematically\nextract and correlate forensic artifacts such as user connections and executed\nqueries. Through a series of experiments, I demonstrate MemTraceDB's\neffectiveness and reveal a critical empirical finding: the MySQL query stack\nhas a finite operational capacity of approximately 9,997 queries. This\ndiscovery allows me to establish a practical, data-driven formula for\ndetermining the optimal frequency for memory snapshot collection, providing a\nclear, actionable guideline for investigators. The result is a\nforensically-sound reconstruction of user activity, independent of compromised\ndisk-based logs."}
{"id": "2509.05893", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.05893", "abs": "https://arxiv.org/abs/2509.05893", "authors": ["Colin Roberts", "Vivek Nair", "Dawn Song"], "title": "Wrangling Entropy: Next-Generation Multi-Factor Key Derivation, Credential Hashing, and Credential Generation Functions", "comment": "Work in progress. Learn more about MFKDF at https://mfkdf.com and\n  Multifactor at https://multifactor.com", "summary": "The Multi-Factor Key Derivation Function (MFKDF) offered a novel solution to\nthe classic problem of usable client-side key management by incorporating\nmultiple popular authentication factors into a key derivation process, but was\nlater shown to be vulnerable to cryptanalysis that degraded its security over\nmultiple invocations. In this paper, we present the Entropy State Transition\nModeling Framework (ESTMF), a novel cryptanalytic technique designed to reveal\npernicious leaks of entropy across multiple invocations of a cryptographic key\nderivation or hash function, and show that it can be used to correctly identify\neach of the known vulnerabilities in the original MFKDF construction. We then\nuse these findings to propose a new construction for ``MFKDF2,'' a\nnext-generation multi-factor key derivation function that can be proven to be\nend-to-end secure using the ESTMF. Finally, we discuss how MFKDF2 can be\nextended to support more authentication factors and usability features than the\nprevious MFKDF construction, and derive several generalizable best-practices\nfor the construction of new KDFs in the future."}
{"id": "2509.05921", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.05921", "abs": "https://arxiv.org/abs/2509.05921", "authors": ["Kun Li", "Cheng Wang", "Minghui Xu", "Yue Zhang", "Xiuzhen Cheng"], "title": "Dataset Ownership in the Era of Large Language Models", "comment": "15 pages, 1 table, accepted by the 2025 International Conference on\n  Blockchain and Web3.0 Technology Innovation and Application Exchange (BWTAC)", "summary": "As datasets become critical assets in modern machine learning systems,\nensuring robust copyright protection has emerged as an urgent challenge.\nTraditional legal mechanisms often fail to address the technical complexities\nof digital data replication and unauthorized use, particularly in opaque or\ndecentralized environments. This survey provides a comprehensive review of\ntechnical approaches for dataset copyright protection, systematically\ncategorizing them into three main classes: non-intrusive methods, which detect\nunauthorized use without modifying data; minimally-intrusive methods, which\nembed lightweight, reversible changes to enable ownership verification; and\nmaximally-intrusive methods, which apply aggressive data alterations, such as\nreversible adversarial examples, to enforce usage restrictions. We synthesize\nkey techniques, analyze their strengths and limitations, and highlight open\nresearch challenges. This work offers an organized perspective on the current\nlandscape and suggests future directions for developing unified, scalable, and\nethically sound solutions to protect datasets in increasingly complex machine\nlearning ecosystems."}
{"id": "2509.06026", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06026", "abs": "https://arxiv.org/abs/2509.06026", "authors": ["Xinyu Gao", "Xiangtao Meng", "Yingkai Dong", "Zheng Li", "Shanqing Guo"], "title": "DCMI: A Differential Calibration Membership Inference Attack Against Retrieval-Augmented Generation", "comment": null, "summary": "While Retrieval-Augmented Generation (RAG) effectively reduces hallucinations\nby integrating external knowledge bases, it introduces vulnerabilities to\nmembership inference attacks (MIAs), particularly in systems handling sensitive\ndata. Existing MIAs targeting RAG's external databases often rely on model\nresponses but ignore the interference of non-member-retrieved documents on RAG\noutputs, limiting their effectiveness. To address this, we propose DCMI, a\ndifferential calibration MIA that mitigates the negative impact of\nnon-member-retrieved documents. Specifically, DCMI leverages the sensitivity\ngap between member and non-member retrieved documents under query perturbation.\nIt generates perturbed queries for calibration to isolate the contribution of\nmember-retrieved documents while minimizing the interference from\nnon-member-retrieved documents. Experiments under progressively relaxed\nassumptions show that DCMI consistently outperforms baselines--for example,\nachieving 97.42% AUC and 94.35% Accuracy against the RAG system with Flan-T5,\nexceeding the MBA baseline by over 40%. Furthermore, on real-world RAG\nplatforms such as Dify and MaxKB, DCMI maintains a 10%-20% advantage over the\nbaseline. These results highlight significant privacy risks in RAG systems and\nemphasize the need for stronger protection mechanisms. We appeal to the\ncommunity's consideration of deeper investigations, like ours, against the data\nleakage risks in rapidly evolving RAG systems. Our code is available at\nhttps://github.com/Xinyu140203/RAG_MIA."}
{"id": "2509.06071", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.06071", "abs": "https://arxiv.org/abs/2509.06071", "authors": ["Yang Lou", "Haibo Hu", "Qun Song", "Qian Xu", "Yi Zhu", "Rui Tan", "Wei-Bin Lee", "Jianping Wang"], "title": "Asymmetry Vulnerability and Physical Attacks on Online Map Construction for Autonomous Driving", "comment": "CCS'25 (a shorter version of this paper will appear in the conference\n  proceeding)", "summary": "High-definition maps provide precise environmental information essential for\nprediction and planning in autonomous driving systems. Due to the high cost of\nlabeling and maintenance, recent research has turned to online HD map\nconstruction using onboard sensor data, offering wider coverage and more timely\nupdates for autonomous vehicles. However, the robustness of online map\nconstruction under adversarial conditions remains underexplored. In this paper,\nwe present a systematic vulnerability analysis of online map construction\nmodels, which reveals that these models exhibit an inherent bias toward\npredicting symmetric road structures. In asymmetric scenes like forks or\nmerges, this bias often causes the model to mistakenly predict a straight\nboundary that mirrors the opposite side. We demonstrate that this vulnerability\npersists in the real-world and can be reliably triggered by obstruction or\ntargeted interference. Leveraging this vulnerability, we propose a novel\ntwo-stage attack framework capable of manipulating online constructed maps.\nFirst, our method identifies vulnerable asymmetric scenes along the victim AV's\npotential route. Then, we optimize the location and pattern of camera-blinding\nattacks and adversarial patch attacks. Evaluations on a public AD dataset\ndemonstrate that our attacks can degrade mapping accuracy by up to 9.9%, render\nup to 44% of targeted routes unreachable, and increase unsafe planned\ntrajectory rates, colliding with real-world road boundaries, by up to 27%.\nThese attacks are also validated on a real-world testbed vehicle. We further\nanalyze root causes of the symmetry bias, attributing them to training data\nimbalance, model architecture, and map element representation. To the best of\nour knowledge, this study presents the first vulnerability assessment of online\nmap construction models and introduces the first digital and physical attack\nagainst them."}
{"id": "2509.06112", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.06112", "abs": "https://arxiv.org/abs/2509.06112", "authors": ["Yanwei Gong", "Ruichen Zhang", "Xiaoqing Wang", "Xiaolin Chang", "Bo Ai", "Junchao Fan", "Bocheng Ju", "Dusit Niyato"], "title": "Towards Reliable Service Provisioning for Dynamic UAV Clusters in Low-Altitude Economy Networks", "comment": null, "summary": "Unmanned Aerial Vehicle (UAV) cluster services are crucial for promoting the\nlow-altitude economy by enabling scalable, flexible, and adaptive aerial\nnetworks. To meet diverse service demands, clusters must dynamically\nincorporate a New UAVs (NUAVs) or an Existing UAV (EUAV). However, achieving\nsustained service reliability remains challenging due to the need for efficient\nand scalable NUAV authentication, privacy-preserving cross-cluster\nauthentication for EUAVs, and robust protection of the cluster session key,\nincluding both forward and backward secrecy. To address these challenges, we\npropose a Lightweight and Privacy-Preserving Cluster Authentication and Session\nKey Update (LP2-CASKU) scheme tailored for dynamic UAV clusters in low-altitude\neconomy networks. LP2-CASKU integrates an efficient batch authentication\nmechanism that simultaneously authenticates multiple NUAVs with minimal\ncommunication overhead. It further introduces a lightweight cross-cluster\nauthentication mechanism that ensures EUAV anonymity and unlinkability.\nAdditionally, a secure session key update mechanism is incorporated to maintain\nkey confidentiality over time, thereby preserving both forward and backward\nsecrecy. We provide a comprehensive security analysis and evaluate LP2-CASKU\nperformance through both theoretical analysis and OMNeT++ simulations.\nExperimental results demonstrate that, compared to the baseline, LP2-CASKU\nachieves a latency reduction of 82.8%-90.8% by across different UAV swarm\nconfigurations and network bitrates, demonstrating strong adaptability to\ndynamic communication environments. Besides, under varying UAV swarm\nconfigurations, LP2-CASKU reduces the energy consumption by approximately\n37.6-72.6%, while effectively supporting privacy-preserving authentication in\nhighly dynamic UAV cluster environments."}
{"id": "2509.06127", "categories": ["cs.CR", "94A60, 94A62, 11T71, 68P30"], "pdf": "https://arxiv.org/pdf/2509.06127", "abs": "https://arxiv.org/abs/2509.06127", "authors": ["Soumya Bhoumik", "Sarbari Mitra", "Rohit Raj Sharma", "Kuldeep Namdeo"], "title": "CSI-IBBS: Identity-Based Blind Signature using CSIDH", "comment": null, "summary": "Identity-based cryptography (IBC), proposed by Adi Shamir, revolutionized\npublic key authentication by eliminating the need for certificates, enabling a\nmore efficient and scalable approach to cryptographic systems. Meanwhile, in\n\\cite{Katsumata2024group}, Katsumata et al. were the first to present the blind\nsignature protocol based on the hardness assumption of isogeny with provable\nsecurity, which resembles the Schnorr blind signature. Building upon these\nfoundational concepts, we propose an Identity-Based Blind Signature Scheme with\nan Honest Zero-Knowledge Verifier utilizing the CSIDH framework. This scheme\ncombines blind signatures for privacy preservation with zero-knowledge proofs\nto ensure the verifier's honesty without revealing any additional information.\n  Leveraging the quantum-resistant properties of CSIDH, a post-quantum secure\nscheme based on supersingular isogenies, our scheme offers strong protection\nagainst quantum adversaries while maintaining computational efficiency. We\nanalyze the security of the introduced protocol in the standard cryptographic\nmodel and demonstrate its effectiveness in safeguarding privacy and verifier\nhonesty. Furthermore, we present a performance evaluation, confirming the\npractical viability of this quantum-resistant cryptographic solution for\nprivacy-preserving applications. This work advances the creation of secure, and\nscalable cryptographic systems for the post-quantum era."}
{"id": "2509.06133", "categories": ["cs.CR", "cs.DC", "cs.SE", "cs.SY", "eess.SY", "C.2.4; K.6.5; D.4.6"], "pdf": "https://arxiv.org/pdf/2509.06133", "abs": "https://arxiv.org/abs/2509.06133", "authors": ["Pradyumna Kaushal"], "title": "VehiclePassport: A GAIA-X-Aligned, Blockchain-Anchored Privacy-Preserving, Zero-Knowledge Digital Passport for Smart Vehicles", "comment": "13 pages, 5 figures. Whitepaper submission; LaTeX source with\n  compiled .bbl. Includes architecture diagrams, tables, and code listings\n  (TypeScript & Solidity)", "summary": "Modern vehicles accumulate fragmented lifecycle records across OEMs, owners,\nand service centers that are difficult to verify and prone to fraud. We propose\nVehiclePassport, a GAIA-X-aligned digital passport anchored on blockchain with\nzero-knowledge proofs (ZKPs) for privacy-preserving verification.\nVehiclePassport immutably commits to manufacturing, telemetry, and service\nevents while enabling selective disclosure via short-lived JWTs and Groth16\nproofs. Our open-source reference stack anchors hashes on Polygon zkEVM at\n<$0.02 per event, validates proofs in <10 ms, and scales to millions of\nvehicles. This architecture eliminates paper-based KYC, ensures GDPR-compliant\ntraceability, and establishes a trustless foundation for insurance, resale, and\nregulatory applications in global mobility data markets."}
{"id": "2509.06136", "categories": ["cs.CR", "68M25 (Primary), 68T07 (Secondary)", "K.6.5; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.06136", "abs": "https://arxiv.org/abs/2509.06136", "authors": ["Yangheran Piao", "Jingjie Li", "Daniel W. Woods"], "title": "Measuring the Vulnerability Disclosure Policies of AI Vendors", "comment": "20 pages, 5 figures", "summary": "As AI is increasingly integrated into products and critical systems,\nresearchers are paying greater attention to identifying related\nvulnerabilities. Effective remediation depends on whether vendors are willing\nto accept and respond to AI vulnerability reports. In this paper, we examine\nthe disclosure policies of 264 AI vendors. Using a mixed-methods approach, our\nquantitative analysis finds that 36% of vendors provide no disclosure channel,\nand only 18% explicitly mention AI-related risks. Vulnerabilities involving\ndata access, authorization, and model extraction are generally considered\nin-scope, while jailbreaking and hallucination are frequently excluded. Through\nqualitative analysis, we further identify three vendor postures toward AI\nvulnerabilities - proactive clarification (n = 46, include active supporters,\nAI integrationists, and back channels), silence (n = 115, include self-hosted\nand hosted vendors), and restrictive (n = 103). Finally, by comparing vendor\npolicies against 1,130 AI incidents and 359 academic publications, we show that\nbug bounty policy evolution has lagged behind both academic research and\nreal-world events."}
{"id": "2509.06202", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.06202", "abs": "https://arxiv.org/abs/2509.06202", "authors": ["Fatemeh Roshanzadeh", "Hamid Barati", "Ali Barati"], "title": "Lightweight Intrusion Detection System Using a Hybrid CNN and ConvNeXt-Tiny Model for Internet of Things Networks", "comment": null, "summary": "The rapid expansion of Internet of Things (IoT) systems across various\ndomains such as industry, smart cities, healthcare, manufacturing, and\ngovernment services has led to a significant increase in security risks,\nthreatening data integrity, confidentiality, and availability. Consequently,\nensuring the security and resilience of IoT systems has become a critical\nrequirement. In this paper, we propose a lightweight and efficient intrusion\ndetection system (IDS) for IoT environments, leveraging a hybrid model of CNN\nand ConvNeXt-Tiny. The proposed method is designed to detect and classify\ndifferent types of network attacks, particularly botnet and malicious traffic,\nwhile the lightweight ConvNeXt-Tiny architecture enables effective deployment\nin resource-constrained devices and networks. A real-world dataset comprising\nboth benign and malicious network packets collected from practical IoT\nscenarios was employed in the experiments. The results demonstrate that the\nproposed method achieves high accuracy while significantly reducing training\nand inference time compared to more complex models. Specifically, the system\nattained 99.63% accuracy in the testing phase, 99.67% accuracy in the training\nphase, and an error rate of 0.0107 across eight classes, while maintaining\nshort response times and low resource consumption. These findings highlight the\neffectiveness of the proposed method in detecting and classifying attacks in\nreal-world IoT environments, indicating that the lightweight architecture can\nserve as a practical alternative to complex and resource-intensive approaches\nin IoT network security."}
{"id": "2509.06264", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06264", "abs": "https://arxiv.org/abs/2509.06264", "authors": ["Qin Yang", "Nicholas Stout", "Meisam Mohammady", "Han Wang", "Ayesha Samreen", "Christopher J Quinn", "Yan Yan", "Ashish Kundu", "Yuan Hong"], "title": "PLRV-O: Advancing Differentially Private Deep Learning via Privacy Loss Random Variable Optimization", "comment": "Source code is available at https://github.com/datasec-lab/plrvo.\n  This is the full version of the paper to appear in CCS'25", "summary": "Differentially Private Stochastic Gradient Descent (DP-SGD) is a standard\nmethod for enforcing privacy in deep learning, typically using the Gaussian\nmechanism to perturb gradient updates. However, conventional mechanisms such as\nGaussian and Laplacian noise are parameterized only by variance or scale. This\nsingle degree of freedom ties the magnitude of noise directly to both privacy\nloss and utility degradation, preventing independent control of these two\nfactors. The problem becomes more pronounced when the number of composition\nrounds T and batch size B vary across tasks, as these variations induce\ntask-dependent shifts in the privacy-utility trade-off, where small changes in\nnoise parameters can disproportionately affect model accuracy. To address this\nlimitation, we introduce PLRV-O, a framework that defines a broad search space\nof parameterized DP-SGD noise distributions, where privacy loss moments are\ntightly characterized yet can be optimized more independently with respect to\nutility loss. This formulation enables systematic adaptation of noise to\ntask-specific requirements, including (i) model size, (ii) training duration,\n(iii) batch sampling strategies, and (iv) clipping thresholds under both\ntraining and fine-tuning settings. Empirical results demonstrate that PLRV-O\nsubstantially improves utility under strict privacy constraints. On CIFAR-10, a\nfine-tuned ViT achieves 94.03% accuracy at epsilon approximately 0.5, compared\nto 83.93% with Gaussian noise. On SST-2, RoBERTa-large reaches 92.20% accuracy\nat epsilon approximately 0.2, versus 50.25% with Gaussian."}
{"id": "2509.06326", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06326", "abs": "https://arxiv.org/abs/2509.06326", "authors": ["Ruisi Zhang", "Yifei Zhao", "Neusha Javidnia", "Mengxin Zheng", "Farinaz Koushanfar"], "title": "AttestLLM: Efficient Attestation Framework for Billion-scale On-device LLMs", "comment": null, "summary": "As on-device LLMs(e.g., Apple on-device Intelligence) are widely adopted to\nreduce network dependency, improve privacy, and enhance responsiveness,\nverifying the legitimacy of models running on local devices becomes critical.\nExisting attestation techniques are not suitable for billion-parameter Large\nLanguage Models (LLMs), struggling to remain both time- and memory-efficient\nwhile addressing emerging threats in the LLM era. In this paper, we present\nAttestLLM, the first-of-its-kind attestation framework to protect the\nhardware-level intellectual property (IP) of device vendors by ensuring that\nonly authorized LLMs can execute on target platforms. AttestLLM leverages an\nalgorithm/software/hardware co-design approach to embed robust watermarking\nsignatures onto the activation distributions of LLM building blocks. It also\noptimizes the attestation protocol within the Trusted Execution Environment\n(TEE), providing efficient verification without compromising inference\nthroughput. Extensive proof-of-concept evaluations on LLMs from Llama, Qwen,\nand Phi families for on-device use cases demonstrate AttestLLM's attestation\nreliability, fidelity, and efficiency. Furthermore, AttestLLM enforces model\nlegitimacy and exhibits resilience against model replacement and forgery\nattacks."}
{"id": "2509.06338", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06338", "abs": "https://arxiv.org/abs/2509.06338", "authors": ["Shuai Yuan", "Zhibo Zhang", "Yuxi Li", "Guangdong Bai", "Wang Kailong"], "title": "Embedding Poisoning: Bypassing Safety Alignment via Embedding Semantic Shift", "comment": "16 pages,9 figures", "summary": "The widespread distribution of Large Language Models (LLMs) through public\nplatforms like Hugging Face introduces significant security challenges. While\nthese platforms perform basic security scans, they often fail to detect subtle\nmanipulations within the embedding layer. This work identifies a novel class of\ndeployment phase attacks that exploit this vulnerability by injecting\nimperceptible perturbations directly into the embedding layer outputs without\nmodifying model weights or input text. These perturbations, though\nstatistically benign, systematically bypass safety alignment mechanisms and\ninduce harmful behaviors during inference. We propose Search based Embedding\nPoisoning(SEP), a practical, model agnostic framework that introduces carefully\noptimized perturbations into embeddings associated with high risk tokens. SEP\nleverages a predictable linear transition in model responses, from refusal to\nharmful output to semantic deviation to identify a narrow perturbation window\nthat evades alignment safeguards. Evaluated across six aligned LLMs, SEP\nachieves an average attack success rate of 96.43% while preserving benign task\nperformance and evading conventional detection mechanisms. Our findings reveal\na critical oversight in deployment security and emphasize the urgent need for\nembedding level integrity checks in future LLM defense strategies."}
{"id": "2509.06368", "categories": ["cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.06368", "abs": "https://arxiv.org/abs/2509.06368", "authors": ["Kunlin Cai", "Jinghuai Zhang", "Ying Li", "Zhiyuan Wang", "Xun Chen", "Tianshi Li", "Yuan Tian"], "title": "From Perception to Protection: A Developer-Centered Study of Security and Privacy Threats in Extended Reality (XR)", "comment": "NDSS 2026", "summary": "The immersive nature of XR introduces a fundamentally different set of\nsecurity and privacy (S&P) challenges due to the unprecedented user\ninteractions and data collection that traditional paradigms struggle to\nmitigate. As the primary architects of XR applications, developers play a\ncritical role in addressing novel threats. However, to effectively support\ndevelopers, we must first understand how they perceive and respond to different\nthreats. Despite the growing importance of this issue, there is a lack of\nin-depth, threat-aware studies that examine XR S&P from the developers'\nperspective. To fill this gap, we interviewed 23 professional XR developers\nwith a focus on emerging threats in XR. Our study addresses two research\nquestions aiming to uncover existing problems in XR development and identify\nactionable paths forward.\n  By examining developers' perceptions of S&P threats, we found that: (1) XR\ndevelopment decisions (e.g., rich sensor data collection, user-generated\ncontent interfaces) are closely tied to and can amplify S&P threats, yet\ndevelopers are often unaware of these risks, resulting in cognitive biases in\nthreat perception; and (2) limitations in existing mitigation methods, combined\nwith insufficient strategic, technical, and communication support, undermine\ndevelopers' motivation, awareness, and ability to effectively address these\nthreats. Based on these findings, we propose actionable and stakeholder-aware\nrecommendations to improve XR S&P throughout the XR development process. This\nwork represents the first effort to undertake a threat-aware,\ndeveloper-centered study in the XR domain -- an area where the immersive,\ndata-rich nature of the XR technology introduces distinctive challenges."}
{"id": "2509.06504", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.06504", "abs": "https://arxiv.org/abs/2509.06504", "authors": ["Hailong Chang", "Guozhu Meng", "Shuhui Xiao", "Kai Chen", "Kun Sun", "Yilin Li"], "title": "When Code Crosses Borders: A Security-Centric Evaluation of LLM-based Code Translation", "comment": null, "summary": "With the growing demand for cross-language codebase migration, evaluating\nLLMs' security implications in translation tasks has become critical. Existing\nevaluations primarily focus on syntactic or functional correctness at the\nfunction level, neglecting the critical dimension of security.\n  To enable security evaluation, we construct STED (Security-centric\nTranslation Evaluation Dataset), the first dataset specifically designed for\nevaluating the security implications of LLM-based code translation. It\ncomprises 720 security-related code samples across five programming languages\nand nine high-impact CWE categories, sourced from CVE/NVD and manually verified\nfor translation tasks. Our evaluation framework consists of two independent\nassessment modules: (1) rigorous evaluation by security researchers, and (2)\nautomated analysis via LLM-as-a-judge. Together they evaluate three critical\naspects: functional correctness, vulnerability preservation, and vulnerability\nintroduction rates.\n  Our large-scale evaluation of five state-of-the-art LLMs across 6,000\ntranslation instances reveals significant security degradation, with 28.6-45%\nof translations introducing new vulnerabilities--particularly for web-related\nflaws like input validation, where LLMs show consistent weaknesses.\nFurthermore, we develop a Retrieval-Augmented Generation (RAG)-based mitigation\nstrategy that reduces translation-induced vulnerabilities by 32.8%, showing the\npotential of knowledge-enhanced prompting."}
{"id": "2509.06509", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.06509", "abs": "https://arxiv.org/abs/2509.06509", "authors": ["Zilong Wang", "Gideon Mohr", "Klaus von Gleissenthall", "Jan Reineke", "Marco Guarnieri"], "title": "Synthesis of Sound and Precise Leakage Contracts for Open-Source RISC-V Processors", "comment": "Technical report containing full formalization and proofs of all\n  results. A short version of this report (with the same title) appears in the\n  proceedings of the 32nd ACM SIGSAC Conference on Computer and Communication\n  Security (CCS 2025)", "summary": "Leakage contracts have been proposed as a new security abstraction at the\ninstruction set architecture level. Leakage contracts aim to capture the\ninformation that processors may leak via microarchitectural side channels.\nRecently, the first tools have emerged to verify whether a processor satisfies\na given contract. However, coming up with a contract that is both sound and\nprecise for a given processor is challenging, time-consuming, and error-prone,\nas it requires in-depth knowledge of the timing side channels introduced by\nmicroarchitectural optimizations.\n  In this paper, we address this challenge by proposing LeaSyn, the first tool\nfor automatically synthesizing leakage contracts that are both sound and\nprecise for processor designs at register-transfer level. Starting from a\nuser-provided contract template that captures the space of possible contracts,\nLeaSyn automatically constructs a contract, alternating between contract\nsynthesis, which ensures precision based on an empirical characterization of\nthe processor's leaks, and contract verification, which ensures soundness.\n  Using LeaSyn, we automatically synthesize contracts for six open-source\nRISC-V CPUs for a variety of contract templates. Our experiments indicate that\nLeaSyn's contracts are sound and more precise (i.e., represent the actual leaks\nin the target processor more faithfully) than contracts constructed by existing\napproaches."}
{"id": "2509.06548", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG", "I.2.6; K.6.5"], "pdf": "https://arxiv.org/pdf/2509.06548", "abs": "https://arxiv.org/abs/2509.06548", "authors": ["Jack Wilkie", "Hanan Hindy", "Ivan Andonovic", "Christos Tachtatzis", "Robert Atkinson"], "title": "Signal-Based Malware Classification Using 1D CNNs", "comment": "Accepted for publication in Springer Cybersecurity (2025)", "summary": "Malware classification is a contemporary and ongoing challenge in\ncyber-security: modern obfuscation techniques are able to evade traditional\nstatic analysis, while dynamic analysis is too resource intensive to be\ndeployed at a large scale. One prominent line of research addresses these\nlimitations by converting malware binaries into 2D images by heuristically\nreshaping them into a 2D grid before resizing using Lanczos resampling. These\nimages can then be classified based on their textural information using\ncomputer vision approaches. While this approach can detect obfuscated malware\nmore effectively than static analysis, the process of converting files into 2D\nimages results in significant information loss due to both quantisation noise,\ncaused by rounding to integer pixel values, and the introduction of 2D\ndependencies which do not exist in the original data. This loss of signal\nlimits the classification performance of the downstream model. This work\naddresses these weaknesses by instead resizing the files into 1D signals which\navoids the need for heuristic reshaping, and additionally these signals do not\nsuffer from quantisation noise due to being stored in a floating-point format.\nIt is shown that existing 2D CNN architectures can be readily adapted to\nclassify these 1D signals for improved performance. Furthermore, a bespoke 1D\nconvolutional neural network, based on the ResNet architecture and\nsqueeze-and-excitation layers, was developed to classify these signals and\nevaluated on the MalNet dataset. It was found to achieve state-of-the-art\nperformance on binary, type, and family level classification with F1 scores of\n0.874, 0.503, and 0.507, respectively, paving the way for future models to\noperate on the proposed signal modality."}
{"id": "2509.06549", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.06549", "abs": "https://arxiv.org/abs/2509.06549", "authors": ["Timo Glaser", "Alexander May", "Julian Nowakowski"], "title": "Super-Quadratic Quantum Speed-ups and Guessing Many Likely Keys", "comment": null, "summary": "We study the fundamental problem of guessing cryptographic keys, drawn from\nsome non-uniform probability distribution $D$, as e.g. in LPN, LWE or for\npasswords. The optimal classical algorithm enumerates keys in decreasing order\nof likelihood. The optimal quantum algorithm, due to Montanaro (2011), is a\nsophisticated Grover search.\n  We give the first tight analysis for Montanaro's algorithm, showing that its\nruntime is $2^{H_{2/3}(D)/2}$, where $H_{\\alpha}(\\cdot)$ denotes Renyi entropy\nwith parameter $\\alpha$. Interestingly, this is a direct consequence of an\ninformation theoretic result called Arikan's Inequality (1996) -- which has so\nfar been missed in the cryptographic community -- that tightly bounds the\nruntime of classical key guessing by $2^{H_{1/2}(D)}$. Since $H_{2/3}(D) <\nH_{1/2}(D)$ for every non-uniform distribution $D$, we thus obtain a\nsuper-quadratic quantum speed-up $s>2$ over classical key guessing.\n  As another main result, we provide the first thorough analysis of guessing in\na multi-key setting. Specifically, we consider the task of attacking many keys\nsampled independently from some distribution $D$, and aim to guess a fraction\nof them. For product distributions $D = \\chi^n$, we show that any constant\nfraction of keys can be guessed within $2^{H(D)}$ classically and $2 ^{H(D)/2}$\nquantumly per key, where $H(\\chi)$ denotes Shannon entropy. In contrast,\nArikan's Inequality implies that guessing a single key costs $2^{H_{1/2}(D)}$\nclassically and $2^{H_{2/3}(D)/2}$ quantumly. Since $H(D) < H_{2/3}(D) <\nH_{1/2}(D)$, this shows that in a multi-key setting the guessing cost per key\nis substantially smaller than in a single-key setting, both classically and\nquantumly."}
{"id": "2509.06562", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.06562", "abs": "https://arxiv.org/abs/2509.06562", "authors": ["I. Buchinskiy", "M. Kotov", "A. Ponmaheshkumar", "R. Perumal"], "title": "Marginal sets in semigroups and semirings", "comment": null, "summary": "In 2019, V. A. Roman'kov introduced the concept of marginal sets for groups.\nHe developed a theory of marginal sets and demonstrated how these sets can be\napplied to improve some key exchange schemes. In this paper, we extend his\nideas and introduce the concept of marginal sets for semigroups and semirings.\nFor tropical matrix semigroups and semirings, we describe how some marginal\nsets can be constructed. We apply marginal sets to improve some key exchange\nschemes over semigroups."}
{"id": "2509.06571", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.06571", "abs": "https://arxiv.org/abs/2509.06571", "authors": ["Tristan Caulfield"], "title": "A Simple Data Exfiltration Game", "comment": null, "summary": "Data exfiltration is a growing problem for business who face costs related to\nthe loss of confidential data as well as potential extortion. This work\npresents a simple game theoretic model of network data exfiltration. In the\nmodel, the attacker chooses the exfiltration route and speed, and the defender\nselects monitoring thresholds to detect unusual activity. The attacker is\nrewarded for exfiltrating data, and the defender tries to minimize the costs of\ndata loss and of responding to alerts."}
{"id": "2509.06572", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.06572", "abs": "https://arxiv.org/abs/2509.06572", "authors": ["Shuli Zhao", "Qinsheng Hou", "Zihan Zhan", "Yanhao Wang", "Yuchong Xie", "Yu Guo", "Libo Chen", "Shenghong Li", "Zhi Xue"], "title": "Mind Your Server: A Systematic Study of Parasitic Toolchain Attacks on the MCP Ecosystem", "comment": null, "summary": "Large language models (LLMs) are increasingly integrated with external\nsystems through the Model Context Protocol (MCP), which standardizes tool\ninvocation and has rapidly become a backbone for LLM-powered applications.\nWhile this paradigm enhances functionality, it also introduces a fundamental\nsecurity shift: LLMs transition from passive information processors to\nautonomous orchestrators of task-oriented toolchains, expanding the attack\nsurface, elevating adversarial goals from manipulating single outputs to\nhijacking entire execution flows. In this paper, we reveal a new class of\nattacks, Parasitic Toolchain Attacks, instantiated as MCP Unintended Privacy\nDisclosure (MCP-UPD). These attacks require no direct victim interaction;\ninstead, adversaries embed malicious instructions into external data sources\nthat LLMs access during legitimate tasks. The malicious logic infiltrates the\ntoolchain and unfolds in three phases: Parasitic Ingestion, Privacy Collection,\nand Privacy Disclosure, culminating in stealthy exfiltration of private data.\nOur root cause analysis reveals that MCP lacks both context-tool isolation and\nleast-privilege enforcement, enabling adversarial instructions to propagate\nunchecked into sensitive tool invocations. To assess the severity, we design\nMCP-SEC and conduct the first large-scale security census of the MCP ecosystem,\nanalyzing 12,230 tools across 1,360 servers. Our findings show that the MCP\necosystem is rife with exploitable gadgets and diverse attack methods,\nunderscoring systemic risks in MCP platforms and the urgent need for defense\nmechanisms in LLM-integrated environments."}
{"id": "2509.06595", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.06595", "abs": "https://arxiv.org/abs/2509.06595", "authors": ["Irdin Pekaric", "Philipp Zech", "Tom Mattson"], "title": "LLMs in Cybersecurity: Friend or Foe in the Human Decision Loop?", "comment": null, "summary": "Large Language Models (LLMs) are transforming human decision-making by acting\nas cognitive collaborators. Yet, this promise comes with a paradox: while LLMs\ncan improve accuracy, they may also erode independent reasoning, promote\nover-reliance and homogenize decisions. In this paper, we investigate how LLMs\nshape human judgment in security-critical contexts. Through two exploratory\nfocus groups (unaided and LLM-supported), we assess decision accuracy,\nbehavioral resilience and reliance dynamics. Our findings reveal that while\nLLMs enhance accuracy and consistency in routine decisions, they can\ninadvertently reduce cognitive diversity and improve automation bias, which is\nespecially the case among users with lower resilience. In contrast,\nhigh-resilience individuals leverage LLMs more effectively, suggesting that\ncognitive traits mediate AI benefit."}
{"id": "2509.06614", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.06614", "abs": "https://arxiv.org/abs/2509.06614", "authors": ["Margarita Capretto", "Martín Ceresa", "Antonio Fernández Anta", "Pedro Moreno Sánchez", "César Sánchez"], "title": "A Secure Sequencer and Data Availability Committee for Rollups (Extended Version)", "comment": null, "summary": "Blockchains face a scalability limitation, partly due to the throughput\nlimitations of consensus protocols, especially when aiming to obtain a high\ndegree of decentralization. Layer 2 Rollups (L2s) are a faster alternative to\nconventional blockchains. L2s perform most computations offchain using\nminimally blockchains (L1) under-the-hood to guarantee correctness. A sequencer\nis a service that receives offchain L2 transaction requests, batches these\ntransactions, and commits compressed or hashed batches to L1. Using hashing\nneeds less L1 space, which is beneficial for gas cost, but requires a data\navailability committee (DAC) service to translate hashes into their\ncorresponding batches of transaction requests. The behavior of sequencers and\nDACs influence the evolution of the L2 blockchain, presenting a potential\nsecurity threat and delaying L2 adoption. We propose in this paper fraud-proof\nmechanisms, arbitrated by L1 contracts, to detect and generate evidence of\ndishonest behavior of the sequencer and DAC. We study how these fraud-proofs\nlimit the power of adversaries that control different number of sequencer and\nDACs members, and provide incentives for their honest behavior. We designed\nthese fraud-proof mechanisms as two player games. Unlike the generic\nfraud-proofs in current L2s (designed to guarantee the correct execution of\ntransactions), our fraud-proofs are over pred-etermined algorithms that verify\nthe properties that determine the correctness of the DAC. Arbitrating over\nconcrete algorithms makes our fraud-proofs more efficient, easier to\nunderstand, and simpler to prove correct. We provide as an artifact a\nmechanization in LEAN4 of our fraud-proof games, including (1) the verified\nstrategies that honest players should play to win all games as well as (2)\nmechanisms to detect dishonest claims."}
{"id": "2509.06626", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.06626", "abs": "https://arxiv.org/abs/2509.06626", "authors": ["Jan Matter", "Muoi Tran"], "title": "Network-level Censorship Attacks in the InterPlanetary File System", "comment": null, "summary": "The InterPlanetary File System (IPFS) has been successfully established as\nthe de facto standard for decentralized data storage in the emerging Web3.\nDespite its decentralized nature, IPFS nodes, as well as IPFS content\nproviders, have converged to centralization in large public clouds.\nCentralization introduces BGP routing-based attacks, such as passive\ninterception and BGP hijacking, as potential threats. Although this attack\nvector has been investigated for many other Web3 protocols, such as Bitcoin and\nEthereum, to the best of our knowledge, it has not been analyzed for the IPFS\nnetwork. In our work, we bridge this gap and demonstrate that BGP routing\nattacks can be effectively leveraged to censor content in IPFS. For the\nanalysis, we collected 3,000 content blocks called CIDs and conducted a\nsimulation of BGP hijacking and passive interception against them. We find that\na single malicious AS can censor 75% of the IPFS content for more than 57% of\nall requester nodes. Furthermore, we show that even with a small set of only 62\nhijacked prefixes, 70% of the full attack effectiveness can already be reached.\nWe further propose and validate countermeasures based on global collaborative\ncontent replication among all nodes in the IPFS network, together with\nadditional robust backup content provider nodes that are well-hardened against\nBGP hijacking. We hope this work raises awareness about the threat BGP\nrouting-based attacks pose to IPFS and triggers further efforts to harden the\nlive IPFS network against them."}
{"id": "2509.06703", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06703", "abs": "https://arxiv.org/abs/2509.06703", "authors": ["Gabriele Digregorio", "Marco Di Gennaro", "Stefano Zanero", "Stefano Longari", "Michele Carminati"], "title": "When Secure Isn't: Assessing the Security of Machine Learning Model Sharing", "comment": null, "summary": "The rise of model-sharing through frameworks and dedicated hubs makes Machine\nLearning significantly more accessible. Despite their benefits, these tools\nexpose users to underexplored security risks, while security awareness remains\nlimited among both practitioners and developers. To enable a more\nsecurity-conscious culture in Machine Learning model sharing, in this paper we\nevaluate the security posture of frameworks and hubs, assess whether\nsecurity-oriented mechanisms offer real protection, and survey how users\nperceive the security narratives surrounding model sharing. Our evaluation\nshows that most frameworks and hubs address security risks partially at best,\noften by shifting responsibility to the user. More concerningly, our analysis\nof frameworks advertising security-oriented settings and complete model sharing\nuncovered six 0-day vulnerabilities enabling arbitrary code execution. Through\nthis analysis, we debunk the misconceptions that the model-sharing problem is\nlargely solved and that its security can be guaranteed by the file format used\nfor sharing. As expected, our survey shows that the surrounding security\nnarrative leads users to consider security-oriented settings as trustworthy,\ndespite the weaknesses shown in this work. From this, we derive takeaways and\nsuggestions to strengthen the security of model-sharing ecosystems."}
{"id": "2509.06754", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.06754", "abs": "https://arxiv.org/abs/2509.06754", "authors": ["Yiqi Tang"], "title": "Image Encryption Scheme Based on Hyper-Chaotic Map and Self-Adaptive Diffusion", "comment": null, "summary": "In the digital age, image encryption technology acts as a safeguard,\npreventing unauthorized access to images. This paper proposes an innovative\nimage encryption scheme that integrates a novel 2D hyper-chaotic map with a\nnewly developed self-adaptive diffusion method. The 2D hyper-chaotic map,\nnamely the 2D-RA map, is designed by hybridizing the Rastrigin and Ackley\nfunctions. The chaotic performance of the 2D-RA map is validated through a\nseries of measurements, including the Bifurcation Diagram, Lyapunov Exponent\n(LE), Initial Value Sensitivity, 0 - 1 Test, Correlation Dimension (CD), and\nKolmogorov Entropy (KE). The results demonstrate that the chaotic performance\nof the 2D-RA map surpasses that of existing advanced chaotic functions.\nAdditionally, the self-adaptive diffusion method is employed to enhance the\nuniformity of grayscale distribution. The performance of the image encryption\nscheme is evaluated using a series of indicators. The results show that the\nproposed image encryption scheme significantly outperforms current\nstate-of-the-art image encryption techniques."}
{"id": "2509.06796", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06796", "abs": "https://arxiv.org/abs/2509.06796", "authors": ["Yuntao Du", "Yuetian Chen", "Hanshen Xiao", "Bruno Ribeiro", "Ninghui Li"], "title": "Imitative Membership Inference Attack", "comment": "Code is available at: https://github.com/zealscott/IMIA", "summary": "A Membership Inference Attack (MIA) assesses how much a target machine\nlearning model reveals about its training data by determining whether specific\nquery instances were part of the training set. State-of-the-art MIAs rely on\ntraining hundreds of shadow models that are independent of the target model,\nleading to significant computational overhead. In this paper, we introduce\nImitative Membership Inference Attack (IMIA), which employs a novel imitative\ntraining technique to strategically construct a small number of target-informed\nimitative models that closely replicate the target model's behavior for\ninference. Extensive experimental results demonstrate that IMIA substantially\noutperforms existing MIAs in various attack settings while only requiring less\nthan 5% of the computational cost of state-of-the-art approaches."}
{"id": "2509.06920", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY", "C.2.0; I.2.7; K.4.1; H.3.3"], "pdf": "https://arxiv.org/pdf/2509.06920", "abs": "https://arxiv.org/abs/2509.06920", "authors": ["Haywood Gelman", "John D. Hastings", "David Kenley"], "title": "An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and Detection", "comment": "6 pages, 5 figures, 5 tables", "summary": "Insider threats are a growing organizational problem due to the complexity of\nidentifying their technical and behavioral elements. A large research body is\ndedicated to the study of insider threats from technological, psychological,\nand educational perspectives. However, research in this domain has been\ngenerally dependent on datasets that are static and limited access which\nrestricts the development of adaptive detection models. This study introduces a\nnovel, ethically grounded approach that uses the large language model (LLM)\nClaude Sonnet 3.7 to dynamically synthesize syslog messages, some of which\ncontain indicators of insider threat scenarios. The messages reflect real-world\ndata distributions by being highly imbalanced (1% insider threats). The syslogs\nwere analyzed for insider threats by both Claude Sonnet 3.7 and GPT-4o, with\ntheir performance evaluated through statistical metrics including precision,\nrecall, MCC, and ROC AUC. Sonnet 3.7 consistently outperformed GPT-4o across\nnearly all metrics, particularly in reducing false alarms and improving\ndetection accuracy. The results show strong promise for the use of LLMs in\nsynthetic dataset generation and insider threat detection."}
{"id": "2509.06921", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06921", "abs": "https://arxiv.org/abs/2509.06921", "authors": ["Safayat Bin Hakim", "Muhammad Adil", "Alvaro Velasquez", "Shouhuai Xu", "Houbing Herbert Song"], "title": "Neuro-Symbolic AI for Cybersecurity: State of the Art, Challenges, and Opportunities", "comment": null, "summary": "Traditional Artificial Intelligence (AI) approaches in cybersecurity exhibit\nfundamental limitations: inadequate conceptual grounding leading to\nnon-robustness against novel attacks; limited instructibility impeding\nanalyst-guided adaptation; and misalignment with cybersecurity objectives.\nNeuro-Symbolic (NeSy) AI has emerged with the potential to revolutionize\ncybersecurity AI. However, there is no systematic understanding of this\nemerging approach. These hybrid systems address critical cybersecurity\nchallenges by combining neural pattern recognition with symbolic reasoning,\nenabling enhanced threat understanding while introducing concerning autonomous\noffensive capabilities that reshape threat landscapes. In this survey, we\nsystematically characterize this field by analyzing 127 publications spanning\n2019-July 2025. We introduce a Grounding-Instructibility-Alignment (G-I-A)\nframework to evaluate these systems, focusing on both cyber defense and cyber\noffense across network security, malware analysis, and cyber operations. Our\nanalysis shows advantages of multi-agent NeSy architectures and identifies\ncritical implementation challenges including standardization gaps,\ncomputational complexity, and human-AI collaboration requirements that\nconstrain deployment. We show that causal reasoning integration is the most\ntransformative advancement, enabling proactive defense beyond correlation-based\napproaches. Our findings highlight dual-use implications where autonomous\nsystems demonstrate substantial capabilities in zero-day exploitation while\nachieving significant cost reductions, altering threat dynamics. We provide\ninsights and future research directions, emphasizing the urgent need for\ncommunity-driven standardization frameworks and responsible development\npractices that ensure advancement serves defensive cybersecurity objectives\nwhile maintaining societal alignment."}
