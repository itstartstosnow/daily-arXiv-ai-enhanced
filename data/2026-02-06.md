<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 28]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Extracting Recurring Vulnerabilities from Black-Box LLM-Generated Software](https://arxiv.org/abs/2602.04894)
*Tomer Kordonsky,Maayan Yamin,Noam Benzimra,Amit LeVi,Avi Mendelson*

Main category: cs.CR

TL;DR: 该论文提出FSTab框架，通过前端特征预测LLM生成代码的后端漏洞，并评估模型在不同场景下重现相同漏洞的一致性，揭示了LLM代码生成的安全风险。


<details>
  <summary>Details</summary>
Motivation: LLM生成的代码常包含可预测的模板化漏洞，但现有方法难以在不访问后端代码的情况下识别这些漏洞。需要一种黑盒方法来预测LLM生成软件中的漏洞持续性。

Method: 提出Feature-Security Table (FSTab)框架，包含两个组件：1) 基于前端特征和LLM知识的黑盒攻击，预测后端漏洞；2) 模型中心化评估，量化模型在不同程序、语义保留重述和应用领域中重现相同漏洞的一致性。

Result: 在GPT-5.2、Claude-4.5 Opus、Gemini-3 Pro等先进代码LLM上评估，FSTab在跨领域迁移中表现优异：即使目标领域未包含在训练中，在Internal Tools上攻击成功率高达94%，漏洞覆盖率达93%。

Conclusion: LLM生成的软件存在未被充分探索的攻击面，代码生成存在显著安全风险。FSTab框架能有效预测漏洞并评估模型漏洞重现一致性，为LLM代码生成安全提供重要工具。

Abstract: LLMs are increasingly used for code generation, but their outputs often follow recurring templates that can induce predictable vulnerabilities. We study \emph{vulnerability persistence} in LLM-generated software and introduce \emph{Feature--Security Table (FSTab)} with two components. First, FSTab enables a black-box attack that predicts likely backend vulnerabilities from observable frontend features and knowledge of the source LLM, without access to backend code or source code. Second, FSTab provides a model-centric evaluation that quantifies how consistently a given model reproduces the same vulnerabilities across programs, semantics-preserving rephrasings, and application domains. We evaluate FSTab on state-of-the-art code LLMs, including GPT-5.2, Claude-4.5 Opus, and Gemini-3 Pro, across diverse application domains. Our results show strong cross-domain transfer: even when the target domain is excluded from training, FSTab achieves up to 94\% attack success and 93\% vulnerability coverage on Internal Tools (Claude-4.5 Opus). These findings expose an underexplored attack surface in LLM-generated software and highlight the security risks of code generation. Our code is available at: https://anonymous.4open.science/r/FSTab-024E.

</details>


### [2] [Privacy Amplification Persists under Unlimited Synthetic Data Release](https://arxiv.org/abs/2602.04895)
*Clément Pierquin,Aurélien Bellet,Marc Tommasi,Matthieu Boussard*

Main category: cs.CR

TL;DR: 在合成数据发布中，即使发布无限数量的合成记录，隐私放大现象仍然存在，这改进了先前仅在模型维度远大于合成记录数时的渐近结果。


<details>
  <summary>Details</summary>
Motivation: 研究合成数据发布中的隐私放大现象，即通过仅发布合成数据而非私有生成模型本身来改善差分隐私保证。先前工作（Pierquin et al., 2025）的放大保证仅适用于模型维度远大于合成记录数的渐近情况，限制了实际应用。

Method: 在有限参数假设下，分析合成数据发布的隐私放大效应。与先前工作不同，本研究证明即使发布无限数量的合成记录，隐私放大仍然存在。

Result: 在有限参数假设下，隐私放大现象在发布无限数量合成记录时仍然存在，这改进了Pierquin et al. (2025)的界限，为更复杂的发布机制提供更严格的隐私保证奠定了基础。

Conclusion: 合成数据发布中的隐私放大效应比先前认为的更强大，即使在发布无限合成记录时仍然存在。这一结构洞察可能指导开发更复杂的发布机制的更严格隐私保证。

Abstract: We study privacy amplification by synthetic data release, a phenomenon in which differential privacy guarantees are improved by releasing only synthetic data rather than the private generative model itself. Recent work by Pierquin et al. (2025) established the first formal amplification guarantees for a linear generator, but they apply only in asymptotic regimes where the model dimension far exceeds the number of released synthetic records, limiting their practical relevance. In this work, we show a surprising result: under a bounded-parameter assumption, privacy amplification persists even when releasing an unbounded number of synthetic records, thereby improving upon the bounds of Pierquin et al. (2025). Our analysis provides structural insights that may guide the development of tighter privacy guarantees for more complex release mechanisms.

</details>


### [3] [Steering Externalities: Benign Activation Steering Unintentionally Increases Jailbreak Risk for Large Language Models](https://arxiv.org/abs/2602.04896)
*Chen Xiong,Zhiyuan He,Pin-Yu Chen,Ching-Yun Ko,Tsung-Yi Ho*

Main category: cs.CR

TL;DR: 激活引导技术虽能提升LLM实用性，但会无意中削弱安全防护，使模型更容易受到攻击


<details>
  <summary>Details</summary>
Motivation: 激活引导是一种实用的后训练模型对齐技术，可在不重新训练的情况下引导预训练模型实现特定行为目标（如合规性或指令遵循）。然而，这种能力无意中引入了关键且未被充分探索的安全风险。

Method: 识别"引导外部性"现象，即来自完全良性数据集（如强制执行严格合规性或特定输出格式如JSON）的引导向量会无意中侵蚀安全防护。通过实验验证这些干预措施如何作为力量倍增器，创建新的漏洞。

Result: 实验显示，这些干预措施使标准基准测试中的攻击成功率超过80%，通过绕过初始安全对齐来增加越狱漏洞。良性激活引导会系统性地侵蚀"安全边际"，使模型更容易受到黑盒攻击。

Conclusion: 研究结果揭示了部署中的关键盲点：推理时实用性改进必须严格审计，以防止意外的安全外部性。激活引导虽然能提升模型实用性，但必须仔细评估其对安全性的影响。

Abstract: Activation steering is a practical post-training model alignment technique to enhance the utility of Large Language Models (LLMs). Prior to deploying a model as a service, developers can steer a pre-trained model toward specific behavioral objectives, such as compliance or instruction adherence, without the need for retraining. This process is as simple as adding a steering vector to the model's internal representations. However, this capability unintentionally introduces critical and under-explored safety risks. We identify a phenomenon termed Steering Externalities, where steering vectors derived from entirely benign datasets-such as those enforcing strict compliance or specific output formats like JSON-inadvertently erode safety guardrails. Experiments reveal that these interventions act as a force multiplier, creating new vulnerabilities to jailbreaks and increasing attack success rates to over 80% on standard benchmarks by bypassing the initial safety alignment. Ultimately, our results expose a critical blind spot in deployment: benign activation steering systematically erodes the "safety margin," rendering models more vulnerable to black-box attacks and proving that inference-time utility improvements must be rigorously audited for unintended safety externalities.

</details>


### [4] [Semantic-level Backdoor Attack against Text-to-Image Diffusion Models](https://arxiv.org/abs/2602.04898)
*Tianxin Chen,Wenbo Jiang,Hongqiao Chen,Zhirun Zheng,Cheng Huang*

Main category: cs.CR

TL;DR: 提出SemBD语义级后门攻击，通过连续语义区域而非离散文本模式植入后门，对抗现有防御方法


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型后门攻击依赖固定文本触发器和单一实体目标，容易被枚举防御和注意力一致性检测发现

Method: 在表示层面植入后门，将触发器定义为连续语义区域；通过蒸馏编辑交叉注意力层的键值投影矩阵；加入语义正则化防止不完整语义误激活；采用多实体后门目标避免高度一致的交叉注意力模式

Result: 实验显示SemBD达到100%攻击成功率，同时对最先进的输入级防御保持强鲁棒性

Conclusion: SemBD展示了语义级后门攻击的有效性，对现有防御方法构成挑战，需要开发新的防御机制

Abstract: Text-to-image (T2I) diffusion models are widely adopted for their strong generative capabilities, yet remain vulnerable to backdoor attacks. Existing attacks typically rely on fixed textual triggers and single-entity backdoor targets, making them highly susceptible to enumeration-based input defenses and attention-consistency detection. In this work, we propose Semantic-level Backdoor Attack (SemBD), which implants backdoors at the representation level by defining triggers as continuous semantic regions rather than discrete textual patterns. Concretely, SemBD injects semantic backdoors by distillation-based editing of the key and value projection matrices in cross-attention layers, enabling diverse prompts with identical semantic compositions to reliably activate the backdoor attack. To further enhance stealthiness, SemBD incorporates a semantic regularization to prevent unintended activation under incomplete semantics, as well as multi-entity backdoor targets that avoid highly consistent cross-attention patterns. Extensive experiments demonstrate that SemBD achieves a 100% attack success rate while maintaining strong robustness against state-of-the-art input-level defenses.

</details>


### [5] [Phantom Transfer: Data-level Defences are Insufficient Against Data Poisoning](https://arxiv.org/abs/2602.04899)
*Andrew Draganov,Tolga H. Dur,Anandmayi Bhongade,Mary Phuong*

Main category: cs.CR

TL;DR: 提出了一种名为"幻影转移"的数据投毒攻击，即使攻击方式完全公开也无法通过数据过滤防御，攻击对GPT-4等模型有效，即使完全重述数据集也无法阻止


<details>
  <summary>Details</summary>
Motivation: 当前的数据投毒防御方法存在局限性，需要证明即使攻击方式完全公开，现有的数据级防御仍然无法有效阻止高级投毒攻击

Method: 改进潜意识学习技术使其适用于实际场景，创建即使知道投毒方式也无法过滤的攻击，通过修改数据集植入密码触发的行为模式

Result: 攻击对包括GPT-4在内的多种模型有效，即使完全重述数据集也无法阻止攻击，能够植入密码触发行为并绕过现有防御

Conclusion: 数据级防御不足以阻止复杂的数据投毒攻击，未来研究应关注模型审计和白盒安全方法

Abstract: We present a data poisoning attack -- Phantom Transfer -- with the property that, even if you know precisely how the poison was placed into an otherwise benign dataset, you cannot filter it out. We achieve this by modifying subliminal learning to work in real-world contexts and demonstrate that the attack works across models, including GPT-4.1. Indeed, even fully paraphrasing every sample in the dataset using a different model does not stop the attack. We also discuss connections to steering vectors and show that one can plant password-triggered behaviours into models while still beating defences.
  This suggests that data-level defences are insufficient for stopping sophisticated data poisoning attacks. We suggest that future work should focus on model audits and white-box security methods.

</details>


### [6] [PriMod4AI: Lifecycle-Aware Privacy Threat Modeling for AI Systems using LLM](https://arxiv.org/abs/2602.04927)
*Gautam Savaliya,Robert Aufschläger,Abhishek Subedi,Michael Heigl,Martin Schramm*

Main category: cs.CR

TL;DR: PriMod4AI提出了一种混合隐私威胁建模框架，结合LINDDUN传统隐私威胁分类和AI特有的模型中心隐私攻击，通过知识库嵌入和LLM引导实现全面的隐私威胁识别。


<details>
  <summary>Details</summary>
Motivation: AI系统在处理敏感或高维数据时引入复杂隐私风险，传统LINDDUN框架无法覆盖模型中心隐私攻击（如成员推理和模型反转），需要统一方法来应对传统和AI驱动的隐私威胁。

Method: PriMod4AI采用混合方法：1) 整合LINDDUN知识库和模型中心隐私攻击知识库；2) 将知识库嵌入向量数据库进行语义检索；3) 结合数据流图系统级元数据；4) 使用检索增强和数据流特定的提示生成来引导LLM识别、解释和分类隐私威胁。

Result: 在两个AI系统上的评估表明，PriMod4AI能够广泛覆盖传统隐私类别，同时识别模型中心隐私威胁。框架在不同LLM间产生一致、基于知识的输出，观察到的协议分数反映了这种一致性。

Conclusion: PriMod4AI提供了一个统一的隐私威胁建模框架，有效整合了传统LINDDUN威胁和AI特有的隐私攻击，为AI系统生命周期各阶段提供全面、基于分类的隐私威胁评估。

Abstract: Artificial intelligence systems introduce complex privacy risks throughout their lifecycle, especially when processing sensitive or high-dimensional data. Beyond the seven traditional privacy threat categories defined by the LINDDUN framework, AI systems are also exposed to model-centric privacy attacks such as membership inference and model inversion, which LINDDUN does not cover. To address both classical LINDDUN threats and additional AI-driven privacy attacks, PriMod4AI introduces a hybrid privacy threat modeling approach that unifies two structured knowledge sources, a LINDDUN knowledge base representing the established taxonomy, and a model-centric privacy attack knowledge base capturing threats outside LINDDUN. These knowledge bases are embedded into a vector database for semantic retrieval and combined with system level metadata derived from Data Flow Diagram. PriMod4AI uses retrieval-augmented and Data Flow specific prompt generation to guide large language models (LLMs) in identifying, explaining, and categorizing privacy threats across lifecycle stages. The framework produces justified and taxonomy-grounded threat assessments that integrate both classical and AI-driven perspectives. Evaluation on two AI systems indicates that PriMod4AI provides broad coverage of classical privacy categories while additionally identifying model-centric privacy threats. The framework produces consistent, knowledge-grounded outputs across LLMs, as reflected in agreement scores in the observed range.

</details>


### [7] [Attack Selection Reduces Safety in Concentrated AI Control Settings against Trusted Monitoring](https://arxiv.org/abs/2602.04930)
*Joachim Schaeffer,Arjun Khandelwal,Tyler Tracy*

Main category: cs.CR

TL;DR: 研究AI如何通过对抗性选择攻击来规避监控，发现在BigCodeBench后门设置中，让攻击模型推理监控器会显著降低安全性


<details>
  <summary>Details</summary>
Motivation: 未来AI部署可能会受到恶意行为监控，但AI可能通过对抗性选择攻击来规避监控（攻击选择），这种能力令人担忧，需要研究其影响

Method: 让红队创建攻击策略，在BigCodeBench后门设置中尝试插入攻击而不被监控器发现；将攻击选择分解为两个问题：将攻击映射到质量分数，将质量分数映射到提交概率；将攻击选择构建为分类问题

Result: 安全性对FPR（误报率）比TPR（真阳性率）更敏感；提示攻击模型推理监控器并谨慎选择攻击时，安全性从基线99%降至59%（0.5%审计预算下）；激发模型的攻击选择能力对避免控制评估中过于乐观的安全评分至关重要

Conclusion: 激发模型的攻击选择能力对于准确评估AI安全性至关重要，否则可能导致过于乐观的安全评估；攻击选择能力显著影响监控效果

Abstract: Future AI deployments will likely be monitored for malicious behaviour. The ability of these AIs to subvert monitors by adversarially selecting against them - attack selection - is particularly concerning. To study this, we let a red team create attack policies that attempt to insert attacks into code without being caught by a monitor in the concentrated BigCodeBench backdooring setting. We decompose attack selection into two problems: mapping attacks to a quality score and mapping quality scores to submission probabilities. We frame attack selection as a classification problem and show that safety is significantly more sensitive to FPR than TPR. We find that prompting the attacker model to reason about the monitor while being cautious with attack selection reduces safety from a baseline of 99% to 59% at 0.5% auditing budget, emphasizing that eliciting attack selection capabilities of models is vital to avoid overly optimistic safety scores in control evaluations.

</details>


### [8] [The Birthmark Standard: Privacy-Preserving Photo Authentication via Hardware Roots of Trust and Consortium Blockchain](https://arxiv.org/abs/2602.04933)
*Sam Ryan*

Main category: cs.CR

TL;DR: 提出Birthmark Standard认证架构，利用相机传感器固有噪声模式生成硬件根密钥，通过联盟区块链存储认证记录，解决AI生成图像破坏照片证据可信度的问题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI系统快速发展导致摄影证据可信度崩溃，现有认证方法（如C2PA）存在技术脆弱性（元数据易被剥离）和结构性依赖（企业控制验证基础设施），商业利益可能与公共利益冲突。

Method: 利用制造独特的传感器熵（非均匀性校正图和PRNU模式）生成硬件根认证密钥；相机创建匿名化认证证书证明传感器真实性；认证记录存储在新闻机构运营的联盟区块链上，确保元数据丢失后仍可验证。

Result: 使用ProVerif形式化验证隐私属性，证明在Dolev-Yao敌手假设下满足制造商非相关性和区块链观察者非识别性；原型实现验证完整加密流程；性能分析显示相机开销低于100ms，百万级日认证量下验证延迟低于500ms。

Conclusion: Birthmark Standard提供硬件根认证架构，通过传感器固有特征和去中心化存储解决AI时代照片认证难题，平衡真实性验证与隐私保护，为新闻和公共话语提供可靠证据基础。

Abstract: The rapid advancement of generative AI systems has collapsed the credibility landscape for photographic evidence. Modern image generation models produce photorealistic images undermining the evidentiary foundation upon which journalism and public discourse depend. Existing authentication approaches, such as the Coalition for Content Provenance and Authenticity (C2PA), embed cryptographically signed metadata directly into image files but suffer from two critical failures: technical vulnerability to metadata stripping during social media reprocessing, and structural dependency on corporate-controlled verification infrastructure where commercial incentives may conflict with public interest. We present the Birthmark Standard, an authentication architecture leveraging manufacturing-unique sensor entropy from non-uniformity correction (NUC) maps and PRNU patterns to generate hardware-rooted authentication keys. During capture, cameras create anonymized authentication certificates proving sensor authenticity without exposing device identity via a key table architecture maintaining anonymity sets exceeding 1,000 devices. Authentication records are stored on a consortium blockchain operated by journalism organizations rather than commercial platforms, enabling verification that survives all metadata loss. We formally verify privacy properties using ProVerif, proving observational equivalence for Manufacturer Non-Correlation and Blockchain Observer Non-Identification under Dolev-Yao adversary assumptions. The architecture is validated through prototype implementation using Raspberry Pi 4 hardware, demonstrating the complete cryptographic pipeline. Performance analysis projects camera overhead below 100ms and verification latency below 500ms at scale of one million daily authentications.

</details>


### [9] [SynthForensics: A Multi-Generator Benchmark for Detecting Synthetic Video Deepfakes](https://arxiv.org/abs/2602.04939)
*Roberto Leotta,Salvatore Alfio Sambataro,Claudio Vittorio Ragaglia,Mirko Casu,Yuri Petralia,Francesco Guarnera,Luca Guarnera,Sebastiano Battiato*

Main category: cs.CR

TL;DR: 提出了首个面向纯合成视频深度伪造检测的人类中心化基准SynthForensics，包含6,815个来自5种不同架构开源T2V模型的视频，实验显示现有检测器在该领域性能显著下降且泛化能力有限。


<details>
  <summary>Details</summary>
Motivation: 随着文本到视频(T2V)模型的快速发展，合成媒体质量已接近真实，且开源生成器的普及使普通用户也能在消费级硬件上创建高保真合成内容，这使得现有的面部中心和基于篡改的检测基准变得过时，迫切需要新的检测基准来应对这一威胁。

Method: 构建了SynthForensics基准，包含6,815个来自5种架构不同的开源T2V模型的独特视频；采用两阶段人工参与验证确保语义和视觉质量；每个视频提供四种版本（原始、无损、轻度压缩、重度压缩）以支持真实世界鲁棒性测试；并研究了在该基准上训练以缓解性能差距的效果。

Result: 实验表明现有最先进的检测器在新领域表现脆弱且泛化能力有限：平均AUC下降29.19%，部分方法表现甚至不如随机猜测，顶级模型在重度压缩下损失超过30个点；通过在SynthForensics上训练可以缓解性能差距，实现对未见生成器的鲁棒泛化（93.81% AUC），但会降低与传统基于篡改的深度伪造检测的向后兼容性。

Conclusion: SynthForensics是首个针对纯合成视频深度伪造检测的人类中心化基准，揭示了现有检测器在应对新兴T2V生成技术时的严重局限性，为未来检测方法的发展提供了重要评估平台，同时公开数据集和生成元数据将促进该领域研究。

Abstract: The landscape of synthetic media has been irrevocably altered by text-to-video (T2V) models, whose outputs are rapidly approaching indistinguishability from reality. Critically, this technology is no longer confined to large-scale labs; the proliferation of efficient, open-source generators is democratizing the ability to create high-fidelity synthetic content on consumer-grade hardware. This makes existing face-centric and manipulation-based benchmarks obsolete. To address this urgent threat, we introduce SynthForensics, to the best of our knowledge the first human-centric benchmark for detecting purely synthetic video deepfakes. The benchmark comprises 6,815 unique videos from five architecturally distinct, state-of-the-art open-source T2V models. Its construction was underpinned by a meticulous two-stage, human-in-the-loop validation to ensure high semantic and visual quality. Each video is provided in four versions (raw, lossless, light, and heavy compression) to enable real-world robustness testing. Experiments demonstrate that state-of-the-art detectors are both fragile and exhibit limited generalization when evaluated on this new domain: we observe a mean performance drop of $29.19\%$ AUC, with some methods performing worse than random chance, and top models losing over 30 points under heavy compression. The paper further investigates the efficacy of training on SynthForensics as a means to mitigate these observed performance gaps, achieving robust generalization to unseen generators ($93.81\%$ AUC), though at the cost of reduced backward compatibility with traditional manipulation-based deepfakes. The complete dataset and all generation metadata, including the specific prompts and inference parameters for every video, will be made publicly available at [link anonymized for review].

</details>


### [10] [System-Level Isolation for Mixed-Criticality RISC-V SoCs: A "World" Reality Check](https://arxiv.org/abs/2602.05002)
*Luis Cunha,Jose Martins,Manuel Rodriguez,Tiago Gomes,Sandro Pinto,Uwe Moslehner,Kai Dieffenbach,Glenn Farrall,Kajetan Nuernberger,Thomas Roecker*

Main category: cs.CR

TL;DR: 本文对RISC-V硬件隔离原语（IOPMP、World-based checker等）进行对比分析，评估其在异构SoC中的安全性、性能和面积权衡，提出改进方案可降低约5%的SoC面积。


<details>
  <summary>Details</summary>
Motivation: 随着RISC-V在汽车、物联网等领域的应用增长，这些领域对SWaP-C（尺寸、重量、功耗、成本）有严格要求，推动了异构SoC的发展。然而，异构集成带来了混合关键性环境下的安全隔离挑战，现有RISC-V隔离原语的互操作性、可扩展性和实时性支持尚不明确。

Method: 实现并比较三种硬件隔离机制：IOPMP、基于World的检查器，以及改进的RISC-V World检查器（解决了基线规范的关键限制）。从实际异构SoC设计角度评估它们在安全保证和PPA（功耗-性能-面积）方面的权衡。

Result: 基于World的检查器引入固定、配置无关的访问延迟，相比其他方案具有更低的最坏情况延迟，且随系统规模可预测地扩展。在宏观层面，提出的改进方案相比基线设计可减少约5%的SoC面积。

Conclusion: 所有成果将开源发布，这些发现有望直接促进RISC-V规范的演进和标准化，以及未来RISC-V SoC的设计。研究为混合关键性异构SoC提供了实用的硬件隔离解决方案。

Abstract: As RISC-V adoption accelerates, domains such as automotive, the Internet of Things (IoT), and industrial control are attracting growing attention. These domains are subject to stringent Size, Weight, Power, and Cost (SWaP-C) constraints, which have driven a shift toward heterogeneous Systems-on-Chip (SoCs) integrating general-purpose CPUs, tightly coupled accelerators, and diverse I/O devices with different integrity levels. While such integration improves cost efficiency and performance, it introduces a fundamental safety and security challenge: enforcing system-level isolation in mixed-criticality environments. Although RISC-V International has proposed several hardware isolation primitives, including RISC-V Worlds, IOPMP, and SmMTT, their interoperability, scalability, and suitability for real-time systems remain insufficiently understood. In this paper, we present a comparative analysis of these primitives from the perspective of practical heterogeneous SoC designs. We implement an IOPMP, a World-based checker, and a modified RISC-V World checker that addresses key limitations of the baseline specification, and evaluate their trade-offs in terms of security guarantees and power-performance-area (PPA). Our results show that the World-based checker introduces a fixed, configuration-independent access latency, achieving lower worst-case delay than the evaluated alternatives while scaling predictably with system size. At the macro level, we estimate that the proposed modifications reduce SoC area by up to approximately 5% compared to a baseline design. All artifacts will be released as open source, and we expect these findings to directly contribute to the evolution and ratification of RISC-V specifications, as well as to the design of future RISC-V SoCs.

</details>


### [11] [Do Vision-Language Models Respect Contextual Integrity in Location Disclosure?](https://arxiv.org/abs/2602.05023)
*Ruixin Yang,Ethan Mendes,Arthur Wang,James Hays,Sauvik Das,Wei Xu,Alan Ritter*

Main category: cs.CR

TL;DR: 论文提出VLM-GEOPRIVACY基准，评估视觉语言模型在图像地理位置推断中的隐私风险，发现当前模型与人类隐私期望不匹配，呼吁设计具备上下文条件隐私推理的多模态系统。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型和前沿多模态大推理模型在图像地理位置推断方面表现出色，这带来了严重的隐私风险——这些广泛可访问的模型可能被滥用以从随意分享的照片中推断敏感位置，甚至达到街道级别的精度，超出了分享者同意或意图披露的细节水平。需要评估模型如何尊重上下文完整性来平衡隐私和实用性。

Method: 提出了VLM-GEOPRIVACY基准，挑战视觉语言模型解释真实世界图像中的潜在社会规范和上下文线索，并确定适当的位置披露水平。评估了14个领先的视觉语言模型。

Result: 尽管模型能够精确地定位图像地理位置，但它们与人类隐私期望的匹配度很差。模型经常在敏感上下文中过度披露信息，并且容易受到基于提示的攻击。

Conclusion: 研究结果呼吁在多模态系统中采用新的设计原则，以纳入上下文条件的隐私推理，使模型能够根据图像中的元素推理确定适当的信息披露水平，平衡隐私和实用性。

Abstract: Vision-language models (VLMs) have demonstrated strong performance in image geolocation, a capability further sharpened by frontier multimodal large reasoning models (MLRMs). This poses a significant privacy risk, as these widely accessible models can be exploited to infer sensitive locations from casually shared photos, often at street-level precision, potentially surpassing the level of detail the sharer consented or intended to disclose. While recent work has proposed applying a blanket restriction on geolocation disclosure to combat this risk, these measures fail to distinguish valid geolocation uses from malicious behavior. Instead, VLMs should maintain contextual integrity by reasoning about elements within an image to determine the appropriate level of information disclosure, balancing privacy and utility. To evaluate how well models respect contextual integrity, we introduce VLM-GEOPRIVACY, a benchmark that challenges VLMs to interpret latent social norms and contextual cues in real-world images and determine the appropriate level of location disclosure. Our evaluation of 14 leading VLMs shows that, despite their ability to precisely geolocate images, the models are poorly aligned with human privacy expectations. They often over-disclose in sensitive contexts and are vulnerable to prompt-based attacks. Our results call for new design principles in multimodal systems to incorporate context-conditioned privacy reasoning.

</details>


### [12] [VEXA: Evidence-Grounded and Persona-Adaptive Explanations for Scam Risk Sensemaking](https://arxiv.org/abs/2602.05056)
*Heajun An,Connor Ng,Sandesh Sharma Dulal,Junghwan Kim,Jin-Hee Cho*

Main category: cs.CR

TL;DR: VEXA框架：结合梯度SHAP归因与理论化漏洞角色，为学习者生成证据基础、角色自适应的诈骗解释，提升语义可靠性而不增加语言复杂性。


<details>
  <summary>Details</summary>
Motivation: 在线诈骗日益复杂，生成式AI使欺骗更加流畅和情境感知。虽然基于Transformer的检测器有很强的预测性能，但其解释对非专家不透明或与模型决策不一致，需要更有效的学习者导向解释方法。

Method: 提出VEXA框架，整合梯度SHAP归因与理论化的漏洞角色，生成证据基础、角色自适应的诈骗解释。通过多通道数据集评估，确保解释基于检测器证据，同时引入角色条件化实现可解释的风格变化。

Result: 评估显示：基于检测器证据的解释提高了语义可靠性而不增加语言复杂性；角色条件化引入可解释的风格变化而不破坏证据对齐。证据基础决定语义正确性，角色适应在忠实性约束下操作于呈现层面。

Conclusion: VEXA证明了角色自适应、证据基础解释的可行性，为非正式情境下可信赖、学习者导向的安全解释提供了设计指导。证据基础控制语义正确性，角色适应在呈现层面操作。

Abstract: Online scams across email, short message services, and social media increasingly challenge everyday risk assessment, particularly as generative AI enables more fluent and context-aware deception. Although transformer-based detectors achieve strong predictive performance, their explanations are often opaque to non-experts or misaligned with model decisions. We propose VEXA, an evidence-grounded and persona-adaptive framework for generating learner-facing scam explanations by integrating GradientSHAP-based attribution with theory-informed vulnerability personas. Evaluation across multi-channel datasets shows that grounding explanations in detector-derived evidence improves semantic reliability without increasing linguistic complexity, while persona conditioning introduces interpretable stylistic variation without disrupting evidential alignment. These results reveal a key design insight: evidential grounding governs semantic correctness, whereas persona-based adaptation operates at the level of presentation under constraints of faithfulness. Together, VEXA demonstrates the feasibility of persona-adaptive, evidence-grounded explanations and provides design guidance for trustworthy, learner-facing security explanations in non-formal contexts.

</details>


### [13] [Bypassing AI Control Protocols via Agent-as-a-Proxy Attacks](https://arxiv.org/abs/2602.05066)
*Jafar Isbarov,Murat Kantarcioglu*

Main category: cs.CR

TL;DR: 论文提出了一种名为"Agent-as-a-Proxy"的新型攻击方法，能够绕过基于监控的AI代理防御系统，即使使用前沿大规模监控模型也无法有效防御。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理自动化关键工作负载，它们仍然容易受到间接提示注入攻击。当前防御依赖于监控协议，通过联合评估代理的思维链和工具使用动作来确保与用户意图一致。然而，这些监控防御可能存在根本性脆弱性。

Method: 提出Agent-as-a-Proxy攻击方法，将AI代理作为传递机制，同时绕过代理和监控系统。在AgentDojo基准测试中，针对AlignmentCheck和Extract-and-Evaluate监控器，使用GPT-4o mini和Llama-3.1-70B等代理攻击大规模监控模型如Qwen2.5-72B。

Result: 在AgentDojo基准测试中，针对不同监控LLM下的AlignmentCheck和Extract-and-Evaluate监控器实现了高攻击成功率。即使使用前沿规模监控模型，也能被类似能力的代理绕过，表明当前基于监控的代理防御具有根本脆弱性。

Conclusion: 当前基于监控的AI代理防御系统无论模型规模如何都存在根本性脆弱性。即使前沿规模监控模型也无法有效防御Agent-as-a-Proxy攻击，这挑战了可扩展监督的现有假设，表明需要更强大的防御机制。

Abstract: As AI agents automate critical workloads, they remain vulnerable to indirect prompt injection (IPI) attacks. Current defenses rely on monitoring protocols that jointly evaluate an agent's Chain-of-Thought (CoT) and tool-use actions to ensure alignment with user intent. We demonstrate that these monitoring-based defenses can be bypassed via a novel Agent-as-a-Proxy attack, where prompt injection attacks treat the agent as a delivery mechanism, bypassing both agent and monitor simultaneously. While prior work on scalable oversight has focused on whether small monitors can supervise large agents, we show that even frontier-scale monitors are vulnerable. Large-scale monitoring models like Qwen2.5-72B can be bypassed by agents with similar capabilities, such as GPT-4o mini and Llama-3.1-70B. On the AgentDojo benchmark, we achieve a high attack success rate against AlignmentCheck and Extract-and-Evaluate monitors under diverse monitoring LLMs. Our findings suggest current monitoring-based agentic defenses are fundamentally fragile regardless of model scale.

</details>


### [14] [Beware Untrusted Simulators -- Reward-Free Backdoor Attacks in Reinforcement Learning](https://arxiv.org/abs/2602.05089)
*Ethan Rathbun,Wo Wei Lin,Alina Oprea,Christopher Amato*

Main category: cs.CR

TL;DR: 论文提出了一种名为"Daze"的新型对抗攻击方法，能够在强化学习智能体中植入动作级后门，通过修改模拟器动力学来触发特定危险动作，且无需控制或观察奖励信号。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习严重依赖模拟环境进行训练，但模拟器存在安全盲点。攻击者可以恶意修改已发布的模拟器动力学，在智能体中植入后门，使其在特定触发条件下执行危险动作，而传统后门攻击需要完全控制训练流程的假设在实际模拟器场景中不切实际。

Method: 提出"Daze"攻击方法，通过修改模拟器动力学来植入动作级后门，无需改变或观察智能体的奖励信号。攻击在智能体训练过程中隐蔽地修改环境响应，使智能体在遇到预设"触发器"时可靠地执行目标动作。论文提供了Daze在通用强化学习任务中有效性的形式化证明。

Result: Daze攻击在离散和连续动作空间领域都表现出可靠性和隐蔽性，能够成功植入后门。论文首次展示了强化学习后门攻击能够迁移到真实的机器人硬件上，验证了攻击的实际威胁性。

Conclusion: 模拟器安全是强化学习训练流程中的关键薄弱环节，Daze攻击展示了通过修改模拟器动力学植入后门的可行性。这强调了需要加强整个强化学习训练流程的安全防护，防止恶意攻击，特别是考虑到攻击能够迁移到真实硬件环境。

Abstract: Simulated environments are a key piece in the success of Reinforcement Learning (RL), allowing practitioners and researchers to train decision making agents without running expensive experiments on real hardware. Simulators remain a security blind spot, however, enabling adversarial developers to alter the dynamics of their released simulators for malicious purposes. Therefore, in this work we highlight a novel threat, demonstrating how simulator dynamics can be exploited to stealthily implant action-level backdoors into RL agents. The backdoor then allows an adversary to reliably activate targeted actions in an agent upon observing a predefined ``trigger'', leading to potentially dangerous consequences. Traditional backdoor attacks are limited in their strong threat models, assuming the adversary has near full control over an agent's training pipeline, enabling them to both alter and observe agent's rewards. As these assumptions are infeasible to implement within a simulator, we propose a new attack ``Daze'' which is able to reliably and stealthily implant backdoors into RL agents trained for real world tasks without altering or even observing their rewards. We provide formal proof of Daze's effectiveness in guaranteeing attack success across general RL tasks along with extensive empirical evaluations on both discrete and continuous action space domains. We additionally provide the first example of RL backdoor attacks transferring to real, robotic hardware. These developments motivate further research into securing all components of the RL training pipeline to prevent malicious attacks.

</details>


### [15] [Crypto-asset Taxonomy for Investors and Regulators](https://arxiv.org/abs/2602.05098)
*Xiao Zhang,Juan Ignacio Ibañez,Jiahua Xu*

Main category: cs.CR

TL;DR: 该论文开发了一个连接技术设计、市场结构和监管的多维加密资产分类框架，填补了该领域缺乏统一分类体系的空白。


<details>
  <summary>Details</summary>
Motivation: 加密资产已成为电子市场的主要组成部分，交易量和市场份额不断增长，但目前缺乏统一全面的资产级别分类框架，需要建立连接技术设计、市场结构和监管的分类体系。

Method: 基于既定的分类指南和现有模型，从理论、监管框架和案例研究中推导维度，然后对前100种资产进行映射，并提供详细的案例研究。

Result: 分类框架涵盖技术标准、关键资源中心化程度、资产功能、法律分类以及铸造、收益、赎回等机制设计。资产映射和案例研究揭示了重复的设计模式，捕捉了现有分类边界上的边缘案例特征，并记录了名义去中心化资产中的中心化控制。

Conclusion: 该论文为加密市场的系统研究提供了框架，支持监管机构评估代币风险，并为投资者和数字平台设计者在构建或参与电子市场时提供了比较资产的工具。

Abstract: Crypto-assets are a main segment of electronic markets, with growing trade volume and market share, yet there's no unified and comprehensive asset level taxonomy framework. This paper develops a multidimensional taxonomy for crypto-assets that connects technical design to market structure and regulation. Building on established taxonomy guideline and existing models, we derive dimensions from theory, regulatory frameworks, and case studies. We then map top 100 assets within the structure and provide several detailed case studies. The taxonomy covers technology standard, centralisation of critical resources, asset function, legal classification and mechanism designs of minting, yield, redemption. The asset mapping and case studies reveal recurring design patterns, capture features of edge cases that sit on boundaries of current categorisations, and document centralised control of nominal decentralised assets. This paper provides framework for systematic study for crypto markets, supports regulators in assessing token risks, and offers investors and digital platform designers a tool to compare assets when building or participate in electronic markets.

</details>


### [16] [SynAT: Enhancing Security Knowledge Bases via Automatic Synthesizing Attack Tree from Crowd Discussions](https://arxiv.org/abs/2602.05329)
*Ziyou Jiang,Lin Shi,Guowei Yang,Xuyan Ma,Fenglong Li,Qing Wang*

Main category: cs.CR

TL;DR: SynAT：一种从众包安全帖子自动合成攻击树的方法，利用LLM和事件关系提取技术构建攻击树，用于增强安全知识库


<details>
  <summary>Details</summary>
Motivation: 传统安全知识库存在信息发布滞后问题，维护困难，难以应对新兴安全风险。而在线知识共享平台上的众包安全讨论包含大量安全知识，可用于增强安全知识库。

Method: 1. 使用大语言模型和提示学习筛选包含攻击信息的句子范围；2. 采用基于转换的事件和关系提取模型同时提取事件和关系；3. 应用启发式规则将提取的事件和关系合成为攻击树。

Result: 在5,070个Stack Overflow安全帖子上进行实验评估，SynAT在事件和关系提取方面优于所有基线方法，在攻击树合成中实现了最高的树相似度。已成功应用于增强华为安全知识库以及公共安全知识库CVE和CAPEC。

Conclusion: SynAT能够有效从众包安全帖子中自动合成攻击树，证明了其实用性和对安全知识库增强的价值。

Abstract: Cyber attacks have become a serious threat to the security of software systems. Many organizations have built their security knowledge bases to safeguard against attacks and vulnerabilities. However, due to the time lag in the official release of security information, these security knowledge bases may not be well maintained, and using them to protect software systems against emergent security risks can be challenging. On the other hand, the security posts on online knowledge-sharing platforms contain many crowd security discussions and the knowledge in those posts can be used to enhance the security knowledge bases. This paper proposes SynAT, an automatic approach to synthesize attack trees from crowd security posts. Given a security post, SynAT first utilize the Large Language Model (LLM) and prompt learning to restrict the scope of sentences that may contain attack information; then it utilizes a transition-based event and relation extraction model to extract the events and relations simultaneously from the scope; finally, it applies heuristic rules to synthesize the attack trees with the extracted events and relations. An experimental evaluation is conducted on 5,070 Stack Overflow security posts, and the results show that SynAT outperforms all baselines in both event and relation extraction, and achieves the highest tree similarity in attack tree synthesis. Furthermore, SynAT has been applied to enhance HUAWEI's security knowledge base as well as public security knowledge bases CVE and CAPEC, which demonstrates SynAT's practicality.

</details>


### [17] [Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening](https://arxiv.org/abs/2602.05386)
*Zhenxiong Yu,Zhi Yang,Zhiheng Jin,Shuhe Wang,Heng Zhang,Yanlin Fei,Lingfeng Zeng,Fangqi Lou,Shuo Zhang,Tu Hu,Jingping Liu,Rongze Chen,Xingyu Zhu,Kunyi Wang,Chaofa Yuan,Xin Guo,Zhaowei Liu,Feipeng Zhang,Jie Huang,Huacan Wang,Ronghao Chen,Liwen Zhang*

Main category: cs.CR

TL;DR: 提出Spider-Sense框架，基于内在风险感知的事件驱动防御机制，让LLM代理在感知风险时才触发防御，通过分层防御机制平衡效率与精度。


<details>
  <summary>Details</summary>
Motivation: 现有代理防御机制采用强制检查范式，在预定义阶段强制触发安全验证。作者认为有效的代理安全应该是内在的和选择性的，而非架构解耦和强制性的。

Method: 提出Spider-Sense框架，基于内在风险感知的事件驱动防御，让代理保持潜在警惕性，仅在感知风险时触发防御。采用分层防御机制：通过轻量级相似性匹配解决已知模式，将模糊案例升级到深度内部推理，消除对外部模型的依赖。

Result: 在S²Bench基准测试中，Spider-Sense实现了竞争性或优越的防御性能，达到最低的攻击成功率（ASR）和误报率（FPR），仅带来8.3%的延迟开销。

Conclusion: Spider-Sense框架通过内在风险感知和事件驱动防御，为LLM代理提供了更有效、更高效的安全解决方案，相比强制检查范式具有显著优势。

Abstract: As large language models (LLMs) evolve into autonomous agents, their real-world applicability has expanded significantly, accompanied by new security challenges. Most existing agent defense mechanisms adopt a mandatory checking paradigm, in which security validation is forcibly triggered at predefined stages of the agent lifecycle. In this work, we argue that effective agent security should be intrinsic and selective rather than architecturally decoupled and mandatory. We propose Spider-Sense framework, an event-driven defense framework based on Intrinsic Risk Sensing (IRS), which allows agents to maintain latent vigilance and trigger defenses only upon risk perception. Once triggered, the Spider-Sense invokes a hierarchical defence mechanism that trades off efficiency and precision: it resolves known patterns via lightweight similarity matching while escalating ambiguous cases to deep internal reasoning, thereby eliminating reliance on external models. To facilitate rigorous evaluation, we introduce S$^2$Bench, a lifecycle-aware benchmark featuring realistic tool execution and multi-stage attacks. Extensive experiments demonstrate that Spider-Sense achieves competitive or superior defense performance, attaining the lowest Attack Success Rate (ASR) and False Positive Rate (FPR), with only a marginal latency overhead of 8.3\%.

</details>


### [18] [BadTemplate: A Training-Free Backdoor Attack via Chat Template Against Large Language Models](https://arxiv.org/abs/2602.05401)
*Zihan Wang,Hongwei Li,Rui Zhang,Wenbo Jiang,Guowen Xu*

Main category: cs.CR

TL;DR: BadTemplate是一种无需训练的LLM后门攻击方法，通过操控聊天模板在系统提示中注入恶意指令，实现高达100%的攻击成功率


<details>
  <summary>Details</summary>
Motivation: 揭示聊天模板在LLM训练和推理中的广泛应用带来了新的攻击面，攻击者可通过控制模板向系统提示注入任意字符串而不被用户察觉

Method: 提出BadTemplate攻击方法：通过精心设计的恶意指令插入高优先级系统提示，无需模型重训练，实现持久后门行为

Result: 在5个基准数据集、6个开源和3个闭源LLM上的实验表明，BadTemplate攻击成功率高达100%，显著优于传统基于提示的后门攻击

Conclusion: 聊天模板在LLM供应链中存在严重安全风险，BadTemplate攻击简单、可扩展且难以检测，需要开发有效的防御机制

Abstract: Chat template is a common technique used in the training and inference stages of Large Language Models (LLMs). It can transform input and output data into role-based and templated expressions to enhance the performance of LLMs. However, this also creates a breeding ground for novel attack surfaces. In this paper, we first reveal that the customizability of chat templates allows an attacker who controls the template to inject arbitrary strings into the system prompt without the user's notice. Building on this, we propose a training-free backdoor attack, termed BadTemplate. Specifically, BadTemplate inserts carefully crafted malicious instructions into the high-priority system prompt, thereby causing the target LLM to exhibit persistent backdoor behaviors. BadTemplate outperforms traditional backdoor attacks by embedding malicious instructions directly into the system prompt, eliminating the need for model retraining while achieving high attack effectiveness with minimal cost. Furthermore, its simplicity and scalability make it easily and widely deployed in real-world systems, raising serious risks of rapid propagation, economic damage, and large-scale misinformation. Furthermore, detection by major third-party platforms HuggingFace and LLM-as-a-judge proves largely ineffective against BadTemplate. Extensive experiments conducted on 5 benchmark datasets across 6 open-source and 3 closed-source LLMs, compared with 3 baselines, demonstrate that BadTemplate achieves up to a 100% attack success rate and significantly outperforms traditional prompt-based backdoors in both word-level and sentence-level attacks. Our work highlights the potential security risks raised by chat templates in the LLM supply chain, thereby supporting the development of effective defense mechanisms.

</details>


### [19] [LTRAS: A Linkable Threshold Ring Adaptor Signature Scheme for Efficient and Private Cross-Chain Transactions](https://arxiv.org/abs/2602.05431)
*Yi Liang,Jinguang Han*

Main category: cs.CR

TL;DR: 提出LTRAS方案，结合适配器签名、门限环签名和可链接性，解决区块链支付通道中的隐私和效率问题


<details>
  <summary>Details</summary>
Motivation: 区块链支付通道需要适配器签名保证原子性，但标准适配器签名会泄露签名者身份，而环签名虽能提供匿名性却带来高通信开销，特别是在UTXO区块链的多账户支付场景中

Method: 提出可链接门限环适配器签名(LTRAS)方案，整合适配器签名的条件绑定、门限环签名的多账户支付能力以及可链接性防止双花，提供形式化定义、安全模型和具体构造

Result: 理论分析和实验实现表明，在大环大小和多账户支付场景下，LTRAS方案相比现有方案显著降低了计算和通信开销

Conclusion: LTRAS方案能有效增强区块链交易的隐私和效率，特别适用于跨链原子交换等应用场景

Abstract: Despite the advantages of decentralization and immutability, blockchain technology faces significant scalability and throughput limitations, which has prompted the exploration of off-chain solutions like payment channels. Adaptor signatures have been considered a promising primitive for constructing such channels due to their support for atomicity, offering an alternative to traditional hash-timelock contracts. However, standard adaptor signatures may reveal signer identity, raising potential privacy concerns. While ring signatures can mitigate this issue by providing anonymity, they often introduce high communication overhead, particularly in multi-account payment settings commonly used in UTXO-based blockchains like Monero. To address these limitations, we propose a Linkable Threshold Ring Adaptor Signature (LTRAS) scheme, which integrates the conditional binding of adaptor signatures, the multi-account payment of threshold ring signatures, and the linkability for preventing double-spending. The formal definition, security model and concrete construction of LTRAS are provided. We also analyze its security and evaluate its performance through theoretical analysis and experimental implementation. Experimental results demonstrate that our scheme achieve significantly lower computation and communication overhead compared to existing schemes in large ring sizes and multi-account payment scenarios. Finally, we discuss its application in cross-chain atomic swaps, demonstrating its potential for enhancing privacy and efficiency in blockchain transactions.

</details>


### [20] [Clouding the Mirror: Stealthy Prompt Injection Attacks Targeting LLM-based Phishing Detection](https://arxiv.org/abs/2602.05484)
*Takashi Koide,Hiroki Nakano,Daiki Chiba*

Main category: cs.CR

TL;DR: 该论文首次全面评估了多模态LLM钓鱼检测中的提示注入攻击，提出了攻击分类法，展示了先进模型仍易受攻击，并提出了有效的防御框架InjectDefuser。


<details>
  <summary>Details</summary>
Motivation: 虽然基于LLM的钓鱼检测方法前景广阔，但LLM天生易受提示注入攻击。攻击者完全控制钓鱼网站的各个元素，可以利用LLM与人类之间的感知不对称性进行隐蔽攻击。目前对钓鱼检测中提示注入的具体风险和有效缓解策略研究不足。

Method: 提出了一个二维分类法（攻击技术和攻击面），涵盖现实的提示注入策略。基于此分类法实施多样化攻击，并对多个代表性LLM检测系统进行实证研究。然后提出了InjectDefuser防御框架，结合提示硬化、基于白名单的检索增强和输出验证。

Result: 实验结果显示，即使是GPT-5等最先进的模型在钓鱼检测中仍然易受提示注入攻击。InjectDefuser防御框架在多个模型上显著降低了攻击成功率，提高了检测系统的可靠性。

Conclusion: 该研究阐明了提示注入攻击的风险格局，并为下一代钓鱼检测提供了实用的防御方案，提高了基于LLM的钓鱼检测系统的可靠性和安全性。

Abstract: Phishing sites continue to grow in volume and sophistication. Recent work leverages large language models (LLMs) to analyze URLs, HTML, and rendered content to decide whether a website is a phishing site. While these approaches are promising, LLMs are inherently vulnerable to prompt injection (PI). Because attackers can fully control various elements of phishing sites, this creates the potential for PI that exploits the perceptual asymmetry between LLMs and humans: instructions imperceptible to end users can still be parsed by the LLM and can stealthily manipulate its judgment. The specific risks of PI in phishing detection and effective mitigation strategies remain largely unexplored. This paper presents the first comprehensive evaluation of PI against multimodal LLM-based phishing detection. We introduce a two-dimensional taxonomy, defined by Attack Techniques and Attack Surfaces, that captures realistic PI strategies. Using this taxonomy, we implement diverse attacks and empirically study several representative LLM-based detection systems. The results show that phishing detection with state-of-the-art models such as GPT-5 remains vulnerable to PI. We then propose InjectDefuser, a defense framework that combines prompt hardening, allowlist-based retrieval augmentation, and output validation. Across multiple models, InjectDefuser significantly reduces attack success rates. Our findings clarify the PI risk landscape and offer practical defenses that improve the reliability of next-generation phishing countermeasures.

</details>


### [21] [GNSS SpAmming: a spoofing-based GNSS denial-of-service attack](https://arxiv.org/abs/2602.05517)
*Sergio Angulo Cosín,Javier Junquera-Sánchez,Carlos Hernando-Ramiro,José-Antonio Gómez-Sánchez*

Main category: cs.CR

TL;DR: 论文提出了一种新型GNSS攻击"SpAmming"，结合了干扰和欺骗两种手段，通过CDMA多路复用技术使接收器失去对合法卫星信号的访问，现有反制措施难以应对。


<details>
  <summary>Details</summary>
Motivation: 现有GNSS系统面临干扰（拒绝信号访问）和欺骗（冒充合法卫星）两种攻击，虽然已有大量反制措施，但作者发现了一种结合两者的新型攻击方式，需要研究其威胁和应对方法。

Method: 利用大多数GNSS中的CDMA多路复用技术，通过欺骗攻击使接收器失去对合法卫星信号的访问。使用SDR系统开发实验原型，针对冷启动、热启动和已获取PVT解算的接收器进行测试，评估不同攻击配置的效果。

Result: 攻击对冷启动接收器特别有效，但在其他场景下也有效果，尤其是配合其他攻击时。实验验证了攻击的可行性，并展示了不同配置下的影响程度。

Conclusion: SpAmming攻击结合了干扰和欺骗的特点，现有反制措施难以应对。需要开发新的检测和对抗方法，并进一步研究其对认证服务（如OSNMA）的影响，以改进对类似攻击的响应能力。

Abstract: GNSSs are vulnerable to attacks of two kinds: jamming (i.e. denying access to the signal) and spoofing (i.e. impersonating a legitimate satellite). These attacks have been extensively studied, and we have a myriad of countermeasures to mitigate them. In this paper we expose a new type of attack: SpAmming, which combines both approaches to achieve the same effects in a more subtle way.
  Exploiting the CDMA multiplexing present in most GNSSs, and through a spoofing attack, this approach leads the receiver to lose access to the signal of a legitimate satellite, which would be equivalent to a denial of service; but in this case the existing countermeasures against jamming or spoofing would not allow safeguarding its effectiveness, as it is neither of them.
  An experimental proof-of-concept is presented in which its impact is evaluated as a function of the previous state of the receiver. Using an SDR-based system developed at the Space Security Centre, the attack is executed against a cold-started receiver, a warm-started receiver, and a receiver that has already acquired the PVT solution and is navigating. Different attack configurations are also tested, starting from a raw emission of the false signal, to surgical Doppler effect configuration, code offset, etc. Although it is shown to be particularly successful against cold-started receivers, the results show that it is also effective in other scenarios, especially if accompanied by other attacks. We will conclude the article by outlining possible countermeasures to detect and, eventually, counteract it; and possible avenues of research to better understand its impact, especially for authenticated services such as OSNMA, and to characterize it in order to improve the response to similar attacks.

</details>


### [22] [Deep Learning for Contextualized NetFlow-Based Network Intrusion Detection: Methods, Data, Evaluation and Deployment](https://arxiv.org/abs/2602.05594)
*Abdelkader El Mahdaouy,Issam Ait Yahia,Soufiane Oualil,Ismail Berrada*

Main category: cs.CR

TL;DR: 该论文综述了基于流的网络入侵检测中上下文感知深度学习的研究进展，提出了四维分类法，并强调了严格评估和实际部署的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着加密技术的普及，基于负载内容的检测方法失效，需要依赖流级遥测数据。现有基于学习的检测器通常将每个流记录视为独立样本，但实际攻击活动中的证据分布在多个流和主机中，需要上下文感知的方法来捕捉攻击的时空模式。

Method: 论文提出了一个四维分类法来组织现有方法：1) 时间上下文（捕捉跨时间的模式），2) 图或关系上下文（捕捉主机间关系），3) 多模态上下文（整合不同类型数据），4) 多分辨率上下文（在不同时间尺度分析）。同时强调严格评估和实际部署约束。

Result: 文献表明，当攻击产生可测量的时间或关系结构时，上下文可以显著改善检测效果。但改进的幅度和可靠性高度依赖于严格的因果评估和捕捉现实多样性的数据集。论文还识别了常见的评估缺陷和实际部署挑战。

Conclusion: 上下文感知深度学习在基于流的入侵检测中具有潜力，但需要更严格的评估方法和更现实的数据集。未来的研究应关注实际部署约束，包括流状态管理、内存增长、延迟预算和模型压缩等实际问题。

Abstract: Network Intrusion Detection Systems (NIDS) have progressively shifted from signature-based techniques toward machine learning and, more recently, deep learning methods. Meanwhile, the widespread adoption of encryption has reduced payload visibility, weakening inspection pipelines that depend on plaintext content and increasing reliance on flow-level telemetry such as NetFlow and IPFIX. Many current learning-based detectors still frame intrusion detection as per-flow classification, implicitly treating each flow record as an independent sample. This assumption is often violated in realistic attack campaigns, where evidence is distributed across multiple flows and hosts, spanning minutes to days through staged execution, beaconing, lateral movement, and exfiltration. This paper synthesizes recent research on context-aware deep learning for flow-based intrusion detection. We organize existing methods into a four-dimensional taxonomy covering temporal context, graph or relational context, multimodal context, and multi-resolution context. Beyond modeling, we emphasize rigorous evaluation and operational realism. We review common failure modes that can inflate reported results, including temporal leakage, data splitting, dataset design flaws, limited dataset diversity, and weak cross-dataset generalization. We also analyze practical constraints that shape deployability, such as streaming state management, memory growth, latency budgets, and model compression choices. Overall, the literature suggests that context can meaningfully improve detection when attacks induce measurable temporal or relational structure, but the magnitude and reliability of these gains depend strongly on rigorous, causal evaluation and on datasets that capture realistic diversity.

</details>


### [23] [ADCA: Attention-Driven Multi-Party Collusion Attack in Federated Self-Supervised Learning](https://arxiv.org/abs/2602.05612)
*Jiayao Wang,Yiping Zhang,Jiale Zhang,Wenliang Yuan,Qilin Wu,Junwu Zhu,Dongfang Zhao*

Main category: cs.CR

TL;DR: ADCA是一种针对联邦自监督学习的后门攻击方法，通过恶意客户端联盟和注意力机制聚合，有效解决传统攻击中触发器被稀释的问题，显著提高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有联邦自监督学习后门攻击存在局限性：使用全局统一触发器容易被检测、在聚合过程中被稀释、在异构客户端环境中缺乏鲁棒性。需要设计更隐蔽、更持久的攻击方法。

Method: 提出注意力驱动的多方共谋攻击（ADCA）：1）恶意客户端在本地预训练中分解全局触发器寻找最优本地模式；2）恶意客户端形成共谋联盟，建立协作优化机制；3）使用注意力机制动态聚合模型更新，探索最佳协作策略；4）聚合参数作为联盟内下一轮训练的初始状态。

Result: 在多个联邦自监督学习场景和四个数据集上的实验表明，ADCA在攻击成功率和持久性方面显著优于现有方法，证明了其有效性和鲁棒性。

Conclusion: ADCA通过恶意客户端联盟和注意力驱动的协作优化，有效缓解了良性更新对后门信息的稀释问题，为联邦自监督学习的安全性研究提供了重要参考。

Abstract: Federated Self-Supervised Learning (FSSL) integrates the privacy advantages of distributed training with the capability of self-supervised learning to leverage unlabeled data, showing strong potential across applications. However, recent studies have shown that FSSL is also vulnerable to backdoor attacks. Existing attacks are limited by their trigger design, which typically employs a global, uniform trigger that is easily detected, gets diluted during aggregation, and lacks robustness in heterogeneous client environments. To address these challenges, we propose the Attention-Driven multi-party Collusion Attack (ADCA). During local pre-training, malicious clients decompose the global trigger to find optimal local patterns. Subsequently, these malicious clients collude to form a malicious coalition and establish a collaborative optimization mechanism within it. In this mechanism, each submits its model updates, and an attention mechanism dynamically aggregates them to explore the best cooperative strategy. The resulting aggregated parameters serve as the initial state for the next round of training within the coalition, thereby effectively mitigating the dilution of backdoor information by benign updates. Experiments on multiple FSSL scenarios and four datasets show that ADCA significantly outperforms existing methods in Attack Success Rate (ASR) and persistence, proving its effectiveness and robustness.

</details>


### [24] [Time-Complexity Characterization of NIST Lightweight Cryptography Finalists](https://arxiv.org/abs/2602.05641)
*Najmul Hasan,Prashanth BusiReddyGari*

Main category: cs.CR

TL;DR: 该论文提出了一个符号模型，用于理论分析NIST轻量级密码学项目中10个最终候选算法的时间复杂度，填补了现有经验评估缺乏统一理论理解的空白。


<details>
  <summary>Details</summary>
Motivation: 随着数字身份系统和物联网验证等新兴技术的发展，轻量级密码学变得至关重要。这些技术需要在处理能力、内存和能源资源有限的设备上提供强大的密码学保证。虽然NIST的轻量级密码学项目对10个最终候选算法进行了经验评估，但缺乏对其时间复杂度的统一理论理解。

Method: 引入一个符号模型，将每个密码方案分解为初始化、数据处理和最终化三个阶段，从而能够对所有10个最终候选算法进行形式化的时间复杂度推导。

Result: 该模型阐明了设计参数如何影响在受限移动和嵌入式环境中的计算扩展性，为区分算法效率提供了基础，并指导在受限环境中选择能够支持安全系统的密码原语。

Conclusion: 该框架填补了轻量级密码算法理论分析的空白，为在资源受限环境中选择和设计高效密码原语提供了理论基础，有助于支持新兴技术在常规使用中的安全需求。

Abstract: Lightweight cryptography is becoming essential as emerging technologies in digital identity systems and Internet of Things verification continue to demand strong cryptographic assurance on devices with limited processing power, memory, and energy resources. As these technologies move into routine use, they demand cryptographic primitives that maintain strong security and deliver predictable performance through clear theoretical models of time complexity. Although NIST's lightweight cryptography project provides empirical evaluations of the ten finalist algorithms, a unified theoretical understanding of their time-complexity behavior remains absent. This work introduces a symbolic model that decomposes each scheme into initialization, data-processing, and finalization phases, enabling formal time-complexity derivation for all ten finalists. The results clarify how design parameters shape computational scaling on constrained mobile and embedded environments. The framework provides a foundation needed to distinguish algorithmic efficiency and guides the choice of primitives capable of supporting security systems in constrained environments.

</details>


### [25] [Interpreting Manifolds and Graph Neural Embeddings from Internet of Things Traffic Flows](https://arxiv.org/abs/2602.05817)
*Enrique Feito-Casares,Francisco M. Melgarejo-Meseguer,Elena Casiraghi,Giorgio Valentini,José-Luis Rojo-Álvarez*

Main category: cs.CR

TL;DR: 提出一个可解释的管道，将高维GNN嵌入映射到潜在流形上，生成可直接可视化的低维表示，用于物联网网络监控和入侵检测。


<details>
  <summary>Details</summary>
Motivation: 物联网生态系统快速扩张导致网络拓扑日益复杂和异构，传统监控工具依赖聚合指标或静态表示，无法捕捉设备间不断演化的关系和结构依赖。虽然图神经网络(GNN)能从关系数据中学习，但其内部表示通常不透明，难以解释，不适用于安全关键操作。

Method: 引入可解释的管道，通过将高维嵌入映射到潜在流形上，生成可直接可视化的低维表示。该投影支持可解释的网络状态监控和互操作性，同时集成特征归因技术来解码塑造流形结构的具体特征。

Result: 该框架在入侵检测方面实现了0.830的分类F1分数，同时还能突出显示概念漂移等现象。

Conclusion: 该方法弥合了高维GNN嵌入与人类可理解的网络行为之间的差距，为网络管理员和安全分析师提供了新的见解。

Abstract: The rapid expansion of Internet of Things (IoT) ecosystems has led to increasingly complex and heterogeneous network topologies. Traditional network monitoring and visualization tools rely on aggregated metrics or static representations, which fail to capture the evolving relationships and structural dependencies between devices. Although Graph Neural Networks (GNNs) offer a powerful way to learn from relational data, their internal representations often remain opaque and difficult to interpret for security-critical operations. Consequently, this work introduces an interpretable pipeline that generates directly visualizable low-dimensional representations by mapping high-dimensional embeddings onto a latent manifold. This projection enables the interpretable monitoring and interoperability of evolving network states, while integrated feature attribution techniques decode the specific characteristics shaping the manifold structure. The framework achieves a classification F1-score of 0.830 for intrusion detection while also highlighting phenomena such as concept drift. Ultimately, the presented approach bridges the gap between high-dimensional GNN embeddings and human-understandable network behavior, offering new insights for network administrators and security analysts.

</details>


### [26] [FHAIM: Fully Homomorphic AIM For Private Synthetic Data Generation](https://arxiv.org/abs/2602.05838)
*Mayank Kumar,Qian Lou,Paulo Barreto,Martine De Cock,Sikha Pentyala*

Main category: cs.CR

TL;DR: FHAIM是首个基于全同态加密的框架，用于在加密表格数据上训练基于边际的合成数据生成器，解决了隐私保护与数据利用的矛盾。


<details>
  <summary>Details</summary>
Motivation: AI发展受限于隐私和法规，许多重要领域（如医疗、教育、金融）的宝贵数据被隔离，现有合成数据生成服务需要数据持有者信任服务提供商访问私有数据，存在隐私风险。

Method: FHAIM将广泛使用的AIM算法适配到全同态加密环境，开发了新颖的FHE协议，确保私有数据在整个训练过程中保持加密状态，并仅以差分隐私保证发布。

Result: 实证分析表明，FHAIM在保持可行运行时间的同时，保留了AIM算法的性能。

Conclusion: FHAIM为隐私保护的合成数据生成提供了可行的解决方案，使数据持有者能够在保护隐私的前提下利用数据，促进AI在敏感领域的应用。

Abstract: Data is the lifeblood of AI, yet much of the most valuable data remains locked in silos due to privacy and regulations. As a result, AI remains heavily underutilized in many of the most important domains, including healthcare, education, and finance. Synthetic data generation (SDG), i.e. the generation of artificial data with a synthesizer trained on real data, offers an appealing solution to make data available while mitigating privacy concerns, however existing SDG-as-a-service workflow require data holders to trust providers with access to private data.We propose FHAIM, the first fully homomorphic encryption (FHE) framework for training a marginal-based synthetic data generator on encrypted tabular data. FHAIM adapts the widely used AIM algorithm to the FHE setting using novel FHE protocols, ensuring that the private data remains encrypted throughout and is released only with differential privacy guarantees. Our empirical analysis show that FHAIM preserves the performance of AIM while maintaining feasible runtimes.

</details>


### [27] [Persistent Human Feedback, LLMs, and Static Analyzers for Secure Code Generation and Vulnerability Detection](https://arxiv.org/abs/2602.05868)
*Ehsan Firouzi,Mohammad Ghafari*

Main category: cs.CR

TL;DR: 研究发现静态安全分析工具（CodeQL和Semgrep）在评估LLM生成代码安全性时存在显著不一致性，仅65%和61%的报告与人工验证结果匹配，质疑其作为唯一评估工具的可靠性，并提出基于人类反馈的动态框架。


<details>
  <summary>Details</summary>
Motivation: 现有研究过度依赖静态分析工具评估LLM的代码安全生成和漏洞检测能力，但这些工具的可靠性尚未得到充分验证。需要了解这些工具在实际评估中的准确性和一致性。

Method: 收集1,080个LLM生成的代码样本，建立人工验证的真实基准，然后使用CodeQL和Semgrep两种主流静态安全工具进行分析，比较工具输出与人工验证结果的一致性。

Result: 61%的样本实际安全，但Semgrep和CodeQL分别报告60%和80%为安全。尽管总体统计看似一致，但逐样本分析显示显著差异：Semgrep仅65%的报告正确匹配真实情况，CodeQL仅61%正确匹配。

Conclusion: 静态分析工具不能作为代码安全的唯一评估标准，需要专家反馈。为此提出一个概念框架，将人类反馈持久存储在动态检索增强生成管道中，使LLM能够复用历史反馈来改进安全代码生成和漏洞检测。

Abstract: Existing literature heavily relies on static analysis tools to evaluate LLMs for secure code generation and vulnerability detection. We reviewed 1,080 LLM-generated code samples, built a human-validated ground-truth, and compared the outputs of two widely used static security tools, CodeQL and Semgrep, against this corpus. While 61% of the samples were genuinely secure, Semgrep and CodeQL classified 60% and 80% as secure, respectively. Despite the apparent agreement in aggregate statistics, per-sample analysis reveals substantial discrepancies: only 65% of Semgrep's and 61% of CodeQL's reports correctly matched the ground truth. These results question the reliability of static analysis tools as sole evaluators of code security and underscore the need for expert feedback. Building on this insight, we propose a conceptual framework that persistently stores human feedback in a dynamic retrieval-augmented generation pipeline, enabling LLMs to reuse past feedback for secure code generation and vulnerability detection.

</details>


### [28] [Characterizing and Modeling the GitHub Security Advisories Review Pipeline](https://arxiv.org/abs/2602.06009)
*Claudio Segal,Paulo Segal,Carlos Eduardo de Schuller Banjar,Felipe Paixão,Hudson Silva Borges,Paulo Silveira Neto,Eduardo Santana de Almeida,Joanna C. S. Santos,Anton Kocheturov,Gaurav Kumar Srivastava,Daniel Sadoc Menasché*

Main category: cs.CR

TL;DR: 对GitHub安全公告(GHSA)审核流程的大规模实证研究，分析了28.8万+公告，发现快慢两种审核路径，并建立了排队模型


<details>
  <summary>Details</summary>
Motivation: GitHub安全公告已成为开源漏洞披露的核心组件，但只有部分公告经过GitHub审核，且审核机制尚不明确，需要深入理解这一流程

Method: 对2019-2025年超过288,000个安全公告进行大规模实证研究，分析哪些公告更可能被审核、量化审核延迟，并识别两种不同的审核延迟机制

Result: 发现两种审核延迟机制：快路径以GitHub仓库公告为主，慢路径以NVD优先公告为主；建立了基于公告处理管道结构的排队模型来解释这种二分法

Conclusion: 揭示了GitHub安全公告审核流程的双重机制，为理解开源漏洞披露生态系统提供了重要见解

Abstract: GitHub Security Advisories (GHSA) have become a central component of open-source vulnerability disclosure and are widely used by developers and security tools. A distinctive feature of GHSA is that only a fraction of advisories are reviewed by GitHub, while the mechanisms associated with this review process remain poorly understood. In this paper, we conduct a large-scale empirical study of GHSA review processes, analyzing over 288,000 advisories spanning 2019--2025. We characterize which advisories are more likely to be reviewed, quantify review delays, and identify two distinct review-latency regimes: a fast path dominated by GitHub Repository Advisories (GRAs) and a slow path dominated by NVD-first advisories. We further develop a queueing model that accounts for this dichotomy based on the structure of the advisory processing pipeline.

</details>
