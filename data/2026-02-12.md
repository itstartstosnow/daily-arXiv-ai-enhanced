<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 35]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Reverse-Engineering Model Editing on Language Models](https://arxiv.org/abs/2602.10134)
*Zhiyu Sun,Minrui Luo,Yu Wang,Zhili Chen,Tianxing He*

Main category: cs.CR

TL;DR: LLM模型编辑方法存在安全漏洞：参数更新会泄露被编辑的数据，攻击者可通过低秩结构分析恢复敏感信息。论文提出KSTER攻击方法，并设计子空间伪装防御策略。


<details>
  <summary>Details</summary>
Motivation: 当前主流的"定位-编辑"模型编辑方法虽然能有效修改模型参数而不需重新训练，但作者发现这些参数更新会无意中成为侧信道，使攻击者能够恢复被编辑的敏感数据，这暴露了模型编辑范式的关键安全漏洞。

Method: 提出KSTER两阶段逆向工程攻击：1) 密钥空间重建：利用更新矩阵的低秩结构，通过谱分析恢复编辑主题的"指纹"；2) 熵减提示恢复：基于熵的提示恢复攻击重建编辑的语义上下文。同时提出子空间伪装防御策略，使用语义诱饵混淆更新指纹。

Result: 在多个LLM上的广泛实验表明，KSTER攻击能够以高成功率恢复被编辑的数据。子空间伪装防御策略在不影响编辑效用的前提下，有效降低了重建风险。

Conclusion: 模型编辑方法存在严重的安全漏洞，参数更新会泄露敏感信息。论文提出的KSTER攻击揭示了这一威胁，而子空间伪装防御为保护模型编辑隐私提供了可行方案。

Abstract: Large language models (LLMs) are pretrained on corpora containing trillions of tokens and, therefore, inevitably memorize sensitive information. Locate-then-edit methods, as a mainstream paradigm of model editing, offer a promising solution by modifying model parameters without retraining. However, in this work, we reveal a critical vulnerability of this paradigm: the parameter updates inadvertently serve as a side channel, enabling attackers to recover the edited data. We propose a two-stage reverse-engineering attack named \textit{KSTER} (\textbf{K}ey\textbf{S}paceRecons\textbf{T}ruction-then-\textbf{E}ntropy\textbf{R}eduction) that leverages the low-rank structure of these updates. First, we theoretically show that the row space of the update matrix encodes a ``fingerprint" of the edited subjects, enabling accurate subject recovery via spectral analysis. Second, we introduce an entropy-based prompt recovery attack that reconstructs the semantic context of the edit. Extensive experiments on multiple LLMs demonstrate that our attacks can recover edited data with high success rates. Furthermore, we propose \textit{subspace camouflage}, a defense strategy that obfuscates the update fingerprint with semantic decoys. This approach effectively mitigates reconstruction risks without compromising editing utility. Our code is available at https://github.com/reanatom/EditingAtk.git.

</details>


### [2] [Anonymization-Enhanced Privacy Protection for Mobile GUI Agents: Available but Invisible](https://arxiv.org/abs/2602.10139)
*Lepeng Zhao,Zhenhua Zou,Shuo Li,Zhuotao Liu*

Main category: cs.CR

TL;DR: 提出基于匿名化的移动GUI代理隐私保护框架，通过将敏感信息替换为类型保留的占位符，实现"可用但不可见"的敏感数据访问，在AndroidLab和PrivScreen基准测试中取得最佳隐私-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 移动GUI代理通过捕获和处理整个屏幕内容来执行智能手机任务，这带来了严重的隐私风险，会暴露电话号码、地址、消息和财务信息等敏感个人数据。现有防御方法要么减少UI暴露，要么只混淆任务无关内容，或者依赖用户授权，但都无法在保护任务关键敏感信息的同时保持代理的无缝可用性。

Method: 提出匿名化隐私保护框架，采用"可用但不可见"原则：敏感信息对任务执行保持可用，但从不直接暴露给云端代理。系统使用PII感知识别模型检测敏感UI内容，并用确定性的类型保留占位符（如PHONE_NUMBER#a1b2c）替换，保留语义类别同时移除识别细节。采用分层架构，包括PII检测器、UI转换器、安全交互代理和隐私网关，确保用户指令、XML层次结构和截图的匿名化一致性，在匿名化接口上调解所有代理操作，并在需要对原始值进行推理时支持范围狭窄的本地计算。

Result: 在AndroidLab和PrivScreen基准测试上的广泛实验表明，该框架显著减少了多个模型的隐私泄露，同时仅带来适度的效用下降，在现有方法中实现了最佳观察到的隐私-效用权衡。

Conclusion: 该匿名化隐私保护框架成功解决了移动GUI代理的隐私风险问题，通过"可用但不可见"的原则和分层架构设计，在保护敏感信息的同时保持了代理的可用性，为移动自动化任务提供了有效的隐私保护解决方案。

Abstract: Mobile Graphical User Interface (GUI) agents have demonstrated strong capabilities in automating complex smartphone tasks by leveraging multimodal large language models (MLLMs) and system-level control interfaces. However, this paradigm introduces significant privacy risks, as agents typically capture and process entire screen contents, thereby exposing sensitive personal data such as phone numbers, addresses, messages, and financial information. Existing defenses either reduce UI exposure, obfuscate only task-irrelevant content, or rely on user authorization, but none can protect task-critical sensitive information while preserving seamless agent usability.
  We propose an anonymization-based privacy protection framework that enforces the principle of available-but-invisible access to sensitive data: sensitive information remains usable for task execution but is never directly visible to the cloud-based agent. Our system detects sensitive UI content using a PII-aware recognition model and replaces it with deterministic, type-preserving placeholders (e.g., PHONE_NUMBER#a1b2c) that retain semantic categories while removing identifying details. A layered architecture comprising a PII Detector, UI Transformer, Secure Interaction Proxy, and Privacy Gatekeeper ensures consistent anonymization across user instructions, XML hierarchies, and screenshots, mediates all agent actions over anonymized interfaces, and supports narrowly scoped local computations when reasoning over raw values is necessary.
  Extensive experiments on the AndroidLab and PrivScreen benchmarks show that our framework substantially reduces privacy leakage across multiple models while incurring only modest utility degradation, achieving the best observed privacy-utility trade-off among existing methods.

</details>


### [3] [Privacy by Voice: Modeling Youth Privacy-Protective Behavior in Smart Voice Assistants](https://arxiv.org/abs/2602.10142)
*Molly Campbell,Ajay Kumar Shrestha*

Main category: cs.CR

TL;DR: 研究探讨加拿大青少年（16-24岁）如何与智能语音助手协商隐私，发现隐私自我效能是保护行为的最强预测因素，而算法透明度与信任通过自我效能完全中介影响保护行为。


<details>
  <summary>Details</summary>
Motivation: 智能语音助手已深度融入青少年生活，但青少年隐私保护行为的驱动机制仍不清楚。需要了解青少年如何与SVAs协商隐私，以填补这一研究空白。

Method: 基于五个关键构念（感知隐私风险、感知收益、算法透明度与信任、隐私自我效能、隐私保护行为）开发结构模型，对N=469名加拿大青少年进行横断面调查，使用偏最小二乘结构方程模型分析。

Result: 隐私自我效能是隐私保护行为的最强预测因素；算法透明度与信任对保护行为的影响完全由自我效能中介；感知收益直接阻碍保护行为，但通过轻微提升自我效能间接促进保护；政策过载和隐藏控制会削弱必要的自我效能。

Conclusion: 研究验证并扩展了先前定性工作，量化了从感知到行动的路径，提出了设计原则：在保持智能语音助手实用性的同时，通过增强自我效能来赋能青少年数字公民。

Abstract: Smart Voice Assistants (SVAs) are deeply embedded in the lives of youth, yet the mechanisms driving the privacy-protective behaviors among young users remain poorly understood. This study investigates how Canadian youth (aged 16-24) negotiate privacy with SVAs by developing and testing a structural model grounded in five key constructs: perceived privacy risks (PPR), perceived benefits (PPBf), algorithmic transparency and trust (ATT), privacy self-efficacy (PSE), and privacy-protective behaviors (PPB). A cross-sectional survey of N=469 youth was analyzed using partial least squares structural equation modeling. Results reveal that PSE is the strongest predictor of PPB, while the effect of ATT on PPB is fully mediated by PSE. This identifies a critical efficacy gap, where youth's confidence must first be built up for them to act. The model confirms that PPBf directly discourages protective action, yet also indirectly fosters it by slightly boosting self-efficacy. These findings empirically validate and extend earlier qualitative work, quantifying how policy overload and hidden controls erode the self-efficacy necessary for protective action. This study contributes an evidence-based pathway from perception to action and translates it into design imperatives that empower young digital citizens without sacrificing the utility of SVAs.

</details>


### [4] [Red-teaming the Multimodal Reasoning: Jailbreaking Vision-Language Models via Cross-modal Entanglement Attacks](https://arxiv.org/abs/2602.10148)
*Yu Yan,Sheng Sun,Shengjia Cheng,Teli Liu,Mingfeng Li,Min Liu*

Main category: cs.CR

TL;DR: CrossTALK是一种针对视觉语言模型的可扩展越狱攻击方法，通过跨模态信息纠缠来绕过安全对齐机制


<details>
  <summary>Details</summary>
Motivation: 现有黑盒越狱攻击使用简单固定的图像-文本组合，缺乏攻击复杂性可扩展性，无法应对VLM不断发展的推理能力

Method: 1) 知识可扩展重构：将有害任务扩展为多跳链式指令；2) 跨模态线索纠缠：将可视化实体迁移到图像中建立多模态推理链接；3) 跨模态场景嵌套：使用多模态上下文指令引导VLM生成详细有害输出

Result: 实验显示CrossTALK实现了最先进的攻击成功率

Conclusion: 提出的跨模态纠缠攻击方法能够超越VLM训练和泛化的安全对齐模式，为红队测试提供有效的可扩展攻击框架

Abstract: Vision-Language Models (VLMs) with multimodal reasoning capabilities are high-value attack targets, given their potential for handling complex multimodal harmful tasks. Mainstream black-box jailbreak attacks on VLMs work by distributing malicious clues across modalities to disperse model attention and bypass safety alignment mechanisms. However, these adversarial attacks rely on simple and fixed image-text combinations that lack attack complexity scalability, limiting their effectiveness for red-teaming VLMs' continuously evolving reasoning capabilities. We propose \textbf{CrossTALK} (\textbf{\underline{Cross}}-modal en\textbf{\underline{TA}}ng\textbf{\underline{L}}ement attac\textbf{\underline{K}}), which is a scalable approach that extends and entangles information clues across modalities to exceed VLMs' trained and generalized safety alignment patterns for jailbreak. Specifically, {knowledge-scalable reframing} extends harmful tasks into multi-hop chain instructions, {cross-modal clue entangling} migrates visualizable entities into images to build multimodal reasoning links, and {cross-modal scenario nesting} uses multimodal contextual instructions to steer VLMs toward detailed harmful outputs. Experiments show our COMET achieves state-of-the-art attack success rate.

</details>


### [5] [Exploring Semantic Labeling Strategies for Third-Party Cybersecurity Risk Assessment Questionnaires](https://arxiv.org/abs/2602.10149)
*Ali Nour Eldin,Mohamed Sellami,Walid Gaaloul*

Main category: cs.CR

TL;DR: 本文探索使用语义标签组织TPRA网络安全问题，比较直接LLM标注与半监督语义标注管道，发现语义标签能提升检索对齐性，SSSL方法能显著减少LLM使用和成本。


<details>
  <summary>Details</summary>
Motivation: 第三方风险评估(TPRA)是核心网络安全实践，但现有检索方法依赖关键词或表层相似性，无法捕捉隐式评估范围和控制语义，需要更有效的组织和检索策略。

Method: 比较两种方法：1) 直接使用LLM进行问题级标注；2) 混合半监督语义标注(SSSL)管道：在嵌入空间聚类问题，用LLM标注小规模代表性子集，通过k-最近邻将标签传播到剩余问题。同时比较基于直接问题相似性和标签空间的下游检索。

Result: 语义标签在标签具有区分性和一致性时能改善检索对齐性。SSSL方法能够从小规模标注子集泛化到大型存储库，同时显著减少LLM使用和成本。

Conclusion: 语义标签方法能有效提升TPRA问题检索质量，SSSL管道提供了一种成本效益高的标注策略，适用于大规模网络安全问题库的组织和检索。

Abstract: Third-Party Risk Assessment (TPRA) is a core cybersecurity practice for evaluating suppliers against standards such as ISO/IEC 27001 and NIST. TPRA questionnaires are typically drawn from large repositories of security and compliance questions, yet tailoring assessments to organizational needs remains a largely manual process. Existing retrieval approaches rely on keyword or surface-level similarity, which often fails to capture implicit assessment scope and control semantics.
  This paper explores strategies for organizing and retrieving TPRA cybersecurity questions using semantic labels that describe both control domains and assessment scope. We compare direct question-level labeling with a Large Language Model (LLM) against a hybrid semi-supervised semantic labeling (SSSL) pipeline that clusters questions in embedding space, labels a small representative subset using an LLM, and propagates labels to remaining questions using k-Nearest Neighbors; we also compare downstream retrieval based on direct question similarity versus retrieval in the label space. We find that semantic labels can improve retrieval alignment when labels are discriminative and consistent, and that SSSL can generalize labels from a small labeled subset to large repositories while substantially reducing LLM usage and cost.

</details>


### [6] [Basic Legibility Protocols Improve Trusted Monitoring](https://arxiv.org/abs/2602.10153)
*Ashwin Sreevatsa,Sebastian Prasanna,Cody Rushing*

Main category: cs.CR

TL;DR: AI控制研究提出可读性协议，通过让不受信任的AI模型用注释详细记录代码，帮助较弱但可信的监控模型更好地评估其行为，从而提高安全性而不牺牲任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统可信监控方法中，较弱但可信的模型监控较强但不可信的模型时，当不可信模型的行为超出监控者理解范围就会失效。需要开发能鼓励不可信模型采取更易评估行动的控制协议。

Method: 引入可读性协议，在APPS编码环境中进行控制评估，让不可信模型通过代码注释详细记录其代码逻辑。与先前删除注释防止欺骗的方法不同，本研究允许并鼓励注释。

Result: 1) 注释协议相比删除注释的基线方法，在保持任务性能的同时提高了安全性；2) 注释对诚实代码益处更大，因为诚实代码有自然解释能消除监控疑虑，而恶意代码通常缺乏合理解释；3) 随着监控模型能力增强，注释带来的安全收益也增加，因为更强的监控能更好区分真实解释和表面合理的解释。

Conclusion: 可读性协议是一种有效的AI控制方法，通过鼓励不可信模型提供可解释的文档，使较弱但可信的监控模型能更可靠地检测恶意行为，且随着监控能力增强效果更好。

Abstract: The AI Control research agenda aims to develop control protocols: safety techniques that prevent untrusted AI systems from taking harmful actions during deployment. Because human oversight is expensive, one approach is trusted monitoring, where weaker, trusted models oversee stronger, untrusted models$\unicode{x2013}$but this often fails when the untrusted model's actions exceed the monitor's comprehension. We introduce legibility protocols, which encourage the untrusted model to take actions that are easier for a monitor to evaluate.
  We perform control evaluations in the APPS coding setting, where an adversarial agent attempts to write backdoored code without detection. We study legibility protocols that allow the untrusted model to thoroughly document its code with comments$\unicode{x2013}$in contrast to prior work, which removed comments to prevent deceptive ones. We find that: (i) commenting protocols improve safety without sacrificing task performance relative to comment-removal baselines; (ii) commenting disproportionately benefits honest code, which typically has a natural explanation that resolves monitor suspicion, whereas backdoored code frequently lacks an easy justification; (iii) gains from commenting increase with monitor strength, as stronger monitors better distinguish genuine justifications from only superficially plausible ones.

</details>


### [7] [PRISM-XR: Empowering Privacy-Aware XR Collaboration with Multimodal Large Language Models](https://arxiv.org/abs/2602.10154)
*Jiangong Chen,Mingyu Zhu,Bin Li*

Main category: cs.CR

TL;DR: PRISM-XR是一个保护隐私的多模态大语言模型集成框架，用于扩展现实环境中的多用户协作，通过边缘服务器预处理敏感数据，实现高效、准确且隐私保护的内容同步。


<details>
  <summary>Details</summary>
Motivation: 当前XR环境中使用MLLM存在两大问题：1) XR头显捕获的视觉数据包含真实世界背景，可能泄露敏感信息（如信用卡、人脸身份），上传到云端MLLM存在严重隐私风险；2) 现有XR API的协同机制依赖耗时且侵犯隐私的环境扫描，难以适应MLLM集成的动态环境。

Method: 提出PRISM-XR框架：1) 在边缘服务器上进行智能帧预处理，过滤敏感数据并移除无关上下文后再与云端生成式AI模型通信；2) 引入轻量级注册过程和完全可定制的内容共享机制，实现高效、准确且隐私保护的内容同步。

Result: 数值评估显示：平台满足用户请求的准确率接近90%，注册时间小于0.27秒，空间不一致性小于3.5厘米。IRB批准的用户研究（28名参与者）表明：系统能在超过90%的场景中自动过滤高敏感对象，同时保持强大的整体可用性。

Conclusion: PRISM-XR成功解决了MLLM集成XR环境中的隐私和协同挑战，通过边缘预处理和轻量级同步机制，在保护用户隐私的同时实现了高效的多用户协作，为隐私感知的XR-MLLM集成提供了可行方案。

Abstract: Multimodal Large Language Models (MLLMs) enhance collaboration in Extended Reality (XR) environments by enabling flexible object and animation creation through the combination of natural language and visual inputs. However, visual data captured by XR headsets includes real-world backgrounds that may contain irrelevant or sensitive user information, such as credit cards left on the table or facial identities of other users. Uploading those frames to cloud-based MLLMs poses serious privacy risks, particularly when such data is processed without explicit user consent. Additionally, existing colocation and synchronization mechanisms in commercial XR APIs rely on time-consuming, privacy-invasive environment scanning and struggle to adapt to the highly dynamic nature of MLLM-integrated XR environments. In this paper, we propose PRISM-XR, a novel framework that facilitates multi-user collaboration in XR by providing privacy-aware MLLM integration. PRISM-XR employs intelligent frame preprocessing on the edge server to filter sensitive data and remove irrelevant context before communicating with cloud generative AI models. Additionally, we introduce a lightweight registration process and a fully customizable content-sharing mechanism to enable efficient, accurate, and privacy-preserving content synchronization among users. Our numerical evaluation results indicate that the proposed platform achieves nearly 90% accuracy in fulfilling user requests and less than 0.27 seconds registration time while maintaining spatial inconsistencies of less than 3.5 cm. Furthermore, we conducted an IRB-approved user study with 28 participants, demonstrating that our system could automatically filter highly sensitive objects in over 90% of scenarios while maintaining strong overall usability.

</details>


### [8] [MalMoE: Mixture-of-Experts Enhanced Encrypted Malicious Traffic Detection Under Graph Drift](https://arxiv.org/abs/2602.10157)
*Yunpeng Tan,Qingyang Li,Mingxin Yang,Yannan Hu,Lei Zhang,Xinggong Zhang*

Main category: cs.CR

TL;DR: MalMoE：基于专家混合的图辅助加密流量检测系统，通过选择最佳专家模型应对图漂移问题，实现精确实时检测


<details>
  <summary>Details</summary>
Motivation: 加密流量虽然保障了传输安全，但使得恶意流量检测面临挑战，因为无法查看数据包有效载荷。现有的图基方法面临图漂移问题，即图的流统计特征或拓扑信息随时间变化，影响检测准确性。

Method: 提出MalMoE系统，采用专家混合（MoE）架构：1）设计类似1跳GNN的专家模型，处理不同特征的图漂移；2）重新设计门控模型，根据实际漂移情况选择专家；3）采用两阶段训练策略和数据增强，指导门控进行路由选择。

Result: 在开源、合成和真实数据集上的实验表明，MalMoE能够实现精确且实时的加密流量检测，有效应对图漂移问题。

Conclusion: MalMoE通过专家混合架构有效解决了图漂移问题，为加密流量环境下的恶意流量检测提供了准确可靠的解决方案。

Abstract: Encryption has been commonly used in network traffic to secure transmission, but it also brings challenges for malicious traffic detection, due to the invisibility of the packet payload. Graph-based methods are emerging as promising solutions by leveraging multi-host interactions to promote detection accuracy. But most of them face a critical problem: Graph Drift, where the flow statistics or topological information of a graph change over time. To overcome these drawbacks, we propose a graph-assisted encrypted traffic detection system, MalMoE, which applies Mixture of Experts (MoE) to select the best expert model for drift-aware classification. Particularly, we design 1-hop-GNN-like expert models that handle different graph drifts by analyzing graphs with different features. Then, the redesigned gate model conducts expert selection according to the actual drift. MalMoE is trained with a stable two-stage training strategy with data augmentation, which effectively guides the gate on how to perform routing. Experiments on open-source, synthetic, and real-world datasets show that MalMoE can perform precise and real-time detection.

</details>


### [9] [Omni-Safety under Cross-Modality Conflict: Vulnerabilities, Dynamics Mechanisms and Efficient Alignment](https://arxiv.org/abs/2602.10161)
*Kun Wang,Zherui Li,Zhenhong Zhou,Yitong Zhang,Yan Mi,Kun Yang,Yiming Zhang,Junhao Dong,Zhongxiang Sun,Qiankun Li,Yang Liu*

Main category: cs.CR

TL;DR: 该论文研究了全模态大语言模型的安全漏洞，提出了模态-语义解耦原则，构建了AdvBench-Omni数据集，发现了拒绝向量幅度收缩导致的中层溶解现象，并提出了基于奇异值分解提取黄金拒绝向量和OmniSteer轻量适配器方法，显著提升了有害输入的拒绝成功率。


<details>
  <summary>Details</summary>
Motivation: 全模态大语言模型虽然扩展了多模态能力，但也引入了跨模态安全风险。目前缺乏对全模态交互中漏洞的系统性理解，需要填补这一研究空白。

Method: 1. 建立模态-语义解耦原则；2. 构建AdvBench-Omni数据集；3. 通过机制分析发现中层溶解现象和模态不变纯拒绝方向；4. 使用奇异值分解提取黄金拒绝向量；5. 提出OmniSteer轻量适配器方法，自适应调节干预强度。

Result: 方法将有害输入的拒绝成功率从69.9%提升到91.2%，同时有效保留了所有模态的通用能力。

Conclusion: 该研究系统揭示了全模态大语言模型的安全漏洞机制，提出的OmniSteer方法能有效提升模型安全性而不损害多模态能力，为全模态模型安全研究提供了新视角。

Abstract: Omni-modal Large Language Models (OLLMs) greatly expand LLMs' multimodal capabilities but also introduce cross-modal safety risks. However, a systematic understanding of vulnerabilities in omni-modal interactions remains lacking. To bridge this gap, we establish a modality-semantics decoupling principle and construct the AdvBench-Omni dataset, which reveals a significant vulnerability in OLLMs. Mechanistic analysis uncovers a Mid-layer Dissolution phenomenon driven by refusal vector magnitude shrinkage, alongside the existence of a modal-invariant pure refusal direction. Inspired by these insights, we extract a golden refusal vector using Singular Value Decomposition and propose OmniSteer, which utilizes lightweight adapters to modulate intervention intensity adaptively. Extensive experiments show that our method not only increases the Refusal Success Rate against harmful inputs from 69.9% to 91.2%, but also effectively preserves the general capabilities across all modalities. Our code is available at: https://github.com/zhrli324/omni-safety-research.

</details>


### [10] [Limits of Residual-Based Detection for Physically Consistent False Data Injection](https://arxiv.org/abs/2602.10162)
*Chenhan Xiao,Yang Weng*

Main category: cs.CR

TL;DR: 论文揭示了交流电力系统状态估计中基于残差的虚假数据注入攻击检测存在根本性局限：当攻击产生的篡改测量值保持在由交流潮流关系和测量冗余诱导的测量流形上时，残差检测器无法将其与正常数据区分。


<details>
  <summary>Details</summary>
Motivation: 当前交流电力系统状态估计主要依赖基于残差的拓扑感知检测方法，其假设恶意测量可以通过物理不一致性反映在异常残差行为中被识别。但这一假设并不总是成立，需要揭示其根本局限性。

Method: 提出数据驱动的构造机制，结合交流潮流的通用功能结构生成物理一致、流形约束的扰动，为残差检测器可被绕过提供具体证据。在多个交流测试系统上进行数值研究。

Result: 数值研究表明，当虚假数据注入攻击产生的篡改测量保持在测量流形上时，基于残差的检测器无法有效检测。这种可检测性限制是测量流形本身的属性，不依赖于攻击者对物理系统模型的详细了解。

Conclusion: 研究结果凸显了交流状态估计中基于残差检测的根本限制，表明需要超越测量一致性测试的补充防御措施。

Abstract: False data injection attacks (FDIAs) pose a persistent challenge to AC power system state estimation. In current practice, detection relies primarily on topology-aware residual-based tests that assume malicious measurements can be distinguished from normal operation through physical inconsistency reflected in abnormal residual behavior. This paper shows that this assumption does not always hold: when FDIA scenarios produce manipulated measurements that remain on the measurement manifold induced by AC power flow relations and measurement redundancy, residual-based detectors may fail to distinguish them from nominal data. The resulting detectability limitation is a property of the measurement manifold itself and does not depend on the attacker's detailed knowledge of the physical system model. To make this limitation observable in practice, we present a data-driven constructive mechanism that incorporates the generic functional structure of AC power flow to generate physically consistent, manifold-constrained perturbations, providing a concrete witness of how residual-based detectors can be bypassed. Numerical studies on multiple AC test systems characterize the conditions under which detection becomes challenging and illustrate its failure modes. The results highlight fundamental limits of residual-based detection in AC state estimation and motivate the need for complementary defenses beyond measurement consistency tests.

</details>


### [11] [MerkleSpeech: Public-Key Verifiable, Chunk-Localised Speech Provenance via Perceptual Fingerprints and Merkle Commitments](https://arxiv.org/abs/2602.10166)
*Tatsunori Ono*

Main category: cs.CR

TL;DR: MerkleSpeech：一种用于语音来源验证的双层系统，结合鲁棒水印和严格密码学完整性验证，支持片段级溯源和拼接感知的时间线分析。


<details>
  <summary>Details</summary>
Motivation: 现有语音溯源方法存在局限：水印系统缺乏第三方可验证的密码学证明，而C2PA等标准在重新编码或常规处理时会失效。需要一种能在常见分发变换中存活，并提供密码学完整性验证的解决方案。

Method: 系统采用双层验证：1）鲁棒水印归属层（WM-only），在常见变换中存活；2）严格密码学完整性层（MSv1），验证片段指纹在Merkle树中的包含关系。通过计算短语音片段的感知指纹，构建Merkle树并签名根节点，嵌入包含内容标识符和元数据的水印载荷，支持从存储库检索Merkle包含证明。

Result: 系统实现了拼接感知的时间线分析，能指示哪些区域通过各层验证以及失败原因。实验针对重采样、带通滤波和加性噪声等场景，实现了极低的误报率，特别考虑了神经编解码器对后处理水印的压力。

Conclusion: MerkleSpeech提供了公开密钥可验证、片段级定位的语音溯源解决方案，结合了水印的鲁棒性和密码学的严格性，能有效应对现实工作流中的拼接、引用、修剪和平台级变换。

Abstract: Speech provenance goes beyond detecting whether a watermark is present. Real workflows involve splicing, quoting, trimming, and platform-level transforms that may preserve some regions while altering others. Neural watermarking systems have made strides in robustness and localised detection, but most deployments produce outputs with no third-party verifiable cryptographic proof tying a time segment to an issuer-signed original. Provenance standards like C2PA adopt signed manifests and Merkle-based fragment validation, yet their bindings target encoded assets and break under re-encoding or routine processing.
  We propose MerkleSpeech, a system for public-key verifiable, chunk-localised speech provenance offering two tiers of assurance. The first, a robust watermark attribution layer (WM-only), survives common distribution transforms and answers "was this chunk issued by a known party?". The second, a strict cryptographic integrity layer (MSv1), verifies Merkle inclusion of the chunk's fingerprint under an issuer signature. The system computes perceptual fingerprints over short speech chunks, commits them in a Merkle tree whose root is signed with an issuer key, and embeds a compact in-band watermark payload carrying a random content identifier and chunk metadata sufficient to retrieve Merkle inclusion proofs from a repository. Once the payload is extracted, all subsequent verification steps (signature check, fingerprint recomputation, Merkle inclusion) use only public information. The result is a splice-aware timeline indicating which regions pass each tier and why any given region fails. We describe the protocol, provide pseudocode, and present experiments targeting very low false positive rates under resampling, bandpass filtering, and additive noise, informed by recent audits identifying neural codecs as a major stressor for post-hoc audio watermarks.

</details>


### [12] [Non-Fungible Blockchain Tokens for Traceable Online-Quality Assurance of Milled Workpieces](https://arxiv.org/abs/2602.10169)
*Nicolai Maisch,Shengjian Chen,Alexander Robertus,Samed Ajdinović,Armin Lechler,Alexander Verl,Oliver Riedel*

Main category: cs.CR

TL;DR: 利用NFT和区块链技术安全存储和传输铣削工件质量数据，实现自动化可追溯性


<details>
  <summary>Details</summary>
Motivation: 解决在线质量保证过程中质量相关数据的安全存储和传输问题，减少耗时且昂贵的重复手动质量检查

Method: 使用NFT在以太坊区块链上安全存储AAS格式的质量数据，通过智能合约铸造NFT，将元数据存储在IPFS中，支持灵活添加新处理步骤的数据

Result: 实现了质量数据的区块链安全存储和传输系统，支持整个价值链的自动化可追溯性

Conclusion: 该概念为铣削工件质量数据提供了安全、可互操作的存储和传输解决方案，能够显著减少重复手动质量检查的需求

Abstract: This work presents a concept and implementation for the secure storage and transfer of quality-relevant data of milled workpieces from online-quality assurance processes enabled by real-time simulation models. It utilises Non-Fungible Tokens (NFT) to securely and interoperably store quality data in the form of an Asset Administration Shell (AAS) on a public Ethereum blockchain. Minted by a custom smart contract, the NFTs reference the metadata saved in the Interplanetary File System (IPFS), allowing new data from additional processing steps to be added in a flexible yet secure manner. The concept enables automated traceability throughout the value chain, minimising the need for time-consuming and costly repetitive manual quality checks.

</details>


### [13] [Breaking 5G on The Lower Layer](https://arxiv.org/abs/2602.10250)
*Subangkar Karmaker Shanto,Imtiaz Karim,Elisa Bertino*

Main category: cs.CR

TL;DR: 论文展示了5G系统中两个物理层/MAC层攻击：SIB1欺骗攻击通过修改广播参数强制UE刷新系统信息增加功耗；TA操纵攻击在随机接入过程中注入恶意定时提前量导致上行失步和DoS


<details>
  <summary>Details</summary>
Motivation: 随着3GPP系统在上层加强安全，物理层和MAC层相对缺乏研究，而5G新版本增加了下层控制消息和过程，为实际攻击创造了新机会

Method: 在受控实验室测试环境中，使用商用智能手机和开源5G网络软件，实现两种攻击：1）SIB1欺骗攻击操纵未受保护的广播字段；2）在随机接入过程中注入攻击者选择的TA偏移量

Result: 实验证明：SIB1欺骗能强制UE刷新系统信息，增加电池消耗；TA偏移超过小容差即可可靠触发无线链路失败，使设备陷入重复重建尝试，造成拒绝服务

Conclusion: 紧凑的下层控制消息对可用性和功耗有显著影响，需要在初始接入和广播过程中部署防御措施

Abstract: As 3GPP systems have strengthened security at the upper layers of the cellular stack, plaintext PHY and MAC layers have remained relatively understudied, though interest in them is growing. In this work, we explore lower-layer exploitation in modern 5G, where recent releases have increased the number of lower-layer control messages and procedures, creating new opportunities for practical attacks. We present two practical attacks and evaluate them in a controlled lab testbed. First, we reproduce a SIB1 spoofing attack to study manipulations of unprotected broadcast fields. By repeatedly changing a key parameter, the UE is forced to refresh and reacquire system information, keeping the radio interface active longer than necessary and increasing battery consumption. Second, we demonstrate a new Timing Advance (TA) manipulation attack during the random access procedure. By injecting an attacker-chosen TA offset in the random access response, the victim applies incorrect uplink timing, which leads to uplink desynchronization, radio link failures, and repeated reconnection loops that effectively cause denial of service. Our experiments use commercial smartphones and open-source 5G network software. Experimental results in our testbed demonstrate that TA offsets exceeding a small tolerance reliably trigger radio link failures in our testbed and can keep devices stuck in repeated re-establishment attempts as long as the rogue base station remains present. Overall, our findings highlight that compact lower-layer control messages can have a significant impact on availability and power, and they motivate placing defenses for initial access and broadcast procedures.

</details>


### [14] [5Gone: Uplink Overshadowing Attacks in 5G-SA](https://arxiv.org/abs/2602.10272)
*Simon Erni,Martin Kotuliak,Marc Roeschlin,Richard Baker,Srdjan Capkun*

Main category: cs.CR

TL;DR: 5Gone是一种针对5G独立组网的软件定义无线电上行链路遮蔽攻击方法，利用3GPP标准缺陷进行隐蔽的拒绝服务、隐私和降级攻击，无需专用硬件加速即可实时遮蔽商用100MHz基站。


<details>
  <summary>Details</summary>
Motivation: 传统5G SA攻击通常使用假基站，需要高功率输出且易被检测。需要开发更隐蔽、可扩展的攻击方法，利用标准缺陷进行精准攻击。

Method: 采用软件定义无线电上行链路遮蔽攻击，攻击者与受害者UE在相同时间频率传输但功率略高。基于商用x86计算机实现，无需专用硬件加速，端到端延迟低于500μs。

Result: 成功遮蔽商用100MHz基站，在实验室和真实公共gNodeB上对7款手机模型和3家芯片厂商进行了端到端攻击评估，证明攻击具有高度可扩展性。

Conclusion: 5Gone展示了5G SA标准存在可利用缺陷，能够进行隐蔽且可扩展的攻击，对5G安全提出了新的挑战。

Abstract: 5G presents numerous advantages compared to previous generations: improved throughput, lower latency, and improved privacy protection for subscribers. Attacks against 5G standalone (SA) commonly use fake base stations (FBS), which need to operate at a very high output power level to lure victim phones to connect to them and are thus highly detectable. In this paper, we introduce 5Gone, a powerful software-defined radio (SDR)-based uplink overshadowing attack method against 5G-SA. 5Gone exploits deficiencies in the 3GPP standard to perform surgical, covert denial-of-service, privacy, and downgrade attacks. Uplink overshadowing means that an attacker is transmitting at exactly the same time and frequency as the victim UE, but with a slightly higher output power. 5Gone runs on a COTS x86 computer without any need for dedicated hardware acceleration and can overshadow commercial 100 MHz cells with an E2E latency of less than 500$μ$s, which up to now has not been possible with any software-based UE implementation. We demonstrate that 5Gone is highly scalable, even when many UEs are connecting in parallel, and finally evaluate the attacks end-to-end against 7 phone models and three different chipset vendors both in our lab and in the real-world on public gNodeBs.

</details>


### [15] [The Role of Learning in Attacking Intrusion Detection Systems](https://arxiv.org/abs/2602.10299)
*Kyle Domico,Jean-Charles Noirot Ferrand,Patrick McDaniel*

Main category: cs.CR

TL;DR: 提出一种基于强化学习的轻量级对抗代理，可快速规避基于机器学习的网络入侵检测系统，无需在线优化，适用于资源受限的受感染设备。


<details>
  <summary>Details</summary>
Motivation: 现有网络攻击方法依赖复杂优化，计算开销大，难以在实际环境中部署。需要开发轻量级、高效的对抗攻击方法，使攻击者能在受感染设备上实时规避ML-based NIDS。

Method: 采用强化学习训练轻量级对抗代理：1) 离线训练阶段，使用侦察收集的网络流量数据，让代理学习如何通过扰动恶意流量来规避替代ML模型；2) 部署阶段，将训练好的代理部署在攻击者控制的受感染设备上，使用学习到的攻击策略规避实际NIDS。

Result: 攻击成功率高达48.9%，攻击生成时间仅需5.72ms，内存占用仅0.52MB。在不同NIDS和白盒、灰盒、黑盒威胁模型下均表现有效。

Conclusion: 轻量级学习驱动的对抗代理攻击高效、快速且资源消耗低，未来基于此类代理的僵尸网络可在各种受感染设备环境中广泛部署，对网络安全构成严重威胁。

Abstract: Recent work on network attacks have demonstrated that ML-based network intrusion detection systems (NIDS) can be evaded with adversarial perturbations. However, these attacks rely on complex optimizations that have large computational overheads, making them impractical in many real-world settings. In this paper, we introduce a lightweight adversarial agent that implements strategies (policies) trained via reinforcement learning (RL) that learn to evade ML-based NIDS without requiring online optimization. This attack proceeds by (1) offline training, where the agent learns to evade a surrogate ML model by perturbing malicious flows using network traffic data assumed to be collected via reconnaissance, then (2) deployment, where the trained agent is used in a compromised device controlled by an attacker to evade ML-based NIDS using learned attack strategies. We evaluate our approach across diverse NIDS and several white-, gray-, and black-box threat models. We demonstrate that attacks using these lightweight agents can be highly effective (reaching up to 48.9% attack success rate), extremely fast (requiring as little as 5.72ms to craft an attack), and require negligible resources (e.g., 0.52MB of memory). Through this work, we demonstrate that future botnets driven by lightweight learning-based agents can be highly effective and widely deployable in diverse environments of compromised devices.

</details>


### [16] [SecCodePRM: A Process Reward Model for Code Security](https://arxiv.org/abs/2602.10418)
*Weichen Yu,Ravi Mangal,Yinyi Luo,Kai Hu,Jingxuan He,Corina S. Pasareanu,Matt Fredrikson*

Main category: cs.CR

TL;DR: SecCodePRM是一个面向安全的流程奖励模型，为代码轨迹提供上下文感知的步骤级安全评分，支持完整代码、部分代码漏洞检测和安全代码生成，在实时交互编码中提供密集反馈。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞检测方法（静态分析器或基于LLM/GNN的检测器）通常需要完整代码上下文，提供稀疏的完成时反馈，且随着代码长度增加性能下降，不适合交互式编码和流式生成中的实时前缀级评估。

Method: 提出SecCodePRM安全导向的流程奖励模型，从静态分析器和专家标注中获取步骤级监督标签，使模型能更精确关注与过程间漏洞相关的细粒度区域。采用风险敏感聚合强调高风险步骤，支持推理时扩展通过排名候选延续并偏好更高累积奖励。

Result: SecCodePRM在三个应用场景（完整代码漏洞检测、部分代码漏洞检测、安全代码生成）中均优于先前方法，同时保持代码功能正确性，表明在安全提升的同时没有安全-效用权衡。

Conclusion: SecCodePRM通过步骤级安全评分提供密集实时反馈，适用于长时程生成，在交互式编码环境中实现了更好的安全评估，同时保持了代码功能完整性。

Abstract: Large Language Models are rapidly becoming core components of modern software development workflows, yet ensuring code security remains challenging. Existing vulnerability detection pipelines either rely on static analyzers or use LLM/GNN-based detectors trained with coarse program-level supervision. Both families often require complete context, provide sparse end-of-completion feedback, and can degrade as code length grows, making them ill-suited for real-time, prefix-level assessment during interactive coding and streaming generation. We propose SecCodePRM, a security-oriented process reward model that assigns a context-aware, step-level security score along a code trajectory. To train the model, we derive step-level supervision labels from static analyzers and expert annotations, allowing the model to attend more precisely to fine-grained regions associated with inter-procedural vulnerabilities. SecCodePRM has three applications: full-code vulnerability detection (VD), partial-code VD, and secure code generation (CG). For VD, SecCodePRM uses risk-sensitive aggregation that emphasizes high-risk steps; for CG, SecCodePRM supports inference-time scaling by ranking candidate continuations and favoring higher cumulative reward. This design yields dense, real-time feedback that scales to long-horizon generation. Empirically, SecCodePRM outperforms prior approaches in all three settings, while preserving code functional correctness, suggesting improved security without a safety-utility tradeoff.

</details>


### [17] [The Landscape of Prompt Injection Threats in LLM Agents: From Taxonomy to Analysis](https://arxiv.org/abs/2602.10453)
*Peiran Wang,Xinfeng Li,Chong Xiang,Jinghuai Zhang,Ying Li,Lixia Zhang,Xiaofeng Wang,Yuan Tian*

Main category: cs.CR

TL;DR: 该论文系统分析了大型语言模型代理中的提示注入安全漏洞，建立了攻击与防御的分类体系，并揭示了现有评估方法忽略上下文依赖任务的局限性。作者提出了新的基准AgentPI来评估上下文依赖场景下的代理行为，发现现有防御方法无法同时实现高可信度、高效用和低延迟。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型向自主代理发展，提示注入漏洞成为严重安全威胁。现有研究缺乏对上下文依赖任务的系统分析，导致防御方法和评估基准存在局限性，无法适应现实世界中代理需要依赖环境观察进行决策的场景。

Method: 通过系统性文献综述和定量分析，建立提示注入攻击和防御的分类体系。攻击按有效载荷生成策略（启发式vs优化）分类，防御按干预阶段（文本、模型、执行层面）分类。提出AgentPI基准来评估上下文依赖交互设置下的代理行为，并对代表性防御方法进行实证评估。

Result: 分析发现现有防御和基准主要忽略上下文依赖任务。使用AgentPI评估显示，没有单一防御方法能同时实现高可信度、高效用和低延迟。许多防御在现有基准中看似有效，但实际上是通过抑制上下文输入实现的，在需要上下文推理的现实代理设置中无法泛化。

Conclusion: 该研究揭示了提示注入安全领域的关键局限性，提出了结构化分类体系和新的评估基准。研究为未来安全LLM代理的研究和实际部署提供了指导，并指出了需要解决的关键开放研究问题，特别是在上下文依赖推理场景下的安全防护。

Abstract: The evolution of Large Language Models (LLMs) has resulted in a paradigm shift towards autonomous agents, necessitating robust security against Prompt Injection (PI) vulnerabilities where untrusted inputs hijack agent behaviors. This SoK presents a comprehensive overview of the PI landscape, covering attacks, defenses, and their evaluation practices. Through a systematic literature review and quantitative analysis, we establish taxonomies that categorize PI attacks by payload generation strategies (heuristic vs. optimization) and defenses by intervention stages (text, model, and execution levels). Our analysis reveals a key limitation shared by many existing defenses and benchmarks: they largely overlook context-dependent tasks, in which agents are authorized to rely on runtime environmental observations to determine actions. To address this gap, we introduce AgentPI, a new benchmark designed to systematically evaluate agent behavior under context-dependent interaction settings. Using AgentPI, we empirically evaluate representative defenses and show that no single approach can simultaneously achieve high trustworthiness, high utility, and low latency. Moreover, we show that many defenses appear effective under existing benchmarks by suppressing contextual inputs, yet fail to generalize to realistic agent settings where context-dependent reasoning is essential. This SoK distills key takeaways and open research problems, offering structured guidance for future research and practical deployment of secure LLM agents.

</details>


### [18] [Authenticated Workflows: A Systems Approach to Protecting Agentic AI](https://arxiv.org/abs/2602.10465)
*Mohan Rajagopalan,Vinay Rao*

Main category: cs.CR

TL;DR: 提出首个企业级智能体AI完整信任层——认证工作流，通过保护提示、工具、数据和上下文四个边界，结合密码学消除攻击类别与运行时策略执行，实现确定性安全。


<details>
  <summary>Details</summary>
Motivation: 现有企业AI工作流的防御机制（护栏、语义过滤器）是概率性的，经常被绕过，无法提供可靠的安全保障，需要一种确定性的安全解决方案。

Method: 引入认证工作流，在四个边界（提示、工具、数据、上下文）强制执行意图（符合组织策略）和完整性（密码学认证），使用MAPL AI原生策略语言动态表达约束，通过分层组合和密码学证明实现高效扩展。

Result: 174个测试用例中实现100%召回率和零误报，防护9/10 OWASP Top 10风险，完全缓解两个高影响生产CVEs，通过薄适配器集成9个主流框架无需协议修改。

Conclusion: 认证工作流为企业智能体AI提供了首个完整的信任层，通过密码学证明和策略执行的结合，实现了确定性的安全防护，解决了现有概率性防御的不足。

Abstract: Agentic AI systems automate enterprise workflows but existing defenses--guardrails, semantic filters--are probabilistic and routinely bypassed. We introduce authenticated workflows, the first complete trust layer for enterprise agentic AI. Security reduces to protecting four fundamental boundaries: prompts, tools, data, and context. We enforce intent (operations satisfy organizational policies) and integrity (operations are cryptographically authentic) at every boundary crossing, combining cryptographic elimination of attack classes with runtime policy enforcement. This delivers deterministic security--operations either carry valid cryptographic proof or are rejected. We introduce MAPL, an AI-native policy language that expresses agentic constraints dynamically as agents evolve and invocation context changes, scaling as O(log M + N) policies versus O(M x N) rules through hierarchical composition with cryptographic attestations for workflow dependencies. We prove practicality through a universal security runtime integrating nine leading frameworks (MCP, A2A, OpenAI, Claude, LangChain, CrewAI, AutoGen, LlamaIndex, Haystack) through thin adapters requiring zero protocol modifications. Formal proofs establish completeness and soundness. Empirical validation shows 100% recall with zero false positives across 174 test cases, protection against 9 of 10 OWASP Top 10 risks, and complete mitigation of two high impact production CVEs.

</details>


### [19] [GPU-Fuzz: Finding Memory Errors in Deep Learning Frameworks](https://arxiv.org/abs/2602.10478)
*Zihao Li,Hongyi Lu,Yanan Guo,Zhenkai Zhang,Shuai Wang,Fengwei Zhang*

Main category: cs.CR

TL;DR: GPU-Fuzz是一个针对深度学习框架GPU内存错误的模糊测试工具，通过约束求解生成边界条件测试用例，在三大主流框架中发现13个未知漏洞


<details>
  <summary>Details</summary>
Motivation: GPU内存错误是深度学习框架的关键威胁，可能导致系统崩溃或安全问题，需要高效的方法来定位这些漏洞

Method: 将算子参数建模为形式约束，利用约束求解器生成系统性地探测GPU内核中易错边界条件的测试用例

Result: 在PyTorch、TensorFlow和PaddlePaddle三个主流深度学习框架中发现了13个未知的内存错误漏洞

Conclusion: GPU-Fuzz在发现深度学习框架GPU内存错误方面具有显著效果，证明了其约束求解方法的有效性

Abstract: GPU memory errors are a critical threat to deep learning (DL) frameworks, leading to crashes or even security issues. We introduce GPU-Fuzz, a fuzzer locating these issues efficiently by modeling operator parameters as formal constraints. GPU-Fuzz utilizes a constraint solver to generate test cases that systematically probe error-prone boundary conditions in GPU kernels. Applied to PyTorch, TensorFlow, and PaddlePaddle, we uncovered 13 unknown bugs, demonstrating the effectiveness of GPU-Fuzz in finding memory errors.

</details>


### [20] [Protecting Context and Prompts: Deterministic Security for Non-Deterministic AI](https://arxiv.org/abs/2602.10481)
*Mohan Rajagopalan,Vinay Rao*

Main category: cs.CR

TL;DR: 该论文提出了一种结合密码学验证的提示来源、防篡改上下文和可证明策略推理的LLM安全框架，将LLM安全从被动检测转向主动预防。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型应用容易受到提示注入和上下文操纵攻击，传统安全模型无法有效防护，需要新的安全机制来确保LLM工作流的可信性。

Method: 引入两个新原语：认证提示（提供自包含的溯源验证）和认证上下文（使用防篡改哈希链确保动态输入完整性）。基于这些原语形式化策略代数，提供协议级拜占庭容错，并设计了五层互补防御机制。

Result: 在涵盖6个类别的代表性攻击评估中，实现了100%检测率、零误报和可忽略的开销，首次结合了密码学强制的提示溯源、防篡改上下文和可证明策略推理。

Conclusion: 该研究提出了首个结合密码学验证的LLM安全框架，通过形式化保证将LLM安全范式从反应式检测转变为预防性保证，为组织级LLM部署提供了可靠的安全基础。

Abstract: Large Language Model (LLM) applications are vulnerable to prompt injection and context manipulation attacks that traditional security models cannot prevent. We introduce two novel primitives--authenticated prompts and authenticated context--that provide cryptographically verifiable provenance across LLM workflows. Authenticated prompts enable self-contained lineage verification, while authenticated context uses tamper-evident hash chains to ensure integrity of dynamic inputs. Building on these primitives, we formalize a policy algebra with four proven theorems providing protocol-level Byzantine resistance--even adversarial agents cannot violate organizational policies. Five complementary defenses--from lightweight resource controls to LLM-based semantic validation--deliver layered, preventative security with formal guarantees. Evaluation against representative attacks spanning 6 exhaustive categories achieves 100% detection with zero false positives and nominal overhead. We demonstrate the first approach combining cryptographically enforced prompt lineage, tamper-evident context, and provable policy reasoning--shifting LLM security from reactive detection to preventative guarantees.

</details>


### [21] [Following Dragons: Code Review-Guided Fuzzing](https://arxiv.org/abs/2602.10487)
*Viet Hoang Luu,Amirmohammad Pasdar,Wachiraphan Charoenwet,Toby Murray,Shaanan Cohney,Van-Thuan Pham*

Main category: cs.CR

TL;DR: EyeQ系统利用代码审查中的开发者智能来指导模糊测试，通过提取安全相关信号、定位相关程序区域，并将这些洞察转化为基于注解的模糊测试指导，显著提升了漏洞发现能力。


<details>
  <summary>Details</summary>
Motivation: 现代模糊测试工具虽然能扩展到大型实际软件，但往往无法触及开发者认为最脆弱或安全关键的程序状态。这些状态通常深藏在执行空间中，受到前置条件的限制，或被低价值路径所掩盖。同时，开发者在代码审查中经常发现风险相关的洞察，但这些信息在自动化测试工具中被忽略。

Method: EyeQ系统从代码审查讨论中提取安全相关信号，定位涉及的程序区域，并将这些洞察转化为基于注解的模糊测试指导。该方法建立在现有的注解感知模糊测试之上，无需改变程序语义或开发者工作流程。首先通过人工指导的可行性研究验证，然后使用大型语言模型和精心设计的提示自动化工作流程。

Result: EyeQ显著改善了漏洞发现能力，在安全关键的PHP代码库中发现了40多个先前未知的漏洞，超越了标准模糊测试配置。

Conclusion: 通过利用代码审查中的开发者智能来指导模糊测试，EyeQ能够更有效地发现安全漏洞，证明了将人类洞察与自动化测试工具结合的价值。

Abstract: Modern fuzzers scale to large, real-world software but often fail to exercise the program states developers consider most fragile or security-critical. Such states are typically deep in the execution space, gated by preconditions, or overshadowed by lower-value paths that consume limited fuzzing budgets. Meanwhile, developers routinely surface risk-relevant insights during code review, yet this information is largely ignored by automated testing tools. We present EyeQ, a system that leverages developer intelligence from code reviews to guide fuzzing. EyeQ extracts security-relevant signals from review discussions, localizes the implicated program regions, and translates these insights into annotation-based guidance for fuzzing. The approach operates atop existing annotation-aware fuzzing, requiring no changes to program semantics or developer workflows. We first validate EyeQ through a human-guided feasibility study on a security-focused dataset of PHP code reviews, establishing a strong baseline for review-guided fuzzing. We then automate the workflow using a large language model with carefully designed prompts. EyeQ significantly improves vulnerability discovery over standard fuzzing configurations, uncovering more than 40 previously unknown bugs in the security-critical PHP codebase.

</details>


### [22] [When Skills Lie: Hidden-Comment Injection in LLM Agents](https://arxiv.org/abs/2602.10498)
*Qianli Wang,Boyang Ma,Minghui Xu,Yue Zhang*

Main category: cs.CR

TL;DR: 论文研究LLM代理中Markdown技能文档的隐藏注释提示注入风险，发现恶意指令可通过HTML注释隐藏，影响模型输出敏感工具意图，并提出防御性系统提示解决方案。


<details>
  <summary>Details</summary>
Motivation: LLM代理通常依赖技能文档来描述可用工具和推荐流程，但Markdown技能文档在渲染为HTML时存在安全风险：HTML注释块对人类审阅者不可见，但原始文本仍会完整提供给模型，这可能导致恶意指令被隐藏其中。

Method: 研究隐藏注释提示注入风险，通过在合法技能文档后附加恶意指令的HTML注释块进行实验，测试DeepSeek-V3.2和GLM-4.5-Air模型对这些隐藏指令的响应。

Result: 实验发现DeepSeek-V3.2和GLM-4.5-Air模型确实会受到隐藏注释中恶意指令的影响，产生包含敏感工具意图的输出。同时验证了简短的防御性系统提示能有效防止恶意工具调用，并暴露出可疑的隐藏指令。

Conclusion: LLM代理的技能文档层存在隐藏注释提示注入风险，需要将技能视为不可信来源并禁止敏感操作。提出的防御性系统提示能有效缓解此安全威胁，提高LLM代理的安全性。

Abstract: LLM agents often rely on Skills to describe available tools and recommended procedures. We study a hidden-comment prompt injection risk in this documentation layer: when a Markdown Skill is rendered to HTML, HTML comment blocks can become invisible to human reviewers, yet the raw text may still be supplied verbatim to the model. In experiments, we find that DeepSeek-V3.2 and GLM-4.5-Air can be influenced by malicious instructions embedded in a hidden comment appended to an otherwise legitimate Skill, yielding outputs that contain sensitive tool intentions. A short defensive system prompt that treats Skills as untrusted and forbids sensitive actions prevents these malicious tool calls and instead surfaces the suspicious hidden instructions.

</details>


### [23] [CryptoCatch: Cryptomining Hidden Nowhere](https://arxiv.org/abs/2602.10573)
*Ruisheng Shi,Ziding Lin,Haoran Sun,Qin Wang,Shihan Zhang,Lina Lan,Zhiyuan Peng,Chenfeng Wang*

Main category: cs.CR

TL;DR: 提出一种实用的加密加密货币挖矿流量检测机制，采用两阶段检测框架，结合机器学习和主动探测，实现高精度检测


<details>
  <summary>Details</summary>
Motivation: 传统检测方法（如黑名单和深度包检测）对加密挖矿流量效果不佳，且误报率高，需要更有效的解决方案

Method: 采用两阶段检测框架：第一阶段使用机器学习进行细粒度检测，第二阶段通过主动探测减少分类器的误报

Result: 系统达到F1分数0.99，识别特定加密货币的准确率达99.39%，在多个挖矿池的广泛测试中验证了有效性

Conclusion: 该方法为识别加密挖矿活动提供了更精确可靠的解决方案，有效解决了传统方法的局限性

Abstract: Cryptomining poses significant security risks, yet traditional detection methods like blacklists and Deep Packet Inspection (DPI) are often ineffective against encrypted mining traffic and suffer from high false positive rates. In this paper, we propose a practical encrypted cryptomining traffic detection mechanism. It consists of a two-stage detection framework, which can effectively provide fine-grained detection results by machine learning and reduce false positives from classifiers through active probing. Our system achieves an F1-score of 0.99 and identifies specific cryptocurrencies with a 99.39\% accuracy rate. Extensive testing across various mining pools confirms the effectiveness of our approach, offering a more precise and reliable solution for identifying cryptomining activities.

</details>


### [24] [Invisible Trails? An Identity Alignment Scheme based on Online Tracking](https://arxiv.org/abs/2602.10626)
*Ruisheng Shi,Zhiyuan Peng,Tong Fu,Lina Lan,Qin Wang,Jiaqi Zeng*

Main category: cs.CR

TL;DR: 论文提出了一种有效的身份对齐方案，通过被动和主动攻击方式，利用跟踪数据识别用户在不同网站上的账户，并构建了评估框架和系统原型。


<details>
  <summary>Details</summary>
Motivation: 尽管跟踪公司声称通过匿名化保护用户隐私，但匿名数据仍存在重大隐私风险，攻击者可利用这些数据识别用户在其他网站上的账户并进行针对性身份对齐。

Method: 开发数据收集器获取必要数据集，设计身份对齐算法，构建被动攻击（分析跟踪数据对齐身份）和主动攻击（诱导用户在线交互以提高成功率）两种去匿名化攻击方式，并首次引入在线跟踪身份对齐评估框架。

Result: 研究了影响身份对齐效果的关键因素，对生成的数据集进行了独立评估，并展示了应用于加密货币用例的完整功能系统原型。

Conclusion: 即使数据被匿名化，用户隐私仍面临严重风险，需要更强的隐私保护措施来应对跟踪数据带来的身份对齐威胁。

Abstract: Many tracking companies collect user data and sell it to data markets and advertisers. While they claim to protect user privacy by anonymizing the data, our research reveals that significant privacy risks persist even with anonymized data. Attackers can exploit this data to identify users' accounts on other websites and perform targeted identity alignment. In this paper, we propose an effective identity alignment scheme for accurately identifying targeted users. We develop a data collector to obtain the necessary datasets, an algorithm for identity alignment, and, based on this, construct two types of de-anonymization attacks: the \textit{passive attack}, which analyzes tracker data to align identities, and the \textit{active attack}, which induces users to interact online, leading to higher success rates. Furthermore, we introduce, for the first time, a novel evaluation framework for online tracking-based identity alignment. We investigate the key factors influencing the effectiveness of identity alignment. Additionally, we provide an independent assessment of our generated dataset and present a fully functional system prototype applied to a cryptocurrency use case.

</details>


### [25] [SecureScan: An AI-Driven Multi-Layer Framework for Malware and Phishing Detection Using Logistic Regression and Threat Intelligence Integration](https://arxiv.org/abs/2602.10750)
*Rumman Firdos,Aman Dangi*

Main category: cs.CR

TL;DR: SecureScan是一个AI驱动的三层检测框架，结合逻辑回归分类、启发式分析和VirusTotal外部威胁情报，对URL、文件哈希和二进制文件进行全面检测，在基准数据集上达到93.1%准确率。


<details>
  <summary>Details</summary>
Motivation: 现代恶意软件和钓鱼攻击日益复杂，传统基于签名的入侵检测系统效果下降，需要更智能的检测方法。

Method: 提出三层检测框架：1) 启发式分析过滤已知威胁；2) 机器学习（逻辑回归）分类不确定样本；3) 通过VirusTotal API验证边界案例。引入阈值校准和灰区逻辑(0.45-0.55)减少误报。

Result: 在基准数据集上达到93.1%准确率，精确率0.87，召回率0.92，表现出良好的泛化能力和减少过拟合。轻量级统计模型结合校准验证和外部情报，性能可与复杂深度学习系统媲美。

Conclusion: 轻量级统计模型通过校准验证和外部威胁情报增强，能够实现与复杂深度学习系统相当的可靠性和性能，为实际部署提供了高效解决方案。

Abstract: The growing sophistication of modern malware and phishing campaigns has diminished the effectiveness of traditional signature-based intrusion detection systems. This work presents SecureScan, an AI-driven, triple-layer detection framework that integrates logistic regression-based classification, heuristic analysis, and external threat intelligence via the VirusTotal API for comprehensive triage of URLs, file hashes, and binaries. The proposed architecture prioritizes efficiency by filtering known threats through heuristics, classifying uncertain samples using machine learning, and validating borderline cases with third-party intelligence. On benchmark datasets, SecureScan achieves 93.1 percent accuracy with balanced precision (0.87) and recall (0.92), demonstrating strong generalization and reduced overfitting through threshold-based decision calibration. A calibrated threshold and gray-zone logic (0.45-0.55) were introduced to minimize false positives and enhance real-world stability. Experimental results indicate that a lightweight statistical model, when augmented with calibrated verification and external intelligence, can achieve reliability and performance comparable to more complex deep learning systems.

</details>


### [26] [Architecting Trust: A Framework for Secure IoT Systems Through Trusted Execution and Semantic Middleware](https://arxiv.org/abs/2602.10762)
*Muhammad Imran*

Main category: cs.CR

TL;DR: 该论文提出了一个综合的物联网安全框架，整合可信执行环境、语义中间件和区块链技术，通过分层架构实现深度防御，同时满足资源受限物联网环境的约束。


<details>
  <summary>Details</summary>
Motivation: 物联网安全面临异构环境下的技术和运营挑战，安全威胁不断增加，需要能够应对不同操作条件的综合安全解决方案。

Method: 基于20多项近期研究和现有标准进行系统分析，提出分层安全架构：硬件根信任（外围层）、零信任原则（网络层）、语义安全机制（应用层），并整合TEE、语义中间件和区块链技术。

Result: 在Cortex-M类微控制器上获得加密性能、检测准确率和能耗等定量指标，证明跨层安全集成能在资源受限环境中实现深度防御。

Conclusion: 提出的架构展示了跨层安全集成的有效性，同时指出了后量子迁移、安全联邦模型交换和自动化合规验证等未来研究方向。

Abstract: The Internet of Things (IoT) security landscape requires the architectural solutions that can address the technical and operational challenges across the heterogeneous environments. The IoT systems operate in different conditions, and security issues continue to increase. This paper presents the comprehensive security framework for IoT that should integrate the Trusted Execution Environments (TEEs) with the semantic middleware and blockchain technologies. The work provides a systematic analysis of the architectural patterns based on more than twenty recent research works and the existing standards, and it proposes a layered security architecture. The architecture includes the hardware rooted trust at peripheral level, the zero trust principles at network level, and the semantic security mechanisms at application level. The framework focuses on practical implementation aspects such as the performance overhead, interoperability requirements, and the compliance with new regulations, which are very important for the real IoT deployments. The paper reports quantitative metrics which include the cryptographic performance on Cortex-M class microcontrollers with the detection accuracy rates and the energy consumption values. The proposed architecture shows that cross-layer security integration can provide defense in depth while it still satisfies the constraints of resource-limited IoT environments. The discussion highlights open challenges and the future research directions for the IoT security architectures that include the post-quantum migration, secure federated model exchange and the automated compliance verification.

</details>


### [27] [GoodVibe: Security-by-Vibe for LLM-Based Code Generation](https://arxiv.org/abs/2602.10778)
*Maximilian Thang,Lichao Wu,Sasha Behrouzi,Mohamadreza Rostami,Jona te Lintelo,Stjepan Picek,Ahmad-Reza Sadeghi*

Main category: cs.CR

TL;DR: GoodVibe是一个神经元级别的框架，通过识别和微调安全相关神经元来提升代码语言模型的安全性，同时保持模型通用能力，大幅减少训练参数和计算成本。


<details>
  <summary>Details</summary>
Motivation: LLM在快速非正式开发（vibe coding）中经常生成功能正确但不安全的代码，现有安全改进方法要么成本高且易遗忘，要么粒度粗且控制有限。

Method: 基于安全相关推理集中在少数神经元的洞察，使用梯度归因识别安全关键神经元，进行神经元选择性微调，并通过激活驱动的神经元聚类进一步降低训练成本。

Result: 在6个LLM和C++、Java、Swift、Go等安全关键编程语言上评估，安全性提升达2.5倍，匹配或超越全微调效果，参数减少4700倍，训练计算减少3.6倍以上。

Conclusion: 神经元级优化为代码生成安全提供了有效且可扩展的方法，在保持效率和通用性的同时显著提升安全性。

Abstract: Large language models (LLMs) are increasingly used for code generation in fast, informal development workflows, often referred to as vibe coding, where speed and convenience are prioritized, and security requirements are rarely made explicit. In this setting, models frequently produce functionally correct but insecure code, creating a growing security risk. Existing approaches to improving code security rely on full-parameter fine-tuning or parameter-efficient adaptations, which are either costly and prone to catastrophic forgetting or operate at coarse granularity with limited interpretability and control.
  We present GoodVibe, a neuron-level framework for improving the security of code language models by default. GoodVibe is based on the key insight that security-relevant reasoning is localized to a small subset of neurons. We identify these neurons using gradient-based attribution from a supervised security task and perform neuron-selective fine-tuning that updates only this security-critical subspace. To further reduce training cost, we introduce activation-driven neuron clustering, enabling structured updates with minimal overhead. We evaluate GoodVibe on six LLMs across security-critical programming languages, including C++, Java, Swift, and Go. GoodVibe substantially improves the security of generated code while preserving general model utility, achieving up to a 2.5x improvement over base models, matching or exceeding full fine-tuning with over 4,700x fewer trainable parameters, and reducing training computation by more than 3.6x compared to the parameter-efficient baseline (LoRA). Our results demonstrate that neuron-level optimization offers an effective and scalable approach to securing code generation without sacrificing efficiency or generality.

</details>


### [28] [Agentic Knowledge Distillation: Autonomous Training of Small Language Models for SMS Threat Detection](https://arxiv.org/abs/2602.10869)
*Adel ElZemity,Joshua Sylvester,Budi Arief,Rogério De Lemos*

Main category: cs.CR

TL;DR: 本文提出Agentic Knowledge Distillation方法，利用大语言模型作为自主教师，通过生成合成数据和迭代优化，训练可在设备端部署的小型学生模型用于短信钓鱼检测。


<details>
  <summary>Details</summary>
Motivation: 短信钓鱼攻击激增，但训练有效的设备端检测器需要标记的威胁数据，这些数据很快就会过时。需要一种无需人工干预、能持续更新的安全检测方法。

Method: 提出Agentic Knowledge Distillation方法：使用强大的LLM作为自主教师，生成合成数据并迭代优化小型学生SLM（Qwen2.5-0.5B和SmolLM2-135M），直到性能达到平台期。比较了四种LLM作为教师（Claude Opus 4.5, GPT 5.2 Codex, Gemini 3 Pro, DeepSeek V3.2）。

Result: 不同教师LLM的性能差异显著，最佳配置达到94.31%准确率和96.25%召回率。相比使用相同合成知识和LoRA设置但没有迭代反馈的DPO基线（50-80%准确率），agentic知识蒸馏显著更优（86-94%准确率）。

Conclusion: Agentic知识蒸馏能快速生成适用于边缘部署的有效安全分类器，但结果强烈依赖于所使用的教师LLM。闭环反馈和针对性优化是关键因素。

Abstract: SMS-based phishing (smishing) attacks have surged, yet training effective on-device detectors requires labelled threat data that quickly becomes outdated. To deal with this issue, we present Agentic Knowledge Distillation, which consists of a powerful LLM acts as an autonomous teacher that fine-tunes a smaller student SLM, deployable for security tasks without human intervention. The teacher LLM autonomously generates synthetic data and iteratively refines a smaller on-device student model until performance plateaus. We compare four LLMs in this teacher role (Claude Opus 4.5, GPT 5.2 Codex, Gemini 3 Pro, and DeepSeek V3.2) on SMS spam/smishing detection with two student SLMs (Qwen2.5-0.5B and SmolLM2-135M). Our results show that performance varies substantially depending on the teacher LLM, with the best configuration achieving 94.31% accuracy and 96.25% recall. We also compare against a Direct Preference Optimisation (DPO) baseline that uses the same synthetic knowledge and LoRA setup but without iterative feedback or targeted refinement; agentic knowledge distillation substantially outperforms it (e.g. 86-94% vs 50-80% accuracy), showing that closed-loop feedback and targeted refinement are critical. These findings demonstrate that agentic knowledge distillation can rapidly yield effective security classifiers for edge deployment, but outcomes depend strongly on which teacher LLM is used.

</details>


### [29] [Beyond Permissions: An Empirical Static Analysis of Privacy and Security Risks in Children-Oriented and General-Audience Mobile Apps for Gaming](https://arxiv.org/abs/2602.10877)
*Bakheet Aljedaani*

Main category: cs.CR

TL;DR: 该研究通过静态分析比较了儿童向和普通向Android手游的隐私安全风险，发现儿童游戏虽然请求权限较少，但配置层面风险和第三方SDK追踪问题与普通游戏相似。


<details>
  <summary>Details</summary>
Motivation: 移动游戏应用日益普及，包括越来越多的儿童向游戏。这些应用通常集成了复杂的分析、广告和归因基础设施，可能带来隐私和安全风险。现有研究主要关注追踪行为或盈利模式，而配置层面的隐私暴露和儿童向应用的研究不足。

Method: 采用三阶段方法：1) 设计研究协议；2) 收集Android应用包并进行静态检查；3) 数据分析。研究检查了权限、清单级配置属性（如备份设置、明文网络流量、导出组件）以及嵌入的第三方SDK生态系统，比较儿童向和普通向移动游戏。

Result: 结果显示，虽然儿童向游戏通常请求较少的权限，但它们经常表现出配置层面的风险，并嵌入与普通游戏相似的第三方追踪SDK。架构和配置决策在塑造隐私风险方面起着关键作用，特别是针对儿童的应用。

Conclusion: 该研究对移动游戏中的隐私暴露进行了全面的静态评估，为开发者、平台提供商和研究人员提供了可操作的见解，有助于改进移动应用中的隐私设计实践。

Abstract: Mobile gaming applications (apps) have become increasingly pervasive, including a growing number of games designed for children. Despite their popularity, these apps often integrate complex analytics, advertising, and attribution infrastructures that may introduce privacy and security risks. Existing research has primarily focused on tracking behaviors or monetization models, leaving configuration-level privacy exposure and children-oriented apps underexplored. In this study, we conducted a comparative static analysis of Android mobile games to investigate privacy and security risks beyond permission usage. The analysis follows a three-phase methodology comprising (i) designing study protocol, (ii) Android Package Kit (APK) collection and static inspection, and (iii) data analysis. We examined permissions, manifest-level configuration properties (e.g., backup settings, cleartext network traffic, and exported components), and embedded third-party Software Development Kit (SDK) ecosystems across children-oriented and general-audience mobile games. The extracted indicators are synthesized into qualitative privacy-risk categories to support comparative reporting. The results showed that while children-oriented games often request fewer permissions, they frequently exhibit configuration-level risks and embed third-party tracking SDKs similar to general-audience games. Architectural and configuration decisions play a critical role in shaping privacy risks, particularly for apps targeting children. This study contributes a holistic static assessment of privacy exposure in mobile games and provides actionable insights for developers, platform providers, and researchers seeking to improve privacy-by-design practices in mobile applications.

</details>


### [30] [Resilient Alerting Protocols for Blockchains](https://arxiv.org/abs/2602.10892)
*Marwa Moullem,Lorenz Breidenbach,Ittay Eyal,Ari Juels*

Main category: cs.CR

TL;DR: 该论文研究了智能合约中警报抑制攻击的抵抗问题，提出了三种实现渐近最优贿赂成本的协议方案。


<details>
  <summary>Details</summary>
Motivation: 智能合约管理着巨额资产，依赖外部事件警报，但现有研究未分析其抵抗攻击者通过贿赂抑制警报的韧性。需要形式化这一警报问题并设计抗贿赂方案。

Method: 1) 形式化警报问题为密码经济学博弈；2) 提出同时博弈协议实现二次方贿赂成本上界；3) 设计三种实现协议：强网络同步协议、可信硬件协议、顺序警报协议。

Result: 建立了二次方(O(n²))贿赂成本上界，三种协议均实现渐近最优贿赂抵抗，但各有不同的资源性能权衡：前两者常数时间但线性存储开销，后者乐观无链上存储但最坏情况O(n)时间。

Conclusion: 论文形式化了智能合约警报问题，提出了实现渐近最优贿赂抵抗的协议设计空间，为实际解决方案提供了丰富的设计选择。

Abstract: Smart contracts are stateful programs deployed on blockchains; they secure over a trillion dollars in transaction value per year. High-stakes smart contracts often rely on timely alerts about external events, but prior work has not analyzed their resilience to an attacker suppressing alerts via bribery. We formalize this challenge in a cryptoeconomic setting as the \emph{alerting problem}, giving rise to a game between a bribing adversary and~$n$ rational participants, who pay a penalty if they are caught deviating from the protocol. We establish a quadratic, i.e.,~$O(n^2)$, upper bound, whereas a straightforward alerting protocol only achieves~$O(n)$ bribery cost.
  We present a \emph{simultaneous game} that asymptotically achieves the quadratic upper bound and thus asymptotically-optimal bribery resistance. We then present two protocols that implement our simultaneous game: The first leverages a strong network synchrony assumption. The second relaxes this strong assumption and instead takes advantage of trusted hardware and blockchain proof-of-publication to establish a timed commitment scheme. These two protocols are constant-time but incur a linear storage overhead on the blockchain. We analyze a third, \emph{sequential alerting} protocol that optimistically incurs no on-chain storage overhead, at the expense of~$O(n)$ worst-case execution time. All three protocols achieve asymptotically-optimal bribery costs, but with different resource and performance tradeoffs. Together, they illuminate a rich design space for practical solutions to the alerting problem.

</details>


### [31] [Blind Gods and Broken Screens: Architecting a Secure, Intent-Centric Mobile Agent Operating System](https://arxiv.org/abs/2602.10915)
*Zhenhua Zou,Sheng Guo,Qiuyang Zhan,Lepeng Zhao,Shuo Li,Qi Li,Ke Xu,Mingwei Xu,Zhuotao Liu*

Main category: cs.CR

TL;DR: 本文系统分析了当前移动AI代理的安全漏洞，并提出Aura架构作为替代方案，通过结构化交互模型和四重防御机制显著提升安全性和性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，移动计算正从应用为中心转向系统级自主代理。当前主流的"屏幕即接口"范式存在结构性安全漏洞，且与移动生态系统经济基础冲突，需要从根本上重新设计安全架构。

Method: 1. 以豆包移动助手为代表进行系统安全分析，将威胁分为四个维度：代理身份、外部接口、内部推理和行动执行
2. 提出Aura架构：采用中心辐射拓扑，包含系统代理、应用代理和代理内核三层结构
3. 实施四重防御机制：加密身份绑定、多层语义防火墙、认知完整性保护、细粒度访问控制

Result: 在MobileSafetyBench评估中，相比豆包：低风险任务成功率从约75%提升至94.3%；高风险攻击成功率从约40%降至4.4%；延迟性能提升近一个数量级。

Conclusion: Aura架构通过结构化、代理原生的交互模型，有效解决了"屏幕即接口"范式的安全漏洞，为移动AI代理提供了可行且安全的替代方案。

Abstract: The evolution of Large Language Models (LLMs) has shifted mobile computing from App-centric interactions to system-level autonomous agents. Current implementations predominantly rely on a "Screen-as-Interface" paradigm, which inherits structural vulnerabilities and conflicts with the mobile ecosystem's economic foundations. In this paper, we conduct a systematic security analysis of state-of-the-art mobile agents using Doubao Mobile Assistant as a representative case. We decompose the threat landscape into four dimensions - Agent Identity, External Interface, Internal Reasoning, and Action Execution - revealing critical flaws such as fake App identity, visual spoofing, indirect prompt injection, and unauthorized privilege escalation stemming from a reliance on unstructured visual data.
  To address these challenges, we propose Aura, an Agent Universal Runtime Architecture for a clean-slate secure agent OS. Aura replaces brittle GUI scraping with a structured, agent-native interaction model. It adopts a Hub-and-Spoke topology where a privileged System Agent orchestrates intent, sandboxed App Agents execute domain-specific tasks, and the Agent Kernel mediates all communication. The Agent Kernel enforces four defense pillars: (i) cryptographic identity binding via a Global Agent Registry; (ii) semantic input sanitization through a multilayer Semantic Firewall; (iii) cognitive integrity via taint-aware memory and plan-trajectory alignment; and (iv) granular access control with non-deniable auditing. Evaluation on MobileSafetyBench shows that, compared to Doubao, Aura improves low-risk Task Success Rate from roughly 75% to 94.3%, reduces high-risk Attack Success Rate from roughly 40% to 4.4%, and achieves near-order-of-magnitude latency gains. These results demonstrate Aura as a viable, secure alternative to the "Screen-as-Interface" paradigm.

</details>


### [32] [CVPL: A Geometric Framework for Post-Hoc Linkage Risk Assessment in Protected Tabular Data](https://arxiv.org/abs/2602.11015)
*Valery Khvatov,Alexey Neyman*

Main category: cs.CR

TL;DR: CVPL是一个几何框架，用于评估原始数据与保护后数据之间的链接风险，提供连续的风险估计而非二元合规判断，揭示即使符合k-匿名性也可能存在实质性链接风险。


<details>
  <summary>Details</summary>
Motivation: 传统隐私度量标准（如k-匿名性）提供合规性保证，但无法量化实际数据发布中的链接风险。需要一种能够评估真实链接可能性的后验分析方法。

Method: 提出CVPL几何框架，将链接分析建模为操作管道：分块、向量化、潜在投影和相似性评估。引入阈值感知风险曲面R(λ,τ)捕捉保护强度和攻击者严格度的联合效应，建立具有单调性保证的渐进分块策略。

Result: 在10,000条记录和19种保护配置上的实证验证显示，即使符合k-匿名性，仍可能存在实质性链接风险，且相当部分风险来自非准标识符的行为模式。经典Fellegi-Sunter链接在CVPL限制性假设下成为特例，违反这些假设会导致系统性过度链接偏差。

Conclusion: CVPL提供可解释的诊断工具，识别驱动链接可行性的特征，支持隐私影响评估、保护机制比较和效用-风险权衡分析，为实际链接风险评估提供了更全面的框架。

Abstract: Formal privacy metrics provide compliance-oriented guarantees but often fail to quantify actual linkability in released datasets. We introduce CVPL (Cluster-Vector-Projection Linkage), a geometric framework for post-hoc assessment of linkage risk between original and protected tabular data. CVPL represents linkage analysis as an operator pipeline comprising blocking, vectorization, latent projection, and similarity evaluation, yielding continuous, scenario-dependent risk estimates rather than binary compliance verdicts. We formally define CVPL under an explicit threat model and introduce threshold-aware risk surfaces, R(lambda, tau), that capture the joint effects of protection strength and attacker strictness. We establish a progressive blocking strategy with monotonicity guarantees, enabling anytime risk estimation with valid lower bounds. We demonstrate that the classical Fellegi-Sunter linkage emerges as a special case of CVPL under restrictive assumptions, and that violations of these assumptions can lead to systematic over-linking bias. Empirical validation on 10,000 records across 19 protection configurations demonstrates that formal k-anonymity compliance may coexist with substantial empirical linkability, with a significant portion arising from non-quasi-identifier behavioral patterns. CVPL provides interpretable diagnostics identifying which features drive linkage feasibility, supporting privacy impact assessment, protection mechanism comparison, and utility-risk trade-off analysis.

</details>


### [33] [Mask-Based Window-Level Insider Threat Detection for Campaign Discovery](https://arxiv.org/abs/2602.11019)
*Jericho Cain,Hayden Beadles*

Main category: cs.CR

TL;DR: 提出一种双通道卷积自编码器，通过分离活动存在性和活动强度来改进内部威胁检测，在CERT r4.2数据集上显著优于标准自编码器基线。


<details>
  <summary>Details</summary>
Motivation: 传统UEBA系统基于固定时间窗口检测内部威胁，但现有方法难以有效利用窗口内关于长期攻击活动的信息。需要探索如何更好地从单个时间窗口中提取信息以发现攻击活动。

Method: 提出双通道卷积自编码器，同时重构二元活动掩码和对应的活动值。这种方法让模型将表征能力集中在稀疏的行为结构上，而不是密集的非活动基线。

Result: 在持续1-7天的多日攻击活动中，该方法实现了窗口级精确率-召回率AUC 0.71，显著超过标准无监督自编码器基线，并能在零误报的情况下实现高精度操作点。

Conclusion: 通过明确分离活动存在性和活动强度，可以显著提升内部威胁检测性能，特别是在发现长期攻击活动方面。双通道自编码器架构能有效捕捉稀疏行为模式。

Abstract: User and Entity Behavior Analytics (UEBA) systems commonly detect insider threats by scoring fixed time windows of user activity for anomalous behavior. While this window-level paradigm has proven effective for identifying sharp behavioral deviations, it remains unclear how much information about longer-running attack campaigns is already present within individual windows, and how such information can be leveraged for campaign discovery. In this work, we study unsupervised window-level insider threat detection on the CERT r4.2 dataset and show that explicitly separating activity presence from activity magnitude yields substantial performance gains. We introduce a dual-channel convolutional autoencoder that reconstructs both a binary activity mask and corresponding activity values, allowing the model to focus representational capacity on sparse behavioral structure rather than dense inactive baselines. Across multiday attack campaigns lasting between one and seven days, the proposed approach achieves a window-level precision-recall AUC of 0.71, substantially exceeding standard unsupervised autoencoder baselines and enabling high-precision operating points with zero false alarms.

</details>


### [34] [IU-GUARD: Privacy-Preserving Spectrum Coordination for Incumbent Users under Dynamic Spectrum Sharing](https://arxiv.org/abs/2602.11023)
*Shaoyu Li,Hexuan Yu,Shanghao Shi,Md Mohaimin Al Barat,Yang Xiao,Y. Thomas Hou,Wenjing Lou*

Main category: cs.CR

TL;DR: IU-GUARD是一个保护隐私的动态频谱共享框架，使用可验证凭证和零知识证明，让授权用户在不暴露身份的情况下访问频谱。


<details>
  <summary>Details</summary>
Motivation: 当前动态频谱共享框架中的授权用户保护机制存在严重缺陷：环境感知能力需要昂贵的传感器部署且易受干扰，而授权用户信息能力则要求授权用户向频谱协调系统公开身份和操作参数，这会泄露操作隐私和任务机密。

Method: 提出IU-GUARD框架，利用可验证凭证和零知识证明技术，使授权用户能够向频谱协调系统证明其授权状态，同时仅披露必要的操作参数，实现身份与频谱访问的解耦。

Result: 实现了原型系统，评估显示IU-GUARD在提供强大隐私保护的同时，具有实用的计算和通信开销，适合实时动态频谱共享部署。

Conclusion: IU-GUARD解决了当前频谱共享框架中的隐私和安全限制，为授权用户提供了身份保护，同时保持了频谱协调系统的功能，是实际可行的隐私保护频谱共享解决方案。

Abstract: With the growing demand for wireless spectrum, dynamic spectrum sharing (DSS) frameworks such as the Citizens Broadband Radio Service (CBRS) have emerged as practical solutions to improve utilization while protecting incumbent users (IUs) such as military radars. However, current incumbent protection mechanisms face critical limitations. The Environmental Sensing Capability (ESC) requires costly sensor deployments and remains vulnerable to interference and security risks. Alternatively, the Incumbent Informing Capability (IIC) requires IUs to disclose their identities and operational parameters to the Spectrum Coordination System (SCS), creating linkable records that compromise operational privacy and mission secrecy. We propose IU-GUARD, a privacy-preserving spectrum sharing framework that enables IUs to access spectrum without revealing their identities. Leveraging verifiable credentials (VCs) and zero-knowledge proofs (ZKPs), IU-GUARD allows IUs to prove their authorization to the SCS while disclosing only essential operational parameters. This decouples IU identity from spectrum access, prevents cross-request linkage, and mitigates the risk of centralized SCS data leakage. We implement a prototype, and our evaluation shows that IU-GUARD achieves strong privacy guarantees with practical computation and communication overhead, making it suitable for real-time DSS deployment.

</details>


### [35] [Vulnerabilities in Partial TEE-Shielded LLM Inference with Precomputed Noise](https://arxiv.org/abs/2602.11088)
*Abhishek Saini,Haolin Jiang,Hang Liu*

Main category: cs.CR

TL;DR: 主流TEE加速设计中的静态密钥重用漏洞，可完全破坏LLM的机密性和完整性保护


<details>
  <summary>Details</summary>
Motivation: 第三方设备部署LLM需要保护模型知识产权，TEE是可行方案但性能限制导致设计妥协，使用预计算的静态密钥基础来加速加密操作，这引入了经典密码学漏洞

Method: 分析TEE加速设计中的密钥重用漏洞，提出两种攻击：1) 针对模型机密性系统的攻击，恢复秘密置换和模型权重；2) 针对完整性系统的攻击，完全绕过Soter和TSQP等系统的完整性检查

Result: 攻击具有实际可行性：从LLaMA-3 8B模型中恢复一层秘密约需6分钟，攻击可扩展到405B参数LLM，在各种配置下都能成功

Conclusion: 主流TEE加速设计中的静态密钥重用模式存在严重安全漏洞，需要重新设计安全协议来保护LLM部署

Abstract: The deployment of large language models (LLMs) on third-party devices requires new ways to protect model intellectual property. While Trusted Execution Environments (TEEs) offer a promising solution, their performance limits can lead to a critical compromise: using a precomputed, static secret basis to accelerate cryptographic operations. We demonstrate that this mainstream design pattern introduces a classic cryptographic flaw, the reuse of secret keying material, into the system's protocol. We prove its vulnerability with two distinct attacks: First, our attack on a model confidentiality system achieves a full confidentiality break by recovering its secret permutations and model weights. Second, our integrity attack completely bypasses the integrity checks of systems like Soter and TSQP. We demonstrate the practicality of our attacks against state-of-the-art LLMs, recovering a layer's secrets from a LLaMA-3 8B model in about 6 minutes and showing the attack scales to compromise 405B-parameter LLMs across a variety of configurations.

</details>
