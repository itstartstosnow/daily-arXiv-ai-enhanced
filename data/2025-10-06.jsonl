{"id": "2510.02317", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.02317", "abs": "https://arxiv.org/abs/2510.02317", "authors": ["Anais Jaikissoon"], "title": "Hybrid Horizons: Policy for Post-Quantum Security", "comment": "18 pages, 3 figures, 1 image", "summary": "The Age of Artificial Intelligence is here. In 2025, there are few\nregulations governing artificial intelligence. While the expansion of\nartificial intelligence is going in a relatively good direction, there is a\nrisk that it can be misused. Misuse of technology is nothing new and will\ncontinue to happen. The lack of regulation in artificial intelligence is\nnecessary because it raises the question of how we can move forward without\nknowing what the limits are. While artificial intelligence dominates the\ntechnology industry, new technology is starting to emerge. Quantum cryptography\nis expected to replace classical cryptography; however, the transition from\nclassical to quantum cryptography is expected to occur within the next 10\nyears. The ability to transition from classical to quantum cryptography\nrequires hybrid cryptography. Hybrid cryptography can be used now; however,\nsimilar to artificial intelligence, there is no regulation or support for the\nregulatory infrastructure regarding hybrid machines. This paper will explore\nthe regulatory gaps in hybrid cryptography. The paper will also offer solutions\nto fix the gaps and ensure the transition from classical to quantum\ncryptography is safely and effectively completed."}
{"id": "2510.02319", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.02319", "abs": "https://arxiv.org/abs/2510.02319", "authors": ["Lekkala Sai Teja", "Annepaka Yadagiri", "Sangam Sai Anish", "Siva Gopala Krishna Nuthakki", "Partha Pakray"], "title": "Modeling the Attack: Detecting AI-Generated Text by Quantifying Adversarial Perturbations", "comment": "8 pages, 3 figures", "summary": "The growth of highly advanced Large Language Models (LLMs) constitutes a huge\ndual-use problem, making it necessary to create dependable AI-generated text\ndetection systems. Modern detectors are notoriously vulnerable to adversarial\nattacks, with paraphrasing standing out as an effective evasion technique that\nfoils statistical detection. This paper presents a comparative study of\nadversarial robustness, first by quantifying the limitations of standard\nadversarial training and then by introducing a novel, significantly more\nresilient detection framework: Perturbation-Invariant Feature Engineering\n(PIFE), a framework that enhances detection by first transforming input text\ninto a standardized form using a multi-stage normalization pipeline, it then\nquantifies the transformation's magnitude using metrics like Levenshtein\ndistance and semantic similarity, feeding these signals directly to the\nclassifier. We evaluate both a conventionally hardened Transformer and our\nPIFE-augmented model against a hierarchical taxonomy of character-, word-, and\nsentence-level attacks. Our findings first confirm that conventional\nadversarial training, while resilient to syntactic noise, fails against\nsemantic attacks, an effect we term \"semantic evasion threshold\", where its\nTrue Positive Rate at a strict 1% False Positive Rate plummets to 48.8%. In\nstark contrast, our PIFE model, which explicitly engineers features from the\ndiscrepancy between a text and its canonical form, overcomes this limitation.\nIt maintains a remarkable 82.6% TPR under the same conditions, effectively\nneutralizing the most sophisticated semantic attacks. This superior performance\ndemonstrates that explicitly modeling perturbation artifacts, rather than\nmerely training on them, is a more promising path toward achieving genuine\nrobustness in the adversarial arms race."}
{"id": "2510.02325", "categories": ["cs.CR", "cs.AI", "I.2.11; J.3"], "pdf": "https://arxiv.org/pdf/2510.02325", "abs": "https://arxiv.org/abs/2510.02325", "authors": ["Mohammed A. Shehab"], "title": "Agentic-AI Healthcare: Multilingual, Privacy-First Framework with MCP Agents", "comment": "6 pages, 1 figure. Submitted as a system/vision paper", "summary": "This paper introduces Agentic-AI Healthcare, a privacy-aware, multilingual,\nand explainable research prototype developed as a single-investigator project.\nThe system leverages the emerging Model Context Protocol (MCP) to orchestrate\nmultiple intelligent agents for patient interaction, including symptom\nchecking, medication suggestions, and appointment scheduling. The platform\nintegrates a dedicated Privacy and Compliance Layer that applies role-based\naccess control (RBAC), AES-GCM field-level encryption, and tamper-evident audit\nlogging, aligning with major healthcare data protection standards such as HIPAA\n(US), PIPEDA (Canada), and PHIPA (Ontario). Example use cases demonstrate\nmultilingual patient-doctor interaction (English, French, Arabic) and\ntransparent diagnostic reasoning powered by large language models. As an\napplied AI contribution, this work highlights the feasibility of combining\nagentic orchestration, multilingual accessibility, and compliance-aware\narchitecture in healthcare applications. This platform is presented as a\nresearch prototype and is not a certified medical device."}
{"id": "2510.02342", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.02342", "abs": "https://arxiv.org/abs/2510.02342", "authors": ["Yu Zhang", "Shuliang Liu", "Xu Yang", "Xuming Hu"], "title": "CATMark: A Context-Aware Thresholding Framework for Robust Cross-Task Watermarking in Large Language Models", "comment": null, "summary": "Watermarking algorithms for Large Language Models (LLMs) effectively identify\nmachine-generated content by embedding and detecting hidden statistical\nfeatures in text. However, such embedding leads to a decline in text quality,\nespecially in low-entropy scenarios where performance needs improvement.\nExisting methods that rely on entropy thresholds often require significant\ncomputational resources for tuning and demonstrate poor adaptability to unknown\nor cross-task generation scenarios. We propose \\textbf{C}ontext-\\textbf{A}ware\n\\textbf{T}hreshold watermarking ($\\myalgo$), a novel framework that dynamically\nadjusts watermarking intensity based on real-time semantic context. $\\myalgo$\npartitions text generation into semantic states using logits clustering,\nestablishing context-aware entropy thresholds that preserve fidelity in\nstructured content while embedding robust watermarks. Crucially, it requires no\npre-defined thresholds or task-specific tuning. Experiments show $\\myalgo$\nimproves text quality in cross-tasks without sacrificing detection accuracy."}
{"id": "2510.02349", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02349", "abs": "https://arxiv.org/abs/2510.02349", "authors": ["Hamed Fard", "Tobias Schalau", "Gerhard Wunder"], "title": "An Investigation into the Performance of Non-Contrastive Self-Supervised Learning Methods for Network Intrusion Detection", "comment": null, "summary": "Network intrusion detection, a well-explored cybersecurity field, has\npredominantly relied on supervised learning algorithms in the past two decades.\nHowever, their limitations in detecting only known anomalies prompt the\nexploration of alternative approaches. Motivated by the success of\nself-supervised learning in computer vision, there is a rising interest in\nadapting this paradigm for network intrusion detection. While prior research\nmainly delved into contrastive self-supervised methods, the efficacy of\nnon-contrastive methods, in conjunction with encoder architectures serving as\nthe representation learning backbone and augmentation strategies that determine\nwhat is learned, remains unclear for effective attack detection. This paper\ncompares the performance of five non-contrastive self-supervised learning\nmethods using three encoder architectures and six augmentation strategies.\nNinety experiments are systematically conducted on two network intrusion\ndetection datasets, UNSW-NB15 and 5G-NIDD. For each self-supervised model, the\ncombination of encoder architecture and augmentation method yielding the\nhighest average precision, recall, F1-score, and AUCROC is reported.\nFurthermore, by comparing the best-performing models to two unsupervised\nbaselines, DeepSVDD, and an Autoencoder, we showcase the competitiveness of the\nnon-contrastive methods for attack detection. Code at:\nhttps://github.com/renje4z335jh4/non_contrastive_SSL_NIDS"}
{"id": "2510.02356", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02356", "abs": "https://arxiv.org/abs/2510.02356", "authors": ["Xinjie Shen", "Mufei Li", "Pan Li"], "title": "Measuring Physical-World Privacy Awareness of Large Language Models: An Evaluation Benchmark", "comment": null, "summary": "The deployment of Large Language Models (LLMs) in embodied agents creates an\nurgent need to measure their privacy awareness in the physical world. Existing\nevaluation methods, however, are confined to natural language based scenarios.\nTo bridge this gap, we introduce EAPrivacy, a comprehensive evaluation\nbenchmark designed to quantify the physical-world privacy awareness of\nLLM-powered agents. EAPrivacy utilizes procedurally generated scenarios across\nfour tiers to test an agent's ability to handle sensitive objects, adapt to\nchanging environments, balance task execution with privacy constraints, and\nresolve conflicts with social norms. Our measurements reveal a critical deficit\nin current models. The top-performing model, Gemini 2.5 Pro, achieved only 59\\%\naccuracy in scenarios involving changing physical environments. Furthermore,\nwhen a task was accompanied by a privacy request, models prioritized completion\nover the constraint in up to 86\\% of cases. In high-stakes situations pitting\nprivacy against critical social norms, leading models like GPT-4o and\nClaude-3.5-haiku disregarded the social norm over 15\\% of the time. These\nfindings, demonstrated by our benchmark, underscore a fundamental misalignment\nin LLMs regarding physically grounded privacy and establish the need for more\nrobust, physically-aware alignment."}
{"id": "2510.02357", "categories": ["cs.CR", "cs.AI", "K.4.1; K.4.2; K.6.5; I.2.0"], "pdf": "https://arxiv.org/pdf/2510.02357", "abs": "https://arxiv.org/abs/2510.02357", "authors": ["Grace Billiris", "Asif Gill", "Madhushi Bandara"], "title": "Privacy in the Age of AI: A Taxonomy of Data Risks", "comment": "12 pages, 2 figures, 4 tables", "summary": "Artificial Intelligence (AI) systems introduce unprecedented privacy\nchallenges as they process increasingly sensitive data. Traditional privacy\nframeworks prove inadequate for AI technologies due to unique characteristics\nsuch as autonomous learning and black-box decision-making. This paper presents\na taxonomy classifying AI privacy risks, synthesised from 45 studies identified\nthrough systematic review. We identify 19 key risks grouped under four\ncategories: Dataset-Level, Model-Level, Infrastructure-Level, and Insider\nThreat Risks. Findings reveal a balanced distribution across these dimensions,\nwith human error (9.45%) emerging as the most significant factor. This taxonomy\nchallenges conventional security approaches that typically prioritise technical\ncontrols over human factors, highlighting gaps in holistic understanding. By\nbridging technical and behavioural dimensions of AI privacy, this paper\ncontributes to advancing trustworthy AI development and provides a foundation\nfor future research."}
{"id": "2510.02365", "categories": ["cs.CR", "math.AG", "math.NT"], "pdf": "https://arxiv.org/pdf/2510.02365", "abs": "https://arxiv.org/abs/2510.02365", "authors": ["Dongfang Zhao"], "title": "Bootstrapping as a Morphism: An Arithmetic Geometry Approach to Asymptotically Faster Homomorphic Encryption", "comment": null, "summary": "Fully Homomorphic Encryption (FHE) provides a powerful paradigm for secure\ncomputation, but its practical adoption is severely hindered by the prohibitive\ncomputational cost of its bootstrapping procedure. The complexity of all\ncurrent bootstrapping methods is fundamentally tied to the multiplicative depth\nof the decryption circuit, denoted $L_{dec}$, making it the primary performance\nbottleneck. This paper introduces a new approach to bootstrapping that\ncompletely bypasses the traditional circuit evaluation model. We apply the\ntools of modern arithmetic geometry to reframe the bootstrapping operation as a\ndirect geometric projection. Our framework models the space of ciphertexts as\nan affine scheme and rigorously defines the loci of decryptable and fresh\nciphertexts as distinct closed subschemes. The bootstrapping transformation is\nthen realized as a morphism between these two spaces. Computationally, this\nprojection is equivalent to solving a specific Closest Vector Problem (CVP)\ninstance on a highly structured ideal lattice, which we show can be done\nefficiently using a technique we call algebraic folding. The primary result of\nour work is a complete and provably correct bootstrapping algorithm with a\ncomputational complexity of $O(d \\cdot \\text{poly}(\\log q))$, where $d$ is the\nring dimension and $q$ is the ciphertext modulus. The significance of this\nresult lies in the complete elimination of the factor $L_{dec}$ from the\ncomplexity, representing a fundamental asymptotic improvement over the state of\nthe art. This geometric perspective offers a new and promising pathway toward\nachieving truly practical and high-performance FHE."}
{"id": "2510.02371", "categories": ["cs.CR", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.02371", "abs": "https://arxiv.org/abs/2510.02371", "authors": ["Bochra Al Agha", "Razane Tajeddine"], "title": "Federated Spatiotemporal Graph Learning for Passive Attack Detection in Smart Grids", "comment": null, "summary": "Smart grids are exposed to passive eavesdropping, where attackers listen\nsilently to communication links. Although no data is actively altered, such\nreconnaissance can reveal grid topology, consumption patterns, and operational\nbehavior, creating a gateway to more severe targeted attacks. Detecting this\nthreat is difficult because the signals it produces are faint, short-lived, and\noften disappear when traffic is examined by a single node or along a single\ntimeline. This paper introduces a graph-centric, multimodal detector that fuses\nphysical-layer and behavioral indicators over ego-centric star subgraphs and\nshort temporal windows to detect passive attacks. To capture stealthy\nperturbations, a two-stage encoder is introduced: graph convolution aggregates\nspatial context across ego-centric star subgraphs, while a bidirectional GRU\nmodels short-term temporal dependencies. The encoder transforms heterogeneous\nfeatures into a unified spatio-temporal representation suitable for\nclassification. Training occurs in a federated learning setup under FedProx,\nimproving robustness to heterogeneous local raw data and contributing to the\ntrustworthiness of decentralized training; raw measurements remain on client\ndevices. A synthetic, standards-informed dataset is generated to emulate\nheterogeneous HAN/NAN/WAN communications with wireless-only passive\nperturbations, event co-occurrence, and leak-safe splits. The model achieves a\ntesting accuracy of 98.32% per-timestep (F1_{attack}=0.972) and 93.35%\nper-sequence at 0.15% FPR using a simple decision rule with run-length m=2 and\nthreshold $\\tau=0.55$. The results demonstrate that combining spatial and\ntemporal context enables reliable detection of stealthy reconnaissance while\nmaintaining low false-positive rates, making the approach suitable for non-IID\nfederated smart-grid deployments."}
{"id": "2510.02373", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02373", "abs": "https://arxiv.org/abs/2510.02373", "authors": ["Qianshan Wei", "Tengchao Yang", "Yaochen Wang", "Xinfeng Li", "Lijun Li", "Zhenfei Yin", "Yi Zhan", "Thorsten Holz", "Zhiqiang Lin", "XiaoFeng Wang"], "title": "A-MemGuard: A Proactive Defense Framework for LLM-Based Agent Memory", "comment": null, "summary": "Large Language Model (LLM) agents use memory to learn from past interactions,\nenabling autonomous planning and decision-making in complex environments.\nHowever, this reliance on memory introduces a critical security risk: an\nadversary can inject seemingly harmless records into an agent's memory to\nmanipulate its future behavior. This vulnerability is characterized by two core\naspects: First, the malicious effect of injected records is only activated\nwithin a specific context, making them hard to detect when individual memory\nentries are audited in isolation. Second, once triggered, the manipulation can\ninitiate a self-reinforcing error cycle: the corrupted outcome is stored as\nprecedent, which not only amplifies the initial error but also progressively\nlowers the threshold for similar attacks in the future. To address these\nchallenges, we introduce A-MemGuard (Agent-Memory Guard), the first proactive\ndefense framework for LLM agent memory. The core idea of our work is the\ninsight that memory itself must become both self-checking and self-correcting.\nWithout modifying the agent's core architecture, A-MemGuard combines two\nmechanisms: (1) consensus-based validation, which detects anomalies by\ncomparing reasoning paths derived from multiple related memories and (2) a\ndual-memory structure, where detected failures are distilled into ``lessons''\nstored separately and consulted before future actions, breaking error cycles\nand enabling adaptation. Comprehensive evaluations on multiple benchmarks show\nthat A-MemGuard effectively cuts attack success rates by over 95% while\nincurring a minimal utility cost. This work shifts LLM memory security from\nstatic filtering to a proactive, experience-driven model where defenses\nstrengthen over time. Our code is available in\nhttps://github.com/TangciuYueng/AMemGuard"}
{"id": "2510.02374", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02374", "abs": "https://arxiv.org/abs/2510.02374", "authors": ["Ayda Aghaei Nia"], "title": "A Hybrid CAPTCHA Combining Generative AI with Keystroke Dynamics for Enhanced Bot Detection", "comment": "6 pages, 4 figures", "summary": "Completely Automated Public Turing tests to tell Computers and Humans Apart\n(CAPTCHAs) are a foundational component of web security, yet traditional\nimplementations suffer from a trade-off between usability and resilience\nagainst AI-powered bots. This paper introduces a novel hybrid CAPTCHA system\nthat synergizes the cognitive challenges posed by Large Language Models (LLMs)\nwith the behavioral biometric analysis of keystroke dynamics. Our approach\ngenerates dynamic, unpredictable questions that are trivial for humans but\nnon-trivial for automated agents, while simultaneously analyzing the user's\ntyping rhythm to distinguish human patterns from robotic input. We present the\nsystem's architecture, formalize the feature extraction methodology for\nkeystroke analysis, and report on an experimental evaluation. The results\nindicate that our dual-layered approach achieves a high degree of accuracy in\nbot detection, successfully thwarting both paste-based and script-based\nsimulation attacks, while maintaining a high usability score among human\nparticipants. This work demonstrates the potential of combining cognitive and\nbehavioral tests to create a new generation of more secure and user-friendly\nCAPTCHAs."}
{"id": "2510.02376", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02376", "abs": "https://arxiv.org/abs/2510.02376", "authors": ["Ryan Marinelli", "Angelica Chowdhury"], "title": "Scaling Homomorphic Applications in Deployment", "comment": "5 pages, 6 figures, 1 pseudo code", "summary": "In this endeavor, a proof-of-concept homomorphic application is developed to\ndetermine the production readiness of encryption ecosystems. A movie\nrecommendation app is implemented for this purpose and productionized through\ncontainerization and orchestration. By tuning deployment configurations, the\ncomputational limitations of Fully Homomorphic Encryption (FHE) are mitigated\nthrough additional infrastructure optimizations\n  Index Terms: Reinforcement Learning, Orchestration, Homomorphic Encryption"}
{"id": "2510.02378", "categories": ["cs.CR", "math.ST", "stat.AP", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.02378", "abs": "https://arxiv.org/abs/2510.02378", "authors": ["Jingrong Xie", "Yumin Li"], "title": "Apply Bayes Theorem to Optimize IVR Authentication Process", "comment": null, "summary": "This paper introduces a Bayesian approach to improve Interactive Voice\nResponse (IVR) authentication processes used by financial institutions.\nTraditional IVR systems authenticate users through a static sequence of\ncredentials, assuming uniform effectiveness among them. However, fraudsters\nexploit this predictability, selectively bypassing strong credentials. This\nstudy applies Bayes' Theorem and conditional probability modeling to evaluate\nfraud risk dynamically and adapt credential verification paths."}
{"id": "2510.02379", "categories": ["cs.CR", "cs.PF", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.02379", "abs": "https://arxiv.org/abs/2510.02379", "authors": ["Abel C. H. Chen"], "title": "Hybrid Schemes of NIST Post-Quantum Cryptography Standard Algorithms and Quantum Key Distribution for Key Exchange and Digital Signature", "comment": "in Chinese language", "summary": "Since the security of post-quantum cryptography (PQC) algorithms is based on\nthe hardness of mathematical problems, while the security of quantum key\ndistribution (QKD) relies on the fundamental principles of quantum physics,\neach approach possesses distinct advantages and limitations that can complement\none another. Consequently, recent studies have proposed hybrid schemes that\ncombine QKD and PQC to establish a dual-layered security model. In response to\nthis trend, this study proposes hybrid schemes that integrate QKD with the\nNational Institute of Standards and Technology (NIST) standardized PQC\nalgorithms. These hybrid schemes include two core components: a hybrid QKD-PQC\nkey exchange protocol and a hybrid QKD-PQC digital signature scheme. For the\nhybrid key exchange protocol, this study combines Module-Lattice-based Key\nEncapsulation Mechanisms (ML-KEM) with QKD protocols, specifically BB84 and\nE91, to construct a secure key exchange protocol. In the design of the hybrid\ndigital signature scheme, this study utilizes Module-Lattice-based Digital\nSignature Algorithms (ML-DSA) and Stateless Hash-based Digital Signature\nAlgorithms (SLH-DSA) to generate signature reconstruction values. These values\nare verified using confirmation codes transmitted via the BB84 and E91\nprotocols. The proposed hybrid key exchange protocol is evaluated by examining\nthe shared secret key it produces, particularly with respect to entropy and\nwhether the output is independent and identically distributed (IID).\nFurthermore, the computation time and message lengths of the proposed hybrid\nschemes are evaluated."}
{"id": "2510.02383", "categories": ["cs.CR", "math.NT", "94A60"], "pdf": "https://arxiv.org/pdf/2510.02383", "abs": "https://arxiv.org/abs/2510.02383", "authors": ["Awnon Bhowmik"], "title": "Selmer-Inspired Elliptic Curve Generation", "comment": null, "summary": "Elliptic curve cryptography (ECC) is foundational to modern secure\ncommunication, yet existing standard curves have faced scrutiny for opaque\nparameter-generation practices. This work introduces a Selmer-inspired\nframework for constructing elliptic curves that is both transparent and\nauditable. Drawing from $2$- and $3$-descent methods, we derive binary quartics\nand ternary cubics whose classical invariants deterministically yield candidate\n$(c_4,c_6)$ parameters. Local solubility checks, modeled on Selmer\nadmissibility, filter candidates prior to reconciliation into short-Weierstrass\nform over prime fields. We then apply established cryptographic validations,\nincluding group-order factorization, cofactor bounds, twist security, and\nembedding-degree heuristics. A proof-of-concept implementation demonstrates\nthat the pipeline functions as a retry-until-success Las Vegas algorithm, with\ncomplete transcripts enabling independent verification. Unlike seed-based or\npurely efficiency-driven designs, our approach embeds arithmetic structure into\nparameter selection while remaining compatible with constant-time, side-channel\nresistant implementations. This work broadens the design space for elliptic\ncurves, showing that descent techniques from arithmetic geometry can underpin\ntrust-enhancing, standardization-ready constructions."}
{"id": "2510.02384", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02384", "abs": "https://arxiv.org/abs/2510.02384", "authors": ["Jie Cao", "Qi Li", "Zelin Zhang", "Jianbing Ni"], "title": "Secure and Robust Watermarking for AI-generated Images: A Comprehensive Survey", "comment": null, "summary": "The rapid advancement of generative artificial intelligence (Gen-AI) has\nfacilitated the effortless creation of high-quality images, while\nsimultaneously raising critical concerns regarding intellectual property\nprotection, authenticity, and accountability. Watermarking has emerged as a\npromising solution to these challenges by distinguishing AI-generated images\nfrom natural content, ensuring provenance, and fostering trustworthy digital\necosystems. This paper presents a comprehensive survey of the current state of\nAI-generated image watermarking, addressing five key dimensions: (1)\nformalization of image watermarking systems; (2) an overview and comparison of\ndiverse watermarking techniques; (3) evaluation methodologies with respect to\nvisual quality, capacity, and detectability; (4) vulnerabilities to malicious\nattacks; and (5) prevailing challenges and future directions. The survey aims\nto equip researchers with a holistic understanding of AI-generated image\nwatermarking technologies, thereby promoting their continued development."}
{"id": "2510.02386", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02386", "abs": "https://arxiv.org/abs/2510.02386", "authors": ["Han Wang", "Haoyu Li", "Brian Ko", "Huan Zhang"], "title": "On The Fragility of Benchmark Contamination Detection in Reasoning Models", "comment": null, "summary": "Leaderboards for LRMs have turned evaluation into a competition,\nincentivizing developers to optimize directly on benchmark suites. A shortcut\nto achieving higher rankings is to incorporate evaluation benchmarks into the\ntraining data, thereby yielding inflated performance, known as benchmark\ncontamination. Surprisingly, our studies find that evading contamination\ndetections for LRMs is alarmingly easy. We focus on the two scenarios where\ncontamination may occur in practice: (I) when the base model evolves into LRM\nvia SFT and RL, we find that contamination during SFT can be originally\nidentified by contamination detection methods. Yet, even a brief GRPO training\ncan markedly conceal contamination signals that most detection methods rely on.\nFurther empirical experiments and theoretical analysis indicate that PPO style\nimportance sampling and clipping objectives are the root cause of this\ndetection concealment, indicating that a broad class of RL methods may\ninherently exhibit similar concealment capability; (II) when SFT contamination\nwith CoT is applied to advanced LRMs as the final stage, most contamination\ndetection methods perform near random guesses. Without exposure to non-members,\ncontaminated LRMs would still have more confidence when responding to those\nunseen samples that share similar distributions to the training set, and thus,\nevade existing memorization-based detection methods. Together, our findings\nreveal the unique vulnerability of LRMs evaluations: Model developers could\neasily contaminate LRMs to achieve inflated leaderboards performance while\nleaving minimal traces of contamination, thereby strongly undermining the\nfairness of evaluation and threatening the integrity of public leaderboards.\nThis underscores the urgent need for advanced contamination detection methods\nand trustworthy evaluation protocols tailored to LRMs."}
{"id": "2510.02391", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02391", "abs": "https://arxiv.org/abs/2510.02391", "authors": ["Nik Rollinson", "Nikolaos Polatidis"], "title": "LLM-Generated Samples for Android Malware Detection", "comment": "24 pages", "summary": "Android malware continues to evolve through obfuscation and polymorphism,\nposing challenges for both signature-based defenses and machine learning models\ntrained on limited and imbalanced datasets. Synthetic data has been proposed as\na remedy for scarcity, yet the role of large language models (LLMs) in\ngenerating effective malware data for detection tasks remains underexplored. In\nthis study, we fine-tune GPT-4.1-mini to produce structured records for three\nmalware families: BankBot, Locker/SLocker, and Airpush/StopSMS, using the\nKronoDroid dataset. After addressing generation inconsistencies with prompt\nengineering and post-processing, we evaluate multiple classifiers under three\nsettings: training with real data only, real-plus-synthetic data, and synthetic\ndata alone. Results show that real-only training achieves near perfect\ndetection, while augmentation with synthetic data preserves high performance\nwith only minor degradations. In contrast, synthetic-only training produces\nmixed outcomes, with effectiveness varying across malware families and\nfine-tuning strategies. These findings suggest that LLM-generated malware can\nenhance scarce datasets without compromising detection accuracy, but remains\ninsufficient as a standalone training source."}
{"id": "2510.02395", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.02395", "abs": "https://arxiv.org/abs/2510.02395", "authors": ["Hongbo Liu", "Jiannong Cao", "Bo Yang", "Dongbin Bai", "Yinfeng Cao", "Xiaoming Shen", "Yinan Zhang", "Jinwen Liang", "Shan Jiang", "Mingjin Zhang"], "title": "PolyLink: A Blockchain Based Decentralized Edge AI Platform for LLM Inference", "comment": null, "summary": "The rapid advancement of large language models (LLMs) in recent years has\nrevolutionized the AI landscape. However, the deployment model and usage of LLM\nservices remain highly centralized, creating significant trust issues and costs\nfor end users and developers. To address these issues, we propose PolyLink, a\nblockchain-based decentralized AI platform that decentralizes LLM development\nand inference. Specifically, PolyLink introduces a decentralized crowdsourcing\narchitecture that supports single-device and cross-device model deployment and\ninference across heterogeneous devices at the edge. Moreover, to ensure the\ninference integrity, we design the TIQE protocol, which combines a lightweight\ncross-encoder model and an LLM-as-a-Judge for a high-accuracy inference\nevaluation. Lastly, we integrate a comprehensive token-based incentive model\nwith dynamic pricing and reward mechanisms for all participants. We have\ndeployed PolyLink and conducted an extensive real-world evaluation through\ngeo-distributed deployment across heterogeneous devices. Results indicate that\nthe inference and verification latency is practical. Our security analysis\ndemonstrates that the system is resistant to model degradation attacks and\nvalidator corruptions. PolyLink is now available at\nhttps://github.com/IMCL-PolyLink/PolyLink."}
{"id": "2510.02422", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02422", "abs": "https://arxiv.org/abs/2510.02422", "authors": ["Kedong Xiu", "Churui Zeng", "Tianhang Zheng", "Xinzhe Huang", "Xiaojun Jia", "Di Wang", "Puning Zhao", "Zhan Qin", "Kui Ren"], "title": "Dynamic Target Attack", "comment": null, "summary": "Existing gradient-based jailbreak attacks typically optimize an adversarial\nsuffix to induce a fixed affirmative response. However, this fixed target\nusually resides in an extremely low-density region of a safety-aligned LLM's\noutput distribution conditioned on diverse harmful inputs. Due to the\nsubstantial discrepancy between the target and the original output, existing\nattacks require numerous iterations to optimize the adversarial prompt, which\nmight still fail to induce the low-probability target response from the target\nLLM. In this paper, we propose Dynamic Target Attack (DTA), a new jailbreaking\nframework relying on the target LLM's own responses as targets to optimize the\nadversarial prompts. In each optimization round, DTA iteratively samples\nmultiple candidate responses directly from the output distribution conditioned\non the current prompt, and selects the most harmful response as a temporary\ntarget for prompt optimization. In contrast to existing attacks, DTA\nsignificantly reduces the discrepancy between the target and the output\ndistribution, substantially easing the optimization process to search for an\neffective adversarial prompt.\n  Extensive experiments demonstrate the superior effectiveness and efficiency\nof DTA: under the white-box setting, DTA only needs 200 optimization iterations\nto achieve an average attack success rate (ASR) of over 87\\% on recent\nsafety-aligned LLMs, exceeding the state-of-the-art baselines by over 15\\%. The\ntime cost of DTA is 2-26 times less than existing baselines. Under the\nblack-box setting, DTA uses Llama-3-8B-Instruct as a surrogate model for target\nsampling and achieves an ASR of 85\\% against the black-box target model\nLlama-3-70B-Instruct, exceeding its counterparts by over 25\\%."}
{"id": "2510.02424", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02424", "abs": "https://arxiv.org/abs/2510.02424", "authors": ["Basil Abdullah AL-Zahrani"], "title": "Adaptive Deception Framework with Behavioral Analysis for Enhanced Cybersecurity Defense", "comment": "5 pages, 5 tables, 1 figure", "summary": "This paper presents CADL (Cognitive-Adaptive Deception Layer), an adaptive\ndeception framework achieving 99.88% detection rate with 0.13% false positive\nrate on the CICIDS2017 dataset. The framework employs ensemble machine learning\n(Random Forest, XGBoost, Neural Networks) combined with behavioral profiling to\nidentify and adapt responses to network intrusions. Through a coordinated\nsignal bus architecture, security components share real-time intelligence,\nenabling collective decision-making. The system profiles attackers based on\ntemporal patterns and deploys customized deception strategies across five\nescalation levels. Evaluation on 50,000 CICIDS2017 test samples demonstrates\nthat CADL significantly outperforms traditional intrusion detection systems\n(Snort: 71.2%, Suricata: 68.5%) while maintaining production-ready false\npositive rates. The framework's behavioral analysis achieves 89% accuracy in\nclassifying attacker profiles. We provide open-source implementation and\ntransparent performance metrics, offering an accessible alternative to\ncommercial deception platforms costing $150-400 per host annually."}
{"id": "2510.02475", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2510.02475", "abs": "https://arxiv.org/abs/2510.02475", "authors": ["Weihang Li", "Pete Crowley", "Arya Tschand", "Yu Wang", "Miroslav Pajic", "Daniel Sorin"], "title": "Rigorous Evaluation of Microarchitectural Side-Channels with Statistical Model Checking", "comment": null, "summary": "Rigorous quantitative evaluation of microarchitectural side channels is\nchallenging for two reasons. First, the processors, attacks, and defenses often\nexhibit probabilistic behaviors. These probabilistic behaviors arise due to\nnatural noise in systems (e.g., from co-running processes), probabilistic side\nchannel attacks, and probabilistic obfuscation defenses. Second,\nmicroprocessors are extremely complex. Previous evaluation methods have relied\non abstract or simplified models, which are necessarily less detailed than real\nsystems or cycle-by-cycle simulators, and these models may miss important\nphenomena. Whereas a simple model may suffice for estimating performance,\nsecurity issues frequently manifest in the details.\n  We address this challenge by introducing Statistical Model Checking (SMC) to\nthe quantitative evaluation of microarchitectural side channels. SMC is a\nrigorous statistical technique that can process the results of probabilistic\nexperiments and provide statistical guarantees, and it has been used in\ncomputing applications that depend heavily on statistical guarantees (e.g.,\nmedical implants, vehicular computing). With SMC, we can treat processors as\nopaque boxes, and we do not have to abstract or simplify them. We demonstrate\nthe effectiveness of SMC through three case studies, in which we experimentally\nshow that SMC can evaluate existing security vulnerabilities and defenses and\nprovide qualitatively similar conclusions with greater statistical rigor, while\nmaking no simplifying assumptions or abstractions. We also show that SMC can\nenable a defender to quantify the amount of noise necessary to have a desired\nlevel of confidence that she has reduced an attacker's probability of success\nto less than a desired threshold, thus providing the defender with an\nactionable plan for obfuscation via noise injection."}
{"id": "2510.02519", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.02519", "abs": "https://arxiv.org/abs/2510.02519", "authors": ["Atonu Ghosh", "Akhilesh Mohanasundaram", "Srishivanth R F", "Sudip Misra"], "title": "TLoRa: Implementing TLS Over LoRa for Secure HTTP Communication in IoT", "comment": "10 pages", "summary": "We present TLoRa, an end-to-end architecture for HTTPS communication over\nLoRa by integrating TCP tunneling and a complete TLS 1.3 handshake. It enables\na seamless and secure communication channel between WiFi-enabled end devices\nand the Internet over LoRa using an End Hub (EH) and a Net Relay (NR). The EH\ntethers a WiFi hotspot and a captive portal for user devices to connect and\nrequest URLs. The EH forwards the requested URLs to the NR using a secure\ntunnel over LoRa. The NR, which acts as a server-side proxy, receives and\nresolves the request from the Internet-based server. It then relays back the\nencrypted response from the server over the same secure tunnel. TLoRa operates\nin three phases -session setup, secure tunneling, and rendering. In the first\nphase, it manages the TCP socket and initiates the TLS handshake. In the\nsecond, it creates a secure tunnel and transfers encrypted TLS data over LoRa.\nFinally, it delivers the URL content to the user. TLoRa also implements a\nlightweight TLS record reassembly layer and a queuing mechanism for session\nmultiplexing. We evaluate TLoRa on real hardware using multiple accesses to a\nweb API. Results indicate that it provides a practical solution by successfully\nestablishing a TLS session over LoRa in 9.9 seconds and takes 3.58 seconds to\nfulfill API requests. To the best of our knowledge, this is the first work to\ncomprehensively design, implement, and evaluate the performance of HTTPS access\nover LoRa using full TLS."}
{"id": "2510.02554", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02554", "abs": "https://arxiv.org/abs/2510.02554", "authors": ["Jonathan Sneh", "Ruomei Yan", "Jialin Yu", "Philip Torr", "Yarin Gal", "Sunando Sengupta", "Eric Sommerlade", "Alasdair Paren", "Adel Bibi"], "title": "ToolTweak: An Attack on Tool Selection in LLM-based Agents", "comment": null, "summary": "As LLMs increasingly power agents that interact with external tools, tool use\nhas become an essential mechanism for extending their capabilities. These\nagents typically select tools from growing databases or marketplaces to solve\nuser tasks, creating implicit competition among tool providers and developers\nfor visibility and usage. In this paper, we show that this selection process\nharbors a critical vulnerability: by iteratively manipulating tool names and\ndescriptions, adversaries can systematically bias agents toward selecting\nspecific tools, gaining unfair advantage over equally capable alternatives. We\npresent ToolTweak, a lightweight automatic attack that increases selection\nrates from a baseline of around 20% to as high as 81%, with strong\ntransferability between open-source and closed-source models. Beyond individual\ntools, we show that such attacks cause distributional shifts in tool usage,\nrevealing risks to fairness, competition, and security in emerging tool\necosystems. To mitigate these risks, we evaluate two defenses: paraphrasing and\nperplexity filtering, which reduce bias and lead agents to select functionally\nsimilar tools more equally. All code will be open-sourced upon acceptance."}
{"id": "2510.02563", "categories": ["cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.02563", "abs": "https://arxiv.org/abs/2510.02563", "authors": ["Chenpei Huang", "Lingfeng Yao", "Hui Zhong", "Kyu In Lee", "Lan Zhang", "Xiaoyong Yuan", "Tomoaki Ohtsuki", "Miao Pan"], "title": "Who's Wearing? Ear Canal Biometric Key Extraction for User Authentication on Wireless Earbuds", "comment": null, "summary": "Ear canal scanning/sensing (ECS) has emerged as a novel biometric\nauthentication method for mobile devices paired with wireless earbuds. Existing\nstudies have demonstrated the uniqueness of ear canals by training and testing\nmachine learning classifiers on ECS data. However, implementing practical\nECS-based authentication requires preventing raw biometric data leakage and\ndesigning computationally efficient protocols suitable for resource-constrained\nearbuds. To address these challenges, we propose an ear canal key extraction\nprotocol, \\textbf{EarID}. Without relying on classifiers, EarID extracts unique\nbinary keys directly on the earbuds during authentication. These keys further\nallow the use of privacy-preserving fuzzy commitment scheme that verifies the\nwearer's key on mobile devices. Our evaluation results demonstrate that EarID\nachieves a 98.7\\% authentication accuracy, comparable to machine learning\nclassifiers. The mobile enrollment time (160~ms) and earbuds processing time\n(226~ms) are negligible in terms of wearer's experience. Moreover, our approach\nis robust and attack-resistant, maintaining a false acceptance rate below 1\\%\nacross all adversarial scenarios. We believe the proposed EarID offers a\npractical and secure solution for next-generation wireless earbuds."}
{"id": "2510.02643", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.02643", "abs": "https://arxiv.org/abs/2510.02643", "authors": ["Jack Garrard", "John F. Hardy II", "Carlo daCunha", "Mayank Bakshi"], "title": "Using Preformed Resistive Random Access Memory to Create a Strong Physically Unclonable Function", "comment": null, "summary": "Physically Unclonable Functions (PUFs) are a promising solution for identity\nverification and asymmetric encryption. In this paper, a new Resistive Random\nAccess Memory (ReRAM) PUF-based protocol is presented to create a physical\nReRAM PUF with a large challenge space. This protocol uses differential reads\nfrom unformed ReRAM as the method for response generation. Lastly, this paper\nalso provides an experimental hardware demonstration of this protocol on a\nPhysical ReRAM device, along with providing notable results as a PUF, with\nexcellent performance characteristics."}
{"id": "2510.02694", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.02694", "abs": "https://arxiv.org/abs/2510.02694", "authors": ["Bowei Ning", "Xuejun Zong", "Kan He"], "title": "MALF: A Multi-Agent LLM Framework for Intelligent Fuzzing of Industrial Control Protocols", "comment": null, "summary": "Industrial control systems (ICS) are vital to modern infrastructure but\nincreasingly vulnerable to cybersecurity threats, particularly through\nweaknesses in their communication protocols. This paper presents MALF\n(Multi-Agent LLM Fuzzing Framework), an advanced fuzzing solution that\nintegrates large language models (LLMs) with multi-agent coordination to\nidentify vulnerabilities in industrial control protocols (ICPs). By leveraging\nRetrieval-Augmented Generation (RAG) for domain-specific knowledge and QLoRA\nfine-tuning for protocol-aware input generation, MALF enhances fuzz testing\nprecision and adaptability. The multi-agent framework optimizes seed\ngeneration, mutation strategies, and feedback-driven refinement, leading to\nimproved vulnerability discovery. Experiments on protocols like Modbus/TCP,\nS7Comm, and Ethernet/IP demonstrate that MALF surpasses traditional methods,\nachieving a test case pass rate (TCPR) of 88-92% and generating more exception\ntriggers (ETN). MALF also maintains over 90% seed coverage and Shannon entropy\nvalues between 4.2 and 4.6 bits, ensuring diverse, protocol-compliant\nmutations. Deployed in a real-world Industrial Attack-Defense Range for power\nplants, MALF identified critical vulnerabilities, including three zero-day\nflaws, one confirmed and registered by CNVD. These results validate MALF's\neffectiveness in real-world fuzzing applications. This research highlights the\ntransformative potential of multi-agent LLMs in ICS cybersecurity, offering a\nscalable, automated framework that sets a new standard for vulnerability\ndiscovery and strengthens critical infrastructure security against emerging\nthreats."}
{"id": "2510.02707", "categories": ["cs.CR", "cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.02707", "abs": "https://arxiv.org/abs/2510.02707", "authors": ["Chinthana Wimalasuriya", "Spyros Tragoudas"], "title": "A Statistical Method for Attack-Agnostic Adversarial Attack Detection with Compressive Sensing Comparison", "comment": null, "summary": "Adversarial attacks present a significant threat to modern machine learning\nsystems. Yet, existing detection methods often lack the ability to detect\nunseen attacks or detect different attack types with a high level of accuracy.\nIn this work, we propose a statistical approach that establishes a detection\nbaseline before a neural network's deployment, enabling effective real-time\nadversarial detection. We generate a metric of adversarial presence by\ncomparing the behavior of a compressed/uncompressed neural network pair. Our\nmethod has been tested against state-of-the-art techniques, and it achieves\nnear-perfect detection across a wide range of attack types. Moreover, it\nsignificantly reduces false positives, making it both reliable and practical\nfor real-world applications."}
{"id": "2510.02833", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.02833", "abs": "https://arxiv.org/abs/2510.02833", "authors": ["Zhixin Xie", "Xurui Song", "Jun Luo"], "title": "Attack via Overfitting: 10-shot Benign Fine-tuning to Jailbreak LLMs", "comment": null, "summary": "Despite substantial efforts in safety alignment, recent research indicates\nthat Large Language Models (LLMs) remain highly susceptible to jailbreak\nattacks. Among these attacks, finetuning-based ones that compromise LLMs'\nsafety alignment via fine-tuning stand out due to its stable jailbreak\nperformance. In particular, a recent study indicates that fine-tuning with as\nfew as 10 harmful question-answer (QA) pairs can lead to successful\njailbreaking across various harmful questions. However, such malicious\nfine-tuning attacks are readily detectable and hence thwarted by moderation\nmodels. In this paper, we demonstrate that LLMs can be jailbroken by\nfine-tuning with only 10 benign QA pairs; our attack exploits the increased\nsensitivity of LLMs to fine-tuning data after being overfitted. Specifically,\nour fine-tuning process starts with overfitting an LLM via fine-tuning with\nbenign QA pairs involving identical refusal answers. Further fine-tuning is\nthen performed with standard benign answers, causing the overfitted LLM to\nforget the refusal attitude and thus provide compliant answers regardless of\nthe harmfulness of a question. We implement our attack on the ten LLMs and\ncompare it with five existing baselines. Experiments demonstrate that our\nmethod achieves significant advantages in both attack effectiveness and attack\nstealth. Our findings expose previously unreported security vulnerabilities in\ncurrent LLMs and provide a new perspective on understanding how LLMs' security\nis compromised, even with benign fine-tuning. Our code is available at\nhttps://github.com/ZHIXINXIE/tenBenign."}
{"id": "2510.02944", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.02944", "abs": "https://arxiv.org/abs/2510.02944", "authors": ["Kel Zin Tan", "Prashant Nalini Vasudevan"], "title": "Improved Search-to-Decision Reduction for Random Local Functions", "comment": null, "summary": "A random local function defined by a $d$-ary predicate $P$ is one where each\noutput bit is computed by applying $P$ to $d$ randomly chosen bits of its\ninput. These represent natural distributions of instances for constraint\nsatisfaction problems. They were put forward by Goldreich as candidates for\nlow-complexity one-way functions, and have subsequently been widely studied\nalso as potential pseudo-random generators.\n  We present a new search-to-decision reduction for random local functions\ndefined by any predicate of constant arity. Given any efficient algorithm that\ncan distinguish, with advantage $\\epsilon$, the output of a random local\nfunction with $m$ outputs and $n$ inputs from random, our reduction produces an\nefficient algorithm that can invert such functions with\n$\\tilde{O}(m(n/\\epsilon)^2)$ outputs, succeeding with probability\n$\\Omega(\\epsilon)$. This implies that if a family of local functions is\none-way, then a related family with shorter output length is family of\npseudo-random generators.\n  Prior to our work, all such reductions that were known required the predicate\nto have additional sensitivity properties, whereas our reduction works for any\npredicate. Our results also generalise to some super-constant values of the\narity $d$, and to noisy predicates."}
{"id": "2510.02947", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.02947", "abs": "https://arxiv.org/abs/2510.02947", "authors": ["Aikaterini-Panagiota Stouka", "Conor McMenamin", "Demetris Kyriacou", "Lin Oshitani", "Quentin Botha"], "title": "SoK: Preconfirmations", "comment": "The latest version of this document is hosted on GitHub at:\n  https://github.com/NethermindEth/sok-preconfirmations", "summary": "In recent years, significant research efforts have focused on improving\nblockchain throughput and confirmation speeds without compromising security.\nWhile decreasing the time it takes for a transaction to be included in the\nblockchain ledger enhances user experience, a fundamental delay still remains\nbetween when a transaction is issued by a user and when its inclusion is\nconfirmed in the blockchain ledger. This delay limits user experience gains\nthrough the confirmation uncertainty it brings for users. This inherent delay\nin conventional blockchain protocols has led to the emergence of\npreconfirmation protocols -- protocols that provide users with early guarantees\nof eventual transaction confirmation.\n  This article presents a Systematization of Knowledge (SoK) on\npreconfirmations. We present the core terms and definitions needed to\nunderstand preconfirmations, outline a general framework for preconfirmation\nprotocols, and explore the economics and risks of preconfirmations. Finally, we\nsurvey and apply our framework to several implementations of real-world\npreconfirmation protocols, bridging the gap between theory and practice."}
{"id": "2510.02960", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.02960", "abs": "https://arxiv.org/abs/2510.02960", "authors": ["Khaled Serag", "Zhaozhou Tang", "Sungwoo Kim", "Vireshwar Kumar", "Dave", "Tian", "Saman Zonouz", "Raheem Beyah", "Dongyan Xu", "Z. Berkay Celik"], "title": "SoK: Kicking CAN Down the Road. Systematizing CAN Security Knowledge", "comment": null, "summary": "For decades, the Controller Area Network (CAN) has served as the primary\nin-vehicle bus (IVB) and extended its use to many non-vehicular systems. Over\nthe past years, CAN security has been intensively scrutinized, yielding\nextensive research literature. Despite its wealth, the literature lacks\nstructured systematization, complicating efforts to assess attack severity,\ndefense efficacy, identify security gaps, or root causes. This leaves non\nexperts uncertain about the relevancy of specific attacks or defenses to their\nsystems, inadvertently portraying CAN as irredeemably insecure. Further, the\nintroduction of new IVB technologies--CAN evolutions, add-ons, and alternative\nbuses--with heightened security claims risks fostering the misconception that\nmerely adopting these technologies resolves CAN's security challenges.\n  This paper systematizes existing CAN security knowledge, presenting a\ncomprehensive taxonomy and assessment models of attackers, attacks, and\ndefenses. It identifies replicable attacks and defense gaps, investigating\ntheir root causes as inherent, accidental, unique, or universal. It then\nextrapolates these insights to emerging IVB technologies by formally analyzing\nthree emerging IVBs to identify shared root causes with CAN and assess their\nability to close security gaps. The findings challenge common perceptions,\ndemonstrating that CAN is more securable than perceived, that most insecurity\nroot causes are shared across IVBs, and that merely adopting newer IVB\ntechnology does not solve persistent security issues. The paper concludes by\nhighlighting future research directions to secure IVB communication down the\nroad."}
{"id": "2510.02964", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.02964", "abs": "https://arxiv.org/abs/2510.02964", "authors": ["Yu He", "Yifei Chen", "Yiming Li", "Shuo Shao", "Leyi Qi", "Boheng Li", "Dacheng Tao", "Zhan Qin"], "title": "External Data Extraction Attacks against Retrieval-Augmented Large Language Models", "comment": null, "summary": "In recent years, RAG has emerged as a key paradigm for enhancing large\nlanguage models (LLMs). By integrating externally retrieved information, RAG\nalleviates issues like outdated knowledge and, crucially, insufficient domain\nexpertise. While effective, RAG introduces new risks of external data\nextraction attacks (EDEAs), where sensitive or copyrighted data in its\nknowledge base may be extracted verbatim. These risks are particularly acute\nwhen RAG is used to customize specialized LLM applications with private\nknowledge bases. Despite initial studies exploring these risks, they often lack\na formalized framework, robust attack performance, and comprehensive\nevaluation, leaving critical questions about real-world EDEA feasibility\nunanswered.\n  In this paper, we present the first comprehensive study to formalize EDEAs\nagainst retrieval-augmented LLMs. We first formally define EDEAs and propose a\nunified framework decomposing their design into three components: extraction\ninstruction, jailbreak operator, and retrieval trigger, under which prior\nattacks can be considered instances within our framework. Guided by this\nframework, we develop SECRET: a Scalable and EffeCtive exteRnal data Extraction\naTtack. Specifically, SECRET incorporates (1) an adaptive optimization process\nusing LLMs as optimizers to generate specialized jailbreak prompts for EDEAs,\nand (2) cluster-focused triggering, an adaptive strategy that alternates\nbetween global exploration and local exploitation to efficiently generate\neffective retrieval triggers. Extensive evaluations across 4 models reveal that\nSECRET significantly outperforms previous attacks, and is highly effective\nagainst all 16 tested RAG instances. Notably, SECRET successfully extracts 35%\nof the data from RAG powered by Claude 3.7 Sonnet for the first time, whereas\nother attacks yield 0% extraction. Our findings call for attention to this\nemerging threat."}
{"id": "2510.02999", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02999", "abs": "https://arxiv.org/abs/2510.02999", "authors": ["Xinzhe Huang", "Wenjing Hu", "Tianhang Zheng", "Kedong Xiu", "Xiaojun Jia", "Di Wang", "Zhan Qin", "Kui Ren"], "title": "Untargeted Jailbreak Attack", "comment": null, "summary": "Existing gradient-based jailbreak attacks on Large Language Models (LLMs),\nsuch as Greedy Coordinate Gradient (GCG) and COLD-Attack, typically optimize\nadversarial suffixes to align the LLM output with a predefined target response.\nHowever, by restricting the optimization objective as inducing a predefined\ntarget, these methods inherently constrain the adversarial search space, which\nlimit their overall attack efficacy. Furthermore, existing methods typically\nrequire a large number of optimization iterations to fulfill the large gap\nbetween the fixed target and the original model response, resulting in low\nattack efficiency.\n  To overcome the limitations of targeted jailbreak attacks, we propose the\nfirst gradient-based untargeted jailbreak attack (UJA), aiming to elicit an\nunsafe response without enforcing any predefined patterns. Specifically, we\nformulate an untargeted attack objective to maximize the unsafety probability\nof the LLM response, which can be quantified using a judge model. Since the\nobjective is non-differentiable, we further decompose it into two\ndifferentiable sub-objectives for optimizing an optimal harmful response and\nthe corresponding adversarial prompt, with a theoretical analysis to validate\nthe decomposition. In contrast to targeted jailbreak attacks, UJA's\nunrestricted objective significantly expands the search space, enabling a more\nflexible and efficient exploration of LLM vulnerabilities.Extensive evaluations\ndemonstrate that \\textsc{UJA} can achieve over 80\\% attack success rates\nagainst recent safety-aligned LLMs with only 100 optimization iterations,\noutperforming the state-of-the-art gradient-based attacks such as I-GCG and\nCOLD-Attack by over 20\\%."}
{"id": "2510.03035", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.03035", "abs": "https://arxiv.org/abs/2510.03035", "authors": ["Lambert Hogenhout", "Rinzin Wangmo"], "title": "Protecting Persona Biometric Data: The Case of Facial Privacy", "comment": null, "summary": "The proliferation of digital technologies has led to unprecedented data\ncollection, with facial data emerging as a particularly sensitive commodity.\nCompanies are increasingly leveraging advanced facial recognition technologies,\noften without the explicit consent or awareness of individuals, to build\nsophisticated surveillance capabilities. This practice, fueled by weak and\nfragmented laws in many jurisdictions, has created a regulatory vacuum that\nallows for the commercialization of personal identity and poses significant\nthreats to individual privacy and autonomy. This article introduces the concept\nof Facial Privacy. It analyzes the profound challenges posed by unregulated\nfacial recognition by conducting a comprehensive review of existing legal\nframeworks. It examines and compares regulations such as the GDPR, Brazil's\nLGPD, Canada's PIPEDA, and privacy acts in China, Singapore, South Korea, and\nJapan, alongside sector-specific laws in the United States like the Illinois\nBiometric Information Privacy Act (BIPA). The analysis highlights the societal\nimpacts of this technology, including the potential for discriminatory bias and\nthe long-lasting harm that can result from the theft of immutable biometric\ndata. Ultimately, the paper argues that existing legal loopholes and\nambiguities leave individuals vulnerable. It proposes a new policy framework\nthat shifts the paradigm from data as property to a model of inalienable\nrights, ensuring that fundamental human rights are upheld against unchecked\ntechnological expansion."}
{"id": "2510.03219", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03219", "abs": "https://arxiv.org/abs/2510.03219", "authors": ["Al Nahian Bin Emran", "Rajendra Upadhyay", "Rajendra Paudyal", "Lisa Donnan", "Duminda Wijesekera"], "title": "TPM-Based Continuous Remote Attestation and Integrity Verification for 5G VNFs on Kubernetes", "comment": null, "summary": "In the rapidly evolving landscape of 5G technology, the adoption of\ncloud-based infrastructure for the deployment of 5G services has become\nincreasingly common. Using a service-based architecture, critical 5G\ncomponents, such as the Access and Mobility Management Function (AMF), Session\nManagement Function (SMF), and User Plane Function (UPF), now run as\ncontainerized pods on Kubernetes clusters. Although this approach improves\nscalability, flexibility, and resilience, it also introduces new security\nchallenges, particularly to ensure the integrity and trustworthiness of these\ncomponents. Current 5G security specifications (for example, 3GPP TS 33.501)\nfocus on communication security and assume that network functions remain\ntrustworthy after authentication, consequently lacking mechanisms to\ncontinuously validate the integrity of NVFs at runtime. To close this gap, and\nto align with Zero Trust principles of 'never trust, always verify', we present\na TPM 2.0-based continuous remote attestation solution for core 5G components\ndeployed on Kubernetes. Our approach uses the Linux Integrity Measurement\nArchitecture (IMA) and a Trusted Platform Module (TPM) to provide\nhardware-based runtime validation. We integrate the open-source Keylime\nframework with a custom IMA template that isolates pod-level measurements,\nallowing per-pod integrity verification. A prototype on a k3s cluster\n(consisting of 1 master, 2 worker nodes) was implemented to attest to core\nfunctions, including AMF, SMF and UPF. The experimental results show that the\nsystem detects unauthorized modifications in real time, labels each pod's trust\nstate, and generates detailed audit logs. This work provides hardware-based\ncontinuous attestation for cloud native and edge deployments, strengthening the\nresilience of 5G as critical infrastructure in multi-vendor and\nmission-critical scenarios of 5G."}
