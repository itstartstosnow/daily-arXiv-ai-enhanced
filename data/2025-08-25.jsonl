{"id": "2508.15776", "categories": ["cs.CR", "cs.CE"], "pdf": "https://arxiv.org/pdf/2508.15776", "abs": "https://arxiv.org/abs/2508.15776", "authors": ["Saeid Ghasemshirazi", "Ghazaleh Shirvani", "Marziye Ranjbar Tavakoli", "Bahar Ghaedi", "Mohammad Amin Langarizadeh"], "title": "Implementing Zero Trust Architecture to Enhance Security and Resilience in the Pharmaceutical Supply Chain", "comment": null, "summary": "The pharmaceutical supply chain faces escalating cybersecurity challenges\nthreatening patient safety and operational continuity. This paper examines the\ntransformative potential of zero trust architecture for enhancing security and\nresilience within this critical ecosystem. We explore the challenges posed by\ndata breaches, counterfeiting, and disruptions and introduce the principles of\ncontinuous verification, least-privilege access, and data-centric security\ninherent in zero trust. Real-world case studies illustrate successful\nimplementations. Benefits include heightened security, data protection, and\nadaptable resilience. As recognized by researchers and industrialists, a\nreliable drug tracing system is crucial for ensuring drug safety throughout the\npharmaceutical production process. One of the most pivotal domains within the\npharmaceutical industry and its associated supply chains where zero trust can\nbe effectively implemented is in the management of narcotics, high-health-risk\ndrugs, and abusable substances. By embracing zero trust, the pharmaceutical\nindustry fortifies its supply chain against constantly changing cyber threats,\nensuring the trustworthiness of critical medical operations."}
{"id": "2508.15778", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.15778", "abs": "https://arxiv.org/abs/2508.15778", "authors": ["Yifan Liao", "Yuxin Cao", "Yedi Zhang", "Wentao He", "Yan Xiao", "Xianglong Du", "Zhiyong Huang", "Jin Song Dong"], "title": "Towards Stealthy and Effective Backdoor Attacks on Lane Detection: A Naturalistic Data Poisoning Approach", "comment": "12 pages,7 figures", "summary": "Deep learning-based lane detection (LD) plays a critical role in autonomous\ndriving and advanced driver assistance systems. However, its vulnerability to\nbackdoor attacks presents a significant security concern. Existing backdoor\nattack methods on LD often exhibit limited practical utility due to the\nartificial and conspicuous nature of their triggers. To address this limitation\nand investigate the impact of more ecologically valid backdoor attacks on LD\nmodels, we examine the common data poisoning attack and introduce DBALD, a\nnovel diffusion-based data poisoning framework for generating naturalistic\nbackdoor triggers. DBALD comprises two key components: optimal trigger position\nfinding and stealthy trigger generation. Given the insight that attack\nperformance varies depending on the trigger position, we propose a\nheatmap-based method to identify the optimal trigger location, with gradient\nanalysis to generate attack-specific heatmaps. A region-based editing diffusion\nprocess is then applied to synthesize visually plausible triggers within the\nmost susceptible regions identified previously. Furthermore, to ensure scene\nintegrity and stealthy attacks, we introduce two loss strategies: one for\npreserving lane structure and another for maintaining the consistency of the\ndriving scene. Consequently, compared to existing attack methods, DBALD\nachieves both a high attack success rate and superior stealthiness. Extensive\nexperiments on 4 mainstream LD models show that DBALD exceeds state-of-the-art\nmethods, with an average success rate improvement of +10.87% and significantly\nenhanced stealthiness. The experimental results highlight significant practical\nchallenges in ensuring model robustness against real-world backdoor threats in\nLD."}
{"id": "2508.15808", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15808", "abs": "https://arxiv.org/abs/2508.15808", "authors": ["Benjamin Murphy", "Twm Stone"], "title": "Uplifted Attackers, Human Defenders: The Cyber Offense-Defense Balance for Trailing-Edge Organizations", "comment": null, "summary": "Advances in AI are widely understood to have implications for cybersecurity.\nArticles have emphasized the effect of AI on the cyber offense-defense balance,\nand commentators can be found arguing either that cyber will privilege\nattackers or defenders. For defenders, arguments are often made that AI will\nenable solutions like formal verification of all software--and for some\nwell-equipped companies, this may be true. This conversation, however, does not\nmatch the reality for most companies. \"Trailing-edge organizations,\" as we term\nthem, rely heavily on legacy software, poorly staff security roles, and\nstruggle to implement best practices like rapid deployment of security patches.\nThese decisions may be the result of corporate inertia, but may also be the\nresult of a seemingly-rational calculation that attackers may not bother\ntargeting a firm due to lack of economic incentives, and as a result,\nunderinvestment in defense will not be punished.\n  This approach to security may have been sufficient prior to the development\nof AI systems, but it is unlikely to remain viable in the near future. We argue\nthat continuing improvements in AI's capabilities poses additional risks on two\nfronts: First, increased usage of AI will alter the economics of the marginal\ncyberattack and expose these trailing-edge organizations to more attackers,\nmore frequently. Second, AI's advances will enable attackers to develop\nexploits and launch attacks earlier than they can today--meaning that it is\ninsufficient for these companies to attain parity with today's leading\ndefenders, but must instead aim for faster remediation timelines and more\nresilient software. The situation today portends a dramatically increased\nnumber of attacks in the near future. Moving forward, we offer a range of\nsolutions for both organizations and governments to improve the defensive\nposture of firms which lag behind their peers today."}
{"id": "2508.15839", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15839", "abs": "https://arxiv.org/abs/2508.15839", "authors": ["Yuksel Aydin"], "title": "CIA+TA Risk Assessment for AI Reasoning Vulnerabilities", "comment": null, "summary": "As AI systems increasingly influence critical decisions, they face threats\nthat exploit reasoning mechanisms rather than technical infrastructure. We\npresent a framework for cognitive cybersecurity, a systematic protection of AI\nreasoning processes from adversarial manipulation. Our contributions are\nthreefold. First, we establish cognitive cybersecurity as a discipline\ncomplementing traditional cybersecurity and AI safety, addressing\nvulnerabilities where legitimate inputs corrupt reasoning while evading\nconventional controls. Second, we introduce the CIA+TA, extending traditional\nConfidentiality, Integrity, and Availability triad with Trust (epistemic\nvalidation) and Autonomy (human agency preservation), requirements unique to\nsystems generating knowledge claims and mediating decisions. Third, we present\na quantitative risk assessment methodology with empirically-derived\ncoefficients, enabling organizations to measure cognitive security risks. We\nmap our framework to OWASP LLM Top 10 and MITRE ATLAS, facilitating operational\nintegration. Validation through previously published studies (151 human\nparticipants; 12,180 AI trials) reveals strong architecture dependence:\nidentical defenses produce effects ranging from 96% reduction to 135%\namplification of vulnerabilities. This necessitates pre-deployment Cognitive\nPenetration Testing as a governance requirement for trustworthy AI deployment."}
{"id": "2508.15840", "categories": ["cs.CR", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.15840", "abs": "https://arxiv.org/abs/2508.15840", "authors": ["Robert Dilworth"], "title": "Unveiling Unicode's Unseen Underpinnings in Undermining Authorship Attribution", "comment": null, "summary": "When using a public communication channel -- whether formal or informal, such\nas commenting or posting on social media -- end users have no expectation of\nprivacy: they compose a message and broadcast it for the world to see. Even if\nan end user takes utmost precautions to anonymize their online presence --\nusing an alias or pseudonym; masking their IP address; spoofing their\ngeolocation; concealing their operating system and user agent; deploying\nencryption; registering with a disposable phone number or email; disabling\nnon-essential settings; revoking permissions; and blocking cookies and\nfingerprinting -- one obvious element still lingers: the message itself.\nAssuming they avoid lapses in judgment or accidental self-exposure, there\nshould be little evidence to validate their actual identity, right? Wrong. The\ncontent of their message -- necessarily open for public consumption -- exposes\nan attack vector: stylometric analysis, or author profiling. In this paper, we\ndissect the technique of stylometry, discuss an antithetical counter-strategy\nin adversarial stylometry, and devise enhancements through Unicode\nsteganography."}
{"id": "2508.15848", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15848", "abs": "https://arxiv.org/abs/2508.15848", "authors": ["Yinghan Zhou", "Juan Wen", "Wanli Peng", "Zhengxian Wu", "Ziwei Zhang", "Yiming Xue"], "title": "Self-Disguise Attack: Induce the LLM to disguise itself for AIGT detection evasion", "comment": null, "summary": "AI-generated text (AIGT) detection evasion aims to reduce the detection\nprobability of AIGT, helping to identify weaknesses in detectors and enhance\ntheir effectiveness and reliability in practical applications. Although\nexisting evasion methods perform well, they suffer from high computational\ncosts and text quality degradation. To address these challenges, we propose\nSelf-Disguise Attack (SDA), a novel approach that enables Large Language Models\n(LLM) to actively disguise its output, reducing the likelihood of detection by\nclassifiers. The SDA comprises two main components: the adversarial feature\nextractor and the retrieval-based context examples optimizer. The former\ngenerates disguise features that enable LLMs to understand how to produce more\nhuman-like text. The latter retrieves the most relevant examples from an\nexternal knowledge base as in-context examples, further enhancing the\nself-disguise ability of LLMs and mitigating the impact of the disguise process\non the diversity of the generated text. The SDA directly employs prompts\ncontaining disguise features and optimized context examples to guide the LLM in\ngenerating detection-resistant text, thereby reducing resource consumption.\nExperimental results demonstrate that the SDA effectively reduces the average\ndetection accuracy of various AIGT detectors across texts generated by three\ndifferent LLMs, while maintaining the quality of AIGT."}
{"id": "2508.15850", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15850", "abs": "https://arxiv.org/abs/2508.15850", "authors": ["Ziyu Wang", "Elahe Khatibi", "Farshad Firouzi", "Sanaz Rahimi Mousavi", "Krishnendu Chakrabarty", "Amir M. Rahmani"], "title": "Linkage Attacks Expose Identity Risks in Public ECG Data Sharing", "comment": null, "summary": "The increasing availability of publicly shared electrocardiogram (ECG) data\nraises critical privacy concerns, as its biometric properties make individuals\nvulnerable to linkage attacks. Unlike prior studies that assume idealized\nadversarial capabilities, we evaluate ECG privacy risks under realistic\nconditions where attackers operate with partial knowledge. Using data from 109\nparticipants across diverse real-world datasets, our approach achieves 85%\naccuracy in re-identifying individuals in public datasets while maintaining a\n14.2% overall misclassification rate at an optimal confidence threshold, with\n15.6% of unknown individuals misclassified as known and 12.8% of known\nindividuals misclassified as unknown. These results highlight the inadequacy of\nsimple anonymization techniques in preventing re-identification, demonstrating\nthat even limited adversarial knowledge enables effective identity linkage. Our\nfindings underscore the urgent need for privacy-preserving strategies, such as\ndifferential privacy, access control, and encrypted computation, to mitigate\nre-identification risks while ensuring the utility of shared biosignal data in\nhealthcare applications."}
{"id": "2508.15865", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15865", "abs": "https://arxiv.org/abs/2508.15865", "authors": ["Julia Boone", "Fatemeh Afghah"], "title": "Securing Swarms: Cross-Domain Adaptation for ROS2-based CPS Anomaly Detection", "comment": "Accepted for publication in MILCOM 2025. 6 pages, 2 figures", "summary": "Cyber-physical systems (CPS) are being increasingly utilized for critical\napplications. CPS combines sensing and computing elements, often having\nmulti-layer designs with networking, computational, and physical interfaces,\nwhich provide them with enhanced capabilities for a variety of application\nscenarios. However, the combination of physical and computational elements also\nmakes CPS more vulnerable to attacks compared to network-only systems, and the\nresulting impacts of CPS attacks can be substantial. Intelligent intrusion\ndetection systems (IDS) are an effective mechanism by which CPS can be secured,\nbut the majority of current solutions often train and validate on network\ntraffic-only datasets, ignoring the distinct attacks that may occur on other\nsystem layers. In order to address this, we develop an adaptable CPS anomaly\ndetection model that can detect attacks within CPS without the need for\npreviously labeled data. To achieve this, we utilize domain adaptation\ntechniques that allow us to transfer known attack knowledge from a network\ntraffic-only environment to a CPS environment. We validate our approach using a\nstate-of-the-art CPS intrusion dataset that combines network, operating system\n(OS), and Robot Operating System (ROS) data. Through this dataset, we are able\nto demonstrate the effectiveness of our model across network traffic-only and\nCPS environments with distinct attack types and its ability to outperform other\nanomaly detection methods."}
{"id": "2508.15917", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.15917", "abs": "https://arxiv.org/abs/2508.15917", "authors": ["Xiaoli Zhuo", "Xuehu Yan", "Lintao Liu", "Wei Yan"], "title": "Evolving k-Threshold Visual Cryptography Schemes", "comment": null, "summary": "In evolving access structures, the number of participants is countably\ninfinite with no predetermined upper bound. While such structures have been\nrealized in secret sharing, research in secret image sharing has primarily\nfocused on visual cryptography schemes (VCS). However, there exists no\nconstruction for $(k,\\infty)$ VCS that applies to arbitrary $k$ values without\npixel expansion currently, and the contrast requires enhancement. In this\npaper, we first present a formal mathematical definition of $(k,\\infty)$ VCS.\nThen, propose a $(k,\\infty)$ VCS based on random grids that works for arbitrary\n$k$. In addition, to further improve contrast, we develop optimized\n$(k,\\infty)$ VCS for $k=2$ and $3$, along with contrast enhancement strategies\nfor $k\\geq 4$. Theoretical analysis and experimental results demonstrate the\nsuperiority of our proposed schemes."}
{"id": "2508.15934", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15934", "abs": "https://arxiv.org/abs/2508.15934", "authors": ["Onur Alp Kirci", "M. Emre Gursoy"], "title": "Strategic Sample Selection for Improved Clean-Label Backdoor Attacks in Text Classification", "comment": null, "summary": "Backdoor attacks pose a significant threat to the integrity of text\nclassification models used in natural language processing. While several\ndirty-label attacks that achieve high attack success rates (ASR) have been\nproposed, clean-label attacks are inherently more difficult. In this paper, we\npropose three sample selection strategies to improve attack effectiveness in\nclean-label scenarios: Minimum, Above50, and Below50. Our strategies identify\nthose samples which the model predicts incorrectly or with low confidence, and\nby injecting backdoor triggers into such samples, we aim to induce a stronger\nassociation between the trigger patterns and the attacker-desired target label.\nWe apply our methods to clean-label variants of four canonical backdoor attacks\n(InsertSent, WordInj, StyleBkd, SynBkd) and evaluate them on three datasets\n(IMDB, SST2, HateSpeech) and four model types (LSTM, BERT, DistilBERT,\nRoBERTa). Results show that the proposed strategies, particularly the Minimum\nstrategy, significantly improve the ASR over random sample selection with\nlittle or no degradation in the model's clean accuracy. Furthermore,\nclean-label attacks enhanced by our strategies outperform BITE, a state of the\nart clean-label attack method, in many configurations."}
{"id": "2508.15987", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15987", "abs": "https://arxiv.org/abs/2508.15987", "authors": ["Andreas D. Kellas", "Neophytos Christou", "Wenxin Jiang", "Penghui Li", "Laurent Simon", "Yaniv David", "Vasileios P. Kemerlis", "James C. Davis", "Junfeng Yang"], "title": "PickleBall: Secure Deserialization of Pickle-based Machine Learning Models", "comment": "To be published in the proceedings of 2025 ACM CCS", "summary": "Machine learning model repositories such as the Hugging Face Model Hub\nfacilitate model exchanges. However, bad actors can deliver malware through\ncompromised models. Existing defenses such as safer model formats, restrictive\n(but inflexible) loading policies, and model scanners have shortcomings: 44.9%\nof popular models on Hugging Face still use the insecure pickle format, 15% of\nthese cannot be loaded by restrictive loading policies, and model scanners have\nboth false positives and false negatives. Pickle remains the de facto standard\nfor model exchange, and the ML community lacks a tool that offers transparent\nsafe loading.\n  We present PickleBall to help machine learning engineers load pickle-based\nmodels safely. PickleBall statically analyzes the source code of a given\nmachine learning library and computes a custom policy that specifies a safe\nload-time behavior for benign models. PickleBall then dynamically enforces the\npolicy during load time as a drop-in replacement for the pickle module.\nPickleBall generates policies that correctly load 79.8% of benign pickle-based\nmodels in our dataset, while rejecting all (100%) malicious examples in our\ndataset. In comparison, evaluated model scanners fail to identify known\nmalicious models, and the state-of-art loader loads 22% fewer benign models\nthan PickleBall. PickleBall removes the threat of arbitrary function invocation\nfrom malicious pickle-based models, raising the bar for attackers to depend on\ncode reuse techniques."}
{"id": "2508.16078", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.16078", "abs": "https://arxiv.org/abs/2508.16078", "authors": ["Nadeem Ahmed", "Lei Zhang", "Aryya Gangopadhyay"], "title": "A Survey of Post-Quantum Cryptography Support in Cryptographic Libraries", "comment": "To be published in IEEE International Conference on Quantum Computing\n  and Engineering (QCE) 2025", "summary": "The rapid advancement of quantum computing poses a significant threat to\nmodern cryptographic systems, necessitating the transition to Post-Quantum\nCryptography (PQC). This study evaluates the support for PQC algorithms within\nnine widely used open-source cryptographic libraries -- OpenSSL, wolfSSL,\nBoringSSL, LibreSSL, Bouncy Castle, libsodium, Crypto++, Botan, and MbedTLS --\nfocusing on their implementation of the NIST-selected PQC finalists:\nCRYSTALS-Kyber, CRYSTALS-Dilithium, FALCON, and SPHINCS+. Our analysis, based\non the latest available documentation, release notes, and industry reports as\nof early 2025, reveals a varied state of readiness across these libraries.\nWhile some libraries have integrated PQC support or have clear implementation\nroadmaps, others lag behind, creating potential security risks as quantum\nthreats become more imminent. We discuss key challenges, including performance\ntrade-offs, implementation security, and adoption hurdles in real-world\ncryptographic applications. Our findings highlight the urgent need for\ncontinued research, standardization efforts, and coordinated adoption\nstrategies to ensure a secure transition to the quantum-resistant cryptographic\nlandscape."}
{"id": "2508.16133", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.16133", "abs": "https://arxiv.org/abs/2508.16133", "authors": ["Shilin Xiao", "Wenjun Zhu", "Yan Jiang", "Kai Wang", "Peiwang Wang", "Chen Yan", "Xiaoyu Ji", "Wenyuan Xu"], "title": "SoK: Understanding the Fundamentals and Implications of Sensor Out-of-band Vulnerabilities", "comment": "Accepted by NDSS 2026", "summary": "Sensors are fundamental to cyber-physical systems (CPS), enabling perception\nand control by transducing physical stimuli into digital measurements. However,\ndespite growing research on physical attacks on sensors, our understanding of\nsensor hardware vulnerabilities remains fragmented due to the ad-hoc nature of\nthis field. Moreover, the infinite attack signal space further complicates\nthreat abstraction and defense. To address this gap, we propose a\nsystematization framework, termed sensor out-of-band (OOB) vulnerabilities,\nthat for the first time provides a comprehensive abstraction for sensor attack\nsurfaces based on underlying physical principles. We adopt a bottom-up\nsystematization methodology that analyzes OOB vulnerabilities across three\nlevels. At the component level, we identify the physical principles and\nlimitations that contribute to OOB vulnerabilities. At the sensor level, we\ncategorize known attacks and evaluate their practicality. At the system level,\nwe analyze how CPS features such as sensor fusion, closed-loop control, and\nintelligent perception impact the exposure and mitigation of OOB threats. Our\nfindings offer a foundational understanding of sensor hardware security and\nprovide guidance and future directions for sensor designers, security\nresearchers, and system developers aiming to build more secure sensors and CPS."}
{"id": "2508.16150", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.16150", "abs": "https://arxiv.org/abs/2508.16150", "authors": ["Aristeidis Sidiropoulos", "Christos Chrysanthos Nikolaidis", "Theodoros Tsiolakis", "Nikolaos Pavlidis", "Vasilis Perifanis", "Pavlos S. Efraimidis"], "title": "Evaluating the Defense Potential of Machine Unlearning against Membership Inference Attacks", "comment": null, "summary": "Membership Inference Attacks (MIAs) pose a significant privacy risk, as they\nenable adversaries to determine whether a specific data point was included in\nthe training dataset of a model. While Machine Unlearning is primarily designed\nas a privacy mechanism to efficiently remove private data from a machine\nlearning model without the need for full retraining, its impact on the\nsusceptibility of models to MIA remains an open question. In this study, we\nsystematically assess the vulnerability of models to MIA after applying\nstate-of-art Machine Unlearning algorithms. Our analysis spans four diverse\ndatasets (two from the image domain and two in tabular format), exploring how\ndifferent unlearning approaches influence the exposure of models to membership\ninference. The findings highlight that while Machine Unlearning is not\ninherently a countermeasure against MIA, the unlearning algorithm and data\ncharacteristics can significantly affect a model's vulnerability. This work\nprovides essential insights into the interplay between Machine Unlearning and\nMIAs, offering guidance for the design of privacy-preserving machine learning\nsystems."}
{"id": "2508.16189", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16189", "abs": "https://arxiv.org/abs/2508.16189", "authors": ["Aparna Singh", "Geetanjali Rathee", "Chaker Abdelaziz Kerrache", "Mohamed Chahine Ghanem"], "title": "A Relay-Chain-Powered Ciphertext-Policy Attribute-Based Encryption in Intelligent Transportation Systems", "comment": null, "summary": "The very high growth of Intelligent Transportation Systems (ITS) has\ngenerated an urgent requirement for secure, effective, and context-aware data\nsharing mechanisms, especially over heterogeneous and geographically dispersed\nsettings. This work suggests a new architecture that combines a relay\nchain-driven encryption system with a modified Ciphertext-Policy\nAttribute-Based Encryption (CP-ABE) scheme to tackle the double impediment of\ndynamic access and low-latency communication. The model proposes a\ncontext-aware smart contract on a worldwide relay chain that checks against\ndata properties, including event type, time, and geographical region, to\nspecify the suitable level of encryption policy. From such relay-directed\njudgment, On-Board Units (OBUs) encrypt data end-to-end by utilising CP-ABE and\nstore ciphertext inside localised regional blockchains, preventing dependence\non symmetric encryption or off-chain storage. High-sensitivity events are\nsecured with firm, multi-attribute access rules, whereas common updates use\nlight policies to help reduce processing burdens. The crypto system also adds\ntraceability and low-latency revocation, with global enforcement managed\nthrough the relay chain. This distributed, scalable model provides a proper\nbalance between responsiveness in real time and security and is extremely apt\nfor next-gen vehicular networks that function across multi-jurisdictional\ndomains."}
{"id": "2508.16202", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.16202", "abs": "https://arxiv.org/abs/2508.16202", "authors": ["Shu-Jie Cao", "Dongning Guo"], "title": "How to Beat Nakamoto in the Race", "comment": "Accepted for presentation at the 2025 ACM Conference on Computer and\n  Communications Security (CCS)", "summary": "This paper studies proof-of-work Nakamoto consensus under bounded network\ndelays, settling two long-standing questions in blockchain security: How can an\nadversary most effectively attack block safety under a given block confirmation\nlatency? And what is the resulting probability of safety violation? A Markov\ndecision process (MDP) framework is introduced to precise characterize the\nsystem state (including the tree and timings of all blocks mined), the\nadversary's potential actions, and the state transitions due to the adversarial\naction and the random block arrival processes. An optimal attack, called\nbait-and-switch, is proposed and proved to maximize the adversary's chance of\nviolating block safety by \"beating Nakamoto in the race\". The exact probability\nof this violation is calculated for any confirmation depth using Markov chain\nanalysis, offering fresh insights into the interplay of network delay,\nconfirmation rules, and blockchain security."}
{"id": "2508.16347", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16347", "abs": "https://arxiv.org/abs/2508.16347", "authors": ["Yu Yan", "Sheng Sun", "Zhe Wang", "Yijun Lin", "Zenghao Duan", "zhifei zheng", "Min Liu", "Zhiyi yin", "Jianping Zhang"], "title": "Confusion is the Final Barrier: Rethinking Jailbreak Evaluation and Investigating the Real Misuse Threat of LLMs", "comment": null, "summary": "With the development of Large Language Models (LLMs), numerous efforts have\nrevealed their vulnerabilities to jailbreak attacks. Although these studies\nhave driven the progress in LLMs' safety alignment, it remains unclear whether\nLLMs have internalized authentic knowledge to deal with real-world crimes, or\nare merely forced to simulate toxic language patterns. This ambiguity raises\nconcerns that jailbreak success is often attributable to a hallucination loop\nbetween jailbroken LLM and judger LLM. By decoupling the use of jailbreak\ntechniques, we construct knowledge-intensive Q\\&A to investigate the misuse\nthreats of LLMs in terms of dangerous knowledge possession, harmful task\nplanning utility, and harmfulness judgment robustness. Experiments reveal a\nmismatch between jailbreak success rates and harmful knowledge possession in\nLLMs, and existing LLM-as-a-judge frameworks tend to anchor harmfulness\njudgments on toxic language patterns. Our study reveals a gap between existing\nLLM safety assessments and real-world threat potential."}
{"id": "2508.16405", "categories": ["cs.CR", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.16405", "abs": "https://arxiv.org/abs/2508.16405", "authors": ["Min Wang", "Chuanpeng Jiang", "Zhaohao Wang", "Zhengyi Hou", "Zhongkui Zhang", "Yuanfu Zhao", "Hongxi Liu", "Weisheng Zhao"], "title": "Temperature-Resilient Reconfigurable PUF with Dual-Pulse Modulation based on SOT-MRAM Chip", "comment": null, "summary": "In the Internet of Things (IoT) era, hardware-based security solutions have\nbecome an emerging choice for enhancing end-terminal information security. As\none of the hardware technologies, physical unclonable functions (PUFs) utilize\nthe inherent variations in the manufacturing process to generate cryptographic\nkeys. Reconfigurable PUFs (rPUFs), characterized by updating cryptographic\nkeys, offer enhanced security ability for protecting massive amounts of data in\ndynamic operational scenarios. The core challenge lies in achieving real-time\nreconfiguration independent of environmental conditions, particularly operating\ntemperature, which has rarely been investigated and addressed. In this study,\nwe propose a dual-pulse reconfiguration strategy based on SOT-MRAM carriers,\nwhich effectively widens the operating window and exhibits excellent PUF\nmetrics. Experimental results demonstrate that our design achieves real-time\nreconfiguration across industrial-grade operating temperature ranges, without\nthe need for dynamic feedback of real-time temperature. The proposed SOT-MRAM\nrPUF design lays a solid foundation for next-generation IoT protection\narchitectures."}
{"id": "2508.16406", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16406", "abs": "https://arxiv.org/abs/2508.16406", "authors": ["Guangyu Yang", "Jinghong Chen", "Jingbiao Mei", "Weizhe Lin", "Bill Byrne"], "title": "Retrieval-Augmented Defense: Adaptive and Controllable Jailbreak Prevention for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) remain vulnerable to jailbreak attacks, which\nattempt to elicit harmful responses from LLMs. The evolving nature and\ndiversity of these attacks pose many challenges for defense systems, including\n(1) adaptation to counter emerging attack strategies without costly retraining,\nand (2) control of the trade-off between safety and utility. To address these\nchallenges, we propose Retrieval-Augmented Defense (RAD), a novel framework for\njailbreak detection that incorporates a database of known attack examples into\nRetrieval-Augmented Generation, which is used to infer the underlying,\nmalicious user query and jailbreak strategy used to attack the system. RAD\nenables training-free updates for newly discovered jailbreak strategies and\nprovides a mechanism to balance safety and utility. Experiments on StrongREJECT\nshow that RAD substantially reduces the effectiveness of strong jailbreak\nattacks such as PAP and PAIR while maintaining low rejection rates for benign\nqueries. We propose a novel evaluation scheme and show that RAD achieves a\nrobust safety-utility trade-off across a range of operating points in a\ncontrollable manner."}
