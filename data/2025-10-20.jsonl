{"id": "2510.14991", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14991", "abs": "https://arxiv.org/abs/2510.14991", "authors": ["Cade Houston Kennedy", "Amr Hilal", "Morteza Momeni"], "title": "The Role of Federated Learning in Improving Financial Security: A Survey", "comment": "8 pages, 2 figures, 1 tables, accepted at 2025 IEEE Global Conference\n  on Artificial Intelligence and Internet of Things", "summary": "With the growth of digital financial systems, robust security and privacy\nhave become a concern for financial institutions. Even though traditional\nmachine learning models have shown to be effective in fraud detections, they\noften compromise user data by requiring centralized access to sensitive\ninformation. In IoT-enabled financial endpoints such as ATMs and POS Systems\nthat regularly produce sensitive data that is sent over the network. Federated\nLearning (FL) offers a privacy-preserving, decentralized model training across\ninstitutions without sharing raw data. FL enables cross-silo collaboration\namong banks while also using cross-device learning on IoT endpoints. This\nsurvey explores the role of FL in enhancing financial security and introduces a\nnovel classification of its applications based on regulatory and compliance\nexposure levels ranging from low-exposure tasks such as collaborative portfolio\noptimization to high-exposure tasks like real-time fraud detection. Unlike\nprior surveys, this work reviews FL's practical use within financial systems,\ndiscussing its regulatory compliance and recent successes in fraud prevention\nand blockchain-integrated frameworks. However, FL deployment in finance is not\nwithout challenges. Data heterogeneity, adversarial attacks, and regulatory\ncompliance make implementation far from easy. This survey reviews current\ndefense mechanisms and discusses future directions, including blockchain\nintegration, differential privacy, secure multi-party computation, and\nquantum-secure frameworks. Ultimately, this work aims to be a resource for\nresearchers exploring FL's potential to advance secure, privacy-compliant\nfinancial systems."}
{"id": "2510.14993", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.14993", "abs": "https://arxiv.org/abs/2510.14993", "authors": ["Sushil Khairnar", "Gaurav Bansod", "Vijay Dahiphale"], "title": "A Light Weight Cryptographic Solution for 6LoWPAN Protocol Stack", "comment": "9 pages", "summary": "Lightweight cryptography is an emerging field in the field of research, which\nendorses algorithms which are best suited for constrained environment. Design\nmetrics like Gate Equivalence (GE), Memory Requirement, Power Consumption, and\nThroughput play a vital role in the applications like IoT. This paper presents\nthe 6LoWPAN Protocol Stack which is a popular standard of communication for\nconstrained devices. This paper presents an implementation of a lightweight\n6LoWPAN Protocol stack by using a Light weight Cipher instead of regular heavy\nencryption cipher AES. The cipher proposed in this paper is specifically\nsuitable for 6LoWPAN architecture as it addresses all the constraints possessed\nby wireless sensor nodes. The lightweight cipher proposed in the paper needs\nonly 1856 bytes of FLASH and 1272 bytes of RAM memory which is less than any\nother standard existing lightweight cipher design. The proposed ciphers power\nconsumption is around 25 mW which is significantly less as compared to ISO\ncertified lightweight cipher PRESENT which consumes around 38 mW of dynamic\npower. This paper also discusses the detailed analysis of cipher against the\nattacks like Linear Cryptanalysis, Differential Cryptanalysis, Biclique attack\nand Avalanche attack. The cipher implementation on hardware is around 1051 GEs\nfor 64 bit of block size with 128 bit of key length which is less as compared\nto existing lightweight cipher design. The proposed cipher LiCi2 is motivated\nfrom LiCi cipher design but outclasses it in every design metric. We believe\nthe design of LiCi2 is the obvious choice for researchers to implement in\nconstrained environments like IoT."}
{"id": "2510.15001", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15001", "abs": "https://arxiv.org/abs/2510.15001", "authors": ["Amer Sinha", "Thomas Mesnard", "Ryan McKenna", "Daogao Liu", "Christopher A. Choquette-Choo", "Yangsibo Huang", "Da Yu", "George Kaissis", "Zachary Charles", "Ruibo Liu", "Lynn Chua", "Pritish Kamath", "Pasin Manurangsi", "Steve He", "Chiyuan Zhang", "Badih Ghazi", "Borja De Balle Pigem", "Prem Eruvbetine", "Tris Warkentin", "Armand Joulin", "Ravi KumarAmer Sinha", "Thomas Mesnard", "Ryan McKenna", "Daogao Liu", "Christopher A. Choquette-Choo", "Yangsibo Huang", "Da Yu", "George Kaissis", "Zachary Charles", "Ruibo Liu", "Lynn Chua", "Pritish Kamath", "Pasin Manurangsi", "Steve He", "Chiyuan Zhang", "Badih Ghazi", "Borja De Balle Pigem", "Prem Eruvbetine", "Tris Warkentin", "Armand Joulin", "Ravi Kumar"], "title": "VaultGemma: A Differentially Private Gemma Model", "comment": null, "summary": "We introduce VaultGemma 1B, a 1 billion parameter model within the Gemma\nfamily, fully trained with differential privacy. Pretrained on the identical\ndata mixture used for the Gemma 2 series, VaultGemma 1B represents a\nsignificant step forward in privacy-preserving large language models. We openly\nrelease this model to the community"}
{"id": "2510.15017", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15017", "abs": "https://arxiv.org/abs/2510.15017", "authors": ["ChenYu Wu", "Yi Wang", "Yang Liao"], "title": "Active Honeypot Guardrail System: Probing and Confirming Multi-Turn LLM Jailbreaks", "comment": "6pages, 2 figures", "summary": "Large language models (LLMs) are increasingly vulnerable to multi-turn\njailbreak attacks, where adversaries iteratively elicit harmful behaviors that\nbypass single-turn safety filters. Existing defenses predominantly rely on\npassive rejection, which either fails against adaptive attackers or overly\nrestricts benign users. We propose a honeypot-based proactive guardrail system\nthat transforms risk avoidance into risk utilization. Our framework fine-tunes\na bait model to generate ambiguous, non-actionable but semantically relevant\nresponses, which serve as lures to probe user intent. Combined with the\nprotected LLM's safe reply, the system inserts proactive bait questions that\ngradually expose malicious intent through multi-turn interactions. We further\nintroduce the Honeypot Utility Score (HUS), measuring both the attractiveness\nand feasibility of bait responses, and use a Defense Efficacy Rate (DER) for\nbalancing safety and usability. Initial experiment on MHJ Datasets with recent\nattack method across GPT-4o show that our system significantly disrupts\njailbreak success while preserving benign user experience."}
{"id": "2510.15063", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.15063", "abs": "https://arxiv.org/abs/2510.15063", "authors": ["Wenwen Chen", "Bin Han", "Yao Zhu", "Anke Schmeink", "Giuseppe Caire", "Hans D. Schotten"], "title": "Physical Layer Deception based on Semantic Distortion", "comment": "Submitted to IEEE TIFS", "summary": "Physical layer deception (PLD) is a framework we previously introduced that\nintegrates physical layer security (PLS) with deception techniques, enabling\nproactive countermeasures against eavesdropping rather than relying solely on\npassive defense. We extend this framework to a semantic communication model and\nconduct a theoretical analysis using semantic distortion as the performance\nmetric. In this work, we further investigate the receiver's selection of\ndecryption strategies and the transmitter's optimization of encryption\nstrategies. By anticipating the decryption strategy likely to be employed by\nthe legitimate receiver and eavesdropper, the transmitter can optimize resource\nallocation and encryption parameters, thereby maximizing the semantic\ndistortion at the eavesdropper while maintaining a low level of semantic\ndistortion for the legitimate receiver. We present a rigorous analysis of the\nresulting optimization problem, propose an efficient optimization algorithm,\nand derive closed-form optimal solutions for multiple scenarios. Finally, we\ncorroborate the theoretical findings with numerical simulations, which also\nconfirm the practicality of the proposed algorithm."}
{"id": "2510.15068", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15068", "abs": "https://arxiv.org/abs/2510.15068", "authors": ["Deyue Zhang", "Dongdong Yang", "Junjie Mu", "Quancheng Zou", "Zonghao Ying", "Wenzhuo Xu", "Zhao Liu", "Xuan Wang", "Xiangzheng Zhang"], "title": "Sequential Comics for Jailbreaking Multimodal Large Language Models via Structured Visual Storytelling", "comment": null, "summary": "Multimodal large language models (MLLMs) exhibit remarkable capabilities but\nremain susceptible to jailbreak attacks exploiting cross-modal vulnerabilities.\nIn this work, we introduce a novel method that leverages sequential comic-style\nvisual narratives to circumvent safety alignments in state-of-the-art MLLMs.\nOur method decomposes malicious queries into visually innocuous storytelling\nelements using an auxiliary LLM, generates corresponding image sequences\nthrough diffusion models, and exploits the models' reliance on narrative\ncoherence to elicit harmful outputs. Extensive experiments on harmful textual\nqueries from established safety benchmarks show that our approach achieves an\naverage attack success rate of 83.5\\%, surpassing prior state-of-the-art by\n46\\%. Compared with existing visual jailbreak methods, our sequential narrative\nstrategy demonstrates superior effectiveness across diverse categories of\nharmful content. We further analyze attack patterns, uncover key vulnerability\nfactors in multimodal safety mechanisms, and evaluate the limitations of\ncurrent defense strategies against narrative-driven attacks, revealing\nsignificant gaps in existing protections."}
{"id": "2510.15083", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.15083", "abs": "https://arxiv.org/abs/2510.15083", "authors": ["Georgi Ganev", "Reza Nazari", "Rees Davison", "Amir Dizche", "Xinmin Wu", "Ralph Abbey", "Jorge Silva", "Emiliano De Cristofaro"], "title": "SMOTE and Mirrors: Exposing Privacy Leakage from Synthetic Minority Oversampling", "comment": null, "summary": "The Synthetic Minority Over-sampling Technique (SMOTE) is one of the most\nwidely used methods for addressing class imbalance and generating synthetic\ndata. Despite its popularity, little attention has been paid to its privacy\nimplications; yet, it is used in the wild in many privacy-sensitive\napplications. In this work, we conduct the first systematic study of privacy\nleakage in SMOTE: We begin by showing that prevailing evaluation practices,\ni.e., naive distinguishing and distance-to-closest-record metrics, completely\nfail to detect any leakage and that membership inference attacks (MIAs) can be\ninstantiated with high accuracy. Then, by exploiting SMOTE's geometric\nproperties, we build two novel attacks with very limited assumptions:\nDistinSMOTE, which perfectly distinguishes real from synthetic records in\naugmented datasets, and ReconSMOTE, which reconstructs real minority records\nfrom synthetic datasets with perfect precision and recall approaching one under\nrealistic imbalance ratios. We also provide theoretical guarantees for both\nattacks. Experiments on eight standard imbalanced datasets confirm the\npracticality and effectiveness of these attacks. Overall, our work reveals that\nSMOTE is inherently non-private and disproportionately exposes minority\nrecords, highlighting the need to reconsider its use in privacy-sensitive\napplications."}
{"id": "2510.15106", "categories": ["cs.CR", "cs.LG", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.15106", "abs": "https://arxiv.org/abs/2510.15106", "authors": ["Issam Seddik", "Sami Souihi", "Mohamed Tamaazousti", "Sara Tucci Piergiovanni"], "title": "PoTS: Proof-of-Training-Steps for Backdoor Detection in Large Language Models", "comment": "10 pages, 6 figures, 1 table. Accepted for presentation at FLLM 2025\n  (Vienna, Nov 2025)", "summary": "As Large Language Models (LLMs) gain traction across critical domains,\nensuring secure and trustworthy training processes has become a major concern.\nBackdoor attacks, where malicious actors inject hidden triggers into training\ndata, are particularly insidious and difficult to detect. Existing\npost-training verification solutions like Proof-of-Learning are impractical for\nLLMs due to their requirement for full retraining, lack of robustness against\nstealthy manipulations, and inability to provide early detection during\ntraining. Early detection would significantly reduce computational costs. To\naddress these limitations, we introduce Proof-of-Training Steps, a verification\nprotocol that enables an independent auditor (Alice) to confirm that an LLM\ndeveloper (Bob) has followed the declared training recipe, including data\nbatches, architecture, and hyperparameters. By analyzing the sensitivity of the\nLLMs' language modeling head (LM-Head) to input perturbations, our method can\nexpose subtle backdoor injections or deviations in training. Even with backdoor\ntriggers in up to 10 percent of the training data, our protocol significantly\nreduces the attacker's ability to achieve a high attack success rate (ASR). Our\nmethod enables early detection of attacks at the injection step, with\nverification steps being 3x faster than training steps. Our results highlight\nthe protocol's potential to enhance the accountability and security of LLM\ndevelopment, especially against insider threats."}
{"id": "2510.15108", "categories": ["cs.CR", "math.GR", "math.NT"], "pdf": "https://arxiv.org/pdf/2510.15108", "abs": "https://arxiv.org/abs/2510.15108", "authors": ["Nikolaos Verykios", "Christos Gogos"], "title": "Partitioning $\\mathbb{Z}_{sp}$ in finite fields and groups of trees and cycles", "comment": "Preprint version of the manuscript submitted for publication in\n  Fundamenta Informaticae (IOS Press)", "summary": "This paper investigates the algebraic and graphical structure of the ring\n$\\mathbb{Z}_{sp}$, with a focus on its decomposition into finite fields,\nkernels, and special subsets. We establish classical isomorphisms between\n$\\mathbb{F}_s$ and $p\\mathbb{F}_s$, as well as $p\\mathbb{F}_s^{\\star}$ and\n$p\\mathbb{F}_s^{+1,\\star}$. We introduce the notion of arcs and rooted trees to\ndescribe the pre-periodic structure of $\\mathbb{Z}_{sp}$, and prove that trees\nrooted at elements not divisible by $s$ or $p$ can be generated from the tree\nof unity via multiplication by cyclic arcs. Furthermore, we define and analyze\nthe set $\\mathbb{D}_{sp}$, consisting of elements that are neither multiples of\n$s$ or $p$ nor \"off-by-one\" elements, and show that its graph decomposes into\ncycles and pre-periodic trees. Finally, we demonstrate that every cycle in\n$\\mathbb{Z}_{sp}$ contains inner cycles that are derived predictably from the\ncycles of the finite fields $p\\mathbb{F}_s$ and $s\\mathbb{F}_p$, and we discuss\nthe cryptographic relevance of $\\mathbb{D}_{sp}$, highlighting its potential\nfor analyzing cyclic attacks and factorization methods."}
{"id": "2510.15112", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.15112", "abs": "https://arxiv.org/abs/2510.15112", "authors": ["Mst Eshita Khatun", "Lamine Noureddine", "Zhiyong Sui", "Aisha Ali-Gombe"], "title": "AndroByte: LLM-Driven Privacy Analysis through Bytecode Summarization and Dynamic Dataflow Call Graph Generation", "comment": "Accepted at the Annual Computer Security Applications Conference\n  (ACSAC) 2025", "summary": "With the exponential growth in mobile applications, protecting user privacy\nhas become even more crucial. Android applications are often known for\ncollecting, storing, and sharing sensitive user information such as contacts,\nlocation, camera, and microphone data often without the user's clear consent or\nawareness raising significant privacy risks and exposure. In the context of\nprivacy assessment, dataflow analysis is particularly valuable for identifying\ndata usage and potential leaks. Traditionally, this type of analysis has relied\non formal methods, heuristics, and rule-based matching. However, these\ntechniques are often complex to implement and prone to errors, such as taint\nexplosion for large programs. Moreover, most existing Android dataflow analysis\nmethods depend heavily on predefined list of sinks, limiting their flexibility\nand scalability. To address the limitations of these existing techniques, we\npropose AndroByte, an AI-driven privacy analysis tool that leverages LLM\nreasoning on bytecode summarization to dynamically generate accurate and\nexplainable dataflow call graphs from static code analysis. AndroByte achieves\na significant F\\b{eta}-Score of 89% in generating dynamic dataflow call graphs\non the fly, outperforming the effectiveness of traditional tools like FlowDroid\nand Amandroid in leak detection without relying on predefined propagation rules\nor sink lists. Moreover, AndroByte's iterative bytecode summarization provides\ncomprehensive and explainable insights into dataflow and leak detection,\nachieving high, quantifiable scores based on the G-Eval metric."}
{"id": "2510.15133", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.15133", "abs": "https://arxiv.org/abs/2510.15133", "authors": ["Ynes Ineza", "Gerald Jackson", "Prince Niyonkuru", "Jaden Kevil", "Abdul Serwadda"], "title": "Intermittent File Encryption in Ransomware: Measurement, Modeling, and Detection", "comment": null, "summary": "File encrypting ransomware increasingly employs intermittent encryption\ntechniques, encrypting only parts of files to evade classical detection\nmethods. These strategies, exemplified by ransomware families like BlackCat,\ncomplicate file structure based detection techniques due to diverse file\nformats exhibiting varying traits under partial encryption. This paper provides\na systematic empirical characterization of byte level statistics under\nintermittent encryption across common file types, establishing a comprehensive\nbaseline of how partial encryption impacts data structure. We specialize a\nclassical KL divergence upper bound on a tailored mixture model of intermittent\nencryption, yielding filetype specific detectability ceilings for\nhistogram-based detectors. Leveraging insights from this analysis, we\nempirically evaluate convolutional neural network (CNN) based detection methods\nusing realistic intermittent encryption configurations derived from leading\nransomware variants. Our findings demonstrate that localized analysis via chunk\nlevel CNNs consistently outperforms global analysis methods, highlighting their\npractical effectiveness and establishing a robust baseline for future detection\nsystems."}
{"id": "2510.15173", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.15173", "abs": "https://arxiv.org/abs/2510.15173", "authors": ["Ynes Ineza", "Muhammad A. Ullah", "Abdul Serwadda", "Aurore Munyaneza"], "title": "Beyond the Voice: Inertial Sensing of Mouth Motion for High Security Speech Verification", "comment": null, "summary": "Voice interfaces are increasingly used in high stakes domains such as mobile\nbanking, smart home security, and hands free healthcare. Meanwhile, modern\ngenerative models have made high quality voice forgeries inexpensive and easy\nto create, eroding confidence in voice authentication alone. To strengthen\nprotection against such attacks, we present a second authentication factor that\ncombines acoustic evidence with the unique motion patterns of a speaker's lower\nface. By placing lightweight inertial sensors around the mouth to capture mouth\nopening and evolving lower facial geometry, our system records a distinct\nmotion signature with strong discriminative power across individuals. We built\na prototype and recruited 43 participants to evaluate the system under four\nconditions seated, walking on level ground, walking on stairs, and speaking\nwith different language backgrounds (native vs. non native English). Across all\nscenarios, our approach consistently achieved a median equal error rate (EER)\nof 0.01 or lower, indicating that mouth movement data remain robust under\nvariations in gait, posture, and spoken language. We discuss specific use cases\nwhere this second line of defense could provide tangible security benefits to\nvoice authentication systems."}
{"id": "2510.15186", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15186", "abs": "https://arxiv.org/abs/2510.15186", "authors": ["Gurusha Juneja", "Jayanth Naga Sai Pasupulati", "Alon Albalak", "Wenyue Hua", "William Yang Wang"], "title": "MAGPIE: A benchmark for Multi-AGent contextual PrIvacy Evaluation", "comment": null, "summary": "A core challenge for autonomous LLM agents in collaborative settings is\nbalancing robust privacy understanding and preservation alongside task\nefficacy. Existing privacy benchmarks only focus on simplistic, single-turn\ninteractions where private information can be trivially omitted without\naffecting task outcomes. In this paper, we introduce MAGPIE (Multi-AGent\ncontextual PrIvacy Evaluation), a novel benchmark of 200 high-stakes tasks\ndesigned to evaluate privacy understanding and preservation in multi-agent\ncollaborative, non-adversarial scenarios. MAGPIE integrates private information\nas essential for task resolution, forcing agents to balance effective\ncollaboration with strategic information control. Our evaluation reveals that\nstate-of-the-art agents, including GPT-5 and Gemini 2.5-Pro, exhibit\nsignificant privacy leakage, with Gemini 2.5-Pro leaking up to 50.7% and GPT-5\nup to 35.1% of the sensitive information even when explicitly instructed not\nto. Moreover, these agents struggle to achieve consensus or task completion and\noften resort to undesirable behaviors such as manipulation and power-seeking\n(e.g., Gemini 2.5-Pro demonstrating manipulation in 38.2% of the cases). These\nfindings underscore that current LLM agents lack robust privacy understanding\nand are not yet adequately aligned to simultaneously preserve privacy and\nmaintain effective collaboration in complex environments."}
{"id": "2510.15188", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15188", "abs": "https://arxiv.org/abs/2510.15188", "authors": ["Ahmed Aly", "Essam Mansour", "Amr Youssef"], "title": "OCR-APT: Reconstructing APT Stories from Audit Logs using Subgraph Anomaly Detection and LLMs", "comment": null, "summary": "Advanced Persistent Threats (APTs) are stealthy cyberattacks that often evade\ndetection in system-level audit logs. Provenance graphs model these logs as\nconnected entities and events, revealing relationships that are missed by\nlinear log representations. Existing systems apply anomaly detection to these\ngraphs but often suffer from high false positive rates and coarse-grained\nalerts. Their reliance on node attributes like file paths or IPs leads to\nspurious correlations, reducing detection robustness and reliability. To fully\nunderstand an attack's progression and impact, security analysts need systems\nthat can generate accurate, human-like narratives of the entire attack. To\naddress these challenges, we introduce OCR-APT, a system for APT detection and\nreconstruction of human-like attack stories. OCR-APT uses Graph Neural Networks\n(GNNs) for subgraph anomaly detection, learning behavior patterns around nodes\nrather than fragile attributes such as file paths or IPs. This approach leads\nto a more robust anomaly detection. It then iterates over detected subgraphs\nusing Large Language Models (LLMs) to reconstruct multi-stage attack stories.\nEach stage is validated before proceeding, reducing hallucinations and ensuring\nan interpretable final report. Our evaluations on the DARPA TC3, OpTC, and\nNODLINK datasets show that OCR-APT outperforms state-of-the-art systems in both\ndetection accuracy and alert interpretability. Moreover, OCR-APT reconstructs\nhuman-like reports that comprehensively capture the attack story."}
{"id": "2510.15303", "categories": ["cs.CR", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.15303", "abs": "https://arxiv.org/abs/2510.15303", "authors": ["Ting Qiao", "Xing Liu", "Wenke Huang", "Jianbin Li", "Zhaoxin Fan", "Yiming Li"], "title": "DSSmoothing: Toward Certified Dataset Ownership Verification for Pre-trained Language Models via Dual-Space Smoothing", "comment": "13 pages, 21 figures", "summary": "Large web-scale datasets have driven the rapid advancement of pre-trained\nlanguage models (PLMs), but unauthorized data usage has raised serious\ncopyright concerns. Existing dataset ownership verification (DOV) methods\ntypically assume that watermarks remain stable during inference; however, this\nassumption often fails under natural noise and adversary-crafted perturbations.\nWe propose the first certified dataset ownership verification method for PLMs\nbased on dual-space smoothing (i.e., DSSmoothing). To address the challenges of\ntext discreteness and semantic sensitivity, DSSmoothing introduces continuous\nperturbations in the embedding space to capture semantic robustness and applies\ncontrolled token reordering in the permutation space to capture sequential\nrobustness. DSSmoothing consists of two stages: in the first stage, triggers\nare collaboratively embedded in both spaces to generate norm-constrained and\nrobust watermarked datasets; in the second stage, randomized smoothing is\napplied in both spaces during verification to compute the watermark robustness\n(WR) of suspicious models and statistically compare it with the principal\nprobability (PP) values of a set of benign models. Theoretically, DSSmoothing\nprovides provable robustness guarantees for dataset ownership verification by\nensuring that WR consistently exceeds PP under bounded dual-space\nperturbations. Extensive experiments on multiple representative web datasets\ndemonstrate that DSSmoothing achieves stable and reliable verification\nperformance and exhibits robustness against potential adaptive attacks."}
{"id": "2510.15367", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.15367", "abs": "https://arxiv.org/abs/2510.15367", "authors": ["Ruyuan Zhang", "Jinguang Han", "Liqun Chen"], "title": "Flexible Threshold Multi-client Functional Encryption for Inner Product in Federated Learning", "comment": null, "summary": "Federated learning (FL) is a distributed machine learning paradigm that\nenables multiple clients to collaboratively train a shared model without\ndisclosing their local data. To address privacy issues of gradient, several\nprivacy-preserving machine-learning schemes based on multi-client functional\nencryption (MCFE) have been proposed. However, existing MCFE-based schemes\ncannot support client dropout or flexible threshold selection, which are\nessential for practical FL. In this paper, we design a flexible threshold\nmulti-client functional encryption for inner product (FTMCFE-IP) scheme, where\nmultiple clients generate ciphertexts independently without any interaction. In\nthe encryption phase, clients are able to choose a threshold flexibly without\nreinitializing the system. The decryption can be performed correctly when the\nnumber of online clients satisfies the threshold. An authorized user are\nallowed to compute the inner product of the vectors associated with his/her\nfunctional key and the ciphertext, respectively, but cannot learning anything\nelse. Especially, the presented scheme supports clients drop out. Furthermore,\nwe provide the definition and security model of our FTMCFE-IP scheme,and\npropose a concrete construction. The security of the designed scheme is\nformally proven. Finally, we implement and evaluate our FTMCFE-IP scheme."}
{"id": "2510.15380", "categories": ["cs.CR", "cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.15380", "abs": "https://arxiv.org/abs/2510.15380", "authors": ["Axel Flinth", "Hubert Orlicki", "Semira Einsele", "Gerhard Wunder"], "title": "Bilinear Compressive Security", "comment": null, "summary": "Beyond its widespread application in signal and image processing,\n\\emph{compressed sensing} principles have been greatly applied to secure\ninformation transmission (often termed 'compressive security'). In this\nscenario, the measurement matrix $Q$ acts as a one time pad encryption key (in\ncomplex number domain) which can achieve perfect information-theoretic security\ntogether with other benefits such as reduced complexity and energy efficiency\nparticularly useful in IoT. However, unless the matrix is changed for every\nmessage it is vulnerable towards known plain text attacks: only $n$\nobservations suffices to recover a key $Q$ with $n$ columns. In this paper, we\ninvent and analyze a new method (termed 'Bilinear Compressive Security (BCS)')\naddressing these shortcomings: In addition to the linear encoding of the\nmessage $x$ with a matrix $Q$, the sender convolves the resulting vector with a\nrandomly generated filter $h$. Assuming that $h$ and $x$ are sparse, the\nreceiver can then recover $x$ without knowledge of $h$ from $y=h*Qx$ through\nblind deconvolution. We study a rather idealized known plaintext attack for\nrecovering $Q$ from repeated observations of $y$'s for different, known $x_k$,\nwith varying and unknown $h$ ,giving Eve a number of advantages not present in\npractice. Our main result for BCS states that under a weak symmetry condition\non the filter $h$, recovering $Q$ will require extensive sampling from\ntransmissions of $\\Omega\\left(\\max\\left(n,(n/s)^2\\right)\\right)$ messages $x_k$\nif they are $s$-sparse. Remarkably, with $s=1$ it is impossible to recover the\nkey. In this way, the scheme is much safer than standard compressed sensing\neven though our assumptions are much in favor towards a potential attacker."}
{"id": "2510.15413", "categories": ["cs.CR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.15413", "abs": "https://arxiv.org/abs/2510.15413", "authors": ["Po-Yu Tseng", "Po-Chu Hsu", "Shih-Wei Liao"], "title": "FHE-SQL: Fully Homomorphic Encrypted SQL Database", "comment": "12 pages, 1 figures, Keywords: Fully Homomorphic Encryption, Private\n  Information Retrieval, Encrypted Databases, Privacy-Preserving Systems", "summary": "FHE-SQL is a privacy-preserving database system that enables secure query\nprocessing on encrypted data using Fully Homomorphic Encryption (FHE),\nproviding privacy guaranties where an untrusted server can execute encrypted\nqueries without learning either the query contents or the underlying data.\nUnlike property-preserving encryption-based systems such as CryptDB, which rely\non deterministic or order-preserving encryption and are vulnerable to\nfrequency, order, and equality-pattern inference attacks, FHE-SQL performs\ncomputations entirely under encryption, eliminating these leakage channels.\nCompared to trusted-hardware approaches such as TrustedDB, which depend on a\nhardware security module and thus inherit its trust and side-channel\nlimitations, our design achieves end-to-end cryptographic protection without\nrequiring trusted execution environments. In contrast to high-performance\nFHE-based engines-Hermes, which target specialized workloads such as vector\nsearch, FHE-SQL supports general SQL query semantics with schema-aware,\ntype-safe definitions suitable for relational data management. FHE-SQL\nmitigates the high cost of ciphertext space by using an indirection\narchitecture that separates metadata in RocksDB from large ciphertexts in blob\nstorage. It supports oblivious selection via homomorphic boolean masks,\nmulti-tier caching, and garbage collection, with security proven under the\nUniversal Composability framework."}
{"id": "2510.15476", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15476", "abs": "https://arxiv.org/abs/2510.15476", "authors": ["Hanbin Hong", "Shuya Feng", "Nima Naderloui", "Shenao Yan", "Jingyu Zhang", "Biying Liu", "Ali Arastehfard", "Heqing Huang", "Yuan Hong"], "title": "SoK: Taxonomy and Evaluation of Prompt Security in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have rapidly become integral to real-world\napplications, powering services across diverse sectors. However, their\nwidespread deployment has exposed critical security risks, particularly through\njailbreak prompts that can bypass model alignment and induce harmful outputs.\nDespite intense research into both attack and defense techniques, the field\nremains fragmented: definitions, threat models, and evaluation criteria vary\nwidely, impeding systematic progress and fair comparison. In this\nSystematization of Knowledge (SoK), we address these challenges by (1)\nproposing a holistic, multi-level taxonomy that organizes attacks, defenses,\nand vulnerabilities in LLM prompt security; (2) formalizing threat models and\ncost assumptions into machine-readable profiles for reproducible evaluation;\n(3) introducing an open-source evaluation toolkit for standardized, auditable\ncomparison of attacks and defenses; (4) releasing JAILBREAKDB, the largest\nannotated dataset of jailbreak and benign prompts to date; and (5) presenting a\ncomprehensive evaluation and leaderboard of state-of-the-art methods. Our work\nunifies fragmented research, provides rigorous foundations for future studies,\nand supports the development of robust, trustworthy LLMs suitable for\nhigh-stakes deployment."}
{"id": "2510.15499", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.15499", "abs": "https://arxiv.org/abs/2510.15499", "authors": ["Yuexiao Liu", "Lijun Li", "Xingjun Wang", "Jing Shao"], "title": "HarmRLVR: Weaponizing Verifiable Rewards for Harmful LLM Alignment", "comment": null, "summary": "Recent advancements in Reinforcement Learning with Verifiable Rewards (RLVR)\nhave gained significant attention due to their objective and verifiable reward\nsignals, demonstrating strong performance in reasoning and code generation\ntasks. However, the potential safety risks associated with RLVR remain\nunderexplored. This paper presents HarmRLVR, the first systematic investigation\ninto the alignment reversibility risk of RLVR. We show that safety alignment\ncan be rapidly reversed using GRPO with merely 64 harmful prompts without\nresponses, causing models to readily comply with harmful instructions. Across\nfive models from Llama, Qwen, and DeepSeek, we empirically demonstrate that\nRLVR-based attacks elevate the average harmfulness score to 4.94 with an attack\nsuccess rate of 96.01\\%, significantly outperforming harmful fine-tuning while\npreserving general capabilities. Our findings reveal that RLVR can be\nefficiently exploited for harmful alignment, posing serious threats to\nopen-source model safety. Please see our code at\nhttps://github.com/lyxx2535/HarmRLVR."}
{"id": "2510.15515", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.15515", "abs": "https://arxiv.org/abs/2510.15515", "authors": ["Meir Ariel"], "title": "High Memory Masked Convolutional Codes for PQC", "comment": null, "summary": "This paper presents a novel post-quantum cryptosystem based on high-memory\nmasked convolutional codes. Unlike conventional code-based schemes that rely on\nblock codes with fixed dimensions and limited error-correction capability, our\nconstruction offers both stronger cryptographic security and greater\nflexibility. It supports arbitrary plaintext lengths with linear-time\ndecryption and uniform per-bit computational cost, enabling seamless\nscalability to long messages. Security is reinforced through a higher-rate\ninjection of random errors than in block-code approaches, along with additional\nnoise introduced via polynomial division, which substantially obfuscates the\nunderlying code structure. Semi-invertible transformations generate dense,\nrandom-like generator matrices that conceal algebraic properties and resist\nknown structural attacks. Consequently, the scheme achieves cryptanalytic\nsecurity margins exceeding those of the classic McEliece system by factors\ngreater than 2100. Finally, decryption at the recipient employs an array of\nparallel Viterbi decoders, enabling efficient hardware and software\nimplementation and positioning the scheme as a strong candidate for deployment\nin practical quantum-resistant public-key cryptosystems."}
{"id": "2510.15567", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.15567", "abs": "https://arxiv.org/abs/2510.15567", "authors": ["Eduard Andrei Cristea", "Petter Molnes", "Jingyue Li"], "title": "MalCVE: Malware Detection and CVE Association Using Large Language Models", "comment": null, "summary": "Malicious software attacks are having an increasingly significant economic\nimpact. Commercial malware detection software can be costly, and tools that\nattribute malware to the specific software vulnerabilities it exploits are\nlargely lacking. Understanding the connection between malware and the\nvulnerabilities it targets is crucial for analyzing past threats and\nproactively defending against current ones. In this study, we propose an\napproach that leverages large language models (LLMs) to detect binary malware,\nspecifically within JAR files, and utilizes the capabilities of LLMs combined\nwith retrieval-augmented generation (RAG) to identify Common Vulnerabilities\nand Exposures (CVEs) that malware may exploit. We developed a proof-of-concept\ntool called MalCVE, which integrates binary code decompilation, deobfuscation,\nLLM-based code summarization, semantic similarity search, and CVE\nclassification using LLMs. We evaluated MalCVE using a benchmark dataset of\n3,839 JAR executables. MalCVE achieved a mean malware detection accuracy of\n97%, at a fraction of the cost of commercial solutions. It is also the first\ntool to associate CVEs with binary malware, achieving a recall@10 of 65%, which\nis comparable to studies that perform similar analyses on source code."}
{"id": "2510.15798", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.15798", "abs": "https://arxiv.org/abs/2510.15798", "authors": ["Jinwoo Kim", "Minjae Seo", "Eduard Marin", "Seungsoo Lee", "Jaehyun Nam", "Seungwon Shin"], "title": "Ambusher: Exploring the Security of Distributed SDN Controllers Through Protocol State Fuzzing", "comment": "17 pages, 16 figures", "summary": "Distributed SDN (Software-Defined Networking) controllers have rapidly become\nan integral element of Wide Area Networks (WAN), particularly within SD-WAN,\nproviding scalability and fault-tolerance for expansive network\ninfrastructures. However, the architecture of these controllers introduces new\npotential attack surfaces that have thus far received inadequate attention. In\nresponse to these concerns, we introduce Ambusher, a testing tool designed to\ndiscover vulnerabilities within protocols used in distributed SDN controllers.\nAmbusher achieves this by leveraging protocol state fuzzing, which\nsystematically finds attack scenarios based on an inferred state machine. Since\nlearning states from a cluster is complicated, Ambusher proposes a novel\nmethodology that extracts a single and relatively simple state machine,\nachieving efficient state-based fuzzing. Our evaluation of Ambusher, conducted\non a real SD-WAN deployment spanning two campus networks and one enterprise\nnetwork, illustrates its ability to uncover 6 potential vulnerabilities in the\nwidely used distributed controller platform."}
{"id": "2510.15801", "categories": ["cs.CR", "cs.CY", "cs.HC", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.15801", "abs": "https://arxiv.org/abs/2510.15801", "authors": ["Bonnie Rushing", "Mac-Rufus Umeokolo", "Shouhuai Xu"], "title": "Towards Proactive Defense Against Cyber Cognitive Attacks", "comment": "University of Colorado Colorado Springs and Department of the Air\n  Force, US Air Force Academy. Disclaimer: The views expressed are those of the\n  author and do not reflect the official policy or position of the US Air Force\n  Academy, US Air Force, Department of Defense, or the US Government", "summary": "Cyber cognitive attacks leverage disruptive innovations (DIs) to exploit\npsychological biases and manipulate decision-making processes. Emerging\ntechnologies, such as AI-driven disinformation and synthetic media, have\naccelerated the scale and sophistication of these threats. Prior studies\nprimarily categorize current cognitive attack tactics, lacking predictive\nmechanisms to anticipate future DIs and their malicious use in cognitive\nattacks. This paper addresses these gaps by introducing a novel predictive\nmethodology for forecasting the emergence of DIs and their malicious uses in\ncognitive attacks. We identify trends in adversarial tactics and propose\nproactive defense strategies."}
