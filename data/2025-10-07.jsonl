{"id": "2510.03319", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03319", "abs": "https://arxiv.org/abs/2510.03319", "authors": ["Chenxiang Luo", "David K. Y. Yau", "Qun Song"], "title": "SVDefense: Effective Defense against Gradient Inversion Attacks via Singular Value Decomposition", "comment": null, "summary": "Federated learning (FL) enables collaborative model training without sharing\nraw data but is vulnerable to gradient inversion attacks (GIAs), where\nadversaries reconstruct private data from shared gradients. Existing defenses\neither incur impractical computational overhead for embedded platforms or fail\nto achieve privacy protection and good model utility at the same time.\nMoreover, many defenses can be easily bypassed by adaptive adversaries who have\nobtained the defense details. To address these limitations, we propose\nSVDefense, a novel defense framework against GIAs that leverages the truncated\nSingular Value Decomposition (SVD) to obfuscate gradient updates. SVDefense\nintroduces three key innovations, a Self-Adaptive Energy Threshold that adapts\nto client vulnerability, a Channel-Wise Weighted Approximation that selectively\npreserves essential gradient information for effective model training while\nenhancing privacy protection, and a Layer-Wise Weighted Aggregation for\neffective model aggregation under class imbalance. Our extensive evaluation\nshows that SVDefense outperforms existing defenses across multiple\napplications, including image classification, human activity recognition, and\nkeyword spotting, by offering robust privacy protection with minimal impact on\nmodel accuracy. Furthermore, SVDefense is practical for deployment on various\nresource-constrained embedded platforms. We will make our code publicly\navailable upon paper acceptance."}
{"id": "2510.03320", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03320", "abs": "https://arxiv.org/abs/2510.03320", "authors": ["Raik Dankworth", "Gesina Schwalbe"], "title": "Attack logics, not outputs: Towards efficient robustification of deep neural networks by falsifying concept-based properties", "comment": "13 pages, 2 figures, accepted by \"7th OVERLAY\" workshop", "summary": "Deep neural networks (NNs) for computer vision are vulnerable to adversarial\nattacks, i.e., miniscule malicious changes to inputs may induce unintuitive\noutputs. One key approach to verify and mitigate such robustness issues is to\nfalsify expected output behavior. This allows, e.g., to locally proof security,\nor to (re)train NNs on obtained adversarial input examples. Due to the\nblack-box nature of NNs, current attacks only falsify a class of the final\noutput, such as flipping from $\\texttt{stop_sign}$ to $\\neg\\texttt{stop_sign}$.\nIn this short position paper we generalize this to search for generally\nillogical behavior, as considered in NN verification: falsify constraints\n(concept-based properties) involving further human-interpretable concepts, like\n$\\texttt{red}\\wedge\\texttt{octogonal}\\rightarrow\\texttt{stop_sign}$. For this,\nan easy implementation of concept-based properties on already trained NNs is\nproposed using techniques from explainable artificial intelligence. Further, we\nsketch the theoretical proof that attacks on concept-based properties are\nexpected to have a reduced search space compared to simple class falsification,\nwhilst arguably be more aligned with intuitive robustness targets. As an\noutlook to this work in progress we hypothesize that this approach has\npotential to efficiently and simultaneously improve logical compliance and\nrobustness."}
{"id": "2510.03407", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03407", "abs": "https://arxiv.org/abs/2510.03407", "authors": ["Boniface M. Sindala", "Ragib Hasan"], "title": "Security Analysis and Threat Modeling of Research Management Applications [Extended Version]", "comment": "8 pages, 4 tables, 2 figures, This is an extended version of a paper\n  published in IEEE SoutheastCon 2025. \\c{opyright} 2025 IEEE", "summary": "Research management applications (RMA) are widely used in clinical research\nenvironments to collect, transmit, analyze, and store sensitive data. This data\nis so valuable making RMAs susceptible to security threats. This analysis,\nanalyzes RMAs' security, focusing on Research Electronic Data Capture (REDCap)\nas an example. We explore the strengths and vulnerabilities within RMAs by\nevaluating the architecture, data flow, and security features. We identify and\nassess potential risks using the MITRE ATT\\&CK framework and STRIDE model. We\nassess REDCap's defenses against common attack vectors focusing on security to\nprovide confidentiality, integrity, availability, non-repudiation, and\nauthentication. We conclude by proposing recommendations for enhancing the\nsecurity of RMAs, ensuring that critical research data remains protected\nwithout compromising usability. This research aims to contribute towards a more\nsecure framework for managing sensitive information in research-intensive\nenvironments."}
{"id": "2510.03417", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03417", "abs": "https://arxiv.org/abs/2510.03417", "authors": ["Javad Rafiei Asl", "Sidhant Narula", "Mohammad Ghasemigol", "Eduardo Blanco", "Daniel Takabi"], "title": "NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn LLM Jailbreaks", "comment": "Javad Rafiei Asl and Sidhant Narula are co-first authors", "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nbut remain vulnerable to jailbreak attacks, especially multi-turn jailbreaks\nthat distribute malicious intent across benign exchanges and bypass alignment\nmechanisms. Existing approaches often explore the adversarial space poorly,\nrely on hand-crafted heuristics, or lack systematic query refinement. We\npresent NEXUS (Network Exploration for eXploiting Unsafe Sequences), a modular\nframework for constructing, refining, and executing optimized multi-turn\nattacks. NEXUS comprises: (1) ThoughtNet, which hierarchically expands a\nharmful intent into a structured semantic network of topics, entities, and\nquery chains; (2) a feedback-driven Simulator that iteratively refines and\nprunes these chains through attacker-victim-judge LLM collaboration using\nharmfulness and semantic-similarity benchmarks; and (3) a Network Traverser\nthat adaptively navigates the refined query space for real-time attacks. This\npipeline uncovers stealthy, high-success adversarial paths across LLMs. On\nseveral closed-source and open-source LLMs, NEXUS increases attack success rate\nby 2.1% to 19.4% over prior methods. Code: https://github.com/inspire-lab/NEXUS"}
{"id": "2510.03542", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03542", "abs": "https://arxiv.org/abs/2510.03542", "authors": ["Pouriya Alimoradi", "Ali Barati", "Hamid Barati"], "title": "A Multi-Layer Electronic and Cyber Interference Model for AI-Driven Cruise Missiles: The Case of Khuzestan Province", "comment": null, "summary": "The rapid advancement of Artificial Intelligence has enabled the development\nof cruise missiles endowed with high levels of autonomy, adaptability, and\nprecision. These AI driven missiles integrating deep learning algorithms, real\ntime data processing, and advanced guidance systems pose critical threats to\nstrategic infrastructures, especially under complex geographic and climatic\nconditions such as those found in Irans Khuzestan Province. In this paper, we\npropose a multi layer interference model, encompassing electronic warfare,\ncyberattacks, and deception strategies, to degrade the performance of AI guided\ncruise missiles significantly. Our experimental results, derived from 400\nsimulation runs across four distinct scenarios, demonstrate notable\nimprovements when employing the integrated multi layer approach compared to\nsingle layer or no interference baselines. Specifically, the average missile\ndeviation from its intended target increases from 0.25 to 8.65 under multi\nlayer interference a more than 3300 increase in angular deviation. Furthermore,\nthe target acquisition success rate is reduced from 92.7 in the baseline\nscenario to 31.5, indicating a 66 decrease in successful strikes. While\nresource consumption for multi layer strategies rises by approximately 25\ncompared to single layer methods, the significant drop in missile accuracy and\nreliability justifies the more intensive deployment of jamming power, cyber\nresources, and decoy measures. Beyond these quantitative improvements, the\nproposed framework uses a deep reinforcement learning based defense coordinator\nto adaptively select the optimal configuration of EW, cyber, and deception\ntactics in real time."}
{"id": "2510.03559", "categories": ["cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.03559", "abs": "https://arxiv.org/abs/2510.03559", "authors": ["Zeya Chen", "Jianing Wen", "Ruth Schmidt", "Yaxing Yao", "Toby Jia-Jun Li", "Tianshi Li"], "title": "PrivacyMotiv: Speculative Persona Journeys for Empathic and Motivating Privacy Reviews in UX Design", "comment": "42 pages, 13 figures", "summary": "UX professionals routinely conduct design reviews, yet privacy concerns are\noften overlooked -- not only due to limited tools, but more critically because\nof low intrinsic motivation. Limited privacy knowledge, weak empathy for\nunexpectedly affected users, and low confidence in identifying harms make it\ndifficult to address risks. We present PrivacyMotiv, an LLM-powered system that\nsupports privacy-oriented design diagnosis by generating speculative personas\nwith UX user journeys centered on individuals vulnerable to privacy risks.\nDrawing on narrative strategies, the system constructs relatable and\nattention-drawing scenarios that show how ordinary design choices may cause\nunintended harms, expanding the scope of privacy reflection in UX. In a\nwithin-subjects study with professional UX practitioners (N=16), we compared\nparticipants' self-proposed methods with PrivacyMotiv across two privacy review\ntasks. Results show significant improvements in empathy, intrinsic motivation,\nand perceived usefulness. This work contributes a promising privacy review\napproach which addresses the motivational barriers in privacy-aware UX."}
{"id": "2510.03565", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03565", "abs": "https://arxiv.org/abs/2510.03565", "authors": ["Cory Brynds", "Parker McLeod", "Lauren Caccamise", "Asmita Pal", "Dewan Saiham", "Sazadur Rahman", "Joshua San Miguel", "Di Wu"], "title": "CryptOracle: A Modular Framework to Characterize Fully Homomorphic Encryption", "comment": null, "summary": "Privacy-preserving machine learning has become an important long-term pursuit\nin this era of artificial intelligence (AI). Fully Homomorphic Encryption (FHE)\nis a uniquely promising solution, offering provable privacy and security\nguarantees. Unfortunately, computational cost is impeding its mass adoption.\nModern solutions are up to six orders of magnitude slower than plaintext\nexecution. Understanding and reducing this overhead is essential to the\nadvancement of FHE, particularly as the underlying algorithms evolve rapidly.\nThis paper presents a detailed characterization of OpenFHE, a comprehensive\nopen-source library for FHE, with a particular focus on the CKKS scheme due to\nits significant potential for AI and machine learning applications. We\nintroduce CryptOracle, a modular evaluation framework comprising (1) a\nbenchmark suite, (2) a hardware profiler, and (3) a predictive performance\nmodel. The benchmark suite encompasses OpenFHE kernels at three abstraction\nlevels: workloads, microbenchmarks, and primitives. The profiler is compatible\nwith standard and user-specified security parameters. CryptOracle monitors\napplication performance, captures microarchitectural events, and logs power and\nenergy usage for AMD and Intel systems. These metrics are consumed by a\nmodeling engine to estimate runtime and energy efficiency across different\nconfiguration scenarios, with error geomean of $-7.02\\%\\sim8.40\\%$ for runtime\nand $-9.74\\%\\sim15.67\\%$ for energy. CryptOracle is open source, fully modular,\nand serves as a shared platform to facilitate the collaborative advancements of\napplications, algorithms, software, and hardware in FHE. The CryptOracle code\ncan be accessed at https://github.com/UnaryLab/CryptOracle."}
{"id": "2510.03610", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03610", "abs": "https://arxiv.org/abs/2510.03610", "authors": ["Zachary Ezetta", "Wu-chang Feng"], "title": "PentestMCP: A Toolkit for Agentic Penetration Testing", "comment": null, "summary": "Agentic AI is transforming security by automating many tasks being performed\nmanually. While initial agentic approaches employed a monolithic architecture,\nthe Model-Context-Protocol has now enabled a remote-procedure call (RPC)\nparadigm to agentic applications, allowing for the flexible construction and\ncomposition of multi-function agents. This paper describes PentestMCP, a\nlibrary of MCP server implementations that support agentic penetration testing.\nBy supporting common penetration testing tasks such as network scanning,\nresource enumeration, service fingerprinting, vulnerability scanning,\nexploitation, and post-exploitation, PentestMCP allows a developer to customize\nmulti-agent workflows for performing penetration tests."}
{"id": "2510.03623", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03623", "abs": "https://arxiv.org/abs/2510.03623", "authors": ["Maraz Mia", "Mir Mehedi A. Pritom"], "title": "Explainable but Vulnerable: Adversarial Attacks on XAI Explanation in Cybersecurity Applications", "comment": "10 pages, 9 figures, 4 tables", "summary": "Explainable Artificial Intelligence (XAI) has aided machine learning (ML)\nresearchers with the power of scrutinizing the decisions of the black-box\nmodels. XAI methods enable looking deep inside the models' behavior, eventually\ngenerating explanations along with a perceived trust and transparency. However,\ndepending on any specific XAI method, the level of trust can vary. It is\nevident that XAI methods can themselves be a victim of post-adversarial attacks\nthat manipulate the expected outcome from the explanation module. Among such\nattack tactics, fairwashing explanation (FE), manipulation explanation (ME),\nand backdoor-enabled manipulation attacks (BD) are the notable ones. In this\npaper, we try to understand these adversarial attack techniques, tactics, and\nprocedures (TTPs) on explanation alteration and thus the effect on the model's\ndecisions. We have explored a total of six different individual attack\nprocedures on post-hoc explanation methods such as SHAP (SHapley Additive\nexPlanations), LIME (Local Interpretable Model-agnostic Explanation), and IG\n(Integrated Gradients), and investigated those adversarial attacks in\ncybersecurity applications scenarios such as phishing, malware, intrusion, and\nfraudulent website detection. Our experimental study reveals the actual\neffectiveness of these attacks, thus providing an urgency for immediate\nattention to enhance the resiliency of XAI methods and their applications."}
{"id": "2510.03625", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.03625", "abs": "https://arxiv.org/abs/2510.03625", "authors": ["Joachim Neu", "Javier Nieto", "Ling Ren"], "title": "On the Limits of Consensus under Dynamic Availability and Reconfiguration", "comment": null, "summary": "Proof-of-stake blockchains require consensus protocols that support Dynamic\nAvailability and Reconfiguration (so-called DAR setting), where the former\nmeans that the consensus protocol should remain live even if a large number of\nnodes temporarily crash, and the latter means it should be possible to change\nthe set of operating nodes over time. State-of-the-art protocols for the DAR\nsetting, such as Ethereum, Cardano's Ouroboros, or Snow White, require\nunrealistic additional assumptions, such as social consensus, or that key\nevolution is performed even while nodes are not participating. In this paper,\nwe identify the necessary and sufficient adversarial condition under which\nconsensus can be achieved in the DAR setting without additional assumptions. We\nthen introduce a new and realistic additional assumption: honest nodes dispose\nof their cryptographic keys the moment they express intent to exit from the set\nof operating nodes. To add reconfiguration to any dynamically available\nconsensus protocol, we provide a bootstrapping gadget that is particularly\nsimple and efficient in the common optimistic case of few reconfigurations and\nno double-spending attempts."}
{"id": "2510.03631", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03631", "abs": "https://arxiv.org/abs/2510.03631", "authors": ["Saleh Darzi", "Saif Eddine Nouma", "Kiarash Sedghighadikolaei", "Attila Altay"], "title": "QPADL: Post-Quantum Private Spectrum Access with Verified Location and DoS Resilience", "comment": "13 pages, 3 figures, 1 table, 4 algorithms", "summary": "With advances in wireless communication and growing spectrum scarcity,\nSpectrum Access Systems (SASs) offer an opportunistic solution but face\nsignificant security challenges. Regulations require disclosure of location\ncoordinates and transmission details, exposing user privacy and anonymity\nduring spectrum queries, while the database operations themselves permit\nDenial-of-Service (DoS) attacks. As location-based services, SAS is also\nvulnerable to compromised or malicious users conducting spoofing attacks. These\nthreats are further amplified given the quantum computing advancements. Thus,\nwe propose QPADL, the first post-quantum (PQ) secure framework that\nsimultaneously ensures privacy, anonymity, location verification, and DoS\nresilience while maintaining efficiency for large-scale spectrum access\nsystems. QPADL introduces SAS-tailored private information retrieval for\nlocation privacy, a PQ-variant of Tor for anonymity, and employs advanced\nsignature constructions for location verification alongside client puzzle\nprotocols and rate-limiting technique for DoS defense. We formally assess its\nsecurity and conduct a comprehensive performance evaluation, incorporating GPU\nparallelization and optimization strategies to demonstrate practicality and\nscalability."}
{"id": "2510.03697", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03697", "abs": "https://arxiv.org/abs/2510.03697", "authors": ["Benjamin Marsh", "Paolo Serafino"], "title": "A Time-Bound Signature Scheme for Blockchains", "comment": "Accepted to the 2025 IEEE International Conference on Blockchain", "summary": "We introduce a modified Schnorr signature scheme to allow for time-bound\nsignatures for transaction fee auction bidding and smart contract purposes in a\nblockchain context, ensuring an honest producer can only validate a signature\nbefore a given block height. The immutable blockchain is used as a source of\nuniversal time for the signature scheme. We show the use of such a signature\nscheme leads to lower MEV revenue for builders. We then apply our time-bound\nsignatures to Ethereum's EIP-1559 and show how it can be used to mitigate the\neffect of MEV on predicted equilibrium strategies."}
{"id": "2510.03705", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03705", "abs": "https://arxiv.org/abs/2510.03705", "authors": ["Yulin Chen", "Haoran Li", "Yuan Sui", "Yangqiu Song", "Bryan Hooi"], "title": "Backdoor-Powered Prompt Injection Attacks Nullify Defense Methods", "comment": "EMNLP 2025 Findings", "summary": "With the development of technology, large language models (LLMs) have\ndominated the downstream natural language processing (NLP) tasks. However,\nbecause of the LLMs' instruction-following abilities and inability to\ndistinguish the instructions in the data content, such as web pages from search\nengines, the LLMs are vulnerable to prompt injection attacks. These attacks\ntrick the LLMs into deviating from the original input instruction and executing\nthe attackers' target instruction. Recently, various instruction hierarchy\ndefense strategies are proposed to effectively defend against prompt injection\nattacks via fine-tuning. In this paper, we explore more vicious attacks that\nnullify the prompt injection defense methods, even the instruction hierarchy:\nbackdoor-powered prompt injection attacks, where the attackers utilize the\nbackdoor attack for prompt injection attack purposes. Specifically, the\nattackers poison the supervised fine-tuning samples and insert the backdoor\ninto the model. Once the trigger is activated, the backdoored model executes\nthe injected instruction surrounded by the trigger. We construct a benchmark\nfor comprehensive evaluation. Our experiments demonstrate that backdoor-powered\nprompt injection attacks are more harmful than previous prompt injection\nattacks, nullifying existing prompt injection defense methods, even the\ninstruction hierarchy techniques."}
{"id": "2510.03720", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03720", "abs": "https://arxiv.org/abs/2510.03720", "authors": ["Dongyang Zhan", "Zhaofeng Yu", "Xiangzhan Yu", "Hongli Zhang", "Lin Ye"], "title": "Shrinking the Kernel Attack Surface Through Static and Dynamic Syscall Limitation", "comment": "13 pages, 5 figures. Accepted for publication in IEEE Transactions on\n  Services Computing (TSC), 2023", "summary": "Linux Seccomp is widely used by the program developers and the system\nmaintainers to secure the operating systems, which can block unused syscalls\nfor different applications and containers to shrink the attack surface of the\noperating systems. However, it is difficult to configure the whitelist of a\ncontainer or application without the help of program developers. Docker\ncontainers block about only 50 syscalls by default, and lots of unblocked\nuseless syscalls introduce a big kernel attack surface. To obtain the dependent\nsyscalls, dynamic tracking is a straight-forward approach but it cannot get the\nfull syscall list. Static analysis can construct an over-approximated syscall\nlist, but the list contains many false positives. In this paper, a systematic\ndependent syscall analysis approach, sysverify, is proposed by combining static\nanalysis and dynamic verification together to shrink the kernel attack surface.\nThe semantic gap between the binary executables and syscalls is bridged by\nanalyzing the binary and the source code, which builds the mapping between the\nlibrary APIs and syscalls systematically. To further reduce the attack surface\nat best effort, we propose a dynamic verification approach to intercept and\nanalyze the security of the invocations of indirect-call-related or rarely\ninvoked syscalls with low overhead."}
{"id": "2510.03737", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03737", "abs": "https://arxiv.org/abs/2510.03737", "authors": ["Dongyang Zhan", "Zhaofeng Yu", "Xiangzhan Yu", "Hongli Zhang", "Lin Ye", "Likun Liu"], "title": "Securing Operating Systems Through Fine-grained Kernel Access Limitation for IoT Systems", "comment": "14 pages, 3 figures. Accepted for publication in IEEE Internet of\n  Things Journal (IOTJ), 2023", "summary": "With the development of Internet of Things (IoT), it is gaining a lot of\nattention. It is important to secure the embedded systems with low overhead.\nThe Linux Seccomp is widely used by developers to secure the kernels by\nblocking the access of unused syscalls, which introduces less overhead.\nHowever, there are no systematic Seccomp configuration approaches for IoT\napplications without the help of developers. In addition, the existing Seccomp\nconfiguration approaches are coarse-grained, which cannot analyze and limit the\nsyscall arguments. In this paper, a novel static dependent syscall analysis\napproach for embedded applications is proposed, which can obtain all of the\npossible dependent syscalls and the corresponding arguments of the target\napplications. So, a fine-grained kernel access limitation can be performed for\nthe IoT applications. To this end, the mappings between dynamic library APIs\nand syscalls according with their arguments are built, by analyzing the control\nflow graphs and the data dependency relationships of the dynamic libraries. To\nthe best of our knowledge, this is the first work to generate the fine-grained\nSeccomp profile for embedded applications."}
{"id": "2510.03752", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03752", "abs": "https://arxiv.org/abs/2510.03752", "authors": ["Rohit Chatterjee", "Changrui Mu", "Prashant Nalini Vasudevan"], "title": "Public-Key Encryption from the MinRank Problem", "comment": null, "summary": "We construct a public-key encryption scheme from the hardness of the\n(planted) MinRank problem over uniformly random instances. This corresponds to\nthe hardness of decoding random linear rank-metric codes. Existing\nconstructions of public-key encryption from such problems require hardness for\nstructured instances arising from the masking of efficiently decodable codes.\nCentral to our construction is the development of a new notion of duality for\nrank-metric codes."}
{"id": "2510.03761", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03761", "abs": "https://arxiv.org/abs/2510.03761", "authors": ["Richard A. Dubniczky", "Bertalan Borsos", "Tihanyi Norbert"], "title": "You Have Been LaTeXpOsEd: A Systematic Analysis of Information Leakage in Preprint Archives Using Large Language Models", "comment": null, "summary": "The widespread use of preprint repositories such as arXiv has accelerated the\ncommunication of scientific results but also introduced overlooked security\nrisks. Beyond PDFs, these platforms provide unrestricted access to original\nsource materials, including LaTeX sources, auxiliary code, figures, and\nembedded comments. In the absence of sanitization, submissions may disclose\nsensitive information that adversaries can harvest using open-source\nintelligence. In this work, we present the first large-scale security audit of\npreprint archives, analyzing more than 1.2 TB of source data from 100,000 arXiv\nsubmissions. We introduce LaTeXpOsEd, a four-stage framework that integrates\npattern matching, logical filtering, traditional harvesting techniques, and\nlarge language models (LLMs) to uncover hidden disclosures within\nnon-referenced files and LaTeX comments. To evaluate LLMs' secret-detection\ncapabilities, we introduce LLMSec-DB, a benchmark on which we tested 25\nstate-of-the-art models. Our analysis uncovered thousands of PII leaks,\nGPS-tagged EXIF files, publicly available Google Drive and Dropbox folders,\neditable private SharePoint links, exposed GitHub and Google credentials, and\ncloud API keys. We also uncovered confidential author communications, internal\ndisagreements, and conference submission credentials, exposing information that\nposes serious reputational risks to both researchers and institutions. We urge\nthe research community and repository operators to take immediate action to\nclose these hidden security gaps. To support open science, we release all\nscripts and methods from this study but withhold sensitive findings that could\nbe misused, in line with ethical principles. The source code and related\nmaterial are available at the project website https://github.com/LaTeXpOsEd"}
{"id": "2510.03770", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03770", "abs": "https://arxiv.org/abs/2510.03770", "authors": ["David Megias"], "title": "Complex Domain Approach for Reversible Data Hiding and Homomorphic Encryption: General Framework and Application to Dispersed Data", "comment": null, "summary": "Ensuring the trustworthiness of data from distributed and\nresource-constrained environments, such as Wireless Sensor Networks or IoT\ndevices, is critical. Existing Reversible Data Hiding (RDH) methods for scalar\ndata suffer from low embedding capacity and poor intrinsic mixing between host\ndata and watermark. This paper introduces Hiding in the Imaginary Domain with\nData Encryption (H[i]dden), a novel framework based on complex number\narithmetic for simultaneous information embedding and encryption. The H[i]dden\nframework offers perfect reversibility, in-principle unlimited watermark size,\nand intrinsic data-watermark mixing. The paper further introduces two\nprotocols: H[i]dden-EG, for joint reversible data hiding and encryption, and\nH[i]dden-AggP, for privacy-preserving aggregation of watermarked data, based on\npartially homomorphic encryption. These protocols provide efficient and\nresilient solutions for data integrity, provenance and confidentiality, serving\nas a foundation for new schemes based on the algebraic properties of the\ncomplex domain."}
{"id": "2510.03819", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03819", "abs": "https://arxiv.org/abs/2510.03819", "authors": ["Chunyi Zhang", "Qinghong Wei", "Xiaoqi Li"], "title": "Security Analysis of Ponzi Schemes in Ethereum Smart Contracts", "comment": null, "summary": "The rapid advancement of blockchain technology has precipitated the\nwidespread adoption of Ethereum and smart contracts across a variety of\nsectors. However, this has also given rise to numerous fraudulent activities,\nwith many speculators embedding Ponzi schemes within smart contracts, resulting\nin significant financial losses for investors. Currently, there is a lack of\neffective methods for identifying and analyzing such new types of fraudulent\nactivities. This paper categorizes these scams into four structural types and\nexplores the intrinsic characteristics of Ponzi scheme contract source code\nfrom a program analysis perspective. The Mythril tool is employed to conduct\nstatic and dynamic analyses of representative cases, thereby revealing their\nvulnerabilities and operational mechanisms. Furthermore, this paper employs\nshell scripts and command patterns to conduct batch detection of open-source\nsmart contract code, thereby unveiling the common characteristics of Ponzi\nscheme smart contracts."}
{"id": "2510.03831", "categories": ["cs.CR", "cs.IT", "cs.LG", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.03831", "abs": "https://arxiv.org/abs/2510.03831", "authors": ["Pedro Ivo da Cruz", "Dimitri Silva", "Tito Spadini", "Ricardo Suyama", "Murilo Bellezoni Loiola"], "title": "Pilot Contamination Attacks Detection with Machine Learning for Multi-User Massive MIMO", "comment": "This version of the article has been accepted for publication, after\n  peer review and is subject to Springer Nature's AM terms of use, but is not\n  the Version of Record and does not reflect post-acceptance improvements, or\n  any corrections. The Version of Record is available online at:\n  https://doi.org/10.1007/s11235-024-01163-0", "summary": "Massive multiple-input multiple-output (MMIMO) is essential to modern\nwireless communication systems, like 5G and 6G, but it is vulnerable to active\neavesdropping attacks. One type of such attack is the pilot contamination\nattack (PCA), where a malicious user copies pilot signals from an authentic\nuser during uplink, intentionally interfering with the base station's (BS)\nchannel estimation accuracy. In this work, we propose to use a Decision Tree\n(DT) algorithm for PCA detection at the BS in a multi-user system. We present a\nmethodology to generate training data for the DT classifier and select the best\nDT according to their depth. Then, we simulate different scenarios that could\nbe encountered in practice and compare the DT to a classical technique based on\nlikelihood ratio testing (LRT) submitted to the same scenarios. The results\nrevealed that a DT with only one level of depth is sufficient to outperform the\nLRT. The DT shows a good performance regarding the probability of detection in\nnoisy scenarios and when the malicious user transmits with low power, in which\ncase the LRT fails to detect the PCA. We also show that the reason for the good\nperformance of the DT is its ability to compute a threshold that separates PCA\ndata from non-PCA data better than the LRT's threshold. Moreover, the DT does\nnot necessitate prior knowledge of noise power or assumptions regarding the\nsignal power of malicious users, prerequisites typically essential for LRT and\nother hypothesis testing methodologies."}
{"id": "2510.03992", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03992", "abs": "https://arxiv.org/abs/2510.03992", "authors": ["Jehyeok Yeon", "Isha Chaudhary", "Gagandeep Singh"], "title": "Quantifying Distributional Robustness of Agentic Tool-Selection", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in agentic systems\nwhere they map user intents to relevant external tools to fulfill a task. A\ncritical step in this process is tool selection, where a retriever first\nsurfaces candidate tools from a larger pool, after which the LLM selects the\nmost appropriate one. This pipeline presents an underexplored attack surface\nwhere errors in selection can lead to severe outcomes like unauthorized data\naccess or denial of service, all without modifying the agent's model or code.\nWhile existing evaluations measure task performance in benign settings, they\noverlook the specific vulnerabilities of the tool selection mechanism under\nadversarial conditions. To address this gap, we introduce ToolCert, the first\nstatistical framework that formally certifies tool selection robustness.\nToolCert models tool selection as a Bernoulli success process and evaluates it\nagainst a strong, adaptive attacker who introduces adversarial tools with\nmisleading metadata, and are iteratively refined based on the agent's previous\nchoices. By sampling these adversarial interactions, ToolCert produces a\nhigh-confidence lower bound on accuracy, formally quantifying the agent's\nworst-case performance. Our evaluation with ToolCert uncovers the severe\nfragility: under attacks injecting deceptive tools or saturating retrieval, the\ncertified accuracy bound drops near zero, an average performance drop of over\n60% compared to non-adversarial settings. For attacks targeting the retrieval\nand selection stages, the certified accuracy bound plummets to less than 20%\nafter just a single round of adversarial adaptation. ToolCert thus reveals\npreviously unexamined security threats inherent to tool selection and provides\na principled method to quantify an agent's robustness to such threats, a\nnecessary step for the safe deployment of agentic systems."}
{"id": "2510.03995", "categories": ["cs.CR", "cs.AI", "I.2; E.m"], "pdf": "https://arxiv.org/pdf/2510.03995", "abs": "https://arxiv.org/abs/2510.03995", "authors": ["Nges Brian Njungle", "Eric Jahns", "Milan Stojkov", "Michel A. Kinsy"], "title": "PrivSpike: Employing Homomorphic Encryption for Private Inference of Deep Spiking Neural Networks", "comment": "13 pages, 5 figures", "summary": "Deep learning has become a cornerstone of modern machine learning. It relies\nheavily on vast datasets and significant computational resources for high\nperformance. This data often contains sensitive information, making privacy a\nmajor concern in deep learning. Spiking Neural Networks (SNNs) have emerged as\nan energy-efficient alternative to conventional deep learning approaches.\nNevertheless, SNNs still depend on large volumes of data, inheriting all the\nprivacy challenges of deep learning. Homomorphic encryption addresses this\nchallenge by allowing computations to be performed on encrypted data, ensuring\ndata confidentiality throughout the entire processing pipeline. In this paper,\nwe introduce PRIVSPIKE, a privacy-preserving inference framework for SNNs using\nthe CKKS homomorphic encryption scheme. PRIVSPIKE supports arbitrary depth SNNs\nand introduces two key algorithms for evaluating the Leaky Integrate-and-Fire\nactivation function: (1) a polynomial approximation algorithm designed for\nhigh-performance SNN inference, and (2) a novel scheme-switching algorithm that\noptimizes precision at a higher computational cost. We evaluate PRIVSPIKE on\nMNIST, CIFAR-10, Neuromorphic MNIST, and CIFAR-10 DVS using models from LeNet-5\nand ResNet-19 architectures, achieving encrypted inference accuracies of\n98.10%, 79.3%, 98.1%, and 66.0%, respectively. On a consumer-grade CPU, SNN\nLeNet-5 models achieved inference times of 28 seconds on MNIST and 212 seconds\non Neuromorphic MNIST. For SNN ResNet-19 models, inference took 784 seconds on\nCIFAR-10 and 1846 seconds on CIFAR-10 DVS. These results establish PRIVSPIKE as\na viable and efficient solution for secure SNN inference, bridging the gap\nbetween energy-efficient deep neural networks and strong cryptographic privacy\nguarantees while outperforming prior encrypted SNN solutions."}
{"id": "2510.03996", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03996", "abs": "https://arxiv.org/abs/2510.03996", "authors": ["Nges Brian Njungle", "Eric Jahns", "Michel A. Kinsy"], "title": "FHEON: A Configurable Framework for Developing Privacy-Preserving Neural Networks Using Homomorphic Encryption", "comment": "14 pages, 6 figures", "summary": "The widespread adoption of Machine Learning as a Service raises critical\nprivacy and security concerns, particularly about data confidentiality and\ntrust in both cloud providers and the machine learning models. Homomorphic\nEncryption (HE) has emerged as a promising solution to this problems, allowing\ncomputations on encrypted data without decryption. Despite its potential,\nexisting approaches to integrate HE into neural networks are often limited to\nspecific architectures, leaving a wide gap in providing a framework for easy\ndevelopment of HE-friendly privacy-preserving neural network models similar to\nwhat we have in the broader field of machine learning. In this paper, we\npresent FHEON, a configurable framework for developing privacy-preserving\nconvolutional neural network (CNN) models for inference using HE. FHEON\nintroduces optimized and configurable implementations of privacy-preserving CNN\nlayers including convolutional layers, average pooling layers, ReLU activation\nfunctions, and fully connected layers. These layers are configured using\nparameters like input channels, output channels, kernel size, stride, and\npadding to support arbitrary CNN architectures. We assess the performance of\nFHEON using several CNN architectures, including LeNet-5, VGG-11, VGG- 16,\nResNet-20, and ResNet-34. FHEON maintains encrypted-domain accuracies within\n+/- 1% of their plaintext counterparts for ResNet-20 and LeNet-5 models.\nNotably, on a consumer-grade CPU, the models build on FHEON achieved 98.5%\naccuracy with a latency of 13 seconds on MNIST using LeNet-5, and 92.2%\naccuracy with a latency of 403 seconds on CIFAR-10 using ResNet-20.\nAdditionally, FHEON operates within a practical memory budget requiring not\nmore than 42.3 GB for VGG-16."}
{"id": "2510.04056", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.04056", "abs": "https://arxiv.org/abs/2510.04056", "authors": ["Rijha Safdar", "Danyail Mateen", "Syed Taha Ali", "Wajahat Hussain"], "title": "Real-VulLLM: An LLM Based Assessment Framework in the Wild", "comment": null, "summary": "Artificial Intelligence (AI) and more specifically Large Language Models\n(LLMs) have demonstrated exceptional progress in multiple areas including\nsoftware engineering, however, their capability for vulnerability detection in\nthe wild scenario and its corresponding reasoning remains underexplored.\nPrompting pre-trained LLMs in an effective way offers a computationally\neffective and scalable solution. Our contributions are (i)varied prompt designs\nfor vulnerability detection and its corresponding reasoning in the wild. (ii)a\nreal-world vector data store constructed from the National Vulnerability\nDatabase, that will provide real time context to vulnerability detection\nframework, and (iii)a scoring measure for combined measurement of accuracy and\nreasoning quality. Our contribution aims to examine whether LLMs are ready for\nwild deployment, thus enabling the reliable use of LLMs stronger for the\ndevelopment of secure software's."}
{"id": "2510.04085", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.04085", "abs": "https://arxiv.org/abs/2510.04085", "authors": ["Prabhanjan Ananth", "John Bostanci", "Aditya Gulati", "Yao-Ting Lin"], "title": "Gluing Random Unitaries with Inverses and Applications to Strong Pseudorandom Unitaries", "comment": "55 pages. A preliminary version, merging this paper and\n  arXiv:2509.24432, appears in the proceedings of the 45th Annual International\n  Cryptology Conference (CRYPTO 2025) under the title \"Pseudorandom Unitaries\n  in the Haar Random Oracle Model\". This is Part II of the full version", "summary": "Gluing theorem for random unitaries [Schuster, Haferkamp, Huang, QIP 2025]\nhave found numerous applications, including designing low depth random\nunitaries [Schuster, Haferkamp, Huang, QIP 2025], random unitaries in ${\\sf\nQAC0}$ [Foxman, Parham, Vasconcelos, Yuen'25] and generically shortening the\nkey length of pseudorandom unitaries [Ananth, Bostanci, Gulati, Lin\nEUROCRYPT'25]. We present an alternate method of combining Haar random\nunitaries from the gluing lemma from [Schuster, Haferkamp, Huang, QIP 2025]\nthat is secure against adversaries with inverse query access to the joined\nunitary. As a consequence, we show for the first time that strong pseudorandom\nunitaries can generically have their length extended, and can be constructed\nusing only $O(n^{1/c})$ bits of randomness, for any constant $c$, if any family\nof strong pseudorandom unitaries exists."}
{"id": "2510.04118", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.04118", "abs": "https://arxiv.org/abs/2510.04118", "authors": ["Prakhar Paliwal", "Atul Kabra", "Manjesh Kumar Hanawal"], "title": "Cyber Warfare During Operation Sindoor: Malware Campaign Analysis and Detection Framework", "comment": "Accepted for presentation at the 21st International Conference on\n  Information Systems Security (ICISS 2025)", "summary": "Rapid digitization of critical infrastructure has made cyberwarfare one of\nthe important dimensions of modern conflicts. Attacking the critical\ninfrastructure is an attractive pre-emptive proposition for adversaries as it\ncan be done remotely without crossing borders. Such attacks disturb the support\nsystems of the opponents to launch any offensive activities, crippling their\nfighting capabilities. Cyberattacks during cyberwarfare can not only be used to\nsteal information, but also to spread disinformation to bring down the morale\nof the opponents. Recent wars in Europe, Africa, and Asia have demonstrated the\nscale and sophistication that the warring nations have deployed to take the\nearly upper hand. In this work, we focus on the military action launched by\nIndia, code-named Operation Sindoor, to dismantle terror infrastructure\nemanating from Pakistan and the cyberattacks launched by Pakistan. In\nparticular, we study the malware used by Pakistan APT groups to deploy Remote\nAccess Trojans in Indian systems. We provide details of the tactics and\ntechniques used in the RAT deployment and develop a telemetry framework to\ncollect necessary event logs using Osquery with a custom extension. Finally, we\ndevelop a detection rule that can be readily deployed to detect the presence of\nthe RAT or any exploitation performed by the malware."}
{"id": "2510.04153", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04153", "abs": "https://arxiv.org/abs/2510.04153", "authors": ["Haoqi Wu", "Wei Dai", "Ming Xu", "Li Wang", "Qiang Yan"], "title": "ObCLIP: Oblivious CLoud-Device Hybrid Image Generation with Privacy Preservation", "comment": "Accepted by NeurIPS 2025", "summary": "Diffusion Models have gained significant popularity due to their remarkable\ncapabilities in image generation, albeit at the cost of intensive computation\nrequirement. Meanwhile, despite their widespread deployment in inference\nservices such as Midjourney, concerns about the potential leakage of sensitive\ninformation in uploaded user prompts have arisen. Existing solutions either\nlack rigorous privacy guarantees or fail to strike an effective balance between\nutility and efficiency. To bridge this gap, we propose ObCLIP, a plug-and-play\nsafeguard that enables oblivious cloud-device hybrid generation. By oblivious,\neach input prompt is transformed into a set of semantically similar candidate\nprompts that differ only in sensitive attributes (e.g., gender, ethnicity). The\ncloud server processes all candidate prompts without knowing which one is the\nreal one, thus preventing any prompt leakage. To mitigate server cost, only a\nsmall portion of denoising steps is performed upon the large cloud model. The\nintermediate latents are then sent back to the client, which selects the\ntargeted latent and completes the remaining denoising using a small device\nmodel. Additionally, we analyze and incorporate several cache-based\naccelerations that leverage temporal and batch redundancy, effectively reducing\ncomputation cost with minimal utility degradation. Extensive experiments across\nmultiple datasets demonstrate that ObCLIP provides rigorous privacy and\ncomparable utility to cloud models with slightly increased server cost."}
{"id": "2510.04257", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04257", "abs": "https://arxiv.org/abs/2510.04257", "authors": ["Yanjie Li", "Yiming Cao", "Dong Wang", "Bin Xiao"], "title": "AgentTypo: Adaptive Typographic Prompt Injection Attacks against Black-box Multimodal Agents", "comment": "13 pages, 8 figures. Submitted to IEEE Transactions on Information\n  Forensics & Security", "summary": "Multimodal agents built on large vision-language models (LVLMs) are\nincreasingly deployed in open-world settings but remain highly vulnerable to\nprompt injection, especially through visual inputs. We introduce AgentTypo, a\nblack-box red-teaming framework that mounts adaptive typographic prompt\ninjection by embedding optimized text into webpage images. Our automatic\ntypographic prompt injection (ATPI) algorithm maximizes prompt reconstruction\nby substituting captioners while minimizing human detectability via a stealth\nloss, with a Tree-structured Parzen Estimator guiding black-box optimization\nover text placement, size, and color. To further enhance attack strength, we\ndevelop AgentTypo-pro, a multi-LLM system that iteratively refines injection\nprompts using evaluation feedback and retrieves successful past examples for\ncontinual learning. Effective prompts are abstracted into generalizable\nstrategies and stored in a strategy repository, enabling progressive knowledge\naccumulation and reuse in future attacks. Experiments on the VWA-Adv benchmark\nacross Classifieds, Shopping, and Reddit scenarios show that AgentTypo\nsignificantly outperforms the latest image-based attacks such as AgentAttack.\nOn GPT-4o agents, our image-only attack raises the success rate from 0.23 to\n0.45, with consistent results across GPT-4V, GPT-4o-mini, Gemini 1.5 Pro, and\nClaude 3 Opus. In image+text settings, AgentTypo achieves 0.68 ASR, also\noutperforming the latest baselines. Our findings reveal that AgentTypo poses a\npractical and potent threat to multimodal agents and highlight the urgent need\nfor effective defense."}
{"id": "2510.04261", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.04261", "abs": "https://arxiv.org/abs/2510.04261", "authors": ["Yu Cui", "Sicheng Pan", "Yifei Liu", "Haibin Zhang", "Cong Zuo"], "title": "VortexPIA: Indirect Prompt Injection Attack against LLMs for Efficient Extraction of User Privacy", "comment": null, "summary": "Large language models (LLMs) have been widely deployed in Conversational AIs\n(CAIs), while exposing privacy and security threats. Recent research shows that\nLLM-based CAIs can be manipulated to extract private information from human\nusers, posing serious security threats. However, the methods proposed in that\nstudy rely on a white-box setting that adversaries can directly modify the\nsystem prompt. This condition is unlikely to hold in real-world deployments.\nThe limitation raises a critical question: can unprivileged attackers still\ninduce such privacy risks in practical LLM-integrated applications? To address\nthis question, we propose \\textsc{VortexPIA}, a novel indirect prompt injection\nattack that induces privacy extraction in LLM-integrated applications under\nblack-box settings. By injecting token-efficient data containing false\nmemories, \\textsc{VortexPIA} misleads LLMs to actively request private\ninformation in batches. Unlike prior methods, \\textsc{VortexPIA} allows\nattackers to flexibly define multiple categories of sensitive data. We evaluate\n\\textsc{VortexPIA} on six LLMs, covering both traditional and reasoning LLMs,\nacross four benchmark datasets. The results show that \\textsc{VortexPIA}\nsignificantly outperforms baselines and achieves state-of-the-art (SOTA)\nperformance. It also demonstrates efficient privacy requests, reduced token\nconsumption, and enhanced robustness against defense mechanisms. We further\nvalidate \\textsc{VortexPIA} on multiple realistic open-source LLM-integrated\napplications, demonstrating its practical effectiveness."}
{"id": "2510.04397", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04397", "abs": "https://arxiv.org/abs/2510.04397", "authors": ["Van Nguyen", "Surya Nepal", "Xingliang Yuan", "Tingmin Wu", "Fengchao Chen", "Carsten Rudolph"], "title": "MulVuln: Enhancing Pre-trained LMs with Shared and Language-Specific Knowledge for Multilingual Vulnerability Detection", "comment": null, "summary": "Software vulnerabilities (SVs) pose a critical threat to safety-critical\nsystems, driving the adoption of AI-based approaches such as machine learning\nand deep learning for software vulnerability detection. Despite promising\nresults, most existing methods are limited to a single programming language.\nThis is problematic given the multilingual nature of modern software, which is\noften complex and written in multiple languages. Current approaches often face\nchallenges in capturing both shared and language-specific knowledge of source\ncode, which can limit their performance on diverse programming languages and\nreal-world codebases. To address this gap, we propose MULVULN, a novel\nmultilingual vulnerability detection approach that learns from source code\nacross multiple languages. MULVULN captures both the shared knowledge that\ngeneralizes across languages and the language-specific knowledge that reflects\nunique coding conventions. By integrating these aspects, it achieves more\nrobust and effective detection of vulnerabilities in real-world multilingual\nsoftware systems. The rigorous and extensive experiments on the real-world and\ndiverse REEF dataset, consisting of 4,466 CVEs with 30,987 patches across seven\nprogramming languages, demonstrate the superiority of MULVULN over thirteen\neffective and state-of-the-art baselines. Notably, MULVULN achieves\nsubstantially higher F1-score, with improvements ranging from 1.45% to 23.59%\ncompared to the baseline methods."}
{"id": "2510.04503", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04503", "abs": "https://arxiv.org/abs/2510.04503", "authors": ["Shuai Zhao", "Xinyi Wu", "Shiqian Zhao", "Xiaobao Wu", "Zhongliang Guo", "Yanhao Jia", "Anh Tuan Luu"], "title": "P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs", "comment": null, "summary": "During fine-tuning, large language models (LLMs) are increasingly vulnerable\nto data-poisoning backdoor attacks, which compromise their reliability and\ntrustworthiness. However, existing defense strategies suffer from limited\ngeneralization: they only work on specific attack types or task settings. In\nthis study, we propose Poison-to-Poison (P2P), a general and effective backdoor\ndefense algorithm. P2P injects benign triggers with safe alternative labels\ninto a subset of training samples and fine-tunes the model on this re-poisoned\ndataset by leveraging prompt-based learning. This enforces the model to\nassociate trigger-induced representations with safe outputs, thereby overriding\nthe effects of original malicious triggers. Thanks to this robust and\ngeneralizable trigger-based fine-tuning, P2P is effective across task settings\nand attack types. Theoretically and empirically, we show that P2P can\nneutralize malicious backdoors while preserving task performance. We conduct\nextensive experiments on classification, mathematical reasoning, and summary\ngeneration tasks, involving multiple state-of-the-art LLMs. The results\ndemonstrate that our P2P algorithm significantly reduces the attack success\nrate compared with baseline models. We hope that the P2P can serve as a\nguideline for defending against backdoor attacks and foster the development of\na secure and trustworthy LLM community."}
{"id": "2510.04528", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04528", "abs": "https://arxiv.org/abs/2510.04528", "authors": ["Santhosh KumarRavindran"], "title": "Unified Threat Detection and Mitigation Framework (UTDMF): Combating Prompt Injection, Deception, and Bias in Enterprise-Scale Transformers", "comment": null, "summary": "The rapid adoption of large language models (LLMs) in enterprise systems\nexposes vulnerabilities to prompt injection attacks, strategic deception, and\nbiased outputs, threatening security, trust, and fairness. Extending our\nadversarial activation patching framework (arXiv:2507.09406), which induced\ndeception in toy networks at a 23.9% rate, we introduce the Unified Threat\nDetection and Mitigation Framework (UTDMF), a scalable, real-time pipeline for\nenterprise-grade models like Llama-3.1 (405B), GPT-4o, and Claude-3.5. Through\n700+ experiments per model, UTDMF achieves: (1) 92% detection accuracy for\nprompt injection (e.g., jailbreaking); (2) 65% reduction in deceptive outputs\nvia enhanced patching; and (3) 78% improvement in fairness metrics (e.g.,\ndemographic bias). Novel contributions include a generalized patching algorithm\nfor multi-threat detection, three groundbreaking hypotheses on threat\ninteractions (e.g., threat chaining in enterprise workflows), and a\ndeployment-ready toolkit with APIs for enterprise integration."}
{"id": "2510.04529", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.04529", "abs": "https://arxiv.org/abs/2510.04529", "authors": ["Yuki Takeuchi", "Duo Xu"], "title": "Computational Certified Deletion Property of Magic Square Game and its Application to Classical Secure Key Leasing", "comment": null, "summary": "We present the first construction of a computational Certified Deletion\nProperty (CDP) achievable with classical communication, derived from the\ncompilation of the non-local Magic Square Game (MSG). We leverage the KLVY\ncompiler to transform the non-local MSG into a 2-round interactive protocol,\nrigorously demonstrating that this compilation preserves the game-specific CDP.\nPreviously, the quantum value and rigidity of the compiled game were\ninvestigated. We emphasize that we are the first to investigate CDP (local\nrandomness in [Fu and Miller, Phys. Rev. A 97, 032324 (2018)]) for the compiled\ngame. Then, we combine this CDP with the framework [Kitagawa, Morimae, and\nYamakawa, Eurocrypt 2025] to construct Secure Key Leasing with classical Lessor\n(cSKL). SKL enables the Lessor to lease the secret key to the Lessee and verify\nthat a quantum Lessee has indeed deleted the key. In this paper, we realize\ncSKL for PKE, PRF, and digital signature. Compared to prior works for cSKL, we\nrealize cSKL for PRF and digital signature for the first time. In addition, we\nsucceed in weakening the assumption needed to construct cSKL."}
{"id": "2510.04619", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.04619", "abs": "https://arxiv.org/abs/2510.04619", "authors": ["Ivan Homoliak", "Martin Perešíni", "Marek Tamaškovič", "Timotej Ponek", "Lukáš Hellebrandt", "Kamil Malinka"], "title": "PoS-CoPOR: Proof-of-Stake Consensus Protocol with Native Onion Routing Providing Scalability and DoS-Resistance", "comment": null, "summary": "Proof-of-Stake (PoS) consensus protocols often face a trade-off between\nperformance and security. Protocols that pre-elect leaders for subsequent\nrounds are vulnerable to Denial-of-Service (DoS) attacks, which can disrupt the\nnetwork and compromise liveness. In this work, we present PoS-CoPOR, a\nsingle-chain PoS consensus protocol that mitigates this vulnerability by\nintegrating a native onion routing mechanism into the consensus protocol\nitself. PoS-CoPOR combines stake-weighted probabilistic leader election with an\nanonymization layer that conceals the network identity of the next block\nproposer. This approach prevents targeted DoS attacks on leaders before they\nproduce a block, thus enhancing network resilience. We implemented and\nevaluated PoS-CoPOR, demonstrating its ability to achieve a throughput of up to\n110 tx/s with 6 nodes, even with the overhead of the anonymization layer. The\nresults show that native anonymization can provide robust DoS resistance with\nonly a modest impact on performance, offering a solution to build secure and\nscalable PoS blockchains."}
{"id": "2510.04640", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.04640", "abs": "https://arxiv.org/abs/2510.04640", "authors": ["Ali Asghar", "Andreas Becher", "Daniel Ziener"], "title": "Backing the Wrong Horse: How Bit-Level Netlist Augmentation can Counter Power Side Channel Attacks", "comment": "5 pages, 3 figures", "summary": "The dependence of power-consumption on the processed data is a known\nvulnerability of CMOS circuits, resulting in side channels which can be\nexploited by power-based side channel attacks (SCAs). These attacks can extract\nsensitive information, such as secret keys, from the implementation of\ncryptographic algorithms. Existing countermeasures against power-based side\nchannel attacks focus on analyzing information leakage at the byte level.\nHowever, this approach neglects the impact of individual bits on the overall\nresistance of a cryptographic implementation. In this work, we present a\ncountermeasure based on single-bit leakage. The results suggest that the\nproposed countermeasure cannot be broken by attacks using conventional SCA\nleakage models."}
{"id": "2510.04652", "categories": ["cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.04652", "abs": "https://arxiv.org/abs/2510.04652", "authors": ["Ines Akaichi", "Giorgos Flouris", "Irini Fundulaki", "Sabrina Kirrane"], "title": "Modeling and Managing Temporal Obligations in GUCON Using SPARQL-star and RDF-star", "comment": null, "summary": "In the digital age, data frequently crosses organizational and jurisdictional\nboundaries, making effective governance essential. Usage control policies have\nemerged as a key paradigm for regulating data usage, safeguarding privacy,\nprotecting intellectual property, and ensuring compliance with regulations. A\ncentral mechanism for usage control is the handling of obligations, which arise\nas a side effect of using and sharing data. Effective monitoring of obligations\nrequires capturing usage traces and accounting for temporal aspects such as\nstart times and deadlines, as obligations may evolve over times into different\nstates, such as fulfilled, violated, or expired. While several solutions have\nbeen proposed for obligation monitoring, they often lack formal semantics or\nprovide limited support for reasoning over obligation states. To address these\nlimitations, we extend GUCON, a policy framework grounded in the formal\nsemantics of SPAQRL graph patterns, to explicitly model the temporal aspects of\nan obligation. This extension enables the expressing of temporal obligations\nand supports continuous monitoring of their evolving states based on usage\ntraces stored in temporal knowledge graphs. We demonstrate how this extended\nmodel can be represented using RDF-star and SPARQL-star and propose an\nObligation State Manager that monitors obligation states and assess their\ncompliance with respect to usage traces. Finally, we evaluate both the extended\nmodel and its prototype implementation."}
{"id": "2510.04882", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.04882", "abs": "https://arxiv.org/abs/2510.04882", "authors": ["Elian Morel"], "title": "Enhancing TreePIR for a Single-Server Setting via Resampling", "comment": null, "summary": "Private Information Retrieval (PIR) allows a client to retrieve an entry\n$\\text{DB}[i]$ from a public database $\\text{DB}$ held by one or more servers,\nwithout revealing the queried index $i$. Traditional PIR schemes achieve\nsublinear server computation only under strong assumptions, such as the\npresence of multiple non-colluding servers or the use of public-key\ncryptography. To overcome these limitations, \\textit{preprocessing PIR} schemes\nintroduce a query-independent offline phase where the client collects\n\\textit{hints} that enable efficient private queries during the online phase.\n  In this work, we focus on preprocessing PIR schemes relying solely on\n\\textit{One-Way Functions} (OWFs), which provide minimal cryptographic\nassumptions and practical implementability. We study three main constructions\n-- TreePIR, PIANO, and PPPS -- that explore different trade-offs between\ncommunication, storage, and server trust assumptions. Building upon the\nmechanisms introduced in PIANO and PPPS, we propose an adaptation of TreePIR to\nthe single-server setting by introducing a dual-table hint structure (primary\nand backup tables) and a \\textit{resampling} technique to refresh hints\nefficiently.\n  Our proposed scheme achieves logarithmic upload bandwidth and $O(\\sqrt{n}\\log\nn)$ download complexity while requiring $O(\\sqrt{n}\\log n)$ client storage.\nThis represents a significant improvement over prior single-server\npreprocessing PIR schemes such as PIANO ($O(\\sqrt{n})$ bandwidth) and PPPS\n($O(n^{1/4})$ bandwidth), while maintaining the simplicity and minimal\nassumptions of the OWF-based setting."}
{"id": "2510.04885", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04885", "abs": "https://arxiv.org/abs/2510.04885", "authors": ["Yuxin Wen", "Arman Zharmagambetov", "Ivan Evtimov", "Narine Kokhlikyan", "Tom Goldstein", "Kamalika Chaudhuri", "Chuan Guo"], "title": "RL Is a Hammer and LLMs Are Nails: A Simple Reinforcement Learning Recipe for Strong Prompt Injection", "comment": null, "summary": "Prompt injection poses a serious threat to the reliability and safety of LLM\nagents. Recent defenses against prompt injection, such as Instruction Hierarchy\nand SecAlign, have shown notable robustness against static attacks. However, to\nmore thoroughly evaluate the robustness of these defenses, it is arguably\nnecessary to employ strong attacks such as automated red-teaming. To this end,\nwe introduce RL-Hammer, a simple recipe for training attacker models that\nautomatically learn to perform strong prompt injections and jailbreaks via\nreinforcement learning. RL-Hammer requires no warm-up data and can be trained\nentirely from scratch. To achieve high ASRs against industrial-level models\nwith defenses, we propose a set of practical techniques that enable highly\neffective, universal attacks. Using this pipeline, RL-Hammer reaches a 98% ASR\nagainst GPT-4o and a $72\\%$ ASR against GPT-5 with the Instruction Hierarchy\ndefense. We further discuss the challenge of achieving high diversity in\nattacks, highlighting how attacker models tend to reward-hack diversity\nobjectives. Finally, we show that RL-Hammer can evade multiple prompt injection\ndetectors. We hope our work advances automatic red-teaming and motivates the\ndevelopment of stronger, more principled defenses. Code is available at\nhttps://github.com/facebookresearch/rl-injector."}
{"id": "2510.04987", "categories": ["cs.CR", "I.2"], "pdf": "https://arxiv.org/pdf/2510.04987", "abs": "https://arxiv.org/abs/2510.04987", "authors": ["Avilash Rath", "Weiliang Qi", "Youpeng Li", "Xinda Wang"], "title": "NatGVD: Natural Adversarial Example Attack towards Graph-based Vulnerability Detection", "comment": "10 pages, 2 figures (2 additional figures in Appendices)", "summary": "Graph-based models learn rich code graph structural information and present\nsuperior performance on various code analysis tasks. However, the robustness of\nthese models against adversarial example attacks in the context of\nvulnerability detection remains an open question. This paper proposes NatGVD, a\nnovel attack methodology that generates natural adversarial vulnerable code to\ncircumvent GNN-based and graph-aware transformer-based vulnerability detectors.\nNatGVD employs a set of code transformations that modify graph structure while\npreserving code semantics. Instead of injecting dead or unrelated code like\nprevious works, NatGVD considers naturalness requirements: generated examples\nshould not be easily recognized by humans or program analysis tools. With\nextensive evaluation of NatGVD on state-of-the-art vulnerability detection\nsystems, the results reveal up to 53.04% evasion rate across GNN-based\ndetectors and graph-aware transformer-based detectors. We also explore\npotential defense strategies to enhance the robustness of these systems against\nNatGVD."}
{"id": "2510.05052", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.05052", "abs": "https://arxiv.org/abs/2510.05052", "authors": ["Weiliang Zhao", "Jinjun Peng", "Daniel Ben-Levi", "Zhou Yu", "Junfeng Yang"], "title": "Proactive defense against LLM Jailbreak", "comment": null, "summary": "The proliferation of powerful large language models (LLMs) has necessitated\nrobust safety alignment, yet these models remain vulnerable to evolving\nadversarial attacks, including multi-turn jailbreaks that iteratively search\nfor successful queries. Current defenses, primarily reactive and static, often\nfail to counter these search-based attacks. In this paper, we introduce ProAct,\na novel proactive defense framework designed to disrupt and mislead autonomous\njailbreaking processes. Our core idea is to intentionally provide adversaries\nwith \"spurious responses\" that appear to be results of successful jailbreak\nattacks but contain no actual harmful content. These misleading responses\nprovide false signals to the attacker's internal optimization loop, causing the\nadversarial search to terminate prematurely and effectively jailbreaking the\njailbreak. By conducting extensive experiments across state-of-the-art LLMs,\njailbreaking frameworks, and safety benchmarks, our method consistently and\nsignificantly reduces attack success rates by up to 92\\%. When combined with\nother defense frameworks, it further reduces the success rate of the latest\nattack strategies to 0\\%. ProAct represents an orthogonal defense strategy that\ncan serve as an additional guardrail to enhance LLM safety against the most\neffective jailbreaking attacks."}
