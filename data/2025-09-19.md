<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 21]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Early Approaches to Adversarial Fine-Tuning for Prompt Injection Defense: A 2022 Study of GPT-3 and Contemporary Models](https://arxiv.org/abs/2509.14271)
*Gustavo Sandoval,Denys Fenchenko,Junyao Chen*

Main category: cs.CR

TL;DR: 2022年早期研究，针对大型语言模型的提示注入和目标劫持攻击，提出了对抗性微调防御方法，将攻击成功率从31%降至接近零，为大模型安全防御研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的广泛应用，提示注入攻击成为严重的安全威胁。研究旨在探索有效的防御方法，保护LLMs免受恶意提示操纵。

Method: 研究构建了提示注入和目标劫持两种攻击方式，在不同LLMs上进行测试，并提出并评估了对抗性微调防御技术。

Result: 未使用防御时，GPT-3系列模型的攻击成功率为31%。使用对抗性微调后，较小GPT-3变体的攻击成功率降至接近零，但发现大模型更易受攻击。

Conclusion: 虽然测试的具体模型已过时，但核心方法和实证发现为现代提示注入防御研究（包括指令层级系统和宪法AI方法）奠定了基础，揭示了基于微调的防御方法的局限性。

Abstract: This paper documents early research conducted in 2022 on defending against
prompt injection attacks in large language models, providing historical context
for the evolution of this critical security domain. This research focuses on
two adversarial attacks against Large Language Models (LLMs): prompt injection
and goal hijacking. We examine how to construct these attacks, test them on
various LLMs, and compare their effectiveness. We propose and evaluate a novel
defense technique called Adversarial Fine-Tuning. Our results show that,
without this defense, the attacks succeeded 31\% of the time on GPT-3 series
models. When using our Adversarial Fine-Tuning approach, attack success rates
were reduced to near zero for smaller GPT-3 variants (Ada, Babbage, Curie),
though we note that subsequent research has revealed limitations of
fine-tuning-based defenses. We also find that more flexible models exhibit
greater vulnerability to these attacks. Consequently, large models such as
GPT-3 Davinci are more vulnerable than smaller models like GPT-2. While the
specific models tested are now superseded, the core methodology and empirical
findings contributed to the foundation of modern prompt injection defense
research, including instruction hierarchy systems and constitutional AI
approaches.

</details>


### [2] [FedMentor: Domain-Aware Differential Privacy for Heterogeneous Federated LLMs in Mental Health](https://arxiv.org/abs/2509.14275)
*Nobin Sarwar,Shubhashis Roy Dipta*

Main category: cs.CR

TL;DR: FedMentor是一个联邦学习框架，结合LoRA和领域感知差分隐私，在心理健康等敏感领域实现隐私保护的LLM微调，在保持性能的同时满足隐私预算要求。


<details>
  <summary>Details</summary>
Motivation: 在敏感领域（如心理健康）中，需要在严格保护隐私的同时保持LLM的实用性和安全性，传统联邦学习缺乏足够的隐私保护机制。

Method: 采用联邦学习框架，结合低秩适应（LoRA）和领域感知差分隐私（DP），每个客户端根据数据敏感度应用自定义DP噪声尺度，服务器在效用低于阈值时自适应降低噪声。

Result: 在三个心理健康数据集上，FedMentor相比无隐私的标准联邦学习提高了安全性，安全输出率提升最多3个百分点，毒性降低，同时保持效用（BERTScore F1和ROUGE-L）与非隐私基线相差不到0.5%，接近集中式上限。

Conclusion: FedMentor提供了一个实用的方法，可在医疗保健等敏感领域安全地私有微调LLM，支持单GPU客户端上最多17亿参数的模型，每轮通信需求小于173MB。

Abstract: Privacy-preserving adaptation of Large Language Models (LLMs) in sensitive
domains (e.g., mental health) requires balancing strict confidentiality with
model utility and safety. We propose FedMentor, a federated fine-tuning
framework that integrates Low-Rank Adaptation (LoRA) and domain-aware
Differential Privacy (DP) to meet per-domain privacy budgets while maintaining
performance. Each client (domain) applies a custom DP noise scale proportional
to its data sensitivity, and the server adaptively reduces noise when utility
falls below a threshold. In experiments on three mental health datasets, we
show that FedMentor improves safety over standard Federated Learning without
privacy, raising safe output rates by up to three points and lowering toxicity,
while maintaining utility (BERTScore F1 and ROUGE-L) within 0.5% of the
non-private baseline and close to the centralized upper bound. The framework
scales to backbones with up to 1.7B parameters on single-GPU clients, requiring
< 173 MB of communication per round. FedMentor demonstrates a practical
approach to privately fine-tune LLMs for safer deployments in healthcare and
other sensitive fields.

</details>


### [3] [Beyond Data Privacy: New Privacy Risks for Large Language Models](https://arxiv.org/abs/2509.14278)
*Yuntao Du,Zitao Li,Ninghui Li,Bolin Ding*

Main category: cs.CR

TL;DR: 本文系统分析了大型语言模型(LLMs)在部署阶段出现的新型隐私风险，包括无意数据泄露和恶意数据窃取，呼吁研究社区关注超越训练阶段数据隐私的新威胁。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在自然语言理解、推理和自主决策方面取得显著进展，这些进步也带来了严重的隐私问题。虽然已有大量研究关注模型训练阶段的数据隐私风险，但对部署阶段新出现的威胁关注不足。

Method: 系统性地检查LLMs部署过程中出现的新型隐私漏洞，分析这些漏洞如何被利用进行复杂的大规模隐私攻击，并讨论潜在的缓解策略。

Result: 发现LLMs集成到广泛应用中以及其自主能力的武器化创造了新的隐私漏洞，这些漏洞不仅威胁个人隐私，还危及金融安全和社会信任。

Conclusion: 研究社区需要扩大关注范围，超越数据隐私风险，开发新的防御措施来应对日益强大的LLMs和LLM驱动系统带来的不断演变的威胁。

Abstract: Large Language Models (LLMs) have achieved remarkable progress in natural
language understanding, reasoning, and autonomous decision-making. However,
these advancements have also come with significant privacy concerns. While
significant research has focused on mitigating the data privacy risks of LLMs
during various stages of model training, less attention has been paid to new
threats emerging from their deployment. The integration of LLMs into widely
used applications and the weaponization of their autonomous abilities have
created new privacy vulnerabilities. These vulnerabilities provide
opportunities for both inadvertent data leakage and malicious exfiltration from
LLM-powered systems. Additionally, adversaries can exploit these systems to
launch sophisticated, large-scale privacy attacks, threatening not only
individual privacy but also financial security and societal trust. In this
paper, we systematically examine these emerging privacy risks of LLMs. We also
discuss potential mitigation strategies and call for the research community to
broaden its focus beyond data privacy risks, developing new defenses to address
the evolving threats posed by increasingly powerful LLMs and LLM-powered
systems.

</details>


### [4] [Resisting Quantum Key Distribution Attacks Using Quantum Machine Learning](https://arxiv.org/abs/2509.14282)
*Ali Al-kuwari,Noureldin Mohamed,Saif Al-kuwari,Ahmed Farouk,Bikash K. Behera*

Main category: cs.CR

TL;DR: 本文提出了一种混合量子长短期记忆（QLSTM）模型，用于检测量子密钥分发（QKD）系统中的七种常见攻击，相比传统机器学习模型展现出更好的性能。


<details>
  <summary>Details</summary>
Motivation: 量子计算对现代通信网络安全构成重大威胁，量子密钥分发虽然提供安全解决方案，但实际实现仍存在硬件缺陷和高级攻击漏洞。需要新的方法来增强QKD系统的安全性。

Method: 提出混合量子长短期记忆（QLSTM）模型，结合量子增强学习和经典深度学习，捕捉QKD数据中的复杂时间模式。创建了包含正常操作和七种攻击场景的QKD数据集进行评估。

Result: 混合QLSTM模型在50个训练周期后达到93.7%的准确率，优于传统的LSTM和CNN等经典深度学习模型。

Conclusion: 量子机器学习方法在QKD攻击检测方面表现出有前景的性能，混合技术有潜力增强未来量子通信网络的安全性。

Abstract: The emergence of quantum computing poses significant risks to the security of
modern communication networks as it breaks today's public-key cryptographic
algorithms. Quantum Key Distribution (QKD) offers a promising solution by
harnessing the principles of quantum mechanics to establish secure keys.
However, practical QKD implementations remain vulnerable to hardware
imperfections and advanced attacks such as Photon Number Splitting and
Trojan-Horse attacks. In this work, we investigate the potential of using
quantum machine learning (QML) to detect popular QKD attacks. In particular, we
propose a Hybrid Quantum Long Short-Term Memory (QLSTM) model to improve the
detection of common QKD attacks. By combining quantum-enhanced learning with
classical deep learning, the model captures complex temporal patterns in QKD
data, improving detection accuracy. To evaluate the proposed model, we
introduce a realistic QKD dataset simulating normal QKD operations along with
seven attack scenarios, Intercept-and-Resend, Photon-Number Splitting (PNS),
Trojan-Horse attacks Random Number Generator (RNG), Detector Blinding,
Wavelength-dependent Trojan Horse, and Combined attacks. The dataset includes
quantum security metrics such as Quantum Bit Error Rate (QBER), measurement
entropy, signal and decoy loss rates, and time-based metrics, ensuring an
accurate representation of real-world conditions. Our results demonstrate
promising performance of the quantum machine learning approach compared to
traditional classical machine learning models, highlighting the potential of
hybrid techniques to enhance the security of future quantum communication
networks. The proposed Hybrid QLSTM model achieved an accuracy of 93.7.0\%
after 50 training epochs, outperforming classical deep learning models such as
LSTM, and CNN.

</details>


### [5] [The Sum Leaks More Than Its Parts: Compositional Privacy Risks and Mitigations in Multi-Agent Collaboration](https://arxiv.org/abs/2509.14284)
*Vaidehi Patil,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CR

TL;DR: 该论文首次系统研究了多智能体LLM系统中的组合隐私泄露问题，提出了两种防御策略（ToM和CoDef），发现协作共识防御在隐私-效用权衡方面表现最佳


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在多智能体系统中广泛应用，传统隐私风险评估方法无法捕捉跨交互组合的隐私泄露风险，需要研究组合隐私泄露现象及其防御方法

Method: 开发了建模辅助知识和智能体交互如何联合放大隐私风险的框架，提出了理论思维防御（ToM）和协作共识防御（CoDef）两种策略，并通过实验评估其效果

Result: 思维链单独提供有限保护（约39%敏感信息阻断率），ToM防御显著改善敏感查询阻断（达97%）但降低良性任务成功率，CoDef实现最佳平衡（79.8%平衡结果）

Conclusion: 研究揭示了协作LLM部署中的新型风险类别，为设计针对组合性、上下文驱动的隐私泄露的防护措施提供了可行见解，强调结合显式推理与防御者协作的益处

Abstract: As large language models (LLMs) become integral to multi-agent systems, new
privacy risks emerge that extend beyond memorization, direct inference, or
single-turn evaluations. In particular, seemingly innocuous responses, when
composed across interactions, can cumulatively enable adversaries to recover
sensitive information, a phenomenon we term compositional privacy leakage. We
present the first systematic study of such compositional privacy leaks and
possible mitigation methods in multi-agent LLM systems. First, we develop a
framework that models how auxiliary knowledge and agent interactions jointly
amplify privacy risks, even when each response is benign in isolation. Next, to
mitigate this, we propose and evaluate two defense strategies: (1)
Theory-of-Mind defense (ToM), where defender agents infer a questioner's intent
by anticipating how their outputs may be exploited by adversaries, and (2)
Collaborative Consensus Defense (CoDef), where responder agents collaborate
with peers who vote based on a shared aggregated state to restrict sensitive
information spread. Crucially, we balance our evaluation across compositions
that expose sensitive information and compositions that yield benign
inferences. Our experiments quantify how these defense strategies differ in
balancing the privacy-utility trade-off. We find that while chain-of-thought
alone offers limited protection to leakage (~39% sensitive blocking rate), our
ToM defense substantially improves sensitive query blocking (up to 97%) but can
reduce benign task success. CoDef achieves the best balance, yielding the
highest Balanced Outcome (79.8%), highlighting the benefit of combining
explicit reasoning with defender collaboration. Together, our results expose a
new class of risks in collaborative LLM deployments and provide actionable
insights for designing safeguards against compositional, context-driven privacy
leakage.

</details>


### [6] [A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks](https://arxiv.org/abs/2509.14285)
*S M Asif Hossain,Ruksat Khan Shayoni,Mohd Ruhul Ameen,Akif Islam,M. F. Mridha,Jungpil Shin*

Main category: cs.CR

TL;DR: 本文提出了一种新颖的多智能体防御框架，通过协调的LLM智能体流水线来实时检测和中和提示注入攻击，在ChatGLM和Llama2平台上实现了100%的攻击缓解率。


<details>
  <summary>Details</summary>
Motivation: 提示注入攻击是大型语言模型部署中的主要漏洞，恶意指令可以覆盖系统提示并导致意外行为，需要有效的实时防御机制。

Method: 采用两种架构：顺序链式智能体流水线和分层协调器系统，使用专门的LLM智能体在协调流水线中检测和中和攻击。

Result: 在55种独特提示注入攻击（8个类别，400个攻击实例）的评估中，多智能体流水线将攻击成功率从基线30%（ChatGLM）和20%（Llama2）降低到0%，实现了100%的缓解效果。

Conclusion: 该框架在多种攻击类别中表现出鲁棒性，包括直接覆盖、代码执行尝试、数据泄露和混淆技术，同时保持对合法查询的系统功能。

Abstract: Prompt injection attacks represent a major vulnerability in Large Language
Model (LLM) deployments, where malicious instructions embedded in user inputs
can override system prompts and induce unintended behaviors. This paper
presents a novel multi-agent defense framework that employs specialized LLM
agents in coordinated pipelines to detect and neutralize prompt injection
attacks in real-time. We evaluate our approach using two distinct
architectures: a sequential chain-of-agents pipeline and a hierarchical
coordinator-based system. Our comprehensive evaluation on 55 unique prompt
injection attacks, grouped into 8 categories and totaling 400 attack instances
across two LLM platforms (ChatGLM and Llama2), demonstrates significant
security improvements. Without defense mechanisms, baseline Attack Success
Rates (ASR) reached 30% for ChatGLM and 20% for Llama2. Our multi-agent
pipeline achieved 100% mitigation, reducing ASR to 0% across all tested
scenarios. The framework demonstrates robustness across multiple attack
categories including direct overrides, code execution attempts, data
exfiltration, and obfuscation techniques, while maintaining system
functionality for legitimate queries.

</details>


### [7] [A Simple and Efficient Jailbreak Method Exploiting LLMs' Helpfulness](https://arxiv.org/abs/2509.14297)
*Xuan Luo,Yue Wang,Zefeng He,Geng Tu,Jing Li,Ruifeng Xu*

Main category: cs.CR

TL;DR: HILL是一种新型越狱方法，通过将有害指令转换为学习式问题来绕过LLMs的安全防护，在多个模型上展现出高攻击成功率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法存在漏洞，需要开发越狱方法来模拟恶意攻击并发现LLMs的安全机制缺陷，以加强安全防护。

Method: HILL方法系统地将指令式有害请求转换为带有简单假设性指示词的学习式问题，并引入了两个新指标来全面评估越狱方法的效用。

Result: 在AdvBench数据集上的实验显示，HILL在大多数模型和恶意类别上达到最高攻击成功率，保持高效率且提示简洁，各种防御方法对其效果有限甚至适得其反。

Conclusion: 这项工作揭示了安全措施在学习式诱导下的显著脆弱性，突显了在帮助性和安全性对齐之间平衡的关键挑战，暴露了LLMs安全机制的内在局限性和防御方法的缺陷。

Abstract: Safety alignment aims to prevent Large Language Models (LLMs) from responding
to harmful queries. To strengthen safety protections, jailbreak methods are
developed to simulate malicious attacks and uncover vulnerabilities. In this
paper, we introduce HILL (Hiding Intention by Learning from LLMs), a novel
jailbreak approach that systematically transforms imperative harmful requests
into learning-style questions with only straightforward hypotheticality
indicators. Further, we introduce two new metrics to thoroughly evaluate the
utility of jailbreak methods. Experiments on the AdvBench dataset across a wide
range of models demonstrate HILL's strong effectiveness, generalizability, and
harmfulness. It achieves top attack success rates on the majority of models and
across malicious categories while maintaining high efficiency with concise
prompts. Results of various defense methods show the robustness of HILL, with
most defenses having mediocre effects or even increasing the attack success
rates. Moreover, the assessment on our constructed safe prompts reveals
inherent limitations of LLMs' safety mechanisms and flaws in defense methods.
This work exposes significant vulnerabilities of safety measures against
learning-style elicitation, highlighting a critical challenge of balancing
helpfulness and safety alignments.

</details>


### [8] [Beyond Classification: Evaluating LLMs for Fine-Grained Automatic Malware Behavior Auditing](https://arxiv.org/abs/2509.14335)
*Xinran Zheng,Xingzhi Qian,Yiling He,Shuo Yang,Lorenzo Cavallaro*

Main category: cs.CR

TL;DR: MalEval是一个用于评估LLM在Android恶意软件审计中能力的框架，通过专家验证报告和静态可达性分析来解决现有局限性，并系统评估了7个主流LLM的审计性能。


<details>
  <summary>Details</summary>
Motivation: 当前恶意软件分类虽然检测性能强，但缺乏对恶意活动的因果性和可验证解释。手动审计成本高且缓慢，而LLM的审计潜力因缺乏细粒度标注、良性代码干扰和不可验证输出等问题尚未充分探索。

Method: 提出MalEval框架，包含专家验证报告、更新的敏感API列表、静态可达性分析降噪、函数级结构表示作为中间归因单元，定义了四个分析师对齐任务和领域特定指标。

Result: 系统评估了7个广泛使用的LLM在精选数据集上的表现，揭示了LLM在审计各阶段的潜力和关键局限性。

Conclusion: MalEval为LLM增强的恶意软件行为审计提供了可复现的基准和基础，展示了LLM在该领域的应用前景和需要改进的方向。

Abstract: Automated malware classification has achieved strong detection performance.
Yet, malware behavior auditing seeks causal and verifiable explanations of
malicious activities -- essential not only to reveal what malware does but also
to substantiate such claims with evidence. This task is challenging, as
adversarial intent is often hidden within complex, framework-heavy
applications, making manual auditing slow and costly. Large Language Models
(LLMs) could help address this gap, but their auditing potential remains
largely unexplored due to three limitations: (1) scarce fine-grained
annotations for fair assessment; (2) abundant benign code obscuring malicious
signals; and (3) unverifiable, hallucination-prone outputs undermining
attribution credibility. To close this gap, we introduce MalEval, a
comprehensive framework for fine-grained Android malware auditing, designed to
evaluate how effectively LLMs support auditing under real-world constraints.
MalEval provides expert-verified reports and an updated sensitive API list to
mitigate ground truth scarcity and reduce noise via static reachability
analysis. Function-level structural representations serve as intermediate
attribution units for verifiable evaluation. Building on this, we define four
analyst-aligned tasks -- function prioritization, evidence attribution,
behavior synthesis, and sample discrimination -- together with domain-specific
metrics and a unified workload-oriented score. We evaluate seven widely used
LLMs on a curated dataset of recent malware and misclassified benign apps,
offering the first systematic assessment of their auditing capabilities.
MalEval reveals both promising potential and critical limitations across audit
stages, providing a reproducible benchmark and foundation for future research
on LLM-enhanced malware behavior auditing. MalEval is publicly available at
https://github.com/ZhengXR930/MalEval.git

</details>


### [9] [LLM Jailbreak Detection for (Almost) Free!](https://arxiv.org/abs/2509.14558)
*Guorui Chen,Yifan Xia,Xiaojun Jia,Zhijiang Li,Philip Torr,Jindong Gu*

Main category: cs.CR

TL;DR: 提出了一种免费的越狱检测方法FJD，通过添加肯定指令和温度缩放来区分越狱和良性提示，几乎不增加计算成本


<details>
  <summary>Details</summary>
Motivation: 现有越狱检测方法计算成本高，需要发现更高效的方法来检测LLM的越狱攻击

Method: 在输入前添加肯定指令，通过温度缩放调整logits，利用第一个token的置信度来区分越狱和良性提示，并结合虚拟指令学习提升检测性能

Result: 在对齐的LLM上进行的广泛实验表明，FJD能有效检测越狱提示，且在LLM推理过程中几乎不产生额外计算成本

Conclusion: FJD提供了一种高效、低成本的越狱检测解决方案，通过输出分布差异和置信度分析来提升LLM的安全性

Abstract: Large language models (LLMs) enhance security through alignment when widely
used, but remain susceptible to jailbreak attacks capable of producing
inappropriate content. Jailbreak detection methods show promise in mitigating
jailbreak attacks through the assistance of other models or multiple model
inferences. However, existing methods entail significant computational costs.
In this paper, we first present a finding that the difference in output
distributions between jailbreak and benign prompts can be employed for
detecting jailbreak prompts. Based on this finding, we propose a Free Jailbreak
Detection (FJD) which prepends an affirmative instruction to the input and
scales the logits by temperature to further distinguish between jailbreak and
benign prompts through the confidence of the first token. Furthermore, we
enhance the detection performance of FJD through the integration of virtual
instruction learning. Extensive experiments on aligned LLMs show that our FJD
can effectively detect jailbreak prompts with almost no additional
computational costs during LLM inference.

</details>


### [10] [What Gets Measured Gets Managed: Mitigating Supply Chain Attacks with a Link Integrity Management System](https://arxiv.org/abs/2509.14583)
*Johnny So,Michael Ferdman,Nick Nikiforakis*

Main category: cs.CR

TL;DR: LiMS是一个透明系统，通过可定制的完整性策略来验证和执行Web资源完整性，以防御供应链攻击，初始页面加载开销数百毫秒，重载开销可忽略。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏健壮的方法来验证Web资源完整性，供应链成为Web应用攻击面的主要目标，需要通用且高性能的解决方案。

Method: 设计LiMS系统，使用可定制完整性策略声明资源预期属性，验证并强制执行这些策略，作为构建全面完整性保证的基础模块。

Result: 在450个代表性域名上模拟部署，初始页面加载总体开销数百毫秒，重载开销可忽略；策略构建块适合依赖使用模式，管理开销最小。

Conclusion: LiMS能够以最小开销引导显著的安全改进，提供足够的保证来防御近期供应链攻击，适合实际部署。

Abstract: The web continues to grow, but dependency-monitoring tools and standards for
resource integrity lag behind. Currently, there exists no robust method to
verify the integrity of web resources, much less in a generalizable yet
performant manner, and supply chains remain one of the most targeted parts of
the attack surface of web applications.
  In this paper, we present the design of LiMS, a transparent system to
bootstrap link integrity guarantees in web browsing sessions with minimal
overhead. At its core, LiMS uses a set of customizable integrity policies to
declare the (un)expected properties of resources, verifies these policies, and
enforces them for website visitors. We discuss how basic integrity policies can
serve as building blocks for a comprehensive set of integrity policies, while
providing guarantees that would be sufficient to defend against recent supply
chain attacks detailed by security industry reports. Finally, we evaluate our
open-sourced prototype by simulating deployments on a representative sample of
450 domains that are diverse in ranking and category. We find that our proposal
offers the ability to bootstrap marked security improvements with an overall
overhead of hundreds of milliseconds on initial page loads, and negligible
overhead on reloads, regardless of network speeds. In addition, from examining
archived data for the sample sites, we find that several of the proposed policy
building blocks suit their dependency usage patterns, and would incur minimal
administrative overhead.

</details>


### [11] [ATLANTIS: AI-driven Threat Localization, Analysis, and Triage Intelligence System](https://arxiv.org/abs/2509.14589)
*Taesoo Kim,HyungSeok Han,Soyeon Park,Dae R. Jeong,Dohyeok Kim,Dongkwan Kim,Eunsoo Kim,Jiho Kim,Joshua Wang,Kangsu Kim,Sangwoo Ji,Woosun Song,Hanqing Zhao,Andrew Chin,Gyejin Lee,Kevin Stevens,Mansour Alharthi,Yizhuo Zhai,Cen Zhang,Joonun Jang,Yeongjin Jang,Ammar Askar,Dongju Kim,Fabian Fleischer,Jeongin Cho,Junsik Kim,Kyungjoon Ko,Insu Yun,Sangdon Park,Dowoo Baik,Haein Lee,Hyeon Heo,Minjae Gwon,Minjae Lee,Minwoo Baek,Seunggi Min,Wonyoung Kim,Yonghwi Jin,Younggi Park,Yunjae Choi,Jinho Jung,Gwanhyun Lee,Junyoung Jang,Kyuheon Kim,Yeonghyeon Cha,Youngjoon Kim*

Main category: cs.CR

TL;DR: ATLANTIS是一个在DARPA AI Cyber Challenge中获得第一名的自主网络推理系统，它结合大型语言模型和程序分析技术来自动发现和修补漏洞


<details>
  <summary>Details</summary>
Motivation: 解决现代软件漏洞发现和修补的速度与规模挑战，克服自动化漏洞发现和程序修复的局限性

Method: 集成大型语言模型(LLMs)与程序分析技术，包括符号执行、定向模糊测试和静态分析，支持从C到Java的多样化代码库

Result: 在DEF CON 33的DARPA AIxCC决赛中获得第一名，实现了高精度和广泛覆盖，产生语义正确的补丁

Conclusion: ATLANTIS展示了程序分析与现代AI结合在自动化安全领域的突破，为未来研究提供了可复现的成果

Abstract: We present ATLANTIS, the cyber reasoning system developed by Team Atlanta
that won 1st place in the Final Competition of DARPA's AI Cyber Challenge
(AIxCC) at DEF CON 33 (August 2025). AIxCC (2023-2025) challenged teams to
build autonomous cyber reasoning systems capable of discovering and patching
vulnerabilities at the speed and scale of modern software. ATLANTIS integrates
large language models (LLMs) with program analysis -- combining symbolic
execution, directed fuzzing, and static analysis -- to address limitations in
automated vulnerability discovery and program repair. Developed by researchers
at Georgia Institute of Technology, Samsung Research, KAIST, and POSTECH, the
system addresses core challenges: scaling across diverse codebases from C to
Java, achieving high precision while maintaining broad coverage, and producing
semantically correct patches that preserve intended behavior. We detail the
design philosophy, architectural decisions, and implementation strategies
behind ATLANTIS, share lessons learned from pushing the boundaries of automated
security when program analysis meets modern AI, and release artifacts to
support reproducibility and future research.

</details>


### [12] [Threats and Security Strategies for IoMT Infusion Pumps](https://arxiv.org/abs/2509.14604)
*Ramazan Yener,Muhammad Hassan,Masooda Bashir*

Main category: cs.CR

TL;DR: 本研究通过文献综述分析了IoMT输液泵的网络安全漏洞，识别了设备级缺陷、认证访问控制问题、网络通信弱点、数据安全风险和组织挑战等五类关键漏洞，为医疗IT专业人员和设备制造商提供结构化安全策略建议。


<details>
  <summary>Details</summary>
Motivation: 随着医疗物联网(IoMT)在医疗系统中的广泛应用，输液泵等关键设备的网络安全漏洞日益突出，可能被网络犯罪分子利用进行横向攻击，威胁患者安全和医疗系统安全。

Method: 通过对过去五年132篇论文的针对性文献回顾，筛选并深入分析了7项最新研究，系统识别和分类输液泵的安全漏洞。

Result: 研究发现输液泵存在五类主要漏洞：设备级缺陷、认证和访问控制问题、网络通信弱点、数据安全和隐私风险、操作和组织挑战，这些漏洞可能导致医疗网络内的横向攻击。

Conclusion: 研究强调了输液泵安全漏洞的严重性，为医疗IT专业人员和设备制造商提供了结构化理解，有助于制定针对性的主动安全策略，更好地保护输液泵和患者安全。

Abstract: The integration of the Internet of Medical Things (IoMT) into healthcare
systems has transformed patient care by enabling real-time monitoring, enhanced
diagnostics, and enhanced operational efficiency. However, this increased
connectivity has also expanded the attack surface for cybercriminals, raising
significant cybersecurity and privacy concerns. This study focuses on the
cybersecurity vulnerabilities of IoMT infusion pumps, which are critical
devices in modern healthcare. Through a targeted literature review of the past
five years, we analyzed seven current studies from a pool of 132 papers to
identify security vulnerabilities. Our findings indicate that infusion pumps
face vulnerabilities such as device-level flaws, authentication and access
control issues, network and communication weaknesses, data security and privacy
risks, and operational or organizational challenges that can expose them to
lateral attacks within healthcare networks. Our analysis synthesizes findings
from seven recent studies to clarify how and why infusion pumps remain
vulnerable in each of these areas. By categorizing the security gaps, we
highlight critical risk patterns and their implications. This work underscores
the scope of the issue and provides a structured understanding that is valuable
for healthcare IT professionals and device manufacturers. Ultimately, the
findings can inform the development of targeted, proactive security strategies
to better safeguard infusion pumps and protect patient well-being.

</details>


### [13] [Enterprise AI Must Enforce Participant-Aware Access Control](https://arxiv.org/abs/2509.14608)
*Shashank Shreedhar Bhatt,Tanmay Rajore,Khushboo Aggarwal,Ganesh Ananthanarayanan,Ranveer Chandra,Nishanth Chandran,Suyash Choudhury,Divya Gupta,Emre Kiciman,Sumit Kumar Pandey,Srinath Setty,Rahul Sharma,Teijia Zhao*

Main category: cs.CR

TL;DR: 论文展示了企业环境中LLM微调和RAG架构存在的数据泄露安全风险，提出了基于细粒度访问控制的确定性防御框架，已在Microsoft Copilot Tuning中部署应用。


<details>
  <summary>Details</summary>
Motivation: 企业环境中LLM微调和RAG系统在处理敏感内部数据时存在严重的数据泄露风险，现有防御措施都是概率性的，无法提供可靠保护。

Method: 提出基于细粒度访问控制的确定性安全框架，要求在训练、检索和生成过程中对所有交互用户明确授权内容使用权限。

Result: 展示了现有防御措施（包括提示清理、输出过滤、系统隔离和训练级隐私机制）都无法有效防止数据泄露攻击。

Conclusion: 只有通过确定性的细粒度访问控制强制执行，才能在微调和RAG推理过程中可靠防止敏感数据泄露给未经授权的接收者。

Abstract: Large language models (LLMs) are increasingly deployed in enterprise settings
where they interact with multiple users and are trained or fine-tuned on
sensitive internal data. While fine-tuning enhances performance by
internalizing domain knowledge, it also introduces a critical security risk:
leakage of confidential training data to unauthorized users. These risks are
exacerbated when LLMs are combined with Retrieval-Augmented Generation (RAG)
pipelines that dynamically fetch contextual documents at inference time.
  We demonstrate data exfiltration attacks on AI assistants where adversaries
can exploit current fine-tuning and RAG architectures to leak sensitive
information by leveraging the lack of access control enforcement. We show that
existing defenses, including prompt sanitization, output filtering, system
isolation, and training-level privacy mechanisms, are fundamentally
probabilistic and fail to offer robust protection against such attacks.
  We take the position that only a deterministic and rigorous enforcement of
fine-grained access control during both fine-tuning and RAG-based inference can
reliably prevent the leakage of sensitive data to unauthorized recipients.
  We introduce a framework centered on the principle that any content used in
training, retrieval, or generation by an LLM is explicitly authorized for
\emph{all users involved in the interaction}. Our approach offers a simple yet
powerful paradigm shift for building secure multi-user LLM systems that are
grounded in classical access control but adapted to the unique challenges of
modern AI workflows. Our solution has been deployed in Microsoft Copilot
Tuning, a product offering that enables organizations to fine-tune models using
their own enterprise-specific data.

</details>


### [14] [Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection](https://arxiv.org/abs/2509.14622)
*Yihao Guo,Haocheng Bian,Liutong Zhou,Ze Wang,Zhaoyi Zhang,Francois Kawala,Milan Dean,Ian Fischer,Yuantao Peng,Noyan Tokgozoglu,Ivan Barrientos,Riyaaz Shaik,Rachel Li,Chandru Venkataraman,Reza Shifteh Far,Moses Pawar,Venkat Sundaranatha,Michael Xu,Frank Chu*

Main category: cs.CR

TL;DR: ADRAG是一个两阶段框架，通过对抗蒸馏和检索增强技术，使用149M参数模型实现高效实时的恶意意图检测，性能接近7B模型，延迟降低5.6倍


<details>
  <summary>Details</summary>
Motivation: 现有方法无法实时处理多样复杂的用户查询，需要开发高效且鲁棒的在线恶意意图检测系统

Method: 两阶段框架：训练阶段使用对抗扰动和检索增强的输入训练教师模型；推理阶段通过蒸馏调度器将知识转移到紧凑学生模型，并利用在线更新的知识库进行实时检测

Result: 在10个安全基准测试中，ADRAG达到WildGuard-7B性能的98.5%，在分布外检测上超越GPT-4 3.3%和Llama-Guard-3-8B 9.5%，延迟降低5.6倍，支持300QPS

Conclusion: ADRAG框架在保持高性能的同时显著降低了计算成本和延迟，为实时恶意意图检测提供了有效的解决方案

Abstract: With the deployment of Large Language Models (LLMs) in interactive
applications, online malicious intent detection has become increasingly
critical. However, existing approaches fall short of handling diverse and
complex user queries in real time. To address these challenges, we introduce
ADRAG (Adversarial Distilled Retrieval-Augmented Guard), a two-stage framework
for robust and efficient online malicious intent detection. In the training
stage, a high-capacity teacher model is trained on adversarially perturbed,
retrieval-augmented inputs to learn robust decision boundaries over diverse and
complex user queries. In the inference stage, a distillation scheduler
transfers the teacher's knowledge into a compact student model, with a
continually updated knowledge base collected online. At deployment, the compact
student model leverages top-K similar safety exemplars retrieved from the
online-updated knowledge base to enable both online and real-time malicious
query detection. Evaluations across ten safety benchmarks demonstrate that
ADRAG, with a 149M-parameter model, achieves 98.5% of WildGuard-7B's
performance, surpasses GPT-4 by 3.3% and Llama-Guard-3-8B by 9.5% on
out-of-distribution detection, while simultaneously delivering up to 5.6x lower
latency at 300 queries per second (QPS) in real-time applications.

</details>


### [15] [Threat Modeling for Enhancing Security of IoT Audio Classification Devices under a Secure Protocols Framework](https://arxiv.org/abs/2509.14657)
*Sergio Benlloch-Lopez,Miquel Viel-Vazquez,Javier Naranjo-Alcazar,Jordi Grau-Haro,Pedro Zuccarello*

Main category: cs.CR

TL;DR: 本文提出了一种针对IoT音频分类设备的深度防御架构，通过TPM远程认证、TLS 1.3加密和后量子密码学等技术，在边缘设备、蜂窝网络和云后端三个信任域中保护敏感音频数据的安全。


<details>
  <summary>Details</summary>
Motivation: 随着配备麦克风并能进行设备端音频分类的IoT节点快速普及，这些设备在资源受限条件下运行时会暴露高度敏感数据，需要构建多层次的安全防护体系。

Method: 采用分层防御架构：1) TPM远程认证确保设备启动完整性；2) TLS 1.3结合Kyber和Dilithium提供后量子加密；3) 端到端加密保护音频特征；4) 签名和防回滚AI模型加固固件；5) 3-2-1数据备份策略保护静态数据。

Result: 设计了一套完整的深度防御协议，能够确保恶意或被篡改的设备保持非活动状态，保护传输中和静态数据的安全，并提供后量子密码学韧性。

Conclusion: 该架构通过多层次安全措施有效保护了IoT音频分类设备的敏感数据，并制定了评估物理和逻辑安全性的计划，为资源受限的IoT环境提供了可行的安全解决方案。

Abstract: The rapid proliferation of IoT nodes equipped with microphones and capable of
performing on-device audio classification exposes highly sensitive data while
operating under tight resource constraints. To protect against this, we present
a defence-in-depth architecture comprising a security protocol that treats the
edge device, cellular network and cloud backend as three separate trust
domains, linked by TPM-based remote attestation and mutually authenticated TLS
1.3. A STRIDE-driven threat model and attack-tree analysis guide the design. At
startup, each boot stage is measured into TPM PCRs. The node can only decrypt
its LUKS-sealed partitions after the cloud has verified a TPM quote and
released a one-time unlock key. This ensures that rogue or tampered devices
remain inert. Data in transit is protected by TLS 1.3 and hybridised with Kyber
and Dilithium to provide post-quantum resilience. Meanwhile, end-to-end
encryption and integrity hashes safeguard extracted audio features. Signed,
rollback-protected AI models and tamper-responsive sensors harden firmware and
hardware. Data at rest follows a 3-2-1 strategy comprising a solid-state drive
sealed with LUKS, an offline cold archive encrypted with a hybrid post-quantum
cipher and an encrypted cloud replica. Finally, we set out a plan for
evaluating the physical and logical security of the proposed protocol.

</details>


### [16] [Security Analysis of Web Applications Based on Gruyere](https://arxiv.org/abs/2509.14706)
*Yonghao Ni,Zhongwen Li,Xiaoqi Li*

Main category: cs.CR

TL;DR: 该论文基于OWASP Top 10漏洞分类，使用Gruyere平台分析web安全漏洞，重现漏洞利用过程并提出修复方案，证明历史漏洞原理对现代安全研究仍有重要价值。


<details>
  <summary>Details</summary>
Motivation: 随着互联网技术的快速发展，web系统成为重要基础设施，但安全漏洞频发，web安全成为网络安全领域的关键研究方向，关系到数据保护、隐私保护和业务连续性。

Method: 首先回顾OWASP Top 10总结常见web漏洞类型、原因和影响，通过典型案例说明利用机制；然后采用Gruyere平台作为实验对象分析已知漏洞，提供详细的漏洞重现步骤和综合修复策略，并与现代真实案例进行比较。

Result: 研究发现虽然Gruyere平台的漏洞相对过时，但其底层原理对解释现代安全缺陷仍然高度相关，基于Gruyere的web系统安全分析能加深对漏洞机制的理解。

Conclusion: 基于Gruyere的web安全分析不仅深化了对漏洞机制的理解，还为技术创新和安全防御提供了实际支持，历史漏洞研究对现代web安全仍具有重要参考价值。

Abstract: With the rapid development of Internet technologies, web systems have become
essential infrastructures for modern information exchange and business
operations. However, alongside their expansion, numerous security
vulnerabilities have emerged, making web security a critical research focus
within the broader field of cybersecurity. These issues are closely related to
data protection, privacy preservation, and business continuity, and systematic
research on web security is crucial for mitigating malicious attacks and
enhancing the reliability and robustness of network systems. This paper first
reviews the OWASP Top 10, summarizing the types, causes, and impacts of common
web vulnerabilities, and illustrates their exploitation mechanisms through
representative cases. Building upon this, the Gruyere platform is adopted as an
experimental subject for analyzing known vulnerabilities. The study presents
detailed reproduction steps for specific vulnerabilities, proposes
comprehensive remediation strategies, and further compares Gruyere's
vulnerabilities with contemporary real-world cases. The findings suggest that,
although Gruyere's vulnerabilities are relatively outdated, their underlying
principles remain highly relevant for explaining a wide range of modern
security flaws. Overall, this research demonstrates that web system security
analysis based on Gruyere not only deepens the understanding of vulnerability
mechanisms but also provides practical support for technological innovation and
security defense.

</details>


### [17] [Variables Ordering Optimization in Boolean Characteristic Set Method Using Simulated Annealing and Machine Learning-based Time Prediction](https://arxiv.org/abs/2509.14754)
*Minzhong Luo,Yudong Sun,Yin Long*

Main category: cs.CR

TL;DR: 本文提出了一种结合机器学习时间预测和模拟退火算法的优化框架，用于高效寻找布尔特征集方法的最佳变量排序，显著提升布尔方程系统的求解效率。


<details>
  <summary>Details</summary>
Motivation: 布尔特征集(BCS)方法在求解布尔方程系统时性能高度依赖于变量排序，不同排序下的求解时间差异巨大，这限制了该方法的实际应用效果。

Method: 构建包含变量频率谱和对应BCS求解时间的数据集，训练机器学习预测器估计任意变量排序的求解时间，然后将该预测器作为模拟退火算法的成本函数来寻找最优变量排序。

Result: 实验表明该方法显著优于标准BCS算法、Gröbner基方法和SAT求解器，特别是在大规模系统(n=32)上表现突出。

Conclusion: 该工作不仅为代数密码分析提供了实用的加速工具，还为符号计算中机器学习增强的组合优化奠定了理论基础，并推导了基于随机过程理论的概率时间复杂度界限。

Abstract: Solving systems of Boolean equations is a fundamental task in symbolic
computation and algebraic cryptanalysis, with wide-ranging applications in
cryptography, coding theory, and formal verification. Among existing
approaches, the Boolean Characteristic Set (BCS) method[1] has emerged as one
of the most efficient algorithms for tackling such problems. However, its
performance is highly sensitive to the ordering of variables, with solving
times varying drastically under different orderings for fixed variable counts n
and equations size m. To address this challenge, this paper introduces a novel
optimization framework that synergistically integrates machine learning
(ML)-based time prediction with simulated annealing (SA) to efficiently
identify high-performance variables orderings. Weconstruct a dataset comprising
variable frequency spectrum X and corresponding BCS solving time t for
benchmark systems(e.g., n = m = 28). Utilizing this data, we train an accurate
ML predictor ft(X) to estimate solving time for any given variables ordering.
For each target system, ft serves as the cost function within an SA algorithm,
enabling rapid discovery of low-latency orderings that significantly expedite
subsequent BCS execution. Extensive experiments demonstrate that our method
substantially outperforms the standard BCS algorithm[1], Gr\"obner basis method
[2] and SAT solver[3], particularly for larger-scale systems(e.g., n = 32).
Furthermore, we derive probabilistic time complexity bounds for the overall
algorithm using stochastic process theory, establishing a quantitative
relationship between predictor accuracy and expected solving complexity. This
work provides both a practical acceleration tool for algebraic cryptanalysis
and a theoretical foundation for ML-enhanced combinatorial optimization in
symbolic computation.

</details>


### [18] [Blockchain-Enabled Explainable AI for Trusted Healthcare Systems](https://arxiv.org/abs/2509.14987)
*Md Talha Mohsin*

Main category: cs.CR

TL;DR: 提出了一个区块链集成可解释AI框架(BXHF)，用于解决医疗系统中的安全数据交换和可理解AI临床决策两大挑战，结合区块链的不可篡改性和XAI的可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决医疗信息网络面临的两个关键问题：安全的数据交换和可理解的AI驱动临床决策，确保患者隐私保护和临床决策的透明度。

Method: 采用区块链技术确保患者记录的不可篡改性和可审计性，结合可解释AI方法提供透明的模型预测，通过混合边缘-云架构实现联邦计算。

Result: 开发了一个统一优化框架，确保数据级信任(通过验证和加密记录共享)和决策级信任(提供可审计且临床相关的解释)，支持跨境临床研究、罕见病检测和高风险干预决策等应用场景。

Conclusion: BXHF通过确保透明度、可审计性和法规合规性，提高了AI在医疗保健中的可信度、采用率和有效性，为更安全可靠的临床决策奠定了基础。

Abstract: This paper introduces a Blockchain-Integrated Explainable AI Framework (BXHF)
for healthcare systems to tackle two essential challenges confronting health
information networks: safe data exchange and comprehensible AI-driven clinical
decision-making. Our architecture incorporates blockchain, ensuring patient
records are immutable, auditable, and tamper-proof, alongside Explainable AI
(XAI) methodologies that yield transparent and clinically relevant model
predictions. By incorporating security assurances and interpretability
requirements into a unified optimization pipeline, BXHF ensures both data-level
trust (by verified and encrypted record sharing) and decision-level trust (with
auditable and clinically aligned explanations). Its hybrid edge-cloud
architecture allows for federated computation across different institutions,
enabling collaborative analytics while protecting patient privacy. We
demonstrate the framework's applicability through use cases such as
cross-border clinical research networks, uncommon illness detection and
high-risk intervention decision support. By ensuring transparency,
auditability, and regulatory compliance, BXHF improves the credibility, uptake,
and effectiveness of AI in healthcare, laying the groundwork for safer and more
reliable clinical decision-making.

</details>


### [19] [Watermarking and Anomaly Detection in Machine Learning Models for LORA RF Fingerprinting](https://arxiv.org/abs/2509.15170)
*Aarushi Mahajan,Wayne Burleson*

Main category: cs.CR

TL;DR: 提出了一种结合数字水印和异常检测的射频指纹识别系统，通过ResNet-34和变分自编码器实现高精度设备认证，同时提供所有权证明和防篡改保护。


<details>
  <summary>Details</summary>
Motivation: 现有的射频指纹识别系统虽然通过深度学习提高了准确性，但模型容易受到复制、篡改和规避攻击，需要更强的安全保护机制。

Method: 使用ResNet-34处理对数梅尔频谱图，嵌入三种水印（简单触发、抗噪声对抗训练触发、隐藏梯度/权重签名），并采用带KL预热和自由比特的卷积变分自编码器进行异常检测。

Result: 在LoRa数据集上达到94.6%的识别准确率、98%的水印成功率以及0.94的AUROC值。

Conclusion: 该系统提供了可验证、防篡改的认证方案，有效解决了射频指纹识别模型的安全脆弱性问题。

Abstract: Radio frequency fingerprint identification (RFFI) distinguishes wireless
devices by the small variations in their analog circuits, avoiding heavy
cryptographic authentication. While deep learning on spectrograms improves
accuracy, models remain vulnerable to copying, tampering, and evasion. We
present a stronger RFFI system combining watermarking for ownership proof and
anomaly detection for spotting suspicious inputs. Using a ResNet-34 on log-Mel
spectrograms, we embed three watermarks: a simple trigger, an adversarially
trained trigger robust to noise and filtering, and a hidden gradient/weight
signature. A convolutional Variational Autoencoders (VAE) with Kullback-Leibler
(KL) warm-up and free-bits flags off-distribution queries. On the LoRa dataset,
our system achieves 94.6% accuracy, 98% watermark success, and 0.94 AUROC,
offering verifiable, tamper-resistant authentication.

</details>


### [20] [Beyond Surface Alignment: Rebuilding LLMs Safety Mechanism via Probabilistically Ablating Refusal Direction](https://arxiv.org/abs/2509.15202)
*Yuanbo Xie,Yingjie Zhang,Tianyun Liu,Duohe Ma,Tingwen Liu*

Main category: cs.CR

TL;DR: DeepRefusal是一个针对大语言模型越狱攻击的鲁棒安全对齐框架，通过概率性消融拒绝方向来动态重建拒绝机制，显著降低攻击成功率约95%


<details>
  <summary>Details</summary>
Motivation: 当前安全对齐方法存在安全对齐深度不足和内部防御机制不鲁棒两个主要限制，容易受到预填充和拒绝方向操纵等对抗攻击

Method: 在微调过程中概率性地跨层和token深度消融拒绝方向，强制模型从越狱状态动态重建其拒绝机制

Result: 在四个开源LLM家族和六种代表性攻击上的广泛评估显示，DeepRefusal将攻击成功率降低了约95%，同时保持模型能力且性能退化最小

Conclusion: DeepRefusal框架有效解决了现有安全对齐方法的局限性，提供了对预填充、拒绝方向攻击以及其他未见越狱策略的强大抵御能力

Abstract: Jailbreak attacks pose persistent threats to large language models (LLMs).
Current safety alignment methods have attempted to address these issues, but
they experience two significant limitations: insufficient safety alignment
depth and unrobust internal defense mechanisms. These limitations make them
vulnerable to adversarial attacks such as prefilling and refusal direction
manipulation. We introduce DeepRefusal, a robust safety alignment framework
that overcomes these issues. DeepRefusal forces the model to dynamically
rebuild its refusal mechanisms from jailbreak states. This is achieved by
probabilistically ablating the refusal direction across layers and token depths
during fine-tuning. Our method not only defends against prefilling and refusal
direction attacks but also demonstrates strong resilience against other unseen
jailbreak strategies. Extensive evaluations on four open-source LLM families
and six representative attacks show that DeepRefusal reduces attack success
rates by approximately 95%, while maintaining model capabilities with minimal
performance degradation.

</details>


### [21] [Evil Vizier: Vulnerabilities of LLM-Integrated XR Systems](https://arxiv.org/abs/2509.15213)
*Yicheng Zhang,Zijian Huang,Sophie Chen,Erfan Shayegani,Jiasi Chen,Nael Abu-Ghazaleh*

Main category: cs.CR

TL;DR: 本文分析了XR设备集成LLM的安全漏洞，展示了针对多平台的概念验证攻击，并提出了防御策略。


<details>
  <summary>Details</summary>
Motivation: 随着XR应用越来越多地集成大语言模型来增强用户体验和内容生成，这种集成带来了新的安全威胁，需要系统性地分析和防御。

Method: 通过分析文献和实际系统中的XR-LLM集成方案，建立分类体系，识别共同威胁模型，并在多个XR平台上进行概念验证攻击演示。

Result: 发现尽管不同平台实现方式各异，但都存在攻击者可以通过修改公共上下文来操纵LLM查询的漏洞，导致错误的视听反馈，危害用户安全和隐私。

Conclusion: 提出了针对开发者的缓解策略和最佳实践，包括初步防御原型，并呼吁社区开发新的保护机制来降低这些风险。

Abstract: Extended reality (XR) applications increasingly integrate Large Language
Models (LLMs) to enhance user experience, scene understanding, and even
generate executable XR content, and are often called "AI glasses". Despite
these potential benefits, the integrated XR-LLM pipeline makes XR applications
vulnerable to new forms of attacks. In this paper, we analyze LLM-Integated XR
systems in the literature and in practice and categorize them along different
dimensions from a systems perspective. Building on this categorization, we
identify a common threat model and demonstrate a series of proof-of-concept
attacks on multiple XR platforms that employ various LLM models (Meta Quest 3,
Meta Ray-Ban, Android, and Microsoft HoloLens 2 running Llama and GPT models).
Although these platforms each implement LLM integration differently, they share
vulnerabilities where an attacker can modify the public context surrounding a
legitimate LLM query, resulting in erroneous visual or auditory feedback to
users, thus compromising their safety or privacy, sowing confusion, or other
harmful effects. To defend against these threats, we discuss mitigation
strategies and best practices for developers, including an initial defense
prototype, and call on the community to develop new protection mechanisms to
mitigate these risks.

</details>
