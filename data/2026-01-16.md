<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 18]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Instalación, configuración y utilización de un nodo Bitcoin en Linux](https://arxiv.org/abs/2601.09748)
*Jose Eduardo Ulloa,Diego R. Llanos*

Main category: cs.CR

TL;DR: 本文详细记录了在Linux环境中安装、配置和运行完整比特币节点的全过程，包括手动编译源代码、网络同步、参数调优和系统资源监控，为比特币节点性能研究和区块链数据查询工具开发提供基础。


<details>
  <summary>Details</summary>
Motivation: 比特币节点的安装和配置过程缺乏系统性的技术文档，特别是参数调优和资源使用方面的经验数据不足。本文旨在填补这一空白，为研究人员和开发者提供可靠的节点部署参考。

Method: 采用手动编译比特币核心源代码的方式，在Linux环境中部署完整节点。通过实验研究txindex、prune、dbcache、maxmempool、maxconnections等关键参数的影响，并监控初始区块下载(IBD)过程中的系统资源使用情况。

Result: 详细记录了比特币节点的安装配置流程，分析了主要生成文件的功能，提供了关键参数的实证研究数据，并量化了IBD过程中CPU、内存、磁盘I/O和网络带宽等系统资源的使用模式。

Conclusion: 本文为比特币节点性能优化和区块链数据查询工具开发提供了坚实的技术基础，所记录的配置经验和资源使用数据对未来的研究和应用开发具有重要参考价值。

Abstract: This paper documents the installation, configuration, and operation of a full Bitcoin node in a Linux environment, from manual compilation of the source code to complete synchronization with the network. The technical phases of the process are described, the main files generated by Bitcoin Core are analyzed, and the effects of the parameters txindex, prune, dbcache, maxmempool, and maxconnections are empirically studied. System resources during the block download (IBD) mechanism are also documented, and the operational importance of each resource is explained. This paper provides a solid foundation for future research proposals on Bitcoin node performance or for the development of blockchain data query tools.

</details>


### [2] [Synthetic Data for Veterinary EHR De-identification: Benefits, Limits, and Safety Trade-offs Under Fixed Compute](https://arxiv.org/abs/2601.09756)
*David Brundage*

Main category: cs.CR

TL;DR: 评估LLM生成的合成病历是否能改善兽医去标识化的安全性，发现在固定样本替换下合成数据会增加泄漏风险，而在训练扩展时适度混合合成数据能提升性能，但主要收益来自训练量增加而非合成数据质量本身。


<details>
  <summary>Details</summary>
Motivation: 兽医电子健康记录包含隐私敏感信息，限制了二次使用。PetEVAL提供了兽医去标识化的基准，但该领域资源有限。本研究旨在评估LLM生成的合成病历是否能改善去标识化的安全性。

Method: 使用PetEVAL衍生的语料库（3,750个保留/1,249个训练），采用隐私保护的"仅模板"机制生成10,382个合成病历。使用三种transformer架构（PetBERT、VetBERT、Bio_ClinicalBERT）在不同混合比例下训练。主要评估文档级泄漏率作为安全性指标。

Result: 固定样本替换下，用合成病历替换真实病历会单调增加泄漏率，表明合成数据不能安全替代真实监督。在计算匹配训练中，适度混合合成数据能达到真实数据性能，但高比例合成数据会降低效用。训练扩展时，PetBERT的F1从0.831提升到0.850，泄漏率从6.32%降至4.02%，但这些收益主要反映训练量增加而非合成数据质量。

Conclusion: 合成数据增强对于扩展训练暴露是有效的，但对于安全关键的兽医去标识化任务，它只是补充而非替代真实数据。合成数据与真实数据在笔记长度和标签分布上存在系统性不匹配，这与持续泄漏风险相关。

Abstract: Veterinary electronic health records (vEHRs) contain privacy-sensitive identifiers that limit secondary use. While PetEVAL provides a benchmark for veterinary de-identification, the domain remains low-resource. This study evaluates whether large language model (LLM)-generated synthetic narratives improve de-identification safety under distinct training regimes, emphasizing (i) synthetic augmentation and (ii) fixed-budget substitution. We conducted a controlled simulation using a PetEVAL-derived corpus (3,750 holdout/1,249 train). We generated 10,382 synthetic notes using a privacy-preserving "template-only" regime where identifiers were removed prior to LLM prompting. Three transformer backbones (PetBERT, VetBERT, Bio_ClinicalBERT) were trained under varying mixtures. Evaluation prioritized document-level leakage rate (the fraction of documents with at least one missed identifier) as the primary safety outcome. Results show that under fixed-sample substitution, replacing real notes with synthetic ones monotonically increased leakage, indicating synthetic data cannot safely replace real supervision. Under compute-matched training, moderate synthetic mixing matched real-only performance, but high synthetic dominance degraded utility. Conversely, epoch-scaled augmentation improved performance: PetBERT span-overlap F1 increased from 0.831 to 0.850 +/- 0.014, and leakage decreased from 6.32% to 4.02% +/- 0.19%. However, these gains largely reflect increased training exposure rather than intrinsic synthetic data quality. Corpus diagnostics revealed systematic synthetic-real mismatches in note length and label distribution that align with persistent leakage. We conclude that synthetic augmentation is effective for expanding exposure but is complementary, not substitutive, for safety-critical veterinary de-identification.

</details>


### [3] [A Risk-Stratified Benchmark Dataset for Bad Randomness (SWC-120) Vulnerabilities in Ethereum Smart Contracts](https://arxiv.org/abs/2601.09836)
*Hadis Rezaei,Rahim Taheri,Francesco Palmieri*

Main category: cs.CR

TL;DR: 论文提出了一个包含1,752个以太坊智能合约的基准数据集，用于检测Bad Randomness漏洞，并开发了包含关键词过滤、模式匹配、风险分类、函数级验证和上下文分析的五阶段方法。


<details>
  <summary>Details</summary>
Motivation: 当前以太坊智能合约中依赖区块属性生成随机数存在安全漏洞（SWC-120），现有检测工具只能识别简单模式且缺乏大规模准确标注的数据集，阻碍了工具改进。

Method: 采用五阶段方法：1) 关键词过滤；2) 58个正则表达式模式匹配；3) 风险分类；4) 函数级验证；5) 上下文分析。将合约分为四个风险等级：高风险（无保护）、中风险（仅矿工可攻击）、低风险（仅所有者可攻击）、安全（使用Chainlink VRF或commit-reveal）。

Result: 构建了比RNVulDet大51倍的数据集，函数级验证发现49%初始分类为受保护的合约实际上可被攻击。评估Slither和Mythril显示两者均未能检测出样本中的漏洞合约。

Conclusion: 该研究提供了首个具有函数级验证和风险分层的Bad Randomness漏洞数据集，揭示了现有工具的局限性，为智能合约安全研究提供了重要资源。

Abstract: Many Ethereum smart contracts rely on block attributes such as block.timestamp or blockhash to generate random numbers for applications like lotteries and games. However, these values are predictable and miner-manipulable, creating the Bad Randomness vulnerability (SWC-120) that has led to real-world exploits. Current detection tools identify only simple patterns and fail to verify whether protective modifiers actually guard vulnerable code. A major obstacle to improving these tools is the lack of large, accurately labeled datasets. This paper presents a benchmark dataset of 1,752 Ethereum smart contracts with validated Bad Randomness vulnerabilities. We developed a five-phase methodology comprising keyword filtering, pattern matching with 58 regular expressions, risk classification, function-level validation, and context analysis. The function-level validation revealed that 49% of contracts initially classified as protected were actually exploitable because modifiers were applied to different functions than those containing vulnerabilities. We classify contracts into four risk levels based on exploitability: HIGH_RISK (no protection), MEDIUM_RISK (miner-exploitable only), LOW_RISK (owner-exploitable only), and SAFE (using Chainlink VRF or commit-reveal). Our dataset is 51 times larger than RNVulDet and the first to provide function-level validation and risk stratification. Evaluation of Slither and Mythril revealed significant detection gaps, as both tools identified none of the vulnerable contracts in our sample, indicating limitations in handling complex randomness patterns. The dataset and validation scripts are publicly available to support future research in smart contract security.

</details>


### [4] [AmbShield: Enhancing Physical Layer Security with Ambient Backscatter Devices against Eavesdroppers](https://arxiv.org/abs/2601.09867)
*Yifan Zhang,Yishan Yang,Riku Jäntti,Zheng Yan,Dusit Niyato,Zhu Han*

Main category: cs.CR

TL;DR: AmbShield：利用环境反向散射设备实现物理层安全，无需额外发射功率，通过随机反向散射干扰窃听者，同时增强合法信道容量。


<details>
  <summary>Details</summary>
Motivation: 无线网络中被动窃听威胁机密性，传统物理层安全技术需要额外功耗、精确同步和完美信道状态信息，在实际部署中受限。需要一种低开销、实用的安全方案。

Method: 提出AmbShield方案，利用环境反向散射设备作为友好干扰器和被动中继：1）随机反向散射干扰窃听者；2）反向散射期望信号增强合法设备容量。建立统一分析框架，推导合法和窃听信噪比的概率分布函数和累积分布函数，以及闭式保密中断概率。

Result: 通过蒙特卡洛仿真、理论推导和高信噪比渐近分析验证，AmbShield在各种系统参数下（包括非完美同步和信道估计）均能提供显著安全增益，分析结果为最小化保密中断概率提供了明确的设计指导。

Conclusion: AmbShield是一种实用的物理层安全方案，利用环境反向散射设备实现安全增强，无需额外发射功率和最小部署开销，在非理想条件下仍能有效工作，为资源受限环境提供了可行的安全解决方案。

Abstract: Passive eavesdropping compromises confidentiality in wireless networks, especially in resource-constrained environments where heavyweight cryptography is impractical. Physical layer security (PLS) exploits channel randomness and spatial selectivity to confine information to an intended receiver with modest overhead. However, typical PLS techniques, such as using beamforming, artificial noise, and reconfigurable intelligent surfaces, often involve added active power or specialized deployment, and, in many designs, rely on precise time synchronization and perfect CSI estimation, which limits their practicality. To this end, we propose AmbShield, an AmBD-assisted PLS scheme that leverages naturally distributed AmBDs to simultaneously strengthen the legitimate channel and degrade eavesdroppers' without requiring extra transmit power and with minimal deployment overhead. In AmbShield, AmBDs are exploited as friendly jammers that randomly backscatter to create interference at eavesdroppers, and as passive relays that backscatter the desired signal to enhance the capacity of legitimate devices. We further develop a unified analytical framework that analyzes the exact probability density function (PDF) and cumulative distribution function (CDF) of legitimate and eavesdropper signal-to-interference-noise ratio (SINR), and a closed-form secrecy outage probability (SOP). The analysis provides clear design guidelines on various practical system parameters to minimize SOP. Extensive experiments that include Monte Carlo simulations, theoretical derivations, and high-SNR asymptotic analysis demonstrate the security gains of AmbShield across diverse system parameters under imperfect synchronization and CSI estimation.

</details>


### [5] [A Novel Contrastive Loss for Zero-Day Network Intrusion Detection](https://arxiv.org/abs/2601.09902)
*Jack Wilkie,Hanan Hindy,Craig Michie,Christos Tachtatzis,James Irvine,Robert Atkinson*

Main category: cs.CR

TL;DR: 提出一种新颖的对比损失函数，用于网络入侵检测中的零日攻击检测，相比传统方法在已知和零日攻击检测上都有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在遇到训练数据中未包含的新攻击类型（零日攻击）时性能显著下降，而异常检测器虽然能检测所有攻击类型但误报率过高。需要一种既能处理数据不平衡问题，又能泛化到零日攻击的方法。

Method: 提出一种新颖的对比损失函数，该模型同时使用良性流量样本和已知恶意样本（不包括零日攻击类）来学习良性流量的分布，从而能够泛化到零日攻击检测。

Result: 在Lycos2017数据集上，相比先前模型，在已知攻击检测上AUROC提升0.000065，在零日攻击检测上AUROC提升0.060883。在开放集识别任务中，OpenAUC提升0.170883。

Conclusion: 提出的对比学习方法在保持对数据不平衡鲁棒性的同时，能够有效检测零日攻击，相比传统异常检测方法在性能上有显著改进。

Abstract: Machine learning has achieved state-of-the-art results in network intrusion detection; however, its performance significantly degrades when confronted by a new attack class -- a zero-day attack. In simple terms, classical machine learning-based approaches are adept at identifying attack classes on which they have been previously trained, but struggle with those not included in their training data. One approach to addressing this shortcoming is to utilise anomaly detectors which train exclusively on benign data with the goal of generalising to all attack classes -- both known and zero-day. However, this comes at the expense of a prohibitively high false positive rate. This work proposes a novel contrastive loss function which is able to maintain the advantages of other contrastive learning-based approaches (robustness to imbalanced data) but can also generalise to zero-day attacks. Unlike anomaly detectors, this model learns the distributions of benign traffic using both benign and known malign samples, i.e. other well-known attack classes (not including the zero-day class), and consequently, achieves significant performance improvements. The proposed approach is experimentally verified on the Lycos2017 dataset where it achieves an AUROC improvement of .000065 and .060883 over previous models in known and zero-day attack detection, respectively. Finally, the proposed method is extended to open-set recognition achieving OpenAUC improvements of .170883 over existing approaches.

</details>


### [6] [Malware Classification using Diluted Convolutional Neural Network with Fast Gradient Sign Method](https://arxiv.org/abs/2601.09933)
*Ashish Anand,Bhupendra Singh,Sunil Khemka,Bireswar Banerjee,Vishi Singh Bhatia,Piyush Ranjan*

Main category: cs.CR

TL;DR: 提出FGSM DICNN方法用于Android恶意软件分类，通过稀释卷积扩大感受野捕获分散模式，结合FGSM对抗训练提升准确率，达到99.44%准确率


<details>
  <summary>Details</summary>
Motivation: Android恶意软件日益复杂，传统检测方法需要大量特征且耗时，需要更高效准确的分类方法

Method: 结合稀释卷积神经网络(DICNN)和快速梯度符号方法(FGSM)，DICNN使用稀释卷积扩大感受野捕获分散模式，FGSM在训练中使用一步扰动增强模型鲁棒性

Result: FGSM DICNN模型达到99.44%准确率，优于现有的Custom Deep Neural Network等方法

Conclusion: 该方法在保持高分类准确率的同时减少了对大量特征的依赖，计算成本更低，为Android恶意软件检测提供了有效解决方案

Abstract: Android malware has become an increasingly critical threat to organizations, society and individuals, posing significant risks to privacy, data security and infrastructure. As malware continues to evolve in terms of complexity and sophistication, the mitigation and detection of these malicious software instances have become more time consuming and challenging particularly due to the requirement of large number of features to identify potential malware. To address these challenges, this research proposes Fast Gradient Sign Method with Diluted Convolutional Neural Network (FGSM DICNN) method for malware classification. DICNN contains diluted convolutions which increases receptive field, enabling the model to capture dispersed malware patterns across long ranges using fewer features without adding parameters. Additionally, the FGSM strategy enhance the accuracy by using one-step perturbations during training that provides more defensive advantage of lower computational cost. This integration helps to manage high classification accuracy while reducing the dependence on extensive feature sets. The proposed FGSM DICNN model attains 99.44% accuracy while outperforming other existing approaches such as Custom Deep Neural Network (DCNN).

</details>


### [7] [SoK: Privacy-aware LLM in Healthcare: Threat Model, Privacy Techniques, Challenges and Recommendations](https://arxiv.org/abs/2601.10004)
*Mohoshin Ara Tahera,Karamveer Singh Sidhu,Shuvalaxmi Dass,Sajal Saha*

Main category: cs.CR

TL;DR: 这篇论文系统分析了医疗领域大语言模型在数据预处理、微调和推理三个阶段的隐私安全威胁，提出了威胁模型和防御建议，为医疗AI系统的隐私保护提供路线图。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在医疗领域的广泛应用（如临床决策支持、电子病历总结等），医疗数据的敏感性和医疗工作流程的高风险性带来了显著的隐私和安全挑战。这些风险在不同部署环境（从本地医院系统到区域医疗网络）中更加复杂，需要系统化的安全分析。

Method: 采用系统化知识整理方法，分析LLM在医疗应用的三个核心阶段：数据预处理、微调和推理。建立了详细的威胁模型，包括攻击者特征、能力和攻击面，并系统梳理了现有隐私保护技术如何缓解这些漏洞。

Result: 分析发现现有防御措施在保护敏感临床数据方面仍有局限性，特别是在不同操作层级上。识别了医疗LLM部署中的关键隐私漏洞和攻击面。

Conclusion: 提出了针对不同阶段的建议和未来研究方向，旨在加强医疗环境中LLM的隐私保障。为理解LLM、威胁和医疗隐私的交叉领域提供了基础，为构建更稳健、临床可信的AI系统提供了路线图。

Abstract: Large Language Models (LLMs) are increasingly adopted in healthcare to support clinical decision-making, summarize electronic health records (EHRs), and enhance patient care. However, this integration introduces significant privacy and security challenges, driven by the sensitivity of clinical data and the high-stakes nature of medical workflows. These risks become even more pronounced across heterogeneous deployment environments, ranging from small on-premise hospital systems to regional health networks, each with unique resource limitations and regulatory demands. This Systematization of Knowledge (SoK) examines the evolving threat landscape across the three core LLM phases: Data preprocessing, Fine-tuning, and Inference within realistic healthcare settings. We present a detailed threat model that characterizes adversaries, capabilities, and attack surfaces at each phase, and we systematize how existing privacy-preserving techniques (PPTs) attempt to mitigate these vulnerabilities. While existing defenses show promise, our analysis identifies persistent limitations in securing sensitive clinical data across diverse operational tiers. We conclude with phase-aware recommendations and future research directions aimed at strengthening privacy guarantees for LLMs in regulated environments. This work provides a foundation for understanding the intersection of LLMs, threats, and privacy in healthcare, offering a roadmap toward more robust and clinically trustworthy AI systems.

</details>


### [8] [Privacy Enhanced PEFT: Tensor Train Decomposition Improves Privacy Utility Tradeoffs under DP-SGD](https://arxiv.org/abs/2601.10045)
*Pradip Kunwar,Minh Vu,Maanak Gupta,Manish Bhattarai*

Main category: cs.CR

TL;DR: TTLoRA-DP：一种基于张量分解的隐私保护参数高效微调方法，相比LoRA-DP在保持性能的同时提供更强的隐私保护


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在敏感数据上微调存在隐私风险，传统差分隐私方法应用于参数高效微调（如LoRA）会导致显著的性能损失，需要更好的隐私-效用权衡方案

Method: 提出TTLoRA-DP框架，将张量分解（Tensor Train）与LoRA结合，通过缓存收缩状态扩展ghost clipping算法，实现高效的差分隐私随机梯度下降（DP-SGD），无需计算完整梯度

Result: 在GPT-2微调实验中，TTLoRA-DP相比LoRA-DP提供更强的隐私保护，同时保持相当或更好的下游性能，使用更小的适配器（平均参数减少7.6倍），即使无DP训练也显示更低的成员推理泄露

Conclusion: TTLoRA为参数高效语言模型适配提供了改进隐私-效用权衡的实用路径，结构约束的PEFT架构能有效缩小参数空间同时保持表达能力

Abstract: Fine-tuning large language models on sensitive data poses significant privacy risks, as membership inference attacks can reveal whether individual records were used during training. While Differential Privacy (DP) provides formal protection, applying DP to conventional Parameter-Efficient Fine-Tuning (PEFT) methods such as Low-Rank Adaptation (LoRA) often incurs substantial utility loss. In this work, we show that a more structurally constrained PEFT architecture, Tensor Train Low-Rank Adaptation (TTLoRA), can improve the privacy-utility tradeoff by shrinking the effective parameter space while preserving expressivity. To this end, we develop TTLoRA-DP, a differentially private training framework for TTLoRA. Specifically, we extend the ghost clipping algorithm to Tensor Train cores via cached contraction states, enabling efficient Differentially Private Stochastic Gradient Descent (DP-SGD) with exact per-example gradient norm computation without materializing full per-example gradients. Experiments on GPT-2 fine-tuning over the Enron and Penn Treebank datasets show that TTLoRA-DP consistently strengthens privacy protection relative to LoRA-DP while maintaining comparable or better downstream utility. Moreover, TTLoRA exhibits lower membership leakage even without DP training, using substantially smaller adapters and requiring on average 7.6X fewer parameters than LoRA. Overall, our results demonstrate that TTLoRA offers a practical path to improving the privacy-utility tradeoff in parameter-efficient language model adaptation.

</details>


### [9] [Fuzzychain-edge: A novel Fuzzy logic-based adaptive Access control model for Blockchain in Edge Computing](https://arxiv.org/abs/2601.10105)
*Khushbakht Farooq,Muhammad Ibrahim,Irsa Manzoor,Mukhtaj Khan,Wei Song*

Main category: cs.CR

TL;DR: 提出Fuzzychain-edge框架，结合模糊逻辑、区块链和零知识证明，为边缘计算中的物联网系统提供自适应访问控制，解决数据隐私和安全问题。


<details>
  <summary>Details</summary>
Motivation: 物联网与边缘计算融合带来实时数据共享优势，但也引入数据隐私泄露和安全漏洞等关键挑战。传统访问控制机制和集中式安全系统无法解决这些问题，导致物联网环境面临未授权访问和数据滥用的风险。

Method: 提出Fuzzychain-edge框架，整合三种关键技术：1) 零知识证明保护敏感数据验证过程；2) 模糊逻辑实现基于数据敏感性、信任等级和用户角色的自适应决策；3) 区块链和智能合约提供去中心化、不可篡改的透明访问控制。

Result: 预期成果包括：增强安全性、降低未授权访问可能性、提供透明的数据交易审计追踪、改善数据隐私、提高访问控制准确性、增强用户对物联网系统的信任。

Conclusion: 该研究为物联网环境提供了隐私保护、安全且可追溯的解决方案，为去中心化技术在医疗等关键领域的应用创新奠定基础。

Abstract: The rapid integration of IoT with edge computing has revolutionized various domains, particularly healthcare, by enabling real-time data sharing, remote monitoring, and decision-making. However, it introduces critical challenges, including data privacy breaches, security vulnerabilities, especially in environments dealing with sensitive information. Traditional access control mechanisms and centralized security systems do not address these issues, leaving IoT environments exposed to unauthorized access and data misuse. This research proposes Fuzzychain-edge, a novel Fuzzy logic-based adaptive Access control model for Blockchain in Edge Computing framework designed to overcome these limitations by incorporating Zero-Knowledge Proofs (ZKPs), fuzzy logic, and smart contracts. ZKPs secure sensitive data during access control processes by enabling verification without revealing confidential details, thereby ensuring user privacy. Fuzzy logic facilitates adaptive, context-aware decision-making for access control by dynamically evaluating parameters such as data sensitivity, trust levels, and user roles. Blockchain technology, with its decentralized and immutable architecture, ensures transparency, traceability, and accountability using smart contracts that automate access control processes. The proposed framework addresses key challenges by enhancing security, reducing the likelihood of unauthorized access, and providing a transparent audit trail of data transactions. Expected outcomes include improved data privacy, accuracy in access control, and increased user trust in IoT systems. This research contributes significantly to advancing privacy-preserving, secure, and traceable solutions in IoT environments, laying the groundwork for future innovations in decentralized technologies and their applications in critical domains such as healthcare and beyond.

</details>


### [10] [Advanced Encryption Technique for Multimedia Data Using Sudoku-Based Algorithms for Enhanced Security](https://arxiv.org/abs/2601.10119)
*Mithil Bavishi,Anuj Bohra,Kushal Vadodaria,Abhinav Bohra,Neha Katre,Ramchandra Mangrulkar,Vinaya Sawant*

Main category: cs.CR

TL;DR: 基于数独的加密系统扩展到多媒体数据（图像、音频、视频），通过时间戳增强密钥安全性，采用块置换和XOR替换方法，对暴力破解和差分攻击具有强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 扩展数独加密方法到多媒体领域，解决传统加密在图像、音频、视频等多媒体数据上的适用性问题，同时通过时间戳依赖增强密钥安全性以防止数据泄露。

Method: 采用基于数独的块置换密码系统，可结合XOR替换方法，密钥生成依赖于消息传输时间戳，支持多模态数据处理，实现为块基置换密码和替换密码的混合系统。

Result: 加密后的媒体数据高度安全：图像NPCR值接近100%，音频SNR值超过60dB，加密音频与源音频差异显著，对暴力破解和差分攻击具有强抵抗力。

Conclusion: 该系统成功将数独加密扩展到多媒体领域，通过时间戳依赖的密钥机制增强了安全性，为图像、音频和视频提供了有效的加密解决方案。

Abstract: Encryption and Decryption is the process of sending a message in a ciphered way that appears meaningless and could be deciphered using a key for security purposes to avoid data breaches. This paper expands on the previous work on Sudoku-based encryption methods, applying it to other forms of media including images, audio and video. It also enhances the security of key generation and usage by making it dependent on the timestamp of when the message was transmitted. It is a versatile system that works on multimodal data and functions as a block-based transposition cipher. Instead of shuffling, it can also employ substitution methods like XOR, making it a substitution cipher. The resulting media are highly encrypted and resilient to brute-force and differential attacks. For images, NPCR values approach 100% and for audio, SNR values exceed 60dB. This makes the encrypted audio significantly different from the source, making decryption more difficult.

</details>


### [11] [ReasAlign: Reasoning Enhanced Safety Alignment against Prompt Injection Attack](https://arxiv.org/abs/2601.10173)
*Hao Li,Yankai Yang,G. Edward Suh,Ning Zhang,Chaowei Xiao*

Main category: cs.CR

TL;DR: ReasAlign通过结构化推理步骤和测试时缩放机制，有效防御间接提示注入攻击，在保持实用性的同时显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型构建的智能体系统虽然强大，但极易受到间接提示注入攻击的威胁，恶意指令通过外部数据嵌入可以劫持智能体行为，需要有效的防御方案。

Method: ReasAlign采用模型级解决方案，核心是结构化推理步骤来分析用户查询、检测冲突指令、保持用户任务连续性；同时引入测试时缩放机制，通过偏好优化的评判模型对推理步骤评分并选择最佳轨迹。

Result: 在多个基准测试中，ReasAlign在保持与未防御模型相当的实用性的同时，显著优于最强的现有防御方案Meta SecAlign。在CyberSecEval2基准上，ReasAlign达到94.6%的实用性和仅3.6%的攻击成功率，远超Meta SecAlign的56.4%实用性和74.4%攻击成功率。

Conclusion: ReasAlign在安全性和实用性之间实现了最佳平衡，为现实世界智能体系统建立了强大实用的提示注入攻击防御方案。

Abstract: Large Language Models (LLMs) have enabled the development of powerful agentic systems capable of automating complex workflows across various fields. However, these systems are highly vulnerable to indirect prompt injection attacks, where malicious instructions embedded in external data can hijack agent behavior. In this work, we present ReasAlign, a model-level solution to improve safety alignment against indirect prompt injection attacks. The core idea of ReasAlign is to incorporate structured reasoning steps to analyze user queries, detect conflicting instructions, and preserve the continuity of the user's intended tasks to defend against indirect injection attacks. To further ensure reasoning logic and accuracy, we introduce a test-time scaling mechanism with a preference-optimized judge model that scores reasoning steps and selects the best trajectory. Comprehensive evaluations across various benchmarks show that ReasAlign maintains utility comparable to an undefended model while consistently outperforming Meta SecAlign, the strongest prior guardrail. On the representative open-ended CyberSecEval2 benchmark, which includes multiple prompt-injected tasks, ReasAlign achieves 94.6% utility and only 3.6% ASR, far surpassing the state-of-the-art defensive model of Meta SecAlign (56.4% utility and 74.4% ASR). These results demonstrate that ReasAlign achieves the best trade-off between security and utility, establishing a robust and practical defense against prompt injection attacks in real-world agentic systems. Our code and experimental results could be found at https://github.com/leolee99/ReasAlign.

</details>


### [12] [PADER: Paillier-based Secure Decentralized Social Recommendation](https://arxiv.org/abs/2601.10212)
*Chaochao Chen,Jiaming Qian,Fei Zheng,Yachuan Liu*

Main category: cs.CR

TL;DR: PADER：基于Paillier密码系统的去中心化安全社交推荐系统，保护用户和商家隐私，通过安全多方计算实现推荐模型的训练和推理。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统存在隐私问题，集中式平台收集大量用户和商家数据。为了保护数据隐私，需要设计一个去中心化的安全推荐系统。

Method: 将Paillier密码系统应用于SoReg（社交正则化）模型，将其视为两方安全多项式评估问题。设计了安全加法和乘法协议以支持任意算术电路的安全计算，并提出了适合实数多项式计算的最优数据打包方案。

Result: 实验结果显示，该方法处理一个拥有数百条评分的用户仅需约1秒，使用约50万条评分训练一个epoch只需<3小时，证明该方法在实际应用中具有可行性。

Conclusion: PADER是一个实用的去中心化安全社交推荐系统，有效保护用户和商家隐私，同时保持较高的计算效率，代码已开源。

Abstract: The prevalence of recommendation systems also brings privacy concerns to both the users and the sellers, as centralized platforms collect as much data as possible from them. To keep the data private, we propose PADER: a Paillier-based secure decentralized social recommendation system. In this system, the users and the sellers are nodes in a decentralized network. The training and inference of the recommendation model are carried out securely in a decentralized manner, without the involvement of a centralized platform. To this end, we apply the Paillier cryptosystem to the SoReg (Social Regularization) model, which exploits both user's ratings and social relations. We view the SoReg model as a two-party secure polynomial evaluation problem and observe that the simple bipartite computation may result in poor efficiency. To improve efficiency, we design secure addition and multiplication protocols to support secure computation on any arithmetic circuit, along with an optimal data packing scheme that is suitable for the polynomial computations of real values. Experiment results show that our method only takes about one second to iterate through one user with hundreds of ratings, and training with ~500K ratings for one epoch only takes <3 hours, which shows that the method is practical in real applications. The code is available at https://github.com/GarminQ/PADER.

</details>


### [13] [XuanJia: A Comprehensive Virtualization-Based Code Obfuscator for Binary Protection](https://arxiv.org/abs/2601.10261)
*Xianyu Zou,Xiaoli Gong,Jin Zhang,Shiyang Li,Pen-Chung Yew*

Main category: cs.CR

TL;DR: XuanJia是一个虚拟化二进制混淆框架，通过ABI兼容的异常处理影子机制，在保护代码的同时也保护异常处理语义，消除静态EH元数据泄露，同时保持与操作系统运行时的兼容性。


<details>
  <summary>Details</summary>
Motivation: 现有基于虚拟化的二进制混淆方法为了保持ABI兼容性，会暴露异常处理元数据，这些元数据泄露了栈布局、控制流边界和对象生命周期等丰富的结构信息，可被利用进行逆向工程。

Method: 提出ABI兼容的异常处理影子机制：用ABI兼容的影子展开信息替换原生EH元数据以满足操作系统驱动的展开需求，然后将异常处理安全重定向到受保护的虚拟机中，在那里对真正的EH语义进行解密、反转并使用混淆代码重放。

Result: XuanJia实现了对385个x86指令编码和155个VM处理器模板的支持，在正确性、抗逆向性和性能方面表现良好：保持语义等价性，有效破坏IDA Pro等自动化逆向工具，空间开销可忽略，运行时开销适中。

Conclusion: XuanJia在保持正确性和实用性的同时，实现了对异常处理逻辑的强保护，证明可以消除EH元数据泄露而不牺牲ABI兼容性。

Abstract: Virtualization-based binary obfuscation is widely adopted to protect software intellectual property, yet existing approaches leave exception-handling (EH) metadata unprotected to preserve ABI compatibility. This exposed metadata leaks rich structural information, such as stack layouts, control-flow boundaries, and object lifetimes, which can be exploited to facilitate reverse engineering. In this paper, we present XuanJia, a comprehensive VM-based binary obfuscation framework that provides end-to-end protection for both executable code and exception-handling semantics. At the core of XuanJia is ABI-Compliant EH Shadowing, a novel exception-aware protection mechanism that preserves compatibility with unmodified operating system runtimes while eliminating static EH metadata leakage. XuanJia replaces native EH metadata with ABI-compliant shadow unwind information to satisfy OS-driven unwinding, and securely redirects exception handling into a protected virtual machine where the genuine EH semantics are decrypted, reversed, and replayed using obfuscated code. We implement XuanJia from scratch, supporting 385 x86 instruction encodings and 155 VM handler templates, and design it as an extensible research testbed. We evaluate XuanJia across correctness, resilience, and performance dimensions. Our results show that XuanJia preserves semantic equivalence under extensive dynamic and symbolic testing, effectively disrupts automated reverse-engineering tools such as IDA Pro, and incurs negligible space overhead and modest runtime overhead. These results demonstrate that XuanJia achieves strong protection of exception-handling logic without sacrificing correctness or practicality.

</details>


### [14] [Reasoning Hijacking: Subverting LLM Classification via Decision-Criteria Injection](https://arxiv.org/abs/2601.10294)
*Yuansen Liu,Yixuan Tang,Anthony Kum Hoe Tun*

Main category: cs.CR

TL;DR: 论文提出"推理劫持"攻击范式，通过注入虚假决策标准来操纵模型判断，而不改变高层任务目标，暴露了当前LLM安全研究的盲点。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全研究主要关注目标劫持攻击，但忽视了推理对齐的脆弱性。论文旨在揭示这种不完整的安全视角，展示模型即使保持高层目标一致，其推理过程仍可被操纵的风险。

Method: 提出"推理劫持"攻击范式，具体实现为"标准攻击"。该方法接受高层任务目标，但通过注入虚假的推理捷径来操纵模型的决策逻辑，而不是像目标劫持那样覆盖系统提示。

Result: 在三个不同任务（有害评论、负面评论和垃圾邮件检测）上的实验表明，即使最新模型也倾向于优先使用注入的启发式捷径而非严谨的语义分析。结果在不同骨干模型上保持一致，且能绕过检测目标偏差的防御机制。

Conclusion: 推理劫持攻击暴露了当前LLM安全研究的根本盲点，因为模型"意图"保持与用户指令一致，但推理过程被操纵。这需要开发新的防御机制来保护模型的推理对齐。

Abstract: Current LLM safety research predominantly focuses on mitigating Goal Hijacking, preventing attackers from redirecting a model's high-level objective (e.g., from "summarizing emails" to "phishing users"). In this paper, we argue that this perspective is incomplete and highlight a critical vulnerability in Reasoning Alignment. We propose a new adversarial paradigm: Reasoning Hijacking and instantiate it with Criteria Attack, which subverts model judgments by injecting spurious decision criteria without altering the high-level task goal. Unlike Goal Hijacking, which attempts to override the system prompt, Reasoning Hijacking accepts the high-level goal but manipulates the model's decision-making logic by injecting spurious reasoning shortcut. Though extensive experiments on three different tasks (toxic comment, negative review, and spam detection), we demonstrate that even newest models are prone to prioritize injected heuristic shortcuts over rigorous semantic analysis. The results are consistent over different backbones. Crucially, because the model's "intent" remains aligned with the user's instructions, these attacks can bypass defenses designed to detect goal deviation (e.g., SecAlign, StruQ), exposing a fundamental blind spot in the current safety landscape. Data and code are available at https://github.com/Yuan-Hou/criteria_attack

</details>


### [15] [Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale](https://arxiv.org/abs/2601.10338)
*Yi Liu,Weizhe Wang,Ruitao Feng,Yao Zhang,Guangquan Xu,Gelei Deng,Yuekang Li,Leo Zhang*

Main category: cs.CR

TL;DR: 首次大规模分析AI技能市场安全风险：31,132个技能中26.1%存在漏洞，数据窃取和权限提升最普遍，5.2%有恶意意图，含可执行脚本的技能风险高2.12倍


<details>
  <summary>Details</summary>
Motivation: AI技能框架允许动态扩展代理能力，但技能在隐式信任和最小审查下执行，形成了未充分表征的重大攻击面，需要系统性安全分析

Method: 从两大市场收集42,447个技能，使用SkillScan多阶段检测框架分析31,132个技能，结合静态分析和基于LLM的语义分类

Result: 发现普遍安全风险：26.1%技能至少含一个漏洞，涵盖14种模式四类风险（提示注入、数据窃取、权限提升、供应链风险），数据窃取（13.3%）和权限提升（11.8%）最普遍，5.2%技能有高严重性恶意模式，含可执行脚本的技能漏洞风险高2.12倍

Conclusion: AI技能生态系统存在严重安全漏洞，迫切需要基于能力的权限系统和强制安全审查，以防止攻击向量被进一步利用

Abstract: The rise of AI agent frameworks has introduced agent skills, modular packages containing instructions and executable code that dynamically extend agent capabilities. While this architecture enables powerful customization, skills execute with implicit trust and minimal vetting, creating a significant yet uncharacterized attack surface. We conduct the first large-scale empirical security analysis of this emerging ecosystem, collecting 42,447 skills from two major marketplaces and systematically analyzing 31,132 using SkillScan, a multi-stage detection framework integrating static analysis with LLM-based semantic classification. Our findings reveal pervasive security risks: 26.1% of skills contain at least one vulnerability, spanning 14 distinct patterns across four categories: prompt injection, data exfiltration, privilege escalation, and supply chain risks. Data exfiltration (13.3%) and privilege escalation (11.8%) are most prevalent, while 5.2% of skills exhibit high-severity patterns strongly suggesting malicious intent. We find that skills bundling executable scripts are 2.12x more likely to contain vulnerabilities than instruction-only skills (OR=2.12, p<0.001). Our contributions include: (1) a grounded vulnerability taxonomy derived from 8,126 vulnerable skills, (2) a validated detection methodology achieving 86.7% precision and 82.5% recall, and (3) an open dataset and detection toolkit to support future research. These results demonstrate an urgent need for capability-based permission systems and mandatory security vetting before this attack vector is further exploited.

</details>


### [16] [AgentGuardian: Learning Access Control Policies to Govern AI Agent Behavior](https://arxiv.org/abs/2601.10440)
*Nadya Abaev,Denis Klimov,Gerard Levinov,David Mimran,Yuval Elovici,Asaf Shabtai*

Main category: cs.CR

TL;DR: AgentGuardian是一个保护AI代理操作的安全框架，通过上下文感知的访问控制策略来监管代理行为，防止恶意输入和幻觉驱动错误。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在多个领域的广泛应用，确保它们只执行授权操作并正确处理输入对于维护系统完整性和防止滥用至关重要。

Method: 提出AgentGuardian框架，包含受控阶段监控执行轨迹学习合法行为模式，基于实时输入上下文和控制流依赖关系推导自适应策略来监管代理的工具调用。

Result: 在两个真实世界AI代理应用中的评估表明，AgentGuardian能有效检测恶意或误导性输入，同时保持正常代理功能，其基于控制流的治理机制能缓解幻觉驱动错误和其他编排级故障。

Conclusion: AgentGuardian为AI代理操作提供了一个有效的安全治理框架，通过上下文感知的访问控制策略保护代理免受恶意输入和系统级故障的影响。

Abstract: Artificial intelligence (AI) agents are increasingly used in a variety of domains to automate tasks, interact with users, and make decisions based on data inputs. Ensuring that AI agents perform only authorized actions and handle inputs appropriately is essential for maintaining system integrity and preventing misuse. In this study, we introduce the AgentGuardian, a novel security framework that governs and protects AI agent operations by enforcing context-aware access-control policies. During a controlled staging phase, the framework monitors execution traces to learn legitimate agent behaviors and input patterns. From this phase, it derives adaptive policies that regulate tool calls made by the agent, guided by both real-time input context and the control flow dependencies of multi-step agent actions. Evaluation across two real-world AI agent applications demonstrates that AgentGuardian effectively detects malicious or misleading inputs while preserving normal agent functionality. Moreover, its control-flow-based governance mechanism mitigates hallucination-driven errors and other orchestration-level malfunctions.

</details>


### [17] [Hybrid Encryption with Certified Deletion in Preprocessing Model](https://arxiv.org/abs/2601.10542)
*Kunal Dey,Reihaneh Safavi-Naini*

Main category: cs.CR

TL;DR: 该论文提出了两种在预处理模型中的混合加密与认证删除方案，分别提供信息论认证删除和永久认证删除，支持任意长度消息加密，具有IND-qe-CPA安全性。


<details>
  <summary>Details</summary>
Motivation: 现有认证删除方案要么依赖一次性密码本加密，要么基于可能被未来计算突破的计算假设。需要既能提供强安全保证又能支持长消息的认证删除方案。

Method: 提出预处理模型中的混合加密认证删除框架，结合信息论密钥封装机制和提供认证删除的数据封装机制。第一种方案提供信息论认证删除，第二种提供永久认证删除。

Result: 成功构建了两种pHE-CD方案：1) 信息论认证删除方案，对抗计算无界敌手；2) 永久认证删除方案，删除前为计算安全，删除后为信息论安全。两种方案都支持任意长度消息加密。

Conclusion: 提出的混合加密认证删除框架为量子安全认证删除提供了新途径，使用量子安全DEM-CD可获得量子安全性且密钥长度远小于消息长度，未来可结合量子KEM进一步优化。

Abstract: Certified deletion allows Alice to outsource data to Bob and, at a later time, obtain a verifiable guarantee that the file has been irreversibly deleted at her request. The functionality, while impossible using classical information alone, can be achieved using quantum information. Existing approaches, rely on one-time pad (OTP) encryption, or use computational hardness assumptions that may be vulnerable to future advances in classical or quantum computing. In this work, we introduce and formalize hybrid encryption with certified deletion in the preprocessing model (pHE-CD) and propose two constructions. The constructions combine an information-theoretic key encapsulation mechanism (iKEM) with a data encapsulation mechanism that provides certified deletion (DEM-CD) and, respectively, provide {\em information-theoretic certified deletion}, where both confidentiality and deletion properties are provided against a computationally unbounded adversary; and {\em everlasting certified deletion}, where confidentiality is computational before deletion, and upon successful verification of the deletion certificate, the message becomes information-theoretically hidden from an adversary that is computationally unbounded. Our pHE-CD schemes provide IND-$q_e$-CPA notion of security and support encryption of arbitrarily long messages. In the second construction, using a computationally secure DEM-CD that is quantum-safe (i.e. constructed using quantum coding and AES), we obtain quantum-safe security with keys that are significantly shorter than the message. Instantiating the proposed framework using quantum enabled kem (qKEM) as the iKEM, is a future work.

</details>


### [18] [Be Your Own Red Teamer: Safety Alignment via Self-Play and Reflective Experience Replay](https://arxiv.org/abs/2601.10589)
*Hao Wang,Yanting Wang,Hao Li,Rui Li,Lei Sha*

Main category: cs.CR

TL;DR: SSP让LLM在RL循环中同时扮演攻击者和防御者，通过自我对抗训练实现自主进化的安全对齐，显著优于基于静态对抗数据集的基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的安全对齐方法主要依赖静态的外部红队测试，使用固定的防御提示或预收集的对抗数据集，导致防御僵化、过拟合已知模式，无法泛化到新颖复杂的威胁。

Method: 提出Safety Self-Play (SSP)系统：让单个LLM在统一的强化学习循环中同时扮演攻击者（生成越狱攻击）和防御者（拒绝有害请求）。引入反射经验回放机制，使用UCB采样策略专注于低奖励的失败案例，帮助模型从过去的困难错误中学习。

Result: 大量实验表明，SSP方法能够自主进化出强大的防御能力，显著优于基于静态对抗数据集训练的基线方法，为主动安全对齐建立了新的基准。

Conclusion: 通过让模型成为自己的红队测试者，SSP实现了自主进化的安全对齐，解决了静态防御方法泛化能力不足的问题，为LLM安全提供了更主动、自适应的解决方案。

Abstract: Large Language Models (LLMs) have achieved remarkable capabilities but remain vulnerable to adversarial ``jailbreak'' attacks designed to bypass safety guardrails. Current safety alignment methods depend heavily on static external red teaming, utilizing fixed defense prompts or pre-collected adversarial datasets. This leads to a rigid defense that overfits known patterns and fails to generalize to novel, sophisticated threats. To address this critical limitation, we propose empowering the model to be its own red teamer, capable of achieving autonomous and evolving adversarial attacks. Specifically, we introduce Safety Self- Play (SSP), a system that utilizes a single LLM to act concurrently as both the Attacker (generating jailbreaks) and the Defender (refusing harmful requests) within a unified Reinforcement Learning (RL) loop, dynamically evolving attack strategies to uncover vulnerabilities while simultaneously strengthening defense mechanisms. To ensure the Defender effectively addresses critical safety issues during the self-play, we introduce an advanced Reflective Experience Replay Mechanism, which uses an experience pool accumulated throughout the process. The mechanism employs a Upper Confidence Bound (UCB) sampling strategy to focus on failure cases with low rewards, helping the model learn from past hard mistakes while balancing exploration and exploitation. Extensive experiments demonstrate that our SSP approach autonomously evolves robust defense capabilities, significantly outperforming baselines trained on static adversarial datasets and establishing a new benchmark for proactive safety alignment.

</details>
