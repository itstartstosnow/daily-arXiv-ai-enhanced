<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 45]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [HMARK: Radioactive Multi-Bit Semantic-Latent Watermarking for Diffusion Models](https://arxiv.org/abs/2512.00094)
*Kexin Li,Guozhen Ding,Ilya Grishchenko,David Lie*

Main category: cs.CR

TL;DR: HMARK是一种用于扩散模型的多比特水印方案，通过在语义潜在空间嵌入所有权信息，实现放射性、鲁棒性和不可感知性，有效检测未经授权的训练数据使用。


<details>
  <summary>Details</summary>
Motivation: 现代生成扩散模型依赖大量训练数据，其中可能包含所有权或使用权不明确的图像。放射性水印可以帮助检测未经授权的训练数据使用，但现有水印方案还需要满足不可感知性、鲁棒性和多比特容量等要求。

Method: 提出HMARK多比特水印方案，在图像扩散模型的语义潜在空间（h-space）中编码所有权信息作为秘密比特。利用h-space的可解释性和语义重要性，确保水印信号对应有意义的语义属性。

Result: 实验结果显示，HMARK在多种失真条件下，对使用LoRA在带水印数据上微调的下游对抗模型生成的图像，实现了98.57%的水印检测准确率、95.07%的比特级恢复准确率、100%的召回率和1.0的AUC。

Conclusion: HMARK通过在语义潜在空间嵌入水印，成功实现了放射性、鲁棒性和不可感知性的平衡，为保护图像免受未经授权的训练使用提供了有效的解决方案。

Abstract: Modern generative diffusion models rely on vast training datasets, often including images with uncertain ownership or usage rights. Radioactive watermarks -- marks that transfer to a model's outputs -- can help detect when such unauthorized data has been used for training. Moreover, aside from being radioactive, an effective watermark for protecting images from unauthorized training also needs to meet other existing requirements, such as imperceptibility, robustness, and multi-bit capacity. To overcome these challenges, we propose HMARK, a novel multi-bit watermarking scheme, which encodes ownership information as secret bits in the semantic-latent space (h-space) for image diffusion models. By leveraging the interpretability and semantic significance of h-space, ensuring that watermark signals correspond to meaningful semantic attributes, the watermarks embedded by HMARK exhibit radioactivity, robustness to distortions, and minimal impact on perceptual quality. Experimental results demonstrate that HMARK achieves 98.57% watermark detection accuracy, 95.07% bit-level recovery accuracy, 100% recall rate, and 1.0 AUC on images produced by the downstream adversarial model finetuned with LoRA on watermarked data across various types of distortions.

</details>


### [2] [Guarding Against Malicious Biased Threats (GAMBiT): Experimental Design of Cognitive Sensors and Triggers with Behavioral Impact Analysis](https://arxiv.org/abs/2512.00098)
*Brandon Beltz,Po-Yu Chen,James Doty,Yvonne Fonken,Nikolos Gurney,Hsiang-Wen Hsing,Sofia Hirschmann,Brett Israelsen,Nathan Lau,Mengyun Li,Stacy Marsella,Michael Murray,Jinwoo Oh,Amy Sliva,Kunal Srivastava,Stoney Trent,Peggy Wu,Ya-Ting Yang,Quanyan Zhu*

Main category: cs.CR

TL;DR: GAMBiT是一个认知驱动的网络安全防御框架，利用人类非理性偏差作为新的防御面，通过触发攻击者的认知偏见来降低其攻击效率。


<details>
  <summary>Details</summary>
Motivation: 传统网络安全防御假设攻击者是理性的、追求效用最大化的，但现实中的攻击者存在认知约束和偏见，这些认知特征可以成为新的防御机会。

Method: GAMBiT通过认知触发器激活攻击者的损失厌恶、基础率忽视、沉没成本谬误等偏见，并开发认知传感器从行为和网络数据推断攻击者的认知状态。

Result: 在模拟小型企业网络中进行的三轮人类被试实验（共61人）显示，这些认知操纵显著干扰了攻击者表现，降低了任务进展，使攻击偏离正确路径，并增加了可检测性。

Conclusion: GAMBiT建立了一个新范式，将攻击者的思维变成战场的一部分，使认知操纵成为网络防御的主动向量，证明可以系统性地触发认知偏见来降低攻击者效率并增强防御者优势。

Abstract: This paper introduces GAMBiT (Guarding Against Malicious Biased Threats), a cognitive-informed cyber defense framework that leverages deviations from human rationality as a new defensive surface. Conventional cyber defenses assume rational, utility-maximizing attackers, yet real-world adversaries exhibit cognitive constraints and biases that shape their interactions with complex digital systems. GAMBiT embeds insights from cognitive science into cyber environments through cognitive triggers, which activate biases such as loss aversion, base-rate neglect, and sunk-cost fallacy, and through newly developed cognitive sensors that infer attackers' cognitive states from behavioral and network data. Three rounds of human-subject experiments (total n=61) in a simulated small business network demonstrate that these manipulations significantly disrupt attacker performance, reducing mission progress, diverting actions off the true attack path, and increasing detectability. These results demonstrate that cognitive biases can be systematically triggered to degrade the attacker's efficiency and enhance the defender's advantage. GAMBiT establishes a new paradigm in which the attacker's mind becomes part of the battlefield and cognitive manipulation becomes a proactive vector for cyber defense.

</details>


### [3] [Quantum-Adversary-Resilient Evidence Structures and Migration Strategies for Regulated AI Audit Trails](https://arxiv.org/abs/2512.00110)
*Leo Kao*

Main category: cs.CR

TL;DR: 该论文研究了面向量子计算威胁的密码学证据记录系统，提出了量子安全的形式化定义、构造方案和迁移策略。


<details>
  <summary>Details</summary>
Motivation: 在临床、制药和金融等受监管的AI工作负载中，广泛使用恒定大小的密码学证据记录来构建审计追踪。然而，现有实现通常依赖经典签名方案，其长期安全性受到量子计算能力的威胁。

Method: 1) 形式化量子敌手下的证据结构安全概念（Q-Audit Integrity, Q-Non-Equivocation, Q-Binding）；2) 在量子随机预言机模型（QROM）中分析哈希-签名实例化；3) 提出三种迁移模式：混合签名、旧证据重签名、Merkle根锚定。

Result: 在标准假设下，基于抗量子签名方案的哈希-签名实例化满足量子安全要求。迁移策略分析表明，量子安全审计追踪可实现适度开销，系统迁移能显著延长现有部署的证据生命周期。

Conclusion: 量子安全审计追踪是可实现的，通过形式化安全定义、抗量子构造方案和系统迁移策略，能够保护受监管AI工作负载的长期证据完整性。

Abstract: Constant-size cryptographic evidence records are increasingly used to build audit trails for regulated AI workloads in clinical, pharmaceutical, and financial settings, where each execution is summarized by a compact, verifiable record of code identity, model version, data digests, and platform measurements. Existing instantiations, however, typically rely on classical signature schemes whose long-term security is threatened by quantum-capable adversaries. In this paper we formalize security notions for evidence structures in the presence of quantum adversaries and study post-quantum (PQ) instantiations and migration strategies for deployed audit logs. We recall an abstraction of constant-size evidence structures and introduce game-based definitions of Q-Audit Integrity, Q-Non-Equivocation, and Q-Binding, capturing the inability of a quantum adversary to forge, equivocate, or rebind evidence items. We then analyze a hash-and-sign instantiation in the quantum random-oracle model (QROM), assuming an existentially unforgeable PQ signature scheme against quantum adversaries, and show that the resulting evidence structure satisfies these notions under standard assumptions. Building on this, we present three migration patterns for existing evidence logs: hybrid signatures, re-signing of legacy evidence, and Merkle-root anchoring, and analyze their security, storage, and computational trade-offs. A case study based on an industrial constant-size evidence platform for regulated AI at Codebat Technologies Inc. suggests that quantum-safe audit trails are achievable with moderate overhead and that systematic migration can significantly extend the evidentiary lifetime of existing deployments.

</details>


### [4] [NetDeTox: Adversarial and Efficient Evasion of Hardware-Security GNNs via RL-LLM Orchestration](https://arxiv.org/abs/2512.00119)
*Zeng Wang,Minghao Shao,Akashdeep Saha,Ramesh Karri,Johann Knechtel,Muhammad Shafique,Ozgur Sinanoglu*

Main category: cs.CR

TL;DR: NetDeTox：结合LLM和RL的自动化框架，通过局部重写攻击GNN硬件安全方案，显著降低面积开销


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的硬件安全方案依赖结构模式识别，容易受到对抗性网表重写攻击。现有攻击方法设计开销大，需要更高效、低开销的对抗攻击框架。

Method: NetDeTox采用端到端框架，结合强化学习（RL）识别关键网表组件，大型语言模型（LLM）设计保持功能的重写方案，通过迭代反馈优化对抗重写。

Result: 相比SOTA方法AttackGNN，NetDeTox成功降低所有安全方案的有效性，重写次数更少，面积开销显著降低（GNN-RE降低54.50%，GNN4IP降低25.44%，OMLA降低41.04%）。对于GNN4IP，甚至能优化原始基准面积。

Conclusion: NetDeTox展示了结合LLM和RL进行对抗性网表重写的有效性，具有实用性和可扩展性，为硬件安全GNN模型的鲁棒性评估提供了新方法。

Abstract: Graph neural networks (GNNs) have shown promise in hardware security by learning structural motifs from netlist graphs. However, this reliance on motifs makes GNNs vulnerable to adversarial netlist rewrites; even small-scale edits can mislead GNN predictions. Existing adversarial approaches, ranging from synthesis-recipe perturbations to gate transformations, come with high design overheads. We present NetDeTox, an automated end-to-end framework that orchestrates large language models (LLMs) with reinforcement learning (RL) in a systematic manner, enabling focused local rewriting. The RL agent identifies netlist components critical for GNN-based reasoning, while the LLM devises rewriting plans to diversify motifs that preserve functionality. Iterative feedback between the RL and LLM stages refines adversarial rewritings to limit overheads. Compared to the SOTA work AttackGNN, NetDeTox successfully degrades the effectiveness of all security schemes with fewer rewrites and substantially lower area overheads (reductions of 54.50% for GNN-RE, 25.44% for GNN4IP, and 41.04% for OMLA, respectively). For GNN4IP, ours can even optimize/reduce the original benchmarks' area, in particular for larger circuits, demonstrating the practicality and scalability of NetDeTox.

</details>


### [5] [An Empirical Study on the Security Vulnerabilities of GPTs](https://arxiv.org/abs/2512.00136)
*Tong Wu,Weibin Wu,Zibin Zheng*

Main category: cs.CR

TL;DR: 该论文对GPTs（基于OpenAI大语言模型的定制AI代理）进行安全漏洞实证研究，通过平台-用户视角分析攻击面，设计系统化攻击套件展示信息泄露和工具滥用漏洞，并提出相应防御机制。


<details>
  <summary>Details</summary>
Motivation: GPTs作为基于OpenAI大语言模型的定制AI代理，在写作、研究和编程等领域展现出巨大潜力，目前数量已达300万，应用领域日益多样化。然而，由于这些LLM代理应用共享一致的框架，可能存在系统性安全漏洞且尚未得到充分探索。为了填补这一空白，需要对GPTs的安全漏洞进行实证研究。

Method: 首先基于先前LLM安全研究，采用平台-用户视角对不同系统组件进行全面的攻击面分析。然后根据攻击面分析，设计系统化、多维度的攻击套件，明确以信息泄露和工具滥用为目标，具体展示基于GPT系统各组件面临的安全漏洞。最后提出相应的防御机制。

Result: 研究展示了基于GPT系统各组件面临的具体安全漏洞，包括信息泄露和工具滥用等风险。通过实证分析揭示了这些系统性漏洞的存在和潜在危害。

Conclusion: 通过提高对这些漏洞的认识并提供关键见解，本研究旨在促进GPTs的安全和负责任应用，同时为开发保护用户和系统免受恶意攻击的鲁棒防御机制做出贡献。研究强调了在GPTs广泛应用背景下安全防护的重要性。

Abstract: Equipped with various tools and knowledge, GPTs, one kind of customized AI agents based on OpenAI's large language models, have illustrated great potential in many fields, such as writing, research, and programming. Today, the number of GPTs has reached three millions, with the range of specific expert domains becoming increasingly diverse. However, given the consistent framework shared among these LLM agent applications, systemic security vulnerabilities may exist and remain underexplored. To fill this gap, we present an empirical study on the security vulnerabilities of GPTs. Building upon prior research on LLM security, we first adopt a platform-user perspective to conduct a comprehensive attack surface analysis across different system components. Then, we design a systematic and multidimensional attack suite with the explicit objectives of information leakage and tool misuse based on the attack surface analysis, thereby concretely demonstrating the security vulnerabilities that various components of GPT-based systems face. Finally, we accordingly propose defense mechanisms to address the aforementioned security vulnerabilities. By increasing the awareness of these vulnerabilities and offering critical insights into their implications, this study seeks to facilitate the secure and responsible application of GPTs while contributing to developing robust defense mechanisms that protect users and systems against malicious attacks.

</details>


### [6] [DeFi TrustBoost: Blockchain and AI for Trustworthy Decentralized Financial Decisions](https://arxiv.org/abs/2512.00142)
*Swati Sachan,Dale S. Fickett*

Main category: cs.CR

TL;DR: DeFi TrustBoost Framework结合区块链和可解释AI，解决低财富家庭小企业贷款审批中的信任问题，强调保密性、合规性、抗攻击性和可审计性。


<details>
  <summary>Details</summary>
Motivation: 解决金融机构在为低财富家庭小企业贷款审批时面临的信任挑战，特别是区块链和AI系统需要满足的四个关键需求：保密性、数据保护合规性、抗对抗攻击能力和监管审计合规性。

Method: 结合区块链技术和可解释AI，开发了防篡改的AI决策审计技术，采用链上（区块链内）和链下数据存储策略，促进金融机构内部和跨机构的协作。

Result: 提出了DeFi TrustBoost框架，能够实现自动化AI决策的防篡改审计，并通过混合存储策略支持金融组织间的协作，满足监管要求。

Conclusion: 该框架为DeFi领域的小企业贷款审批提供了可信的技术解决方案，通过区块链和可解释AI的结合，解决了传统金融系统中的信任和合规问题。

Abstract: This research introduces the Decentralized Finance (DeFi) TrustBoost Framework, which combines blockchain technology and Explainable AI to address challenges faced by lenders underwriting small business loan applications from low-wealth households. The framework is designed with a strong emphasis on fulfilling four crucial requirements of blockchain and AI systems: confidentiality, compliance with data protection laws, resistance to adversarial attacks, and compliance with regulatory audits. It presents a technique for tamper-proof auditing of automated AI decisions and a strategy for on-chain (inside-blockchain) and off-chain data storage to facilitate collaboration within and across financial organizations.

</details>


### [7] [Measuring Memecoin Fragility](https://arxiv.org/abs/2512.00377)
*Yuexin Xiang,SM Mahir Shazeed Rish,Qishuang Fu,Yuquan Li,Qin Wang,Tsz Hon Yuen,Jiangshan Yu*

Main category: cs.CR

TL;DR: 本文提出了首个Memecoin生态系统脆弱性框架（ME2F），用于量化模因币在波动性、所有权集中度和情绪放大三个维度的风险，发现政治主题模因币风险最高，而ETH和SOL等基准代币最为稳健。


<details>
  <summary>Details</summary>
Motivation: 模因币作为一种源于互联网文化和社区叙事的加密资产，其市场动态主要由社交媒体传播、名人影响和投机资本驱动，与传统技术驱动的加密货币不同。目前缺乏系统性的框架来捕捉这些生态系统的独特脆弱性。

Method: 提出了Memecoin生态系统脆弱性框架（ME2F），该框架从三个维度形式化模因币风险：1）波动动态评分，捕捉持续和极端的价格波动以及基础链的溢出效应；2）鲸鱼主导评分，量化顶级持有者的所有权集中度；3）情绪放大评分，衡量注意力驱动冲击对市场稳定性的影响。

Result: 应用ME2F分析代表性代币（覆盖超过65%市场份额）发现：1）政治主题代币（TRUMP、MELANIA、LIBRA）风险最高，结合了波动性、所有权集中度和情绪敏感性；2）成熟模因币（DOGE、SHIB、PEPE）处于中等风险范围；3）基准代币（ETH、SOL）由于更深流动性和机构参与而保持稳健。

Conclusion: 研究首次提供了生态系统层面的模因币脆弱性证据，强调了在Web3时代增强市场韧性的治理意义，为监管和风险管理提供了重要参考。

Abstract: Memecoins, emerging from internet culture and community-driven narratives, have rapidly evolved into a unique class of crypto assets. Unlike technology-driven cryptocurrencies, their market dynamics are primarily shaped by viral social media diffusion, celebrity influence, and speculative capital inflows.
  To capture the distinctive vulnerabilities of these ecosystems, we present the first Memecoin Ecosystem Fragility Framework (ME2F). ME2F formalizes memecoin risks in three dimensions: i) Volatility Dynamics Score capturing persistent and extreme price swings together with spillover from base chains; ii) Whale Dominance Score quantifying ownership concentration among top holders; and iii) Sentiment Amplification Score measuring the impact of attention-driven shocks on market stability.
  We apply ME2F to representative tokens (over 65\% market share) and show that fragility is not evenly distributed across the ecosystem. Politically themed tokens such as TRUMP, MELANIA, and LIBRA concentrate the highest risks, combining volatility, ownership concentration, and sensitivity to sentiment shocks. Established memecoins such as DOGE, SHIB, and PEPE fall into an intermediate range. Benchmark tokens ETH and SOL remain consistently resilient due to deeper liquidity and institutional participation. Our findings provide the first ecosystem-level evidence of memecoin fragility and highlight governance implications for enhancing market resilience in the Web3 era.

</details>


### [8] [Red Teaming Large Reasoning Models](https://arxiv.org/abs/2512.00412)
*Jiawei Chen,Yang Yang,Chao Yu,Yu Tian,Zhi Cao,Linghao Li,Hang Su,Zhaoxia Yin*

Main category: cs.CR

TL;DR: RT-LRM是一个评估大型推理模型可信度的统一基准，涵盖真实性、安全性和效率三个维度，通过30个推理任务评估26个模型，发现LRM存在新的安全风险且比LLM更脆弱。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在多步推理任务中表现出色，但引入了新的安全和可靠性风险（如CoT劫持和提示诱导低效），现有评估方法未能充分捕捉这些风险，需要专门的评估基准。

Method: 提出RT-LRM基准，从三个核心维度（真实性、安全性、效率）评估LRM可信度；引入训练范式作为分析视角，设计30个推理任务套件；对26个模型进行广泛实验。

Result: LRM普遍面临可信度挑战，在遇到推理诱导风险时比LLM更脆弱；发现了先前未被充分探索的漏洞；开发了可扩展的工具箱支持标准化可信度研究。

Conclusion: LRM存在新的安全风险需要针对性评估，RT-LRM基准和工具箱为未来可信度研究提供了标准化框架，代码和数据集将开源。

Abstract: Large Reasoning Models (LRMs) have emerged as a powerful advancement in multi-step reasoning tasks, offering enhanced transparency and logical consistency through explicit chains of thought (CoT). However, these models introduce novel safety and reliability risks, such as CoT-hijacking and prompt-induced inefficiencies, which are not fully captured by existing evaluation methods. To address this gap, we propose RT-LRM, a unified benchmark designed to assess the trustworthiness of LRMs. RT-LRM evaluates three core dimensions: truthfulness, safety and efficiency. Beyond metric-based evaluation, we further introduce the training paradigm as a key analytical perspective to investigate the systematic impact of different training strategies on model trustworthiness. We achieve this by designing a curated suite of 30 reasoning tasks from an observational standpoint. We conduct extensive experiments on 26 models and identify several valuable insights into the trustworthiness of LRMs. For example, LRMs generally face trustworthiness challenges and tend to be more fragile than Large Language Models (LLMs) when encountering reasoning-induced risks. These findings uncover previously underexplored vulnerabilities and highlight the need for more targeted evaluations. In addition, we release a scalable toolbox for standardized trustworthiness research to support future advancements in this important field. Our code and datasets will be open-sourced.

</details>


### [9] [BEACON: Automatic Container Policy Generation using Environment-aware Dynamic Analysis](https://arxiv.org/abs/2512.00414)
*Haney Kang,Eduard Marin,Myoungsung You,Diego Perino,Seungwon Shin,Jinwoo Kim*

Main category: cs.CR

TL;DR: BeaCon是一个自动化生成可调节容器安全策略的工具，通过动态分析模拟真实环境，发现隐藏的执行路径，并使用安全与功能评分机制平衡安全性和应用功能。


<details>
  <summary>Details</summary>
Motivation: 现有容器安全策略生成方法无法发现容器在分析阶段隐藏的执行路径，导致安全策略不完整。需要一种能自动生成可调节安全策略的工具，既能保障主机内核安全，又能维持容器应用功能。

Method: 使用动态分析模拟真实环境，采用高效启发式方法探索大量分析空间，设计安全与功能评分机制对系统调用和capability进行优先级排序，基于eBPF内核技术实现原型系统。

Result: 在top 15容器中，BeaCon通过多样化环境平均发现16.5%额外系统调用；成功缓解45个已知漏洞（CVEs）相关风险；在两个知名安全漏洞的概念验证中成功减少攻击面。

Conclusion: BeaCon能有效生成平衡安全与功能的容器安全策略，显著增强容器安全性，为云服务提供商提供可执行的安全措施同时保持租户应用可用性。

Abstract: This paper introduces BeaCon, a novel tool for the automated generation of adjustable container security policies. Unlike prior approaches, BeaCon leverages dynamic analysis to simulate realistic environments, uncovering container execution paths that may remain hidden during the profiling phase. To address the challenge of exploring vast profiling spaces, we employ efficient heuristics to reveal additional system events with minimal effort. In addition, BeaCon incorporates a security and functionality scoring mechanism to prioritize system calls and capabilities based on their impact on the host OS kernel's security and the functionality of containerized applications. By integrating these scores, BeaCon achieves a customized balance between security and functionality, enabling cloud providers to enforce security measures while maintaining tenant availability. We implemented a prototype of BeaCon using eBPF kernel technology and conducted extensive evaluations. Results from the top 15 containers, which revealed significant improvements, demonstrate that BeaCon identifies an average of 16.5% additional syscalls by applying diverse environments. Furthermore, we evaluated its effectiveness in mitigating risks associated with 45 known vulnerabilities (e.g., CVEs), showcasing its potential to significantly enhance container security. Additionally, we performed proof-of-concept demonstrations for two well-known security vulnerabilities, showing that BeaCon successfully reduces attack surface by blocking these exploits.

</details>


### [10] [RECTor: Robust and Efficient Correlation Attack on Tor](https://arxiv.org/abs/2512.00436)
*Binghui Wu,Dinil Mon Divakaran,Levente Csikor,Mohan Gurusamy*

Main category: cs.CR

TL;DR: RECTor：基于机器学习的流量关联框架，使用注意力MIL和GRU编码提取鲁棒流量特征，通过孪生网络和近似最近邻搜索高效匹配，显著提升Tor流量关联攻击的准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: Tor网络虽然通过加密中继隐藏用户身份，但仍易受流量关联攻击。现有方法存在两大局限：1) 对噪声和部分观测的鲁棒性有限；2) 计算复杂度高，可扩展性差，需要进行昂贵的成对匹配。

Method: 提出RECTor框架：1) 使用基于注意力的多实例学习(MIL)和GRU时序编码提取鲁棒的流量表示，即使流量数据不完整或被混淆；2) 通过孪生网络将嵌入映射到共享空间；3) 采用近似最近邻搜索(aNN)进行高效匹配。

Result: RECTor在实证评估中优于DeepCorr、DeepCOFFEA和FlowTracker等最先进基线：在高噪声条件下真阳性率提升高达60%；训练和推理时间减少超过50%；推理成本随流量数量近线性增长，展示强大可扩展性。

Conclusion: RECTor揭示了Tor匿名模型的关键漏洞，表明需要开发先进的模型感知防御机制。该框架在现实条件下实现了高效、鲁棒的流量关联攻击，对匿名网络安全性构成严重威胁。

Abstract: Tor is a widely used anonymity network that conceals user identities by routing traffic through encrypted relays, yet it remains vulnerable to traffic correlation attacks that deanonymize users by matching patterns in ingress and egress traffic. However, existing correlation methods suffer from two major limitations: limited robustness to noise and partial observations, and poor scalability due to computationally expensive pairwise matching. To address these challenges, we propose RECTor, a machine learning-based framework for traffic correlation under realistic conditions. RECTor employs attention-based Multiple Instance Learning (MIL) and GRU-based temporal encoding to extract robust flow representations, even when traffic data is incomplete or obfuscated. These embeddings are mapped into a shared space via a Siamese network and efficiently matched using approximate nearest neighbor (aNN) search. Empirical evaluations show that RECTor outperforms state-of-the-art baselines such as DeepCorr, DeepCOFFEA, and FlowTracker, achieving up to 60% higher true positive rates under high-noise conditions and reducing training and inference time by over 50%. Moreover, RECTor demonstrates strong scalability: inference cost grows near-linearly as the number of flows increases. These findings reveal critical vulnerabilities in Tor's anonymity model and highlight the need for advanced model-aware defenses.

</details>


### [11] [A Unified Framework for Constructing Information-Theoretic Private Information Retrieval](https://arxiv.org/abs/2512.00480)
*Liang Feng Zhang*

Main category: cs.CR

TL;DR: 该论文提出了一种新的离散结构FOASC，并建立了一个统一框架来构建信息论私有信息检索协议，旨在降低通信复杂度。


<details>
  <summary>Details</summary>
Motivation: 私有信息检索协议在保护用户隐私方面很重要，但现有构造主要分为多服务器信息论PIR和单服务器计算PIR。信息论PIR在计算效率和无界服务器安全性方面更有优势，但降低通信复杂度是核心挑战。

Method: 引入新的离散结构——具有跨度能力的正交阵列族（FOASC），并提出一个统一框架来构建信息论PIR协议。该框架能够捕捉文献中最有影响力的IT-PIR协议。

Result: 建立了一个统一的IT-PIR协议构建框架，展示了现有重要IT-PIR协议都可以在该框架下表示。同时提出了关于FOASC的几个有趣开放问题。

Conclusion: FOASC为IT-PIR协议提供了一个新的理论框架，通过解决相关开放问题可能产生创新的IT-PIR协议，有助于降低通信复杂度这一核心挑战。

Abstract: Retrieving up-to-date information from a publicly accessible database poses significant threats to the user's privacy. {\em Private information retrieval} (PIR) protocols allow a user to retrieve any entry from a database, without revealing the identity of the entry being retrieved to the server(s). Such protocols have found numerous applications in both theoretical studies and real-life scenarios. The existing PIR constructions mainly give multi-server {\em information-theoretic} PIR (IT-PIR) protocols or single-server computational PIR (CPIR) protocols. Compared with CPIR, IT-PIR protocols are computationally more efficient and secure in the presence of unbounded servers. The most classical and challenging problem in the realm of IT-PIR is constructing protocols with lower {\em communication complexity}. In this review, we introduce a new discrete structure called {\em families of orthogonal arrays with span capability} (FOASC) and propose a unified framework for constructing IT-PIR protocols. We show how the most influential IT-PIR protocols in the literature can be captured by the framework. We also put forward several interesting open problems concerning FOASC, whose solutions may result in innovative IT-PIR protocols.

</details>


### [12] [TrojanLoC: LLM-based Framework for RTL Trojan Localization](https://arxiv.org/abs/2512.00591)
*Weihua Xiao,Zeng Wang,Minghao Shao,Raghu Vamshi Hemadri,Ozgur Sinanoglu,Muhammad Shafique,Johann Knechtel,Siddharth Garg,Ramesh Karri*

Main category: cs.CR

TL;DR: TrojanLoC：基于LLM的RTL级硬件木马定位框架，通过RTL微调的大语言模型获取模块级和行级嵌入，实现细粒度木马检测与定位，性能显著优于传统图神经网络方法。


<details>
  <summary>Details</summary>
Motivation: 现有硬件木马检测方法通常将设计转换为图结构（如门级网表或数据流图），然后使用图神经网络获取嵌入，这种方法存在三个主要问题：(1) 丢失紧凑的RTL语义信息，(2) 依赖感受野有限的浅层GNN，(3) 主要局限于粗粒度的模块级二进制检测。

Method: 提出TrojanLoC框架：1) 使用RTL微调的大语言模型直接从RTL代码中提取模块级和行级嵌入，捕捉全局设计上下文和局部语义；2) 在这些嵌入上训练任务特定分类器，执行模块级木马检测、类型预测和细粒度行级定位；3) 引入TrojanInS大型合成数据集，包含系统注入的四类效应型木马，并带有精确的行级标注。

Result: TrojanLoC在模块级达到0.99 F1分数的木马检测性能，比基线高0.68；木马类型分类达到0.84宏F1。在行级达到0.93宏F1，能够精确定位与木马相关的RTL代码行。

Conclusion: TrojanLoC通过直接利用RTL语义信息，克服了传统图转换方法的局限性，实现了更精确的硬件木马检测和定位，为RTL级硬件安全提供了有效的细粒度分析工具。

Abstract: Hardware Trojans (HT s) are a persistent threat to integrated circuits, especially when inserted at the register-transfer level (RTL). Existing methods typically first convert the design into a graph, such as a gate-level netlist or an RTL-derived dataflow graph (DFG), and then use a graph neural network (GNN ) to obtain an embedding of that graph, which (i) loses compact RTL semantics, (ii) relies on shallow GNNs with limited receptive field, and (iii) is largely restricted to coarse, module-level binary HT detection. We propose TrojanLoC, an LLM-based framework for RTL-level HT localization. We use an RTL-finetuned LLM to derive module-level and line-level embeddings directly from RTL code, capturing both global design context and local semantics. Next, we train task-specific classifiers on these embeddings to perform module-level Trojan detection, type prediction, and fine-grained line-level localization. We also introduce TrojanInS, a large synthetic dataset of RTL designs with systematically injected Trojans from four effect-based categories, each accompanied by precise line-level annotations. Our experiments show that TrojanLoC achieves strong module-level performance, reaching 0.99 F1-score for Trojan detection, up to 0.68 higher than baseline, and 0.84 macro-F1 for Trojan-type classification. At the line level, TrojanLoc further achieves up to 0.93 macro-F1, enabling fine-grained localization of Trojan-relevant RTL lines

</details>


### [13] [Extended Abstract: Synthesizable Low-overhead Circuit-level Countermeasures and Pro-Active Detection Techniques for Power and EM SCA](https://arxiv.org/abs/2512.00635)
*Archisman Ghosh*

Main category: cs.CR

TL;DR: 该论文提出针对物联网设备侧信道攻击的低开销电路级防护方案，包括零开销集成电感传感器检测电磁侧信道攻击和故障注入攻击，以及实现最低能耗面积的硅验证后量子密码算法Saber。


<details>
  <summary>Details</summary>
Motivation: 物联网设备虽然使用数学安全的加密算法，但硬件实现会泄露功耗、电磁辐射等侧信道信息，现有防护方案开销过高，不适用于能耗受限的物联网设备。同时量子计算机的发展威胁现有加密协议，需要后量子密码算法。

Method: 1. 提出零开销集成电感传感器，结合简单机器学习算法检测电磁侧信道攻击、时钟毛刺故障注入攻击和电压毛刺故障注入攻击。2. 实现硅验证的NIST后量子密码算法Saber（基于模学习取整方案），优化能耗和面积。

Result: 1. 集成电感传感器能有效检测多种攻击类型。2. 实现的Saber算法在所有候选方案中达到最低能耗和最小面积，为NIST标准化做出贡献。

Conclusion: 该论文为物联网设备提供了实用的低开销侧信道攻击防护方案，同时在后量子密码硬件实现方面取得重要进展，有助于构建更安全的物联网系统和应对量子计算威胁。

Abstract: The gamut of todays internet-connected embedded devices has led to increased concerns regarding the security and confidentiality of data. Most internet-connected embedded devices employ mathematically secure cryptographic algorithms to address security vulnerabilities. Despite such mathematical guarantees, as these algorithms are often implemented in silicon, they leak critical information in terms of power consumption, electromagnetic (EM) radiation, timing, cache hits and misses, photonic emission and so on, leading to side-channel analysis (SCA) attacks. This thesis focuses on low overhead generic circuit-level yet synthesizable countermeasures against power and EM SCA. Existing countermeasures (including proposed) still have relatively high overhead which bars them from being used in energy-constraint IoT devices. We propose a zero-overhead integrated inductive sensor which is able to detect i)EM SCA ii) Clock glitch-based Fault Injection Attack (FIA), and iii) Voltage-glitch based Fault Injection Attack by using a simple ML algorithm. Advent of quantum computer research will open new possibilities for theoretical attacks against existing cryptographic protocols. National Institute of Standard & Technology (NIST) has standardized post-quantum cryptographic algorithms to secure crypto-systems against quantum adversary. I contribute to the standardization procedure by introducing the first silicon-verified Saber (a NIST finalist modulo Learning with Rounding scheme) which consumes lowest energy and area till date amongst all the candidates.

</details>


### [14] [Blockchain-based vs. SQL Database Systems for Digital Twin Evidence Management: A Comparative Forensic Analysis](https://arxiv.org/abs/2512.00645)
*Boyd Franken,Hong-Hanh Nguyen-Le,Nhien-An Le-Khac*

Main category: cs.CR

TL;DR: 比较区块链与传统数据库在数字孪生证据管理中的表现，发现区块链在数据完整性方面更优，但传统数据库性能更稳定。


<details>
  <summary>Details</summary>
Motivation: 随着数字孪生和元宇宙技术的出现，数字取证面临前所未有的挑战，需要研究适合这些新兴技术的证据管理系统。

Method: 进行对照实验，比较以太坊区块链+IPFS存储与传统SQL数据库在数字孪生证据管理方面的性能。

Result: 区块链在数据完整性和不可篡改性方面更优（对取证应用至关重要），但传统数据库性能一致性更好；区块链存储时间更快但检索操作变异性更高；两者都能通过哈希验证保持取证完整性。

Conclusion: 区块链的不可变特性为法律程序提供了额外的安全保障，这项研究有助于为元宇宙时代的新兴技术开发稳健的数字取证方法。

Abstract: Digital forensics faces unprecedented challenges with the emergence of digital twins and metaverse technologies. This paper presents the first comparative analysis between blockchain-based and traditional database systems for managing digital twin evidence in forensic investigations. We conducted controlled experiments comparing the Ethereum blockchain with IPFS storage against traditional SQL databases for digital twin evidence management. Our findings reveal that while blockchain provides superior data integrity and immutability, crucial for forensic applications, traditional databases offer better performance consistency. The blockchain implementation showed faster average storage times but higher variability in retrieval operations. Both systems maintained forensic integrity through hash verification, though blockchain's immutable nature provides additional security guarantees essential for legal proceedings. This research contributes to the development of robust digital forensic methodologies for emerging technologies in the metaverse era.

</details>


### [15] [Concept-Guided Backdoor Attack on Vision Language Models](https://arxiv.org/abs/2512.00713)
*Haoyu Shen,Weimin Lyu,Haotian Xu,Tengfei Ma*

Main category: cs.CR

TL;DR: 本文提出概念引导的后门攻击新范式，通过语义概念而非像素级触发器攻击视觉语言模型，包括CTP和CGUB两种方法，在保持正常任务性能的同时实现高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型的后门攻击主要依赖像素级触发器或不可察觉的扰动，这些方法隐蔽性差且容易被基于图像的防御机制检测。需要开发更隐蔽、基于语义概念的攻击方法。

Method: 提出两种概念引导的后门攻击：1) CTP使用自然图像中的显式概念作为触发器，仅对包含目标概念的样本进行投毒；2) CGUB在训练时利用概念瓶颈模型干预内部概念激活，在推理时丢弃CBM分支以保持模型不变，实现系统性的文本标签替换。

Result: 在多个VLM架构和数据集上的实验表明，CTP和CGUB都能实现高攻击成功率，同时对干净任务性能影响适中，证明了概念级漏洞是VLM的新关键攻击面。

Conclusion: 概念引导的后门攻击为视觉语言模型安全研究开辟了新方向，揭示了语义概念层面的安全漏洞，需要开发新的防御机制来应对这类更隐蔽的攻击。

Abstract: Vision-Language Models (VLMs) have achieved impressive progress in multimodal text generation, yet their rapid adoption raises increasing concerns about security vulnerabilities. Existing backdoor attacks against VLMs primarily rely on explicit pixel-level triggers or imperceptible perturbations injected into images. While effective, these approaches reduce stealthiness and remain vulnerable to image-based defenses. We introduce concept-guided backdoor attacks, a new paradigm that operates at the semantic concept level rather than on raw pixels. We propose two different attacks. The first, Concept-Thresholding Poisoning (CTP), uses explicit concepts in natural images as triggers: only samples containing the target concept are poisoned, causing the model to behave normally in all other cases but consistently inject malicious outputs whenever the concept appears. The second, CBL-Guided Unseen Backdoor (CGUB), leverages a Concept Bottleneck Model (CBM) during training to intervene on internal concept activations, while discarding the CBM branch at inference time to keep the VLM unchanged. This design enables systematic replacement of a targeted label in generated text (for example, replacing "cat" with "dog"), even when the replacement behavior never appears in the training data. Experiments across multiple VLM architectures and datasets show that both CTP and CGUB achieve high attack success rates while maintaining moderate impact on clean-task performance. These findings highlight concept-level vulnerabilities as a critical new attack surface for VLMs.

</details>


### [16] [MASCOT: Analyzing Malware Evolution Through A Well-Curated Source Code Dataset](https://arxiv.org/abs/2512.00741)
*Bojing Li,Duo Zhong,Dharani Nadendla,Gabriel Terceros,Prajna Bhandar,Raguvir S,Charles Nicholas*

Main category: cs.CR

TL;DR: 该论文引入了一个包含6032个样本的手动审查恶意软件源代码数据集，从软件工程角度系统评估现代恶意软件的规模、开发成本、代码质量、安全性和依赖性，并提出多视图谱系分析来揭示恶意软件间的演化关系。


<details>
  <summary>Details</summary>
Motivation: 恶意软件爆炸式增长和代码重用形成了复杂的演化关系，现有研究难以捕捉最新演化趋势，且缺乏直观工具来理清恶意软件样本或类别间的复杂联系。

Method: 1) 构建手动审查的恶意软件源代码数据集（6032个样本）；2) 从软件工程角度系统评估恶意软件的规模、开发成本、代码质量、安全性和依赖性；3) 提出多视图谱系分析：整体视图量化样本和类别间的联系强度和方向，详细视图追踪单个样本的演化历史。

Result: 实验结果表明：尽管代码质量仍存在持续缺陷，但恶意软件样本展现出日益增加的复杂性和标准化，与主流软件工程实践发展同步。谱系分析直观揭示了代码重用驱动的谱系扩展和演化，为理解恶意软件生态系统的形成和演化提供了新证据和工具。

Conclusion: 该研究通过大规模恶意软件源代码数据集和系统评估，揭示了恶意软件开发的软件工程化趋势，提出的多视图谱系分析为理解恶意软件演化提供了新的分析框架和工具。

Abstract: In recent years, the explosion of malware and extensive code reuse have formed complex evolutionary connections among malware specimens. The rapid pace of development makes it challenging for existing studies to characterize recent evolutionary trends. In addition, intuitive tools to untangle these intricate connections between malware specimens or categories are urgently needed. This paper introduces a manually-reviewed malware source code dataset containing 6032 specimens. Building on and extending current research from a software engineering perspective, we systematically evaluate the scale, development costs, code quality, as well as security and dependencies of modern malware. We further introduce a multi-view genealogy analysis to clarify malware connections: at an overall view, this analysis quantifies the strength and direction of connections among specimens and categories; at a detailed view, it traces the evolutionary histories of individual specimens. Experimental results indicate that, despite persistent shortcomings in code quality, malware specimens exhibit an increasing complexity and standardization, in step with the development of mainstream software engineering practices. Meanwhile, our genealogy analysis intuitively reveals lineage expansion and evolution driven by code reuse, providing new evidence and tools for understanding the formation and evolution of the malware ecosystem.

</details>


### [17] [Bias Injection Attacks on RAG Databases and Sanitization Defenses](https://arxiv.org/abs/2512.00804)
*Hao Wu,Prateek Saxena*

Main category: cs.CR

TL;DR: 本文提出针对RAG系统的偏见注入攻击，通过插入事实正确但语义偏见的文本来影响LLM回答的意识形态框架，并开发了后检索过滤防御方法BiasDef。


<details>
  <summary>Details</summary>
Motivation: 现有知识投毒攻击主要注入虚假或有毒内容，容易被事实核查或语言分析检测。本文揭示了一种新的隐蔽威胁：偏见注入攻击，通过插入事实正确但语义偏见的段落来影响LLM回答的意识形态框架。

Method: 1) 精确描述偏见注入攻击类别；2) 开发后检索过滤防御方法BiasDef；3) 基于公开问答数据集构建综合基准进行评估。

Result: 1) 提出的攻击能显著诱导LLM回答的视角偏移，有效规避现有基于检索的净化防御；2) BiasDef优于现有方法，减少15%的对抗性段落检索，将回答中的视角偏移减轻6.2倍，同时使良性段落检索增加62%。

Conclusion: 偏见注入攻击是RAG系统的新威胁，需要专门的防御机制。BiasDef作为有效的后检索过滤方法，能显著减轻攻击影响，同时保持良性内容的检索能力。

Abstract: This paper explores attacks and defenses on vector databases in retrieval-augmented generation (RAG) systems. Prior work on knowledge poisoning attacks primarily inject false or toxic content, which fact-checking or linguistic analysis easily detects. We reveal a new and subtle threat: bias injection attacks, which insert factually correct yet semantically biased passages into the knowledge base to covertly influence the ideological framing of answers generated by large language models (LLMs). We demonstrate that these adversarial passages, though linguistically coherent and truthful, can systematically crowd out opposing views from the retrieved context and steer LLM answers toward the attacker's intended perspective.
  We precisely characterize this class of attacks and then develop a post-retrieval filtering defense, BiasDef. We construct a comprehensive benchmark based on public question answering datasets to evaluate them. Our results show that: (1) the proposed attack induces significant perspective shifts in LLM answers, effectively evading existing retrieval-based sanitization defenses; and (2) BiasDef outperforms existing methods by reducing adversarial passages retrieved by 15\% which mitigates perspective shift by 6.2\times in answers, while enabling the retrieval of 62\% more benign passages.

</details>


### [18] [Logic Encryption: This Time for Real](https://arxiv.org/abs/2512.00833)
*Rupesh Raj Karn,Lakshmi Likhitha Mankali,Zeng Wang,Saideep Sreekumar,Prithwish Basu Roy,Ozgur Sinanoglu,Lilas Alrahis,Johann Knechtel*

Main category: cs.CR

TL;DR: 提出基于逻辑加密的IP保护新方法，通过加密逻辑本身来混淆电路结构和功能，相比现有方案在无预言机攻击下表现更优且设计开销更低。


<details>
  <summary>Details</summary>
Motivation: 现代电路面临逆向工程、知识产权盗窃、侧信道攻击等多种威胁，需要有效的IP保护方案。

Method: 基于标准加密算法、密钥位随机化、简单电路设计技术和系统级综合操作，构建端到端的逻辑加密实现方法，采用构造正确的方式。

Result: 方案在对抗多种无预言机攻击方面表现出显著效果，优于现有技术，且设计开销更低。提供了完整的开源实现。

Conclusion: 提出的逻辑加密方法为电路IP保护提供了有效的解决方案，在安全性和设计效率方面都有优势。

Abstract: Modern circuits face various threats like reverse engineering, theft of intellectual property (IP), side-channel attacks, etc. Here, we present a novel approach for IP protection based on logic encryption (LE). Unlike established schemes for logic locking, our work obfuscates the circuit's structure and functionality by encoding and encrypting the logic itself. We devise an end-to-end method for practical LE implementation based on standard cryptographic algorithms, key-bit randomization, simple circuit design techniques, and system-level synthesis operations, all in a correct-by-construction manner. Our extensive analysis demonstrates the remarkable efficacy of our scheme, outperforming prior art against a range of oracle-less attacks covering crucial threat vectors, all with lower design overheads. We provide a full open-source release.

</details>


### [19] [Hesperus is Phosphorus: Mapping Threat Actor Naming Taxonomies at Scale](https://arxiv.org/abs/2512.00857)
*Gonzalo Roa,Manuel Suarez-Roman,Juan Tapiador*

Main category: cs.CR

TL;DR: 本文研究了网络威胁情报（CTI）厂商间威胁行为体（TA）命名不一致问题，提出HiP方法进行名称规范化、整合与聚类，分析了13,371份CTI报告中的3,287个TA名称，揭示了别名集中现象、演变规律及命名扩散因素。


<details>
  <summary>Details</summary>
Motivation: 当前CTI厂商采用分散的专有命名体系，导致威胁行为体命名不一致，给研究人员整合和关联不同CTI报告与TA档案带来严重困难，阻碍了有效的威胁情报共享与分析。

Method: 提出HiP（Hesperus is Phosphorus）方法，用于规范化、整合和聚类可能对应同一实体的TA名称。该方法基于从15个来源收集的大规模数据集，包含13,371份CTI报告、17个厂商分类法、3,287个TA名称及8个映射关系，构建名称图进行分析。

Result: 分析发现：1）别名主要集中在相对较少的TA子集上；2）该现象随时间演变；3）命名扩散受临时活动集群名称、共用工具基础设施、重叠操作等因素影响；4）映射中存在错误和方法缺陷导致某些TA名称集群过大。

Conclusion: 采用统一的TA命名标准面临固有困难，根本障碍在于需要共享各CTI厂商私有的高度敏感遥测数据。研究揭示了当前命名混乱的现状及其对威胁情报整合的负面影响。

Abstract: This paper studies the problem of Threat Actor (TA) naming convention inconsistency across leading Cyber Threat Intelligence (CTI) vendors. The current decentralized and proprietary nomenclature creates confusion and significant obstacles for researchers, including difficulties in integrating and correlating disparate CTI reports and TA profiles. This paper introduces HiP (Hesperus is Phosphorus, a reference to the classic question about the Morning and the Evening Star), a methodology for normalizing, integrating, and clustering TA names presumably corresponding to the same entity. Using HiP, we analyze a large dataset collected from 15 sources and spanning 13,371 CTI reports, 17 vendor taxonomies, 3,287 TA names, and 8 mappings between them. Our analysis of the resulting name graph provides insights on key features of the problem, such as the concentration of aliases on a relatively small subset of TAs, the evolution of this phenomenon over the years, and the factors that could explain TA name proliferation. We also report errors in the mappings and methodological pitfalls that contribute to make certain TA name clusters larger than they should be, including the use of temporary names for activity clusters, the existence of common tools and infrastructure, and overlapping operations. We conclude with a discussion on the inherent difficulties to adopt a TA naming standard, a quest fundamentally hampered by the need to share highly-sensitive telemetry that is private to each CTI vendor.

</details>


### [20] [Mitigating Indirect Prompt Injection via Instruction-Following Intent Analysis](https://arxiv.org/abs/2512.00966)
*Mintong Kang,Chong Xiang,Sanjay Kariyappa,Chaowei Xiao,Bo Li,Edward Suh*

Main category: cs.CR

TL;DR: IntentGuard是一个防御间接提示注入攻击的框架，通过分析LLM的指令跟随意图来识别和中和来自不可信数据的恶意指令。


<details>
  <summary>Details</summary>
Motivation: 间接提示注入攻击（IPIAs）是LLM驱动代理面临的关键威胁，攻击者将恶意指令隐藏在输入数据中，使LLM无意中执行这些指令。现有防御方法主要关注检测恶意文本，但忽略了LLM是否真正意图跟随这些指令的核心问题。

Method: IntentGuard基于指令跟随意图分析，使用指令跟随意图分析器（IIA）识别LLM将输入的哪些部分视为可执行指令。通过三种"思维干预"策略从支持推理的LLM中提取结构化指令列表：1）思维开始预填充，2）思维结束精炼，3）对抗性上下文演示。

Result: 在AgentDojo和Mind2Web两个代理基准测试中，使用Qwen-3-32B和gpt-oss-20B模型评估，IntentGuard在除一个设置外没有效用下降，并能显著降低攻击成功率（例如在Mind2Web场景中将攻击成功率从100%降至8.5%）。

Conclusion: IntentGuard提供了一种有效的防御间接提示注入攻击的通用框架，通过关注LLM的指令跟随意图而非单纯检测恶意文本，实现了强鲁棒性和最小效用损失。

Abstract: Indirect prompt injection attacks (IPIAs), where large language models (LLMs) follow malicious instructions hidden in input data, pose a critical threat to LLM-powered agents. In this paper, we present IntentGuard, a general defense framework based on instruction-following intent analysis. The key insight of IntentGuard is that the decisive factor in IPIAs is not the presence of malicious text, but whether the LLM intends to follow instructions from untrusted data. Building on this insight, IntentGuard leverages an instruction-following intent analyzer (IIA) to identify which parts of the input prompt the model recognizes as actionable instructions, and then flag or neutralize any overlaps with untrusted data segments. To instantiate the framework, we develop an IIA that uses three "thinking intervention" strategies to elicit a structured list of intended instructions from reasoning-enabled LLMs. These techniques include start-of-thinking prefilling, end-of-thinking refinement, and adversarial in-context demonstration. We evaluate IntentGuard on two agentic benchmarks (AgentDojo and Mind2Web) using two reasoning-enabled LLMs (Qwen-3-32B and gpt-oss-20B). Results demonstrate that IntentGuard achieves (1) no utility degradation in all but one setting and (2) strong robustness against adaptive prompt injection attacks (e.g., reducing attack success rates from 100% to 8.5% in a Mind2Web scenario).

</details>


### [21] [Sliced Rényi Pufferfish Privacy: Directional Additive Noise Mechanism and Private Learning with Gradient Clipping](https://arxiv.org/abs/2512.01115)
*Tao Zhang,Yevgeniy Vorobeychik*

Main category: cs.CR

TL;DR: 提出了切片Rényi Pufferfish隐私(SRPP)框架，解决了高维最优传输校准问题，并提供了机制无关的组合规则，特别适用于迭代深度学习算法。


<details>
  <summary>Details</summary>
Motivation: 解决Rényi Pufferfish隐私(RPP)的两个实际缺陷：高维最优传输校准困难，以及缺乏通用的机制无关迭代学习组合规则。

Method: 引入切片Rényi Pufferfish隐私(SRPP)，用方向性比较替代高维比较；提出切片Wasserstein机制进行噪声校准；开发基于历史均匀上限(HUC)的SRPP-SGD方案用于迭代深度学习。

Result: SRPP方法在静态和迭代设置中都实现了有利的隐私-效用权衡，能够进行优雅的加性组合，并提供统计稳定和各向异性的校准。

Conclusion: SRPP框架有效解决了RPP的实际限制，为高维隐私机制设计和迭代学习提供了实用的解决方案，在隐私保护和模型效用之间取得了良好平衡。

Abstract: We study privatization mechanism design and privacy accounting in the Pufferfish family, addressing two practical gaps of Renyi Pufferfish Privacy (RPP): high-dimensional optimal transport (OT) calibration and the absence of a general, mechanism-agnostic composition rule for iterative learning. We introduce Sliced Renyi Pufferfish Privacy (SRPP), which replaces high-dimensional comparisons by directional ones over a set of unit vectors, enabling geometry-aware and tractable guarantees. To calibrate noise without high-dimensional OT, we propose sliced Wasserstein mechanisms that compute per-direction (1-D) sensitivities, yielding closed-form, statistically stable, and anisotropic calibrations. We further define SRPP Envelope (SRPE) as computable upper bounds that are tightly implementable by these sliced Wasserstein mechanisms. For iterative deep learning algorithms, we develop a decompose-then-compose SRPP-SGD scheme with gradient clipping based on a History-Uniform Cap (HUC), a pathwise bound on one-step directional changes that is uniform over optimization history, and a mean-square variant (ms-HUC) that leverages subsampling randomness to obtain on-average SRPP guarantees with improved utility. The resulting HUC and ms-HUC accountants aggregate per-iteration, per-direction Renyi costs and integrate naturally with moments-accountant style analyses. Finally, when multiple mechanisms are trained and privatized independently under a common slicing geometry, our analysis yields graceful additive composition in both worst-case and mean-square regimes. Our experiments indicate that the proposed SRPP-based methods achieve favorable privacy-utility trade-offs in both static and iterative settings.

</details>


### [22] [Reverse Engineering and Control-Aware Security Analysis of the ArduPilot UAV Framework](https://arxiv.org/abs/2512.01164)
*Yasaswini Konapalli,Lotfi Ben Othmane,Cihan Tunc,Feras Benchellal,Likhita Mudagere*

Main category: cs.CR

TL;DR: 该论文分析了ArduPilot开源无人机自动驾驶框架的软件架构和控制模型，研究如何通过合法输入滥用控制模型来诱导恶意行为。


<details>
  <summary>Details</summary>
Motivation: 无人机技术在各领域应用日益广泛，其安全性至关重要。ArduPilot作为最广泛使用的开源自动驾驶框架，存在多种已知漏洞，特别是通信子系统（WiFi、遥测、GPS）的漏洞可能成为关键攻击入口。需要深入理解其控制模型如何可能被滥用。

Method: 首先重构ArduPilot的软件架构和控制模型，然后分析这些控制模型如何可能被滥用，通过合法输入诱导恶意行为。

Result: 论文通过架构重构和控制模型分析，揭示了ArduPilot系统中控制模型可能被滥用的机制，展示了如何利用合法输入诱导恶意行为。

Conclusion: ArduPilot的控制模型存在被滥用的风险，攻击者可能通过合法输入诱导恶意行为，这凸显了无人机自动驾驶系统安全性的重要性，需要加强控制模型的安全防护。

Abstract: Unmanned Aerial Vehicle (UAV) technologies are gaining high interest for many domains, which makes UAV security of utmost importance. ArduPilot is among the most widely used open-source autopilot UAV frameworks; yet, many studies demonstrate the vulnerabilities affecting such systems. Vulnerabilities within its communication subsystems (including WiFi, telemetry, or GPS) expose critical entry points, and vulnerabilities in Ardupilot can affect the control procedure. In this paper, we reconstruct the software architecture and the control models implemented by ArduPilot and then examine how these control models could potentially misused to induce malicious behaviors while relying on legitimate inputs.

</details>


### [23] [DefenSee: Dissecting Threat from Sight and Text - A Multi-View Defensive Pipeline for Multi-modal Jailbreaks](https://arxiv.org/abs/2512.01185)
*Zihao Wang,Kar Wai Fok,Vrizlynn L. L. Thing*

Main category: cs.CR

TL;DR: DefenSee：一种轻量级多模态黑盒防御技术，通过图像变体转录和跨模态一致性检查来增强MLLMs对协同越狱攻击的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）虽然能处理文本、图像和音频，但在面对协同越狱攻击时非常脆弱。现有防御主要针对文本模态，缺乏有效的多模态保护机制，导致MLLMs比纯文本模型更容易受到恶意指令攻击。

Method: 提出DefenSee防御技术，采用图像变体转录和跨模态一致性检查方法。通过生成图像的不同变体并进行转录，然后检查跨模态内容的一致性，模拟人类判断过程来检测潜在攻击。

Result: 在流行的多模态越狱和良性数据集上的实验表明，DefenSee显著提升MLLMs的鲁棒性，同时在良性任务上保持更好性能。在MM-SafetyBench基准测试中，将MiniGPT4的越狱攻击成功率降至1.70%以下，显著优于现有方法。

Conclusion: DefenSee是一种有效且轻量级的多模态防御方案，能够显著增强MLLMs对协同越狱攻击的抵抗力，同时保持模型在正常任务上的性能，为解决多模态模型安全问题提供了新思路。

Abstract: Multi-modal large language models (MLLMs), capable of processing text, images, and audio, have been widely adopted in various AI applications. However, recent MLLMs integrating images and text remain highly vulnerable to coordinated jailbreaks. Existing defenses primarily focus on the text, lacking robust multi-modal protection. As a result, studies indicate that MLLMs are more susceptible to malicious or unsafe instructions, unlike their text-only counterparts. In this paper, we proposed DefenSee, a robust and lightweight multi-modal black-box defense technique that leverages image variants transcription and cross-modal consistency checks, mimicking human judgment. Experiments on popular multi-modal jailbreak and benign datasets show that DefenSee consistently enhances MLLM robustness while better preserving performance on benign tasks compared to SOTA defenses. It reduces the ASR of jailbreak attacks to below 1.70% on MiniGPT4 using the MM-SafetyBench benchmark, significantly outperforming prior methods under the same conditions.

</details>


### [24] [CTF Archive: Capture, Curate, Learn Forever](https://arxiv.org/abs/2512.01233)
*Pratham Gupta,Aditya Gabani,Connor Nelson,Yan Shoshitaishvili*

Main category: cs.CR

TL;DR: CTF Archive平台通过集中归档和配置CTF挑战，解决CTF竞赛短暂性带来的教育价值流失问题，使学习者能够随时访问和练习，专注于概念理解而非环境配置。


<details>
  <summary>Details</summary>
Motivation: CTF竞赛虽然是有力的网络安全教育方式，但其短暂性（24-48小时）和临时基础设施阻碍了持续教育价值。学习者难以重新访问未解决的挑战，因为缺乏完整文档和指导来手动重建和重新托管这些挑战。

Method: 开发CTF Archive平台，集中归档十多年来的数百个CTF挑战，提供完全配置、即用型环境。平台消除环境设置的复杂性，使挑战能够长期保存和访问。

Result: 平台降低了学习门槛，使学习者能够专注于概念理解而非技术故障排除。保存的挑战鼓励学习者按自己的节奏进行深入研究和探索，显著增强概念理解，无需承受实时竞赛压力。

Conclusion: CTF Archive提供了一个可扩展的解决方案，将持久、实用的网络安全学习整合到学术课程中，通过公共可访问性促进包容性教育体验，保存CTF竞赛的教育价值。

Abstract: Capture the Flag (CTF) competitions represent a powerful experiential learning approach within cybersecurity education, blending diverse concepts into interactive challenges. However, the short duration (typically 24-48 hours) and ephemeral infrastructure of these events often impede sustained educational benefit. Learners face substantial barriers in revisiting unsolved challenges, primarily due to the cumbersome process of manually reconstructing and rehosting the challenges without comprehensive documentation or guidance. To address this critical gap, we introduce CTF Archive, a platform designed to preserve the educational value of CTF competitions by centralizing and archiving hundreds of challenges spanning over a decade in fully configured, ready-to-use environments. By removing the complexity of environment setup, CTF Archive allows learners to focus directly on conceptual understanding rather than technical troubleshooting. The availability of these preserved challenges encourages in-depth research and exploration at the learner's pace, significantly enhancing conceptual comprehension without the pressures of live competition. Additionally, public accessibility lowers entry barriers, promoting an inclusive educational experience. Overall, CTF Archive provides a scalable solution to integrate persistent, practical cybersecurity learning into academic curricula.

</details>


### [25] [Benchmarking and Understanding Safety Risks in AI Character Platforms](https://arxiv.org/abs/2512.01247)
*Yiluo Wei,Peixian Zhang,Gareth Tyson*

Main category: cs.CR

TL;DR: 首个大规模AI角色平台安全研究：评估16个平台5000个问题，发现平均不安全回复率达65.1%，远高于基线17.7%，安全性能与角色特征相关，机器学习模型能有效识别不安全角色。


<details>
  <summary>Details</summary>
Motivation: AI角色平台允许用户与AI角色对话，增长迅速但存在安全隐患。其沉浸式、个性化特性结合技术漏洞引发重大安全担忧，但此前缺乏系统性安全评估。

Method: 对16个流行AI角色平台进行大规模安全研究，使用包含5000个问题的基准集，覆盖16个安全类别，评估平台安全性能。

Result: AI角色平台平均不安全回复率高达65.1%，显著高于基线17.7%。安全性能在不同角色间差异显著，与角色人口统计特征和个性特征强相关。机器学习模型能以0.81的F1分数识别不安全角色。

Conclusion: 研究结果为平台治理和内容审核提供重要见解，预测能力可用于改进安全交互机制、角色搜索/推荐和角色创建，促进更安全的AI角色平台发展。

Abstract: AI character platforms, which allow users to engage in conversations with AI personas, are a rapidly growing application domain. However, their immersive and personalized nature, combined with technical vulnerabilities, raises significant safety concerns. Despite their popularity, a systematic evaluation of their safety has been notably absent. To address this gap, we conduct the first large-scale safety study of AI character platforms, evaluating 16 popular platforms using a benchmark set of 5,000 questions across 16 safety categories. Our findings reveal a critical safety deficit: AI character platforms exhibit an average unsafe response rate of 65.1%, substantially higher than the 17.7% average rate of the baselines. We further discover that safety performance varies significantly across different characters and is strongly correlated with character features such as demographics and personality. Leveraging these insights, we demonstrate that our machine learning model is able identify less safe characters with an F1-score of 0.81. This predictive capability can be beneficial for platforms, enabling improved mechanisms for safer interactions, character search/recommendations, and character creation. Overall, the results and findings offer valuable insights for enhancing platform governance and content moderation for safer AI character platforms.

</details>


### [26] [Large Language Models Cannot Reliably Detect Vulnerabilities in JavaScript: The First Systematic Benchmark and Evaluation](https://arxiv.org/abs/2512.01255)
*Qingyuan Fei,Xin Liu,Song Li,Shujiang Wu,Jianwei Hou,Ping Chen,Zifeng Kang*

Main category: cs.CR

TL;DR: 研究人员提出了FORGEJS框架，用于构建JavaScript漏洞检测的基准测试ARENAJS，并评估了7个主流商业LLM，发现它们在JavaScript漏洞检测中表现有限且存在严重鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 现有JavaScript漏洞检测基准存在三个关键问题：覆盖不完整、因不合理标注导致低估LLM能力、使用不现实案例导致高估。需要系统评估LLM在JavaScript漏洞检测中的真实能力。

Method: 提出三个基准构建原则：全面性、不低估、不高估。基于此开发FORGEJS自动基准生成框架，构建ARENAJS基准，并提出JUDGEJS自动评估框架。使用JUDGEJS评估7个商业LLM在ARENAJS上的表现。

Result: 评估结果显示，LLM不仅推理能力有限，还存在严重的鲁棒性缺陷。这表明使用LLM进行可靠的JavaScript漏洞检测仍然是一个开放挑战。

Conclusion: LLM在JavaScript漏洞检测中的能力被高估，当前技术仍不可靠。需要更系统的基准和评估方法来准确衡量LLM在此领域的真实能力。

Abstract: Researchers have proposed numerous methods to detect vulnerabilities in JavaScript, especially those assisted by Large Language Models (LLMs). However, the actual capability of LLMs in JavaScript vulnerability detection remains questionable, necessitating systematic evaluation and comprehensive benchmarks. Unfortunately, existing benchmarks suffer from three critical limitations: (1) incomplete coverage, such as covering a limited subset of CWE types; (2) underestimation of LLM capabilities caused by unreasonable ground truth labeling; and (3) overestimation due to unrealistic cases such as using isolated vulnerable files rather than complete projects.
  In this paper, we introduce, for the first time, three principles for constructing a benchmark for JavaScript vulnerability detection that directly address these limitations: (1) comprehensiveness, (2) no underestimation, and (3) no overestimation. Guided by these principles, we propose FORGEJS, the first automatic benchmark generation framework for evaluating LLMs' capability in JavaScript vulnerability detection. Then, we use FORGEJS to construct ARENAJS-the first systematic benchmark for LLM-based JavaScript vulnerability detection-and further propose JUDGEJS, an automatic evaluation framework.
  We conduct the first systematic evaluation of LLMs for JavaScript vulnerability detection, leveraging JUDGEJS to assess seven popular commercial LLMs on ARENAJS. The results show that LLMs not only exhibit limited reasoning capabilities, but also suffer from severe robustness defects, indicating that reliable JavaScript vulnerability detection with LLMs remains an open challenge.

</details>


### [27] [Systems Security Foundations for Agentic Computing](https://arxiv.org/abs/2512.01295)
*Mihai Christodorescu,Earlence Fernandes,Ashish Hooda,Somesh Jha,Johann Rehberger,Khawaja Shams*

Main category: cs.CR

TL;DR: 本文从计算机系统安全视角提出AI智能体安全与隐私的短期和长期研究问题，强调端到端系统安全而非孤立模型安全，通过11个真实攻击案例研究，定义智能体系统特有的新研究问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全研究主要关注单个模型的加固，但类似软件安全经验表明这不足够。需要从计算机系统安全角度，采用端到端的安全方法，将传统安全原则应用于AI智能体系统。

Method: 采用计算机系统安全视角，构建现实的攻击者模型，应用软件安全经验教训，通过11个真实智能体系统攻击案例研究，分析传统安全原则在AI智能体环境中的挑战。

Result: 识别了将传统安全原则应用于智能体计算的具体挑战，提出了智能体系统特有的新研究问题，为AI/ML从业者和研究者提供了安全实践指导。

Conclusion: AI智能体安全需要系统级方法而非仅模型级加固，应借鉴数十年网络安全经验，采用构建攻击者模型、应用软件安全原则、持续改进安全态势的成熟方法。

Abstract: This paper articulates short- and long-term research problems in AI agent security and privacy, using the lens of computer systems security. This approach examines end-to-end security properties of entire systems, rather than AI models in isolation. While we recognize that hardening a single model is useful, it is important to realize that it is often insufficient. By way of an analogy, creating a model that is always helpful and harmless is akin to creating software that is always helpful and harmless. The collective experience of decades of cybersecurity research and practice shows that this is insufficient. Rather, constructing an informed and realistic attacker model before building a system, applying hard-earned lessons from software security, and continuous improvement of security posture is a tried-and-tested approach to securing real computer systems. A key goal is to examine where research challenges arise when applying traditional security principles in the context of AI agents. A secondary goal of this report is to distill these ideas for AI and ML practitioners and researchers. We discuss the challenges of applying security principles to agentic computing, present 11 case studies of real attacks on agentic systems, and define a series of new research problems specific to the security of agentic systems.

</details>


### [28] [Securing Large Language Models (LLMs) from Prompt Injection Attacks](https://arxiv.org/abs/2512.01326)
*Omar Farooq Khan Suri,John McCrae*

Main category: cs.CR

TL;DR: JATMO防御方法通过任务特定微调减少LLM对提示注入攻击的脆弱性，但HOUYI遗传攻击框架仍能绕过防御，显示微调防御的局限性


<details>
  <summary>Details</summary>
Motivation: 评估JATMO防御方法对抗提示注入攻击的实际效果，了解基于微调的防御策略的鲁棒性和局限性

Method: 使用改进的HOUYI遗传攻击框架（包含自定义适应度评分、修改的变异逻辑和本地测试环境）对JATMO微调的LLaMA 2-7B、Qwen1.5-4B和Qwen1.5-0.5B模型进行测试，并与微调的GPT-3.5-Turbo基线比较

Result: JATMO能降低攻击成功率，但无法完全阻止注入攻击；攻击者利用多语言线索或代码相关干扰仍能绕过防御；生成质量与注入脆弱性之间存在权衡关系

Conclusion: 基于微调的防御方法虽有前景但存在局限性，需要分层、对抗性感知的缓解策略来增强LLM的安全性

Abstract: Large Language Models (LLMs) are increasingly being deployed in real-world applications, but their flexibility exposes them to prompt injection attacks. These attacks leverage the model's instruction-following ability to make it perform malicious tasks. Recent work has proposed JATMO, a task-specific fine-tuning approach that trains non-instruction-tuned base models to perform a single function, thereby reducing susceptibility to adversarial instructions. In this study, we evaluate the robustness of JATMO against HOUYI, a genetic attack framework that systematically mutates and optimizes adversarial prompts. We adapt HOUYI by introducing custom fitness scoring, modified mutation logic, and a new harness for local model testing, enabling a more accurate assessment of defense effectiveness. We fine-tuned LLaMA 2-7B, Qwen1.5-4B, and Qwen1.5-0.5B models under the JATMO methodology and compared them with a fine-tuned GPT-3.5-Turbo baseline. Results show that while JATMO reduces attack success rates relative to instruction-tuned models, it does not fully prevent injections; adversaries exploiting multilingual cues or code-related disruptors still bypass defenses. We also observe a trade-off between generation quality and injection vulnerability, suggesting that better task performance often correlates with increased susceptibility. Our results highlight both the promise and limitations of fine-tuning-based defenses and point toward the need for layered, adversarially informed mitigation strategies.

</details>


### [29] [EmoRAG: Evaluating RAG Robustness to Symbolic Perturbations](https://arxiv.org/abs/2512.01335)
*Xinyun Zhou,Xinfeng Li,Yinan Peng,Ming Xu,Xuanwang Zhang,Miao Yu,Yidong Wang,Xiaojun Jia,Kun Wang,Qingsong Wen,XiaoFeng Wang,Wei Dong*

Main category: cs.CR

TL;DR: 研究发现RAG系统对表情符号等微小符号扰动极度脆弱，单个表情符号就能导致近100%的错误检索，揭示了当前RAG系统鲁棒性假设的重大缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统被广泛认为是增强LLM忠实度的可靠方法，但研究者发现这些系统存在被忽视的严重漏洞——对微小符号扰动（特别是表情符号）极度敏感，可能被恶意利用来操控系统输出。

Method: 通过在通用问答和代码领域进行广泛实验，使用多种最先进的检索器和生成器，系统研究表情符号注入对RAG系统的影响。分析了位置敏感性、参数规模与脆弱性的关系，并评估现有防御措施的有效性。

Result: 发现三个关键现象：1) 单个表情符号注入就能导致近100%的错误检索；2) 表情符号在查询开头位置扰动最严重（F1分数超过0.92）；3) 参数更大的模型反而更脆弱。现有防御措施对此类攻击基本无效。

Conclusion: 当前RAG系统的鲁棒性假设存在严重缺陷，表情符号等微小扰动能造成灾难性误导。需要开发针对性防御机制，并重新思考如何构建真正鲁棒的RAG系统。

Abstract: Retrieval-Augmented Generation (RAG) systems are increasingly central to robust AI, enhancing large language model (LLM) faithfulness by incorporating external knowledge. However, our study unveils a critical, overlooked vulnerability: their profound susceptibility to subtle symbolic perturbations, particularly through near-imperceptible emoticon tokens such as "(@_@)" that can catastrophically mislead retrieval, termed EmoRAG. We demonstrate that injecting a single emoticon into a query makes it nearly 100% likely to retrieve semantically unrelated texts that contain a matching emoticon. Our extensive experiment across general question-answering and code domains, using a range of state-of-the-art retrievers and generators, reveals three key findings: (I) Single-Emoticon Disaster: Minimal emoticon injections cause maximal disruptions, with a single emoticon almost 100% dominating RAG output. (II) Positional Sensitivity: Placing an emoticon at the beginning of a query can cause severe perturbation, with F1-Scores exceeding 0.92 across all datasets. (III) Parameter-Scale Vulnerability: Counterintuitively, models with larger parameters exhibit greater vulnerability to the interference. We provide an in-depth analysis to uncover the underlying mechanisms of these phenomena. Furthermore, we raise a critical concern regarding the robustness assumption of current RAG systems, envisioning a threat scenario where an adversary exploits this vulnerability to manipulate the RAG system. We evaluate standard defenses and find them insufficient against EmoRAG. To address this, we propose targeted defenses, analyzing their strengths and limitations in mitigating emoticon-based perturbations. Finally, we outline future directions for building robust RAG systems.

</details>


### [30] [A Wolf in Sheep's Clothing: Bypassing Commercial LLM Guardrails via Harmless Prompt Weaving and Adaptive Tree Search](https://arxiv.org/abs/2512.01353)
*Rongzhe Wei,Peizhi Niu,Xinjie Shen,Tony Tu,Yifan Li,Ruihan Wu,Eli Chien,Olgica Milenkovic,Pan Li*

Main category: cs.CR

TL;DR: 提出CKA-Agent攻击框架，通过将有害目标分解为多个良性子查询来绕过LLM安全防护，成功率超过95%


<details>
  <summary>Details</summary>
Motivation: 现有越狱攻击大多基于提示优化，仍包含恶意语义信号容易被现代防护机制检测。作者发现LLM内部知识高度互联的特性存在更深层漏洞，允许通过组合良性子查询实现有害目标

Method: 提出相关知识攻击代理(CKA-Agent)，将越狱重构为对目标模型知识库的自适应树状探索。代理发出局部无害查询，利用模型响应指导多路径探索，最终聚合信息实现原始有害目标

Result: 在多个先进商业LLM（Gemini2.5-Flash/Pro、GPT-oss-120B、Claude-Haiku-4.5）上评估，CKA-Agent即使面对强防护也能持续达到超过95%的成功率

Conclusion: 揭示了LLM内部知识结构存在的严重漏洞，迫切需要针对此类知识分解攻击的防御机制

Abstract: Large language models (LLMs) remain vulnerable to jailbreak attacks that bypass safety guardrails to elicit harmful outputs. Existing approaches overwhelmingly operate within the prompt-optimization paradigm: whether through traditional algorithmic search or recent agent-based workflows, the resulting prompts typically retain malicious semantic signals that modern guardrails are primed to detect. In contrast, we identify a deeper, largely overlooked vulnerability stemming from the highly interconnected nature of an LLM's internal knowledge. This structure allows harmful objectives to be realized by weaving together sequences of benign sub-queries, each of which individually evades detection. To exploit this loophole, we introduce the Correlated Knowledge Attack Agent (CKA-Agent), a dynamic framework that reframes jailbreaking as an adaptive, tree-structured exploration of the target model's knowledge base. The CKA-Agent issues locally innocuous queries, uses model responses to guide exploration across multiple paths, and ultimately assembles the aggregated information to achieve the original harmful objective. Evaluated across state-of-the-art commercial LLMs (Gemini2.5-Flash/Pro, GPT-oss-120B, Claude-Haiku-4.5), CKA-Agent consistently achieves over 95% success rates even against strong guardrails, underscoring the severity of this vulnerability and the urgent need for defenses against such knowledge-decomposition attacks. Our codes are available at https://github.com/Graph-COM/CKA-Agent.

</details>


### [31] [INFERMAL: Inferential analysis of maliciously registered domains](https://arxiv.org/abs/2512.01391)
*Yevheniya Nosyk,Maciej Korczyński,Carlos Gañán,Sourena Maroofi,Jan Bayer,Zul Odgerel,Samaneh Tajalizadehkhoob,Andrzej Duda*

Main category: cs.CR

TL;DR: 研究发现域名注册费用降低1美元会导致恶意域名增加49%，免费服务使钓鱼活动激增88%，严格限制可减少63%滥用，而提供API访问的注册商恶意域名激增401%


<details>
  <summary>Details</summary>
Motivation: 网络犯罪分子依赖域名进行网络攻击，现有研究发现恶意注册集中在少数注册商和顶级域名中，但缺乏对驱动恶意注册因素的系统性分析，存在理解不同变量如何影响恶意注册的关键空白

Method: 通过GLM回归分析，编译了包含73个特征的全面列表，涵盖三个主要潜在因素：注册属性、主动验证和反应性安全实践，系统分析恶意行为者在注册新钓鱼域名时的倾向和厌恶

Result: 注册费用每降低1美元对应恶意域名增加49%；免费服务（如网站托管）导致钓鱼活动激增88%；严格限制可减少63%的滥用；提供API访问进行域名注册或账户创建的注册商恶意域名激增401%

Conclusion: 研究揭示了影响恶意域名注册的关键经济和技术因素，有助于域名注册中介制定针对性的反滥用措施，同时需要与他们的经济利益保持一致

Abstract: Cybercriminals have long depended on domain names for phishing, spam, malware distribution, and botnet operation. To facilitate the malicious activities, they continually register new domain names for exploitation. Previous work revealed an abnormally high concentration of malicious registrations in a handful of domain name registrars and top-level domains (TLDs). Anecdotal evidence suggests that low registration prices attract cybercriminals, implying that higher costs may potentially discourage them. However, no existing study has systematically analyzed the factors driving abuse, leaving a critical gap in understanding how different variables influence malicious registrations. In this report, we carefully distill the inclinations and aversions of malicious actors during the registration of new phishing domain names. We compile a comprehensive list of 73 features encompassing three main latent factors: registration attributes, proactive verification, and reactive security practices. Through a GLM regression analysis, we find that each dollar reduction in registration fees corresponds to a 49% increase in malicious domains. The availability of free services, such as web hosting, drives an 88% surge in phishing activities. Conversely, stringent restrictions cut down abuse by 63%, while registrars providing API access for domain registration or account creation experience a staggering 401% rise in malicious domains. This exploration may assist intermediaries involved in domain registration to develop tailored anti-abuse practices, yet aligning them with their economic incentives.

</details>


### [32] [Inside Qubic's Selfish Mining Campaign on Monero: Evidence, Tactics, and Limits](https://arxiv.org/abs/2512.01437)
*Suhyeon Lee,Hyeongyeong Kim*

Main category: cs.CR

TL;DR: 分析Qubic在2025年对Monero的自私挖矿活动，发现其算力份额在23-34%之间，未达51%控制，且收益低于诚实挖矿。


<details>
  <summary>Details</summary>
Motivation: 研究Qubic在Monero网络上宣传的自私挖矿活动的实际效果和影响，验证自私挖矿理论模型在现实中的适用性。

Method: 结合Monero节点数据和Qubic矿池API数据，重建Qubic归属区块和算力，检测自私挖矿策略区间，使用经典自私挖矿模型和改进的马尔可夫链模型进行分析。

Result: 检测到10个符合自私挖矿策略的时间区间，Qubic算力份额在23-34%范围内，但未持续达到51%控制；两种模型均预测其收益低于诚实挖矿，数据基本证实但存在偏差。

Conclusion: Qubic的自私挖矿活动未达到理论预期的收益优势，模型与实测数据的差异源于算力时变性和攻击分段粗糙性，表明现实自私挖矿比理论模型更复杂。

Abstract: We analyze Qubic's advertised selfish mining campaign on Monero in 2025. Combining data from Monero nodes, and the Qubic pool API, we reconstruct Qubic-attributed blocks and hashrate and detect ten intervals consistent with selfish mining strategies. In these intervals, Qubic's average hashrate share rises to the 23-34\% range, yet sustained 51\% control is never observed. We evaluate the campaign against the classical selfish mining model and a modified Markov-chain model that reflects Qubic's conservative release strategy: both predict lower revenue than honest mining at the inferred parameters, and the data largely confirms this while still showing noticeable deviations from the predicted curve. We interpret this gap between model and measurements in terms of Qubic's time-varying hashrate and coarse-grained attack segmentation.

</details>


### [33] [IVE: An Accelerator for Single-Server Private Information Retrieval Using Versatile Processing Elements](https://arxiv.org/abs/2512.01574)
*Sangpyo Kim,Hyesung Ji,Jongmin Kim,Wonseok Choi,Jaiyoung Park,Jung Ho Ahn*

Main category: cs.CR

TL;DR: IVE是一个针对单服务器PIR的加速器，通过DRAM存储、多客户端批处理、片上暂存器和异构内存系统，实现了比现有硬件方案高达1275倍的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 单服务器PIR虽然部署简单且信任假设较少，但由于需要读取整个大型数据库（如SSD），计算和内存带宽需求过高，导致实际应用受限。

Method: 提出IVE加速器，采用DRAM存储大型数据库，通过多客户端批处理分摊访问成本，使用片上暂存器和操作调度算法最大化数据复用，引入sysNTTU功能单元提高面积效率，设计异构内存系统实现数据库规模线性扩展。

Result: IVE实现了比现有PIR硬件解决方案高达1275倍的吞吐量提升，能够支持大型数据库的实用检索。

Conclusion: IVE通过系统化的硬件架构创新，解决了单服务器PIR的性能瓶颈，使其能够实际应用于大型数据库的隐私保护检索场景。

Abstract: Private information retrieval (PIR) is an essential cryptographic protocol for privacy-preserving applications, enabling a client to retrieve a record from a server's database without revealing which record was requested. Single-server PIR based on homomorphic encryption has particularly gained immense attention for its ease of deployment and reduced trust assumptions. However, single-server PIR remains impractical due to its high computational and memory bandwidth demands. Specifically, reading the entirety of large databases from storage, such as SSDs, severely limits its performance. To address this, we propose IVE, an accelerator for single-server PIR with a systematic extension that enables practical retrieval from large databases using DRAM. Recent advances in DRAM capacity allow PIR for large databases to be served entirely from DRAM, removing its dependence on storage bandwidth. Although the memory bandwidth bottleneck still remains, multi-client batching effectively amortizes database access costs across concurrent requests to improve throughput. However, client-specific data remains a bottleneck, whose bandwidth requirements ultimately limits performance. IVE overcomes this by employing a large on-chip scratchpad with an operation scheduling algorithm that maximizes data reuse, further boosting throughput. Additionally, we introduce sysNTTU, a versatile functional unit that enhances area efficiency without sacrificing performance. We also propose a heterogeneous memory system architecture, which enables a linear scaling of database sizes without a throughput degradation. Consequently, IVE achieves up to 1,275x higher throughput compared to prior PIR hardware solutions.

</details>


### [34] [Beyond the Hype: A Large-Scale Empirical Analysis of On-Chain Transactions in NFT Scams](https://arxiv.org/abs/2512.01577)
*Wenkai Li,Zongwei Li,Xiaoqi Li,Chunyi Zhang,Xiaoyan Zhang,Yuqing Zhang*

Main category: cs.CR

TL;DR: 首次系统性地通过图分析探索NFT钓鱼欺诈，揭示了NFT钓鱼诈骗在交易图中的核心行为特征和模式。


<details>
  <summary>Details</summary>
Motivation: NFT作为数字资产所有权的代表形式吸引了众多投资者，但相关的钓鱼欺诈活动造成了重大财产损失。虽然已有许多图分析方法检测恶意诈骗事件，但尚无针对NFT诈骗交易模式的研究。

Method: 收集多个平台发布的NFT钓鱼事件相关交易记录、日志数据和安全报告，经过清洗和统一处理后构建交易图，分析NFT钓鱼诈骗的分布、交易特征和交互模式。

Result: 正常交易占所有交易的96.71%，钓鱼相关账户仅占总账户的0.94%，但出现在8.36%的交易场景中，且在大规模交易网络中与正常账户的交互概率显著更高。NFT钓鱼诈骗者通常集体作案，针对特定账户，倾向于通过多种代币标准与受害者交互，交易周期比正常交易更短，涉及更多多方交易。

Conclusion: 本研究揭示了NFT钓鱼诈骗的核心行为特征，为未来NFT钓鱼诈骗的检测和预防提供了重要参考。

Abstract: Non-fungible tokens (NFTs) serve as a representative form of digital asset ownership and have attracted numerous investors, creators, and tech enthusiasts in recent years. However, related fraud activities, especially phishing scams, have caused significant property losses. There are many graph analysis methods to detect malicious scam incidents, but no research on the transaction patterns of the NFT scams. Therefore, to fill this gap, we are the first to systematically explore NFT phishing frauds through graph analysis, aiming to comprehensively investigate the characteristics and patterns of NFT phishing frauds on the transaction graph. During the research process, we collect transaction records, log data, and security reports related to NFT phishing incidents published on multiple platforms. After collecting, sanitizing, and unifying the data, we construct a transaction graph and analyze the distribution, transaction features, and interaction patterns of NFT phishing scams. We find that normal transactions on the blockchain accounted for 96.71% of all transactions. Although phishing-related accounts accounted for only 0.94% of the total accounts, they appeared in 8.36% of the transaction scenarios, and their interaction probability with normal accounts is significantly higher in large-scale transaction networks. Moreover, NFT phishing scammers often carry out fraud in a collective manner, targeting specific accounts, tend to interact with victims through multiple token standards, have shorter transaction cycles than normal transactions, and involve more multi-party transactions. This study reveals the core behavioral features of NFT phishing scams, providing important references for the detection and prevention of NFT phishing scams in the future.

</details>


### [35] [Confidential, Attestable, and Efficient Inter-CVM Communication with Arm CCA](https://arxiv.org/abs/2512.01594)
*Sina Abdollahi,Amir Al Sadi,Marios Kogias,David Kotz,Hamed Haddadi*

Main category: cs.CR

TL;DR: CAEC系统基于Arm CCA架构，通过引入机密共享内存(CSM)机制，使多个机密虚拟机(CVMs)能够安全地共享内存，同时保持对hypervisor和其他非参与CVM的不可访问性，显著提升跨CVM通信性能。


<details>
  <summary>Details</summary>
Motivation: 现有CVM架构缺乏跨CVM数据共享的一流机制，由于内存模型隔离，跨CVM数据交换成为性能瓶颈。所有跨CVM通信必须通过hypervisor可访问的内存，需要昂贵的加密解密操作来保护机密性和完整性。

Method: 基于Arm机密计算架构(CCA)构建，扩展其固件以支持机密共享内存(CSM)，这是一种在多个CVM之间安全共享的内存区域，同时对hypervisor和非参与CVM保持不可访问。设计完全兼容CCA硬件，仅增加4%的固件代码大小。

Result: CAEC在多种工作负载下提供显著的性能优势。跨CVM通信相比基于加密的机制减少高达209倍的CPU周期。提供高性能、强隔离保证和可验证的共享语义。

Conclusion: CAEC为下一代可信多CVM服务提供了实用且可扩展的基础，适用于边缘和云环境，解决了CVM间数据共享的性能瓶颈问题。

Abstract: Confidential Virtual Machines (CVMs) are increasingly adopted to protect sensitive workloads from privileged adversaries such as the hypervisor. While they provide strong isolation guarantees, existing CVM architectures lack first-class mechanisms for inter-CVM data sharing due to their disjoint memory model, making inter-CVM data exchange a performance bottleneck in compartmentalized or collaborative multi-CVM systems. Under this model, a CVM's accessible memory is either shared with the hypervisor or protected from both the hypervisor and all other CVMs. This design simplifies reasoning about memory ownership; however, it fundamentally precludes plaintext data sharing between CVMs because all inter-CVM communication must pass through hypervisor-accessible memory, requiring costly encryption and decryption to preserve confidentiality and integrity.
  In this paper, we introduce CAEC, a system that enables protected memory sharing between CVMs. CAEC builds on Arm Confidential Compute Architecture (CCA) and extends its firmware to support Confidential Shared Memory (CSM), a memory region securely shared between multiple CVMs while remaining inaccessible to the hypervisor and all non-participating CVMs. CAEC's design is fully compatible with CCA hardware and introduces only a modest increase (4\%) in CCA firmware code size. CAEC delivers substantial performance benefits across a range of workloads. For instance, inter-CVM communication over CAEC achieves up to 209$\times$ reduction in CPU cycles compared to encryption-based mechanisms over hypervisor-accessible shared memory. By combining high performance, strong isolation guarantees, and attestable sharing semantics, CAEC provides a practical and scalable foundation for the next generation of trusted multi-CVM services across both edge and cloud environments.

</details>


### [36] [WhiteLie: A Robust System for Spoofing User Data in Android Platforms](https://arxiv.org/abs/2512.01595)
*Harish Yadav,Vikas Maurya,Abhilash Jindal,Vireshwar Kumar*

Main category: cs.CR

TL;DR: WhiteLie是一个Android用户数据欺骗系统，能在不root设备或修改应用的情况下，向应用提供虚假数据以保护用户隐私，同时避免应用崩溃。


<details>
  <summary>Details</summary>
Motivation: Android权限框架允许用户拒绝应用访问隐私数据，但许多应用在被拒绝权限时会崩溃，迫使用户必须允许访问才能使用应用。现有解决方案需要root设备或修改应用二进制文件，限制了实际部署。

Method: WhiteLie是一个全面的用户数据欺骗系统，能够欺骗多种用户数据并喂给目标应用。它能检测隐私侵犯行为，自动响应并提供虚假数据而非用户真实数据，同时不会导致应用崩溃或中断。系统不需要设备root，也不修改应用二进制，可在标准Android设备上部署。

Result: 在70多个流行Android应用上的实验表明，WhiteLie能够欺骗应用接受虚假数据而不被检测到。评估显示系统在电池使用、CPU消耗和应用执行延迟方面引入的开销可忽略不计。

Conclusion: 研究证明了在现有Android生态系统中实施以用户为中心的隐私增强机制的可行性，WhiteLie为保护用户隐私提供了一种实用且高效的解决方案。

Abstract: Android employs a permission framework that empowers users to either accept or deny sharing their private data (for example, location) with an app. However, many apps tend to crash when they are denied permission, leaving users no choice but to allow access to their data in order to use the app. In this paper, we introduce a comprehensive and robust user data spoofing system, WhiteLie, that can spoof a variety of user data and feed it to target apps. Additionally, it detects privacy-violating behaviours, automatically responding by supplying spoofed data instead of the user's real data, without crashing or disrupting the apps. Unlike prior approaches, WhiteLie requires neither device rooting nor altering the app's binary, making it deployable on stock Android devices. Through experiments on more than 70 popular Android apps, we demonstrate that WhiteLie is able to deceive apps into accepting spoofed data without getting detected. Our evaluation further demonstrates that WhiteLie introduces negligible overhead in terms of battery usage, CPU consumption, and app execution latency. Our findings underscore the feasibility of implementing user-centric privacy-enhancing mechanisms within the existing Android ecosystem.

</details>


### [37] [Towards a Multi-Layer Defence Framework for Securing Near-Real-Time Operations in Open RAN](https://arxiv.org/abs/2512.01596)
*Hamed Alimohammadi,Samara Mayhoub,Sotiris Chatzimiltis,Mohammad Shojafar,Muhammad Nasir Mumtaz Bhutta*

Main category: cs.CR

TL;DR: 提出一个多层防御框架来保护Open RAN中的近实时RAN智能控制器操作，针对三类运行时威胁设计专用检测组件，在测试床上验证了有效检测率和低延迟开销。


<details>
  <summary>Details</summary>
Motivation: Open RAN中近实时控制操作的安全性日益重要，但现有方案对运行时威胁防护不足，这些威胁在系统运行时针对控制回路发起攻击。

Method: 提出多层防御框架，将运行时威胁分为三类：消息级、数据级和控制逻辑级，为每类设计专用检测组件：基于签名的E2消息检查模块、基于LSTM网络的遥测数据中毒检测器、基于执行时哈希挑战-响应的运行时xApp认证机制。

Result: 在FlexRIC和商用RAN仿真器组成的O-RAN测试床上评估，显示有效检测率、低延迟开销和实际集成可行性。保护机制可在近实时约束下运行，对500个用户设备的网络引入少于80毫秒的开销。

Conclusion: 为Open RAN中近实时RIC控制回路提供了可部署、分层、策略驱动的运行时安全架构基础，并提供了可扩展框架，未来可集成更多缓解策略和威胁特定模块。

Abstract: Securing the near-real-time (near-RT) control operations in Open Radio Access Networks (Open RAN) is increasingly critical, yet remains insufficiently addressed, as new runtime threats target the control loop while the system is operational. In this paper, we propose a multi-layer defence framework designed to enhance the security of near-RT RAN Intelligent Controller (RIC) operations. We classify operational-time threats into three categories, message-level, data-level, and control logic-level, and design and implement a dedicated detection and mitigation component for each: a signature-based E2 message inspection module performing structural and semantic validation of signalling exchanges, a telemetry poisoning detector based on temporal anomaly scoring using an LSTM network, and a runtime xApp attestation mechanism based on execution-time hash challenge-response. The framework is evaluated on an O-RAN testbed comprising FlexRIC and a commercial RAN emulator, demonstrating effective detection rates, low latency overheads, and practical integration feasibility. Results indicate that the proposed safeguards can operate within near-RT time constraints while significantly improving protection against runtime attacks, introducing less than 80 ms overhead for a network with 500 User Equipment (UEs). Overall, this work lays the foundation for deployable, layered, and policy-driven runtime security architectures for the near-RT RIC control loop in Open RAN, and provides an extensible framework into which future mitigation policies and threat-specific modules can be integrated.

</details>


### [38] [On the Context-Hiding Property of Shamir-Based Homomorphic Secret Sharing](https://arxiv.org/abs/2512.01604)
*Shuai Feng,Liang Feng Zhang*

Main category: cs.CR

TL;DR: 本文形式化了同态秘密共享（HSS）的上下文隐藏属性，研究了基于Shamir方案的单项式HSS的上下文隐藏特性，并将其扩展到多项式函数。


<details>
  <summary>Details</summary>
Motivation: 在HSS支持的应用（如安全多方计算）中，安全性要求输出份额不会泄露比函数输出更多的输入信息。传统的随机化技术虽然能实现上下文隐藏，但会增加份额大小，因此需要更高效的方法。

Method: 形式化HSS的上下文隐藏属性，专门针对单个函数进行分析；研究基于Shamir方案的单项式HSS的上下文隐藏特性；将研究扩展到多项式函数。

Result: 建立了HSS上下文隐藏的形式化定义，分析了Shamir-based HSS在单项式和多项式函数上的上下文隐藏特性，为设计更高效的HSS方案提供了理论基础。

Conclusion: 通过形式化HSS的上下文隐藏属性并分析具体方案的特性，为开发不增加份额大小的高效上下文隐藏HSS方案奠定了基础，有助于提升安全多方计算等应用的性能。

Abstract: Homomorphic secret sharing (HSS) allows multiple input clients to secretly share their private inputs to a function among several servers such that each server can homomorphically compute the function over its share to produce a share of the function's output. In HSS-enabled applications such as secure multi-party computation (MPC), security requires that the output shares leak no more information about the inputs than the function output. Such security is ensured by the context-hiding property of HSS. The typical rerandomization technique achieves context hiding but increases the share size. To address this, we formalize the context-hiding property of HSS for individual functions, examine the context-hiding property of Shamir-based HSS for monomials, and extend the study to polynomials.

</details>


### [39] [Rethinking Cybersecurity Ontology Classification and Evaluation: Towards a Credibility-Centered Framework](https://arxiv.org/abs/2512.01651)
*Antoine Leblanc,Jacques Robin,Nourhène Ben Rabah,Zequan Huang,Bénédicte Le Grand*

Main category: cs.CR

TL;DR: 本文分析网络安全本体激增现象，认为这不仅是技术质量问题，更是可信度缺失（用户缺乏信任、认可和采用）所致。作者提出修订的可信度评估框架和分类方案，指导根据具体安全需求选择本体。


<details>
  <summary>Details</summary>
Motivation: 网络安全本体数量激增，但现有研究主要关注技术质量问题，忽略了用户信任和采用不足这一关键因素。需要从可信度角度解释本体扩散现象，并提供实用指导帮助用户选择合适本体。

Method: 1. 使用本体分类框架对网络安全本体进行最先进综述和分类；2. 提出修订的可信度评估框架，引入机构支持、学术认可、日常实践验证和工业采用等指标；3. 基于新指标构建分类方案；4. 在ANCILE法卢研究项目中应用该框架进行案例验证。

Result: 发现网络安全本体激增不仅源于技术缺陷，更因可信度缺失。提出的可信度评估框架和分类方案能有效指导本体选择，ANCILE项目案例证明该框架能重塑操作环境中的本体选择过程。

Conclusion: 网络安全本体扩散问题需要超越技术视角，考虑可信度因素。提出的可信度评估框架为选择适合特定安全需求的本体提供了实用工具，有助于提高本体在实际应用中的采用率。

Abstract: This paper analyzes the proliferation of cybersecurity ontologies, arguing that this surge cannot be explained solely by technical shortcomings related to quality, but also by a credibility deficit - a lack of trust, endorsement, and adoption by users. This conclusion is based on our first contribution, which is a state-of-the-art review and categorization of cybersecurity ontologies using the Framework for Ontologies Classification framework. To address this gap, we propose a revised framework for assessing credibility, introducing indicators such as institutional support, academic recognition, day-to-day practitioner validation, and industrial adoption. Based on these new credibility indicators, we construct a classification scheme designed to guide the selection of ontologies that are relevant to specific security needs. We then apply this framework to a concrete use case: the Franco-Luxembourgish research project ANCILE, which illustrates how a credibility-aware evaluation can reshape ontology selection for operational contexts.

</details>


### [40] [Demystifying Feature Engineering in Malware Analysis of API Call Sequences](https://arxiv.org/abs/2512.01666)
*Tianheng Qu,Hongsong Zhu,Limin Sun,Haining Wang,Haiqiang Fei,Zheng He,Zhi Li*

Main category: cs.CR

TL;DR: 本文通过比较知识驱动和NLP驱动的特征工程方法，研究了API调用序列特征选择对恶意软件分类的影响，发现知识驱动方法在较小样本量下表现更优，且模型倾向于关注难以解释的特征。


<details>
  <summary>Details</summary>
Motivation: 恶意软件分析中，API调用序列的特征提取传统上依赖领域专家知识，而NLP方法可实现自动特征提取。本文旨在研究如何为基于API调用序列的恶意软件分析有效选择特征。

Method: 首先在CNN、LSTM和Transformer三种模型下，对知识驱动和NLP驱动的特征工程方法进行性能比较评估。然后分析API调用序列的完整特征集，探究模型关注的特征类型。

Result: 知识驱动特征工程方法在所有指标上普遍优于NLP驱动方法，尤其在较小样本量下表现更佳。分析发现模型倾向于关注句柄和虚拟地址等跨执行变化大且难以解释的特征。

Conclusion: 知识驱动特征工程在恶意软件分类中仍具有优势，但模型关注的某些特征对人工分析不友好，需要在自动特征提取和可解释性之间寻求平衡。

Abstract: Machine learning (ML) has been widely used to analyze API call sequences in malware analysis, which typically requires the expertise of domain specialists to extract relevant features from raw data. The extracted features play a critical role in malware analysis. Traditional feature extraction is based on human domain knowledge, while there is a trend of using natural language processing (NLP) for automatic feature extraction. This raises a question: how do we effectively select features for malware analysis based on API call sequences? To answer it, this paper presents a comprehensive study of investigating the impact of feature engineering upon malware classification.We first conducted a comparative performance evaluation under three models, Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), and Transformer, with respect to knowledge-based and NLP-based feature engineering methods. We observed that models with knowledge-based feature engineering inputs generally outperform those using NLP-based across all metrics, especially under smaller sample sizes. Then we analyzed a complete set of data features from API call sequences, our analysis reveals that models often focus on features such as handles and virtual addresses, which vary across executions and are difficult for human analysts to interpret.

</details>


### [41] [AI-Driven Cybersecurity Testbed for Nuclear Infrastructure: Comprehensive Evaluation Using METL Operational Data](https://arxiv.org/abs/2512.01727)
*Benjamin Blakely,Yeni Li,Akshay Dave,Derek Kultgen,Rick Vilim*

Main category: cs.CR

TL;DR: 该研究系统评估了核基础设施中四种AI网络安全检测方法，使用METL实验平台测试了15种攻击场景，发现变化点检测性能最佳（平均AUC 0.785），为核设施网络安全提供了实用基准和参考架构。


<details>
  <summary>Details</summary>
Motivation: 先进核反应堆系统面临日益增长的网络安全威胁，攻击者利用网络物理接口操纵控制系统，同时规避传统IT安全措施。需要评估人工智能方法在核基础设施网络安全保护中的有效性。

Method: 使用阿贡国家实验室的METL作为实验平台，开发了系统评估框架，包含四种机器学习检测范式：变化点检测、LSTM异常检测、依赖关系违规分析和自编码器重构方法。攻击分类包括15种针对反应堆控制系统的场景，每个场景在五个严重级别上实施。

Result: 进行了300次严格实验，变化点检测表现最佳（平均AUC 0.785），其次是LSTM异常检测（0.636）、依赖关系违规（0.621）和自编码器方法（0.580）。多站点协同攻击最易检测（AUC 0.739），而精度信任衰减攻击最难检测（AUC 0.592）。

Conclusion: 该研究为关键核基础设施提供了AI网络安全能力的实用性能基准和参考架构，为网络物理系统中的操作部署和增强威胁响应奠定了重要基础。

Abstract: Advanced nuclear reactor systems face increasing cybersecurity threats as sophisticated attackers exploit cyber-physical interfaces to manipulate control systems while evading traditional IT security measures. This research presents a comprehensive evaluation of artificial intelligence approaches for cybersecurity protection in nuclear infrastructure, using Argonne National Laboratory's Mechanisms Engineering Test Loop (METL) as an experimental platform. We developed a systematic evaluation framework encompassing four machine learning detection paradigms: Change Point Detection, LSTM-based Anomaly Detection, Dependency Violation analysis, and Autoencoder reconstruction methods. Our comprehensive attack taxonomy includes 15 distinct scenarios targeting reactor control systems, each implemented across five severity tiers to evaluate detection performance under varying attack intensities. The experimental evaluation encompassed 300 rigorous experiments using realistic METL operational data. Change Point Detection emerged as the leading approach with mean AUC performance of 0.785, followed by LSTM Anomaly Detection (0.636), Dependency Violation (0.621), and Autoencoder methods (0.580). Attack detectability varied significantly, with multi-site coordinated attacks proving most detectable (AUC = 0.739) while precision trust decay attacks presented the greatest detection challenge (AUC = 0.592). This work delivers practical performance benchmarks and reference architecture that advance AI-based cybersecurity capabilities for critical nuclear infrastructure, providing essential foundations for operational deployment and enhanced threat response in cyber-physical systems.

</details>


### [42] [A Privacy-Preserving Information-Sharing Protocol for Federated Authentication](https://arxiv.org/abs/2512.01832)
*Francesco Buccafurri,Carmen Licciardi*

Main category: cs.CR

TL;DR: 提出一种基于OPRF的隐私保护身份注册协议，可在联邦认证系统中检测重复/欺诈身份，同时保护用户隐私


<details>
  <summary>Details</summary>
Motivation: 解决联邦认证系统中身份提供者需要检测重复或欺诈身份注册，但又不能泄露用户个人数据或允许跨域关联的隐私保护问题

Method: 使用不经意伪随机函数（OPRF）结合域特定变换，让每个身份提供者生成独立的伪匿名标识符，同时保持输入机密性；中央机构维护盲注册表记录验证结果

Result: 构建了一个通用抽象框架，在保持强隐私保证的同时支持有效的欺诈预防机制，实现全球一致性检查而不暴露敏感信息

Conclusion: 该协议为联邦认证系统提供了隐私保护的身份注册和信息共享方案，平衡了隐私保护与欺诈检测的需求

Abstract: This paper presents a privacy-preserving protocol for identity registration and information sharing in federated authentication systems. The goal is to enable Identity Providers (IdPs) to detect duplicate or fraudulent identity enrollments without revealing users personal data or enabling cross-domain correlation. The protocol relies on Oblivious Pseudorandom Functions (OPRFs) combined with domain-specific transformations, ensuring that each IdP generates independent pseudonymous identifiers derived from a shared cryptographic service while maintaining full input confidentiality. A central authority maintains a blind registry that records successful and failed identity verifications using only pseudonymous identifiers, allowing global consistency checks without exposing sensitive information or linking users across domains. The proposed construction provides a general and abstract framework suitable for a wide range of federated authentication systems, achieving strong privacy guarantees while supporting effective fraud-prevention mechanisms during identity registration.

</details>


### [43] [JPEGs Just Got Snipped: Croppable Signatures Against Deepfake Images](https://arxiv.org/abs/2512.01845)
*Pericle Perazzo,Massimiliano Mattei,Giuseppe Anastasi,Marco Avvenuti,Gianluca Dini,Giuseppe Lettieri,Carlo Vallati*

Main category: cs.CR

TL;DR: 提出一种基于BLS签名的图像认证方法，允许图像裁剪后签名仍然有效，但能检测其他篡改（包括深度伪造），签名大小恒定O(1)，并适配JPEG标准。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术利用深度学习算法创建超逼真但虚假的媒体内容，如面部和声音替换，可能传播虚假信息，损害公众信任。需要一种能区分合法裁剪和恶意篡改的图像认证机制。

Method: 采用BLS签名方案，设计一种特殊的签名机制：签名在图像裁剪后仍然有效，但对其他类型的图像操作（包括深度伪造）会失效。该方法不需要裁剪者知道私钥或被信任，签名大小恒定O(1)。还针对JPEG标准进行了适配。

Result: 方法在实验中对签名图像的大小进行了测试，证明该方案在实际场景中可行，特别适用于通过Web服务器传播图像且裁剪是主要变换的场景。

Conclusion: 提出的BLS签名方案为图像认证提供了一种实用解决方案，能有效区分合法的图像裁剪和恶意的深度伪造等篡改操作，有助于对抗虚假信息传播。

Abstract: Deepfakes are a type of synthetic media created using artificial intelligence, specifically deep learning algorithms. This technology can for example superimpose faces and voices onto videos, creating hyper-realistic but artificial representations. Deepfakes pose significant risks regarding misinformation and fake news, because they can spread false information by depicting public figures saying or doing things they never did, undermining public trust. In this paper, we propose a method that leverages BLS signatures (Boneh, Lynn, and Shacham 2004) to implement signatures that remain valid after image cropping, but are invalidated in all the other types of manipulation, including deepfake creation. Our approach does not require who crops the image to know the signature private key or to be trusted in general, and it is O(1) in terms of signature size, making it a practical solution for scenarios where images are disseminated through web servers and cropping is the primary transformation. Finally, we adapted the signature scheme for the JPEG standard, and we experimentally tested the size of a signed image.

</details>


### [44] [Behind the Curtain: How Shared Hosting Providers Respond to Vulnerability Notifications](https://arxiv.org/abs/2512.01891)
*Giada Stivala,Rafael Mrowczynski,Maria Hellenthal,Giancarlo Pellegrino*

Main category: cs.CR

TL;DR: 首次深入研究了托管服务提供商如何处理漏洞通知，发现低修复率主要源于严格的责任边界、低托管费用和高攻击量，而非通知本身的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管之前的研究关注通知发送方的因素（如发件人信誉、邮件格式等），但漏洞通知的修复率仍然很低。本文从托管服务提供商的角度出发，首次深入探究其内部处理流程和组织因素对通知效果的影响。

Method: 对24家托管服务提供商（包括共享托管和网站开发服务）进行半结构化访谈，涵盖不同公司规模和操作角色，调查他们对漏洞通知的处理流程、组织结构和运营因素。

Result: 研究发现：1）复杂基础设施导致部分提供商难以联系；2）大多数提供商常规处理漏洞通知；3）低修复率主要源于严格的责任边界（网站应用问题被视为客户责任）；4）低托管费用和高日常攻击量阻碍主动和被动安全措施；5）托管商指责网站所有者疏忽，而网站所有者往往低估网站价值或缺乏安全知识。

Conclusion: 漏洞通知修复率低的主要原因是托管服务提供商与网站所有者之间的责任划分问题，以及经济激励不足。需要超越单纯改进通知技术，解决组织层面的激励和责任分配问题。

Abstract: Large-scale vulnerability notifications (VNs) can help hosting provider organizations (HPOs) identify and remediate security vulnerabilities that attackers can exploit in data breaches or phishing campaigns. Previous VN studies have primarily focused on factors under the control of reporters, such as sender reputation, email formatting, and communication channels. Despite these efforts, remediation rates for vulnerability notifications continue to remain consistently low. This paper presents the first in-depth study of how HPOs process vulnerability notifications internally and what organizational and operational factors influence VN effectiveness. We examine the problem from a different perspective to provide the first detailed understanding of the reasons behind persistently low remediation rates. Instead of manipulating parameters of VN campaigns, we interview hosting providers directly, investigating how they handle vulnerability notifications and what factors may influence VN effectiveness, such as VN awareness and reachability, HPOs' service models, and perceived security risks.
  We conducted semi-structured interviews with 24 HPOs across shared hosting and web development services, representing varied company sizes and operator roles. Our findings reveal practical insights on VN processing and abuse workflows. While some providers remain hard to reach due to complex infrastructures, most report routinely handling VNs. However, limited remediation often stems from strict responsibility boundaries, where web application issues are seen as the customer's domain. Low hosting fees and high volumes of daily compromises further discourage both proactive and reactive measures. Our findings show that HPOs blame negligent website owners, and prior works on website owners confirms they often undervalue their sites or lack security know-how.

</details>


### [45] [Improving Phishing Resilience with AI-Generated Training: Evidence on Prompting, Personalization, and Duration](https://arxiv.org/abs/2512.01893)
*Francesco Greco,Giuseppe Desolda,Cesare Tucci,Andrea Esposito,Antonio Curci,Antonio Piccinno*

Main category: cs.CR

TL;DR: LLM生成的钓鱼防范培训内容能显著提升学习效果，复杂个性化策略无明显优势，简单提示即可生成高质量培训材料


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击持续威胁网络安全，但传统用户培训开发成本高、维护困难。生成式AI提供了新机会，但缺乏其教学效果的实证证据

Method: 通过两个对照实验验证LLM作为钓鱼防范培训生成引擎的效果。研究1比较四种提示策略；研究2测试个性化程度和培训时长的影响

Result: AI生成内容能显著提升学习效果，简单"直接-画像"策略即有效；复杂心理测量个性化无优势；较长培训时长能适度提升准确率

Conclusion: 组织可利用LLM规模化生成高质量培训内容，无需复杂用户画像，依靠模型内在能力即可

Abstract: Phishing remains a persistent cybersecurity threat; however, developing scalable and effective user training is labor-intensive and challenging to maintain. Generative Artificial Intelligence offers an interesting opportunity, but empirical evidence on its instructional efficacy remains scarce. This paper provides an experimental validation of Large Language Models (LLMs) as autonomous engines for generating phishing resilience training. Across two controlled studies (N=480), we demonstrate that AI-generated content yields significant pre-post learning gains regardless of the specific prompting strategy employed. Study 1 (N=80) compares four prompting techniques, finding that even a straightforward "direct-profile" strategy--simply embedding user traits into the prompt--produces effective training material. Study 2 (N=400) investigates the scalability of this approach by testing personalization and training duration. Results show that complex psychometric personalization offers no measurable advantage over well-designed generic content, while longer training duration provides a modest boost in accuracy. These findings suggest that organizations can leverage LLMs to generate high-quality, effective training at scale without the need for complex user profiling, relying instead on the inherent capabilities of the model.

</details>
