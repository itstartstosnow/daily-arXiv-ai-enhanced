<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 27]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Blockchain-Based Decentralized Domain Name System](https://arxiv.org/abs/2508.05655)
*Guang Yang,Peter Trinh,Alma Nkemla,Amuru Serikyaku,Edward Tatchim,Osman Sharaf*

Main category: cs.CR

TL;DR: 论文提出了一种基于区块链的去中心化域名系统（DDNS），以解决传统DNS的安全漏洞和中心化问题。


<details>
  <summary>Details</summary>
Motivation: 传统DNS面临投毒攻击、审查机制和中心化故障点等问题，亟需一种更安全、去中心化的替代方案。

Method: 设计了专用的工作量证明区块链，结合IPFS分布式存储，实现端到端信任签名和零信任验证。

Result: 系统实现了15秒域名记录传播时间，支持20种标准DNS记录类型，并具备高扩展性和抗操纵能力。

Conclusion: DDNS展示了区块链技术在域名系统中的潜力，为互联网自由和安全提供了可行解决方案。

Abstract: The current Domain Name System (DNS) infrastructure faces critical
vulnerabilities including poisoning attacks, censorship mechanisms, and
centralized points of failure that compromise internet freedom and security.
Recent incidents such as DNS poisoning attacks on ISP customers highlight the
urgent need for resilient alternatives. This paper presents a novel
blockchain-based Decentralized Domain Name System (DDNS). We designed a
specialized Proof-of-Work blockchain to maximize support for DNS-related
protocols and achieve node decentralization. The system integrates our
blockchain with IPFS for distributed storage, implements cryptographic
primitives for end-to-end trust signatures, and achieves Never Trust, Always
Verify zero-trust verification. Our implementation achieves 15-second domain
record propagation times, supports 20 standard DNS record types, and provides
perpetual free .ddns domains. The system has been deployed across distributed
infrastructure in San Jose, Los Angeles, and Orange County, demonstrating
practical scalability and resistance to traditional DNS manipulation
techniques. Performance evaluation shows the system can handle up to Max Theor.
TPS 1,111.1 tx/s (minimal transactions) and Max Theor. TPS 266.7 tx/s (regular
transactions) for domain operations while maintaining sub-second query
resolution through intelligent caching mechanisms.

</details>


### [2] [Universally Unfiltered and Unseen:Input-Agnostic Multimodal Jailbreaks against Text-to-Image Model Safeguards](https://arxiv.org/abs/2508.05658)
*Song Yan,Hui Wei,Jinlong Fei,Guoliang Yang,Zhengyu Zhao,Zheng Wamg*

Main category: cs.CR

TL;DR: 本文提出了一种名为U3-Attack的多模态越狱攻击方法，旨在绕过文本到图像（T2I）模型的安全防护措施，包括提示过滤器和图像安全检查器。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态越狱攻击方法局限于特定提示和图像的扰动，扩展性差且优化耗时。

Method: U3-Attack通过优化图像背景的对抗性补丁和敏感词的安全改写集，实现通用绕过安全防护。

Result: 实验表明，U3-Attack在开源和商业T2I模型上表现优异，例如在Runway-inpainting模型上成功率比现有最佳方法高4倍。

Conclusion: U3-Attack是一种高效且通用的多模态越狱攻击方法，显著提升了绕过T2I安全防护的能力。

Abstract: Various (text) prompt filters and (image) safety checkers have been
implemented to mitigate the misuse of Text-to-Image (T2I) models in creating
Not-Safe-For-Work (NSFW) content.In order to expose potential security
vulnerabilities of such safeguards, multimodal jailbreaks have been
studied.However, existing jailbreaks are limited to prompt-specific and
image-specific perturbations, which suffer from poor scalability and
time-consuming optimization.To address these limitations, we propose
Universally Unfiltered and Unseen (U3)-Attack, a multimodal jailbreak attack
method against T2I safeguards.Specifically, U3-Attack optimizes an adversarial
patch on the image background to universally bypass safety checkers and
optimizes a safe paraphrase set from a sensitive word to universally bypass
prompt filters while eliminating redundant computations.Extensive experimental
results demonstrate the superiority of our U3-Attack on both open-source and
commercial T2I models.For example, on the commercial Runway-inpainting model
with both prompt filter and safety checker, our U3-Attack achieves $~4\times$
higher success rates than the state-of-the-art multimodal jailbreak attack,
MMA-Diffusion.Content Warning: This paper includes examples of NSFW content.

</details>


### [3] [Can LLMs effectively provide game-theoretic-based scenarios for cybersecurity?](https://arxiv.org/abs/2508.05670)
*Daniele Proverbio,Alessio Buscemi,Alessandro Di Stefano,The Anh Han,German Castignani,Pietro Liò*

Main category: cs.CR

TL;DR: 研究探讨了经典博弈论框架是否能有效捕捉LLM驱动行为，发现LLM的行为受个性特征和语言选择影响，需谨慎应用于网络安全。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在博弈论场景中的行为，评估其对网络安全的影响。

Method: 使用可复现的博弈论框架，测试LLM在零和博弈和囚徒困境中的行为，分析语言和个性特征的影响。

Result: LLM的行为受个性特征和语言选择影响，表现出不一致性，需进一步研究。

Conclusion: LLM在网络安全中的应用需谨慎，需优化模型以提高稳定性。

Abstract: Game theory has long served as a foundational tool in cybersecurity to test,
predict, and design strategic interactions between attackers and defenders. The
recent advent of Large Language Models (LLMs) offers new tools and challenges
for the security of computer systems; In this work, we investigate whether
classical game-theoretic frameworks can effectively capture the behaviours of
LLM-driven actors and bots. Using a reproducible framework for game-theoretic
LLM agents, we investigate two canonical scenarios -- the one-shot zero-sum
game and the dynamic Prisoner's Dilemma -- and we test whether LLMs converge to
expected outcomes or exhibit deviations due to embedded biases. Our experiments
involve four state-of-the-art LLMs and span five natural languages, English,
French, Arabic, Vietnamese, and Mandarin Chinese, to assess linguistic
sensitivity. For both games, we observe that the final payoffs are influenced
by agents characteristics such as personality traits or knowledge of repeated
rounds. Moreover, we uncover an unexpected sensitivity of the final payoffs to
the choice of languages, which should warn against indiscriminate application
of LLMs in cybersecurity applications and call for in-depth studies, as LLMs
may behave differently when deployed in different countries. We also employ
quantitative metrics to evaluate the internal consistency and cross-language
stability of LLM agents, to help guide the selection of the most stable LLMs
and optimising models for secure applications.

</details>


### [4] [DINA: A Dual Defense Framework Against Internal Noise and External Attacks in Natural Language Processing](https://arxiv.org/abs/2508.05671)
*Ko-Wei Chuang,Hen-Hsen Huang,Tsai-Yen Li*

Main category: cs.CR

TL;DR: 论文提出DINA框架，同时防御NLP系统中的内部标签噪声和外部对抗攻击，显著提升模型鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和生成式AI在客服和内容审核中的应用增加，内部标签污染和外部对抗攻击成为双重威胁。

Method: 结合计算机视觉中的噪声标签学习方法和对抗训练，提出DINA框架，统一应对双重威胁。

Result: 在真实在线游戏数据集上的实验表明，DINA显著优于基线模型。

Conclusion: DINA为NLP系统在对抗场景中的安全部署提供了实用策略，对公平和负责任的AI应用有广泛意义。

Abstract: As large language models (LLMs) and generative AI become increasingly
integrated into customer service and moderation applications, adversarial
threats emerge from both external manipulations and internal label corruption.
In this work, we identify and systematically address these dual adversarial
threats by introducing DINA (Dual Defense Against Internal Noise and
Adversarial Attacks), a novel unified framework tailored specifically for NLP.
Our approach adapts advanced noisy-label learning methods from computer vision
and integrates them with adversarial training to simultaneously mitigate
internal label sabotage and external adversarial perturbations. Extensive
experiments conducted on a real-world dataset from an online gaming service
demonstrate that DINA significantly improves model robustness and accuracy
compared to baseline models. Our findings not only highlight the critical
necessity of dual-threat defenses but also offer practical strategies for
safeguarding NLP systems in realistic adversarial scenarios, underscoring
broader implications for fair and responsible AI deployment.

</details>


### [5] [Towards Effective Offensive Security LLM Agents: Hyperparameter Tuning, LLM as a Judge, and a Lightweight CTF Benchmark](https://arxiv.org/abs/2508.05674)
*Minghao Shao,Nanda Rani,Kimberly Milner,Haoran Xi,Meet Udeshi,Saksham Aggarwal,Venkata Sai Charan Putrevu,Sandeep Kumar Shukla,Prashanth Krishnamurthy,Farshad Khorrami,Ramesh Karri,Muhammad Shafique*

Main category: cs.CR

TL;DR: 论文研究了LLM代理系统在CTF挑战中的自动化表现，提出了CTFJudge框架和CTF Competency Index（CCI）指标，并探讨了LLM超参数对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 提升LLM代理在网络安全任务中的自动化能力，特别是CTF挑战的解决效率。

Method: 1. 提出CTFJudge框架评估代理轨迹；2. 设计CCI指标衡量部分正确性；3. 分析LLM超参数对性能的影响；4. 发布CTFTiny基准数据集。

Result: 确定了多代理协作的最佳设置，并开源了CTFTiny和CTFJudge工具。

Conclusion: 为未来LLM代理在网络安全领域的研究奠定了基础。

Abstract: Recent advances in LLM agentic systems have improved the automation of
offensive security tasks, particularly for Capture the Flag (CTF) challenges.
We systematically investigate the key factors that drive agent success and
provide a detailed recipe for building effective LLM-based offensive security
agents. First, we present CTFJudge, a framework leveraging LLM as a judge to
analyze agent trajectories and provide granular evaluation across CTF solving
steps. Second, we propose a novel metric, CTF Competency Index (CCI) for
partial correctness, revealing how closely agent solutions align with
human-crafted gold standards. Third, we examine how LLM hyperparameters, namely
temperature, top-p, and maximum token length, influence agent performance and
automated cybersecurity task planning. For rapid evaluation, we present
CTFTiny, a curated benchmark of 50 representative CTF challenges across binary
exploitation, web, reverse engineering, forensics, and cryptography. Our
findings identify optimal multi-agent coordination settings and lay the
groundwork for future LLM agent research in cybersecurity. We make CTFTiny open
source to public https://github.com/NYU-LLM-CTF/CTFTiny along with CTFJudge on
https://github.com/NYU-LLM-CTF/CTFJudge.

</details>


### [6] [Principle-Guided Verilog Optimization: IP-Safe Knowledge Transfer via Local-Cloud Collaboration](https://arxiv.org/abs/2508.05675)
*Jing Wang,Zheng Li,Lei Li,Fan He,Liyu Lin,Yao Lai,Yan Li,Xiaoyang Zeng,Yufeng Guo*

Main category: cs.CR

TL;DR: 提出了一种保护IP的边缘-云协作框架，结合本地小型LLM和云端强大LLM，显著提升RTL代码优化成功率。


<details>
  <summary>Details</summary>
Motivation: 解决云端LLM处理专有硬件设计时的IP泄露风险，同时利用其优化能力。

Method: 本地小型LLM分析目标设计与草稿代码，生成设计原则；云端LLM基于原则提供优化建议，避免IP泄露。

Result: 框架优化成功率（66.67%）优于单独使用云端LLM（49.81%）或商业模型（55.81%）。

Conclusion: 该框架在性能提升与IP保护间取得平衡，为安全硬件设计优化提供新范式。

Abstract: Recent years have witnessed growing interest in adopting large language
models (LLMs) for Register Transfer Level (RTL) code optimization. While
powerful cloud-based LLMs offer superior optimization capabilities, they pose
unacceptable intellectual property (IP) leakage risks when processing
proprietary hardware designs. In this paper, we propose a new scenario where
Verilog code must be optimized for specific attributes without leaking
sensitive IP information. We introduce the first IP-preserving edge-cloud
collaborative framework that leverages the benefits of both paradigms. Our
approach employs local small LLMs (e.g., Qwen-2.5-Coder-7B) to perform secure
comparative analysis between paired high-quality target designs and novice
draft codes, yielding general design principles that summarize key insights for
improvements. These principles are then used to query stronger cloud LLMs
(e.g., Deepseek-V3) for targeted code improvement, ensuring that only
abstracted and IP-safe guidance reaches external services. Our experimental
results demonstrate that the framework achieves significantly higher
optimization success rates compared to baseline methods. For example, combining
Qwen-2.5-Coder-7B and Deepseek-V3 achieves a 66.67\% optimization success rate
for power utilization, outperforming Deepseek-V3 alone (49.81\%) and even
commercial models like GPT-4o (55.81\%). Further investigation of local and
cloud LLM combinations reveals that different model pairings exhibit varying
strengths for specific optimization objectives, with interesting trends
emerging when varying the number of comparative code pairs. Our work
establishes a new paradigm for secure hardware design optimization that
balances performance gains with IP protection.

</details>


### [7] [Adversarial Attacks on Reinforcement Learning-based Medical Questionnaire Systems: Input-level Perturbation Strategies and Medical Constraint Validation](https://arxiv.org/abs/2508.05677)
*Peizhuo Liu*

Main category: cs.CR

TL;DR: 该论文研究了基于强化学习的医疗问卷系统的安全性，通过六种对抗攻击方法评估其脆弱性，并在医疗约束下生成临床可信的对抗样本，结果显示攻击显著影响诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 基于强化学习的医疗问卷系统在医疗场景中潜力巨大，但其安全性和鲁棒性尚未解决，因此需要评估其对抗攻击的脆弱性。

Method: 将诊断过程建模为马尔可夫决策过程（MDP），实现六种对抗攻击方法（如FGSM、PGD等），并开发包含247条医疗约束的验证框架。

Result: 在NHIS数据集上，成功生成97.6%临床可信的对抗样本，攻击成功率从33.08%（FGSM）到64.70%（AutoAttack），显著降低诊断准确性。

Conclusion: 即使输入受严格医疗约束，基于强化学习的医疗问卷系统仍存在显著脆弱性，需进一步研究提升安全性。

Abstract: RL-based medical questionnaire systems have shown great potential in medical
scenarios. However, their safety and robustness remain unresolved. This study
performs a comprehensive evaluation on adversarial attack methods to identify
and analyze their potential vulnerabilities. We formulate the diagnosis process
as a Markov Decision Process (MDP), where the state is the patient responses
and unasked questions, and the action is either to ask a question or to make a
diagnosis. We implemented six prevailing major attack methods, including the
Fast Gradient Signed Method (FGSM), Projected Gradient Descent (PGD), Carlini &
Wagner Attack (C&W) attack, Basic Iterative Method (BIM), DeepFool, and
AutoAttack, with seven epsilon values each. To ensure the generated adversarial
examples remain clinically plausible, we developed a comprehensive medical
validation framework consisting of 247 medical constraints, including
physiological bounds, symptom correlations, and conditional medical
constraints. We achieved a 97.6% success rate in generating clinically
plausible adversarial samples. We performed our experiment on the National
Health Interview Survey (NHIS) dataset (https://www.cdc.gov/nchs/nhis/), which
consists of 182,630 samples, to predict the participant's 4-year mortality
rate. We evaluated our attacks on the AdaptiveFS framework proposed in
arXiv:2004.00994. Our results show that adversarial attacks could significantly
impact the diagnostic accuracy, with attack success rates ranging from 33.08%
(FGSM) to 64.70% (AutoAttack). Our work has demonstrated that even under strict
medical constraints on the input, such RL-based medical questionnaire systems
still show significant vulnerabilities.

</details>


### [8] [Selection-Based Vulnerabilities: Clean-Label Backdoor Attacks in Active Learning](https://arxiv.org/abs/2508.05681)
*Yuhan Zhi,Longtian Wang,Xiaofei Xie,Chao Shen,Qiang Hu,Xiaohong Guan*

Main category: cs.CR

TL;DR: 本文提出了ALA框架，首次利用主动学习中的获取函数作为攻击面，揭示了主动学习的潜在安全风险。


<details>
  <summary>Details</summary>
Motivation: 主动学习（AL）在资源受限场景中广泛应用，但其安全性未被充分研究。本文旨在揭示AL的潜在弱点，即获取函数可能被恶意利用。

Method: ALA通过优化不易察觉的毒化输入，使其表现出高不确定性分数，从而增加被获取函数选中的概率。

Result: 实验表明，ALA在低毒化预算（0.5%-1.0%）下仍能实现高成功率（高达94%），同时保持模型效用且不易被人类标注者察觉。

Conclusion: 研究提醒用户：获取函数易被利用，主动学习在可信数据场景中需谨慎部署。

Abstract: Active learning(AL), which serves as the representative label-efficient
learning paradigm, has been widely applied in resource-constrained scenarios.
The achievement of AL is attributed to acquisition functions, which are
designed for identifying the most important data to label. Despite this
success, one question remains unanswered: is AL safe? In this work, we
introduce ALA, a practical and the first framework to utilize the acquisition
function as the poisoning attack surface to reveal the weakness of active
learning. Specifically, ALA optimizes imperceptibly poisoned inputs to exhibit
high uncertainty scores, increasing their probability of being selected by
acquisition functions. To evaluate ALA, we conduct extensive experiments across
three datasets, three acquisition functions, and two types of clean-label
backdoor triggers. Results show that our attack can achieve high success rates
(up to 94%) even under low poisoning budgets (0.5%-1.0%) while preserving model
utility and remaining undetectable to human annotators. Our findings remind
active learning users: acquisition functions can be easily exploited, and
active learning should be deployed with caution in trusted data scenarios.

</details>


### [9] [MM-FusionNet: Context-Aware Dynamic Fusion for Multi-modal Fake News Detection with Large Vision-Language Models](https://arxiv.org/abs/2508.05684)
*Junhao He,Tianyu Liu,Jingyuan Zhao,Benjamin Turner*

Main category: cs.CR

TL;DR: MM-FusionNet利用大型视觉语言模型（LVLMs）和多模态动态融合模块（CADFM），通过自适应权重分配提升多模态假新闻检测性能，在LMFND数据集上达到0.938的F1分数。


<details>
  <summary>Details</summary>
Motivation: 多模态假新闻对社会信任构成威胁，传统文本检测方法因图文欺骗性交互而效果有限，需更智能的多模态融合方法。

Method: 提出MM-FusionNet框架，核心为Context-Aware Dynamic Fusion Module（CADFM），通过双向跨模态注意力和动态模态门控网络自适应分配图文特征权重。

Result: 在LMFND数据集上F1分数达0.938，优于现有多模态基线约0.5%，且接近人类水平，展现强鲁棒性和可解释性。

Conclusion: MM-FusionNet通过动态融合多模态信息，显著提升假新闻检测性能，具有实际应用潜力。

Abstract: The proliferation of multi-modal fake news on social media poses a
significant threat to public trust and social stability. Traditional detection
methods, primarily text-based, often fall short due to the deceptive interplay
between misleading text and images. While Large Vision-Language Models (LVLMs)
offer promising avenues for multi-modal understanding, effectively fusing
diverse modal information, especially when their importance is imbalanced or
contradictory, remains a critical challenge. This paper introduces
MM-FusionNet, an innovative framework leveraging LVLMs for robust multi-modal
fake news detection. Our core contribution is the Context-Aware Dynamic Fusion
Module (CADFM), which employs bi-directional cross-modal attention and a novel
dynamic modal gating network. This mechanism adaptively learns and assigns
importance weights to textual and visual features based on their contextual
relevance, enabling intelligent prioritization of information. Evaluated on the
large-scale Multi-modal Fake News Dataset (LMFND) comprising 80,000 samples,
MM-FusionNet achieves a state-of-the-art F1-score of 0.938, surpassing existing
multi-modal baselines by approximately 0.5% and significantly outperforming
single-modal approaches. Further analysis demonstrates the model's dynamic
weighting capabilities, its robustness to modality perturbations, and
performance remarkably close to human-level, underscoring its practical
efficacy and interpretability for real-world fake news detection.

</details>


### [10] [Leveraging large language models for SQL behavior-based database intrusion detection](https://arxiv.org/abs/2508.05690)
*Meital Shlezinger,Shay Akirav,Lei Zhou,Liang Guo,Avi Kessel,Guoliang Li*

Main category: cs.CR

TL;DR: 论文提出了一种基于DistilBERT的两层异常检测方法，结合无监督和监督机器学习技术，用于检测SQL数据库中的异常访问行为。


<details>
  <summary>Details</summary>
Motivation: 数据库异常访问行为（如内部和外部攻击）日益增多，现有方法缺乏细粒度检测能力，容易误判或漏判。

Method: 使用无监督方法（集成异常检测器）标记异常查询，并结合监督方法（微调Transformer模型）精确识别内部攻击。

Result: 该方法能有效区分正常和异常行为，减少数据标注需求，提高检测精度。

Conclusion: 该方法为保护关键数据库系统提供了高效解决方案。

Abstract: Database systems are extensively used to store critical data across various
domains. However, the frequency of abnormal database access behaviors, such as
database intrusion by internal and external attacks, continues to rise.
Internal masqueraders often have greater organizational knowledge, making it
easier to mimic employee behavior effectively. In contrast, external
masqueraders may behave differently due to their lack of familiarity with the
organization. Current approaches lack the granularity needed to detect
anomalies at the operational level, frequently misclassifying entire sequences
of operations as anomalies, even though most operations are likely to represent
normal behavior. On the other hand, some anomalous behaviors often resemble
normal activities, making them difficult for existing detection methods to
identify. This paper introduces a two-tiered anomaly detection approach for
Structured Query Language (SQL) using the Bidirectional Encoder Representations
from Transformers (BERT) model, specifically DistilBERT, a more efficient,
pre-trained version. Our method combines both unsupervised and supervised
machine learning techniques to accurately identify anomalous activities while
minimizing the need for data labeling. First, the unsupervised method uses
ensemble anomaly detectors that flag embedding vectors distant from learned
normal patterns of typical user behavior across the database (out-of-scope
queries). Second, the supervised method uses fine-tuned transformer-based
models to detect internal attacks with high precision (in-scope queries), using
role-labeled classification, even on limited labeled SQL data. Our findings
make a significant contribution by providing an effective solution for
safeguarding critical database systems from sophisticated threats.

</details>


### [11] [AuthPrint: Fingerprinting Generative Models Against Malicious Model Providers](https://arxiv.org/abs/2508.05691)
*Kai Yao,Marc Juarez*

Main category: cs.CR

TL;DR: 论文提出了一种对抗性模型指纹技术，用于验证生成模型输出的来源，即使在模型提供者可能对抗的情况下也能有效工作。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型在高风险领域广泛应用，但缺乏验证输出来源的机制，因此需要一种能在对抗环境下工作的指纹技术。

Method: 通过可信验证器从模型输出空间中提取秘密指纹，并训练模型预测和验证这些指纹，即使模型提供者对抗也能保持鲁棒性。

Result: 实验表明，该方法在GAN和扩散模型上实现了接近零的FPR@95%TPR，并对架构和数据的微小修改及对抗攻击保持鲁棒性。

Conclusion: 该方法为生成模型输出来源验证提供了一种有效且鲁棒的解决方案。

Abstract: Generative models are increasingly adopted in high-stakes domains, yet
current deployments offer no mechanisms to verify the origin of model outputs.
We address this gap by extending model fingerprinting techniques beyond the
traditional collaborative setting to one where the model provider may act
adversarially. To our knowledge, this is the first work to evaluate
fingerprinting for provenance attribution under such a threat model. The
methods rely on a trusted verifier that extracts secret fingerprints from the
model's output space, unknown to the provider, and trains a model to predict
and verify them. Our empirical evaluation shows that our methods achieve
near-zero FPR@95%TPR for instances of GAN and diffusion models, even when
tested on small modifications to the original architecture and training data.
Moreover, the methods remain robust against adversarial attacks that actively
modify the outputs to bypass detection. Source codes are available at
https://github.com/PSMLab/authprint.

</details>


### [12] [DMFI: Dual-Modality Fine-Tuning and Inference Framework for LLM-Based Insider Threat Detection](https://arxiv.org/abs/2508.05694)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Guanggang Geng,Zhiying Li,Jian Weng*

Main category: cs.CR

TL;DR: DMFI是一个双模态框架，结合语义推理和行为感知微调，用于检测内部威胁，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统模型难以捕捉语义意图和复杂行为动态，现有LLM解决方案在提示适应性和模态覆盖上有限。

Method: DMFI将原始日志转换为语义视图和行为抽象，使用两个LoRA增强的LLM独立微调，并通过MLP模块融合输出。

Result: 在CERT数据集上，DMFI的检测准确率优于现有方法。

Conclusion: DMFI结合LLM的语义推理和结构化行为建模，为内部威胁检测提供了可扩展的解决方案。

Abstract: Insider threat detection (ITD) poses a persistent and high-impact challenge
in cybersecurity due to the subtle, long-term, and context-dependent nature of
malicious insider behaviors. Traditional models often struggle to capture
semantic intent and complex behavior dynamics, while existing LLM-based
solutions face limitations in prompt adaptability and modality coverage. To
bridge this gap, we propose DMFI, a dual-modality framework that integrates
semantic inference with behavior-aware fine-tuning. DMFI converts raw logs into
two structured views: (1) a semantic view that processes content-rich artifacts
(e.g., emails, https) using instruction-formatted prompts; and (2) a behavioral
abstraction, constructed via a 4W-guided (When-Where-What-Which) transformation
to encode contextual action sequences. Two LoRA-enhanced LLMs are fine-tuned
independently, and their outputs are fused via a lightweight MLP-based decision
module. We further introduce DMFI-B, a discriminative adaptation strategy that
separates normal and abnormal behavior representations, improving robustness
under severe class imbalance. Experiments on CERT r4.2 and r5.2 datasets
demonstrate that DMFI outperforms state-of-the-art methods in detection
accuracy. Our approach combines the semantic reasoning power of LLMs with
structured behavior modeling, offering a scalable and effective solution for
real-world insider threat detection. Our work demonstrates the effectiveness of
combining LLM reasoning with structured behavioral modeling, offering a
scalable and deployable solution for modern insider threat detection.

</details>


### [13] [MambaITD: An Efficient Cross-Modal Mamba Network for Insider Threat Detection](https://arxiv.org/abs/2508.05695)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Zhiying Li,Guanggang Geng,Jian Weng*

Main category: cs.CR

TL;DR: 本文提出了一种基于Mamba状态空间模型和跨模态自适应融合的新型内部威胁检测框架MambaITD，解决了现有方法在时间动态特征建模、计算效率和跨模态信息孤岛方面的不足。


<details>
  <summary>Details</summary>
Motivation: 企业面临日益增长的内部威胁风险，现有检测方法因时间动态特征建模不足、计算效率低和跨模态信息孤岛问题而无法有效应对。

Method: 通过多源日志预处理模块对齐异构数据，Mamba编码器建模行为序列的长期依赖，结合门控特征融合机制动态融合序列和统计信息，并提出基于最大化类间方差的动态阈值优化方法。

Result: MambaITD在建模效率和特征融合能力上显著优于传统方法，超越了基于Transformer的方法。

Conclusion: MambaITD为内部威胁检测提供了更有效的解决方案。

Abstract: Enterprises are facing increasing risks of insider threats, while existing
detection methods are unable to effectively address these challenges due to
reasons such as insufficient temporal dynamic feature modeling, computational
efficiency and real-time bottlenecks and cross-modal information island
problem. This paper proposes a new insider threat detection framework MambaITD
based on the Mamba state space model and cross-modal adaptive fusion. First,
the multi-source log preprocessing module aligns heterogeneous data through
behavioral sequence encoding, interval smoothing, and statistical feature
extraction. Second, the Mamba encoder models long-range dependencies in
behavioral and interval sequences, and combines the sequence and statistical
information dynamically in combination with the gated feature fusion mechanism.
Finally, we propose an adaptive threshold optimization method based on
maximizing inter-class variance, which dynamically adjusts the decision
threshold by analyzing the probability distribution, effectively identifies
anomalies, and alleviates class imbalance and concept drift. Compared with
traditional methods, MambaITD shows significant advantages in modeling
efficiency and feature fusion capabilities, outperforming Transformer-based
methods, and provides a more effective solution for insider threat detection.

</details>


### [14] [Log2Sig: Frequency-Aware Insider Threat Detection via Multivariate Behavioral Signal Decomposition](https://arxiv.org/abs/2508.05696)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Zhiying Li,Guanggang Geng*

Main category: cs.CR

TL;DR: Log2Sig是一种新型的异常检测框架，通过将用户日志转化为多变量行为频率信号，并结合多尺度频率分解和序列建模，显著提升了内部威胁检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将系统日志建模为扁平事件序列，无法捕捉用户行为中的频率动态和多尺度扰动模式，导致检测效果受限。

Method: Log2Sig利用多变量变分模式分解（MVMD）提取多尺度行为波动特征，并结合Mamba时序编码器和频率信号线性投影，构建双视图行为表示。

Result: 在CERT r4.2和r5.2数据集上，Log2Sig在准确率和F1分数上均显著优于现有基线方法。

Conclusion: Log2Sig通过多尺度频率分析和序列建模的融合，为内部威胁检测提供了一种更有效的解决方案。

Abstract: Insider threat detection presents a significant challenge due to the
deceptive nature of malicious behaviors, which often resemble legitimate user
operations. However, existing approaches typically model system logs as flat
event sequences, thereby failing to capture the inherent frequency dynamics and
multiscale disturbance patterns embedded in user behavior. To address these
limitations, we propose Log2Sig, a robust anomaly detection framework that
transforms user logs into multivariate behavioral frequency signals,
introducing a novel representation of user behavior. Log2Sig employs
Multivariate Variational Mode Decomposition (MVMD) to extract Intrinsic Mode
Functions (IMFs), which reveal behavioral fluctuations across multiple temporal
scales. Based on this, the model further performs joint modeling of behavioral
sequences and frequency-decomposed signals: the daily behavior sequences are
encoded using a Mamba-based temporal encoder to capture long-term dependencies,
while the corresponding frequency components are linearly projected to match
the encoder's output dimension. These dual-view representations are then fused
to construct a comprehensive user behavior profile, which is fed into a
multilayer perceptron for precise anomaly detection. Experimental results on
the CERT r4.2 and r5.2 datasets demonstrate that Log2Sig significantly
outperforms state-of-the-art baselines in both accuracy and F1 score.

</details>


### [15] [System Security Framework for 5G Advanced /6G IoT Integrated Terrestrial Network-Non-Terrestrial Network (TN-NTN) with AI-Enabled Cloud Security](https://arxiv.org/abs/2508.05707)
*Sasa Maric,Rasil Baidar,Robert Abbas,Sam Reisenfeld*

Main category: cs.CR

TL;DR: 本文提出了一种基于AI的云安全框架，用于5G Advanced/6G物联网与地面-非地面网络（TN-NTN）集成的系统级安全。


<details>
  <summary>Details</summary>
Motivation: 由于TN-NTN网络的异构性、规模和分布式特性，新的安全挑战出现，需要实时威胁检测和智能策略执行。

Method: 利用AI原生云平台实现实时威胁检测、安全自动化和智能策略执行，并结合卫星接入功能增强安全性。

Result: 提出了一个全面的AI云安全框架，支持零信任原则、联邦学习、安全编排和分层安全框架。

Conclusion: 未来5G Advanced/6G物联网网络中应实施AI驱动的卫星NTN，以增强安全性和抵御对抗性威胁。

Abstract: The integration of Terrestrial Networks (TN) and Non-Terrestrial Networks
(NTN), including 5G Advanced/6G and the Internet of Things (IoT) technologies,
using Low Earth Orbit (LEO) satellites, high-altitude platforms (HAPS), and
Unmanned Aerial Vehicles (UAVs), is redefining the landscape of global
connectivity. This paper introduces a new system-level security framework for
5G Advanced/6G IoT-integrated TN-NTN architectures with AI-native-enabled cloud
security. Due to the heterogeneity, scale, and distributed nature of these
networks, new security challenges have emerged. Leveraging AI-native cloud
platforms offers powerful capabilities for real-time threat detection, security
automation, and intelligent policy enforcement. The NTN satellite access
function enhances security for discontinuous coverage via satellite
connections. In addition, this paper explores the security risks associated
with integrated 5G Advanced/6G IoT TN-NTN systems, including full network
segmentation, network slicing, and the cloudification of the RAN and core. We
present a comprehensive AI-enabled cloud security framework and conclude with
proposals for implementing AI-powered, satellite-based NTN within future 5G
Advanced/6G IoT networks. Our approach emphasizes zero-trust principles,
federated learning, secure orchestration, a layered security framework, and
resilience against adversarial threats.

</details>


### [16] [On Digital Twins in Defence: Overview and Applications](https://arxiv.org/abs/2508.05717)
*Marco Giberna,Holger Voos,Paulo Tavares,João Nunes,Tobias Sorg,Andrea Masini,Jose Luis Sanchez-Lopez*

Main category: cs.CR

TL;DR: 本文探讨了数字孪生技术在国防领域的应用，包括设计、训练、任务执行等，提出了标准化框架，并分析了实施中的挑战与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 数字孪生技术因其实时监控和优化能力在多个领域受到关注，本文旨在探索其在国防领域的潜力与应用。

Method: 通过分析科学文献、行业实践、政府策略及利益相关者调查，研究数字孪生在国防中的应用与挑战。

Result: 数字孪生可提升国防系统性能、预测能力和运行时间，但实施中仍存在标准化和互操作性等挑战。

Conclusion: 未来需加强框架建设和跨学科合作，以充分发挥数字孪生在国防领域的潜力。

Abstract: Digital twin technology has gained increasing attention across various
sectors due to its ability to create virtual replicas of physical systems,
enabling real-time monitoring, optimization, and simulation. This paper
explores the integration of digital twins within defence applications, focusing
on key use cases ranging from system design and development, operational
planning and training, to mission execution and debriefing. By examining the
application of digital twin technologies across defense platforms, we highlight
their key advantages such as enhanced operational performance, predictive
capabilities, and increased system uptime. Additionally, we introduce a novel
characterization framework for digital twins that aims to standardize and unify
their application across different defence domains to facilitate
interoperability. Thereafter, we discuss the main challenges, gaps and
limitations in implementing and adopting digital twins within defence
organizations by analyzing a combination of scientific literature, current
industry practices, governmental strategies, and the findings from a
comprehensive survey of industrial stakeholders and ministries of defense.
Finally, we outline future research directions and development opportunities,
emphasizing the need for robust frameworks and interdisciplinary collaborations
to fully realize the potential of digital twins in the defence sector.

</details>


### [17] [Secure and Scalable Blockchain Voting: A Comparative Framework and the Role of Large Language Models](https://arxiv.org/abs/2508.05865)
*Kiana Kiashemshaki,Elvis Nnaemeka Chukwuani,Mohammad Jalili Torkamani,Negin Mahmoudi*

Main category: cs.CR

TL;DR: 该论文探讨了区块链技术在电子投票系统中的应用，分析了现有共识机制和加密协议的局限性，并提出了优化策略，同时探索了大型语言模型（LLM）在智能合约生成和验证中的新角色。


<details>
  <summary>Details</summary>
Motivation: 区块链技术为电子投票系统提供了透明性、去中心化和安全性，但其实际应用仍受限于可扩展性、计算复杂性和隐私需求等挑战。

Method: 论文提出了一个比较框架，分析区块链电子投票架构、共识机制和加密协议，并提出了混合共识、轻量级加密和去中心化身份管理等优化策略。

Result: 研究发现为设计安全、可扩展且智能的区块链电子投票系统提供了基础，并提出了LLM在智能合约生成和验证中的潜在应用。

Conclusion: 该研究为构建端到端的区块链电子投票原型奠定了基础，结合了LLM引导的智能合约生成和验证，并通过系统框架和模拟分析支持。

Abstract: Blockchain technology offers a promising foundation for modernizing E-Voting
systems by enhancing transparency, decentralization, and security. Yet,
real-world adoption remains limited due to persistent challenges such as
scalability constraints, high computational demands, and complex privacy
requirements. This paper presents a comparative framework for analyzing
blockchain-based E-Voting architectures, consensus mechanisms, and
cryptographic protocols. We examine the limitations of prevalent models like
Proof of Work, Proof of Stake, and Delegated Proof of Stake, and propose
optimization strategies that include hybrid consensus, lightweight
cryptography, and decentralized identity management. Additionally, we explore
the novel role of Large Language Models (LLMs) in smart contract generation,
anomaly detection, and user interaction. Our findings offer a foundation for
designing secure, scalable, and intelligent blockchain-based E-Voting systems
suitable for national-scale deployment. This work lays the groundwork for
building an end-to-end blockchain E-Voting prototype enhanced by LLM-guided
smart contract generation and validation, supported by a systematic framework
and simulation-based analysis.

</details>


### [18] [Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System](https://arxiv.org/abs/2508.06059)
*Haorui He,Yupeng Li,Bin Benjamin Zhu,Dacheng Wen,Reynold Cheng,Francis C. M. Lau*

Main category: cs.CR

TL;DR: Fact2Fiction是一种针对基于LLM的事实核查系统的首个投毒攻击框架，通过模仿分解策略和利用系统生成的解释来制作恶意证据，攻击成功率比现有方法高8.9%--21.2%。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的事实核查系统安全性不足，容易被攻击者利用来放大虚假信息。

Method: Fact2Fiction模仿系统的分解策略，利用其生成的解释来制作恶意证据，破坏子声明验证。

Result: 实验表明，Fact2Fiction的攻击成功率比现有方法高8.9%--21.2%。

Conclusion: Fact2Fiction揭示了当前事实核查系统的安全弱点，强调了防御措施的必要性。

Abstract: State-of-the-art fact-checking systems combat misinformation at scale by
employing autonomous LLM-based agents to decompose complex claims into smaller
sub-claims, verify each sub-claim individually, and aggregate the partial
results to produce verdicts with justifications (explanatory rationales for the
verdicts). The security of these systems is crucial, as compromised
fact-checkers, which tend to be easily underexplored, can amplify
misinformation. This work introduces Fact2Fiction, the first poisoning attack
framework targeting such agentic fact-checking systems. Fact2Fiction mirrors
the decomposition strategy and exploits system-generated justifications to
craft tailored malicious evidences that compromise sub-claim verification.
Extensive experiments demonstrate that Fact2Fiction achieves 8.9\%--21.2\%
higher attack success rates than state-of-the-art attacks across various
poisoning budgets. Fact2Fiction exposes security weaknesses in current
fact-checking systems and highlights the need for defensive countermeasures.

</details>


### [19] [A Game-Theoretic Foundation for Bitcoin's Price: A Security-Utility Equilibrium](https://arxiv.org/abs/2508.06071)
*Liang Chen*

Main category: cs.CR

TL;DR: 论文提出了一种结构化的博弈论模型（RESUNE）来评估去中心化数字资产（如比特币）的价值，通过市场均衡价格和哈希率的关系内生确定网络安全，并提供了可测试的预测。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖投机信念评估数字资产价值，本文旨在通过理性预期和安全-效用纳什均衡（RESUNE）提供一个更结构化的框架。

Method: 采用RESUNE模型，结合自由进入挖矿模型和全球博弈模型，分析价格、哈希率和网络安全之间的内生关系。

Result: 证明了RESUNE的存在性，并提出了唯一性和稳定性的条件；模型预测价格对需求的直接影响需超过价格对安全的反馈。

Conclusion: 模型将比特币价值分解为交易效用、安全和投机成分，解释了价格对哈希率的单向因果关系，并提出了结构VAR模型进行验证。

Abstract: This paper introduces a structural game-theoretic model to value
decentralized digital assets like Bitcoin. Instead of relying on speculative
beliefs, it frames the asset's price within a Rational-Expectations
Security-Utility Nash Equilibrium (RESUNE). This equilibrium is a fixed point
where the market-clearing price dictates the hash rate through a free-entry
mining model, which in turn endogenously sets the network's security. The
security, defined as one minus the probability of a 51% attack, is determined
via a global games model of attacker coordination, providing a unique and
continuous security function. We prove the existence of a RESUNE and offer
conditions for its uniqueness and stability. The model predicts that the
stabilizing direct effect of price on demand must outweigh the potentially
destabilizing feedback from price to security. The framework generates testable
predictions, such as a protocol halving causing a contraction in both hash rate
and price. A structural Vector Autoregression (VAR) model is proposed to test
this mechanism. The model decomposes Bitcoin's value into transactional
utility, security, and speculative components and explains the observed
unidirectional causality from price to hash rate.

</details>


### [20] [ProvX: Generating Counterfactual-Driven Attack Explanations for Provenance-Based Detection](https://arxiv.org/abs/2508.06073)
*Weiheng Wu,Wei Qiao,Teng Li,Yebo Feng,Zhuo Ma,Jianfeng Ma,Yang Liu*

Main category: cs.CR

TL;DR: ProvX是一个解释框架，用于解释基于图神经网络的入侵检测系统，通过反事实逻辑和优化任务提供可验证的解释，并提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决GNN安全模型的黑盒问题，为安全分析师提供可验证的解释和攻击相关证据。

Method: 引入反事实解释逻辑，将离散搜索问题转化为连续优化任务，并采用分阶段固化策略提升解释精度。

Result: ProvX能定位与真实攻击高度相关的关键图结构，解释必要性平均达51.59%，优于现有方法。

Conclusion: ProvX不仅提供有效解释，还能通过检测-解释-反馈闭环框架增强模型鲁棒性。

Abstract: Provenance graph-based intrusion detection systems are deployed on hosts to
defend against increasingly severe Advanced Persistent Threat. Using Graph
Neural Networks to detect these threats has become a research focus and has
demonstrated exceptional performance. However, the widespread adoption of
GNN-based security models is limited by their inherent black-box nature, as
they fail to provide security analysts with any verifiable explanations for
model predictions or any evidence regarding the model's judgment in relation to
real-world attacks. To address this challenge, we propose ProvX, an effective
explanation framework for exlaining GNN-based security models on provenance
graphs. ProvX introduces counterfactual explanation logic, seeking the minimal
structural subset within a graph predicted as malicious that, when perturbed,
can subvert the model's original prediction. We innovatively transform the
discrete search problem of finding this critical subgraph into a continuous
optimization task guided by a dual objective of prediction flipping and
distance minimization. Furthermore, a Staged Solidification strategy is
incorporated to enhance the precision and stability of the explanations. We
conducted extensive evaluations of ProvX on authoritative datasets. The
experimental results demonstrate that ProvX can locate critical graph
structures that are highly relevant to real-world attacks and achieves an
average explanation necessity of 51.59\%, with these metrics outperforming
current SOTA explainers. Furthermore, we explore and provide a preliminary
validation of a closed-loop Detection-Explanation-Feedback enhancement
framework, demonstrating through experiments that the explanation results from
ProvX can guide model optimization, effectively enhancing its robustness
against adversarial attacks.

</details>


### [21] [Adaptive Backtracking for Privacy Protection in Large Language Models](https://arxiv.org/abs/2508.06087)
*Zhihao Yao,Yuxuan Gu,Xiachong Feng,Weitao Ma,Bo Li,Xiaocheng Feng*

Main category: cs.CR

TL;DR: 论文提出企业导向的隐私保护方法ABack，解决现有方法导致模型性能下降和缺乏评估数据集的问题，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前隐私保护研究主要关注用户隐私，忽视了企业数据泄漏风险，尤其是在检索增强生成范式下。

Method: 提出ABack机制（无需训练）和PriGenQA数据集，并开发自适应攻击者进行严格评估。

Result: ABack在对抗强攻击者时，隐私效用分数提升15%，避免了性能折衷。

Conclusion: ABack为企业隐私保护提供了高效解决方案，解决了现有方法的局限性。

Abstract: The preservation of privacy has emerged as a critical topic in the era of
artificial intelligence. However, current work focuses on user-oriented
privacy, overlooking severe enterprise data leakage risks exacerbated by the
Retrieval-Augmented Generation paradigm. To address this gap, our paper
introduces a novel objective: enterprise-oriented privacy concerns. Achieving
this objective requires overcoming two fundamental challenges: existing methods
such as data sanitization severely degrade model performance, and the field
lacks public datasets for evaluation. We address these challenges with several
solutions. (1) To prevent performance degradation, we propose ABack, a
training-free mechanism that leverages a Hidden State Model to pinpoint the
origin of a leakage intention and rewrite the output safely. (2) To solve the
lack of datasets, we construct PriGenQA, a new benchmark for enterprise privacy
scenarios in healthcare and finance. To ensure a rigorous evaluation, we move
beyond simple static attacks by developing a powerful adaptive attacker with
Group Relative Policy Optimization. Experiments show that against this superior
adversary, ABack improves the overall privacy utility score by up to 15\% over
strong baselines, avoiding the performance trade-offs of prior methods.

</details>


### [22] [Simulation in Cybersecurity: Understanding Techniques, Applications, and Goals](https://arxiv.org/abs/2508.06106)
*Luca Serena,Gabriele D'Angelo,Stefano Ferretti,Moreno Marzolla*

Main category: cs.CR

TL;DR: 本文对网络安全领域的建模与仿真研究进行了全面综述，从四个维度分类现有研究，并分析了不同方法的优缺点。


<details>
  <summary>Details</summary>
Motivation: 网络安全研究中建模与仿真的多样性导致方法论趋势难以识别，现有综述多局限于特定技术或领域，缺乏整体视角。

Method: 通过四个维度（应用领域、威胁类型、仿真技术、仿真目标）对现有研究进行分类和分析。

Result: 总结了不同方法的优缺点，明确了适合仿真研究的网络威胁类型，并分析了特定挑战下的最佳建模范式。

Conclusion: 该综述为网络安全仿真研究提供了整体视角，有助于指导未来研究的方向和方法选择。

Abstract: Modeling and simulation are widely used in cybersecurity research to assess
cyber threats, evaluate defense mechanisms, and analyze vulnerabilities.
However, the diversity of application areas, the variety of cyberattacks
scenarios, and the differing objectives of these simulations makes it difficult
to identify methodological trends. Existing reviews often focus on specific
modeling techniques or application domains, making it challenging to analyze
the field as a whole. To address these limitations, we present a comprehensive
review of the current state of the art, classifying the selected papers based
on four dimensions: the application domain, the types of cyber threats
represented, the simulation techniques employed, and the primary goals of the
simulation. The review discusses the strengths and limitations of different
approaches, identifies which cyber threats are the most suited for
simulation-based investigations, and analyzes which modeling paradigms are most
appropriate for specific cybersecurity challenges.

</details>


### [23] [SLIP: Soft Label Mechanism and Key-Extraction-Guided CoT-based Defense Against Instruction Backdoor in APIs](https://arxiv.org/abs/2508.06153)
*Zhengxian Wu,Juan Wen,Wanli Peng,Haowei Chang,Yinghan Zhou,Yiming Xue*

Main category: cs.CR

TL;DR: SLIP是一种针对黑盒后门攻击的防御机制，通过软标签机制和关键提取引导的CoT方法，显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 随着定制化大语言模型（LLM）代理的发展，黑盒后门攻击威胁加剧，现有防御方法难以应对，亟需新解决方案。

Method: SLIP结合关键提取引导的CoT（KCoT）和软标签机制（SLM），提取任务相关关键词并量化语义相关性，排除异常分数以提高可靠性。

Result: 实验表明，SLIP将攻击成功率从90.2%降至25.13%，同时保持高准确率，优于现有防御方法。

Conclusion: SLIP有效应对黑盒后门攻击，为LLM安全性提供了可靠解决方案。

Abstract: With the development of customized large language model (LLM) agents, a new
threat of black-box backdoor attacks has emerged, where malicious instructions
are injected into hidden system prompts. These attacks easily bypass existing
defenses that rely on white-box access, posing a serious security challenge. To
address this, we propose SLIP, a Soft Label mechanism and key-extraction-guided
CoT-based defense against Instruction backdoors in APIs. SLIP is designed based
on two key insights. First, to counteract the model's oversensitivity to
triggers, we propose a Key-extraction-guided Chain-of-Thought (KCoT). Instead
of only considering the single trigger or the input sentence, KCoT prompts the
agent to extract task-relevant key phrases. Second, to guide the LLM toward
correct answers, our proposed Soft Label Mechanism (SLM) prompts the agent to
quantify the semantic correlation between key phrases and candidate answers.
Crucially, to mitigate the influence of residual triggers or misleading content
in phrases extracted by KCoT, which typically causes anomalous scores, SLM
excludes anomalous scores deviating significantly from the mean and
subsequently averages the remaining scores to derive a more reliable semantic
representation. Extensive experiments on classification and question-answer
(QA) tasks demonstrate that SLIP is highly effective, reducing the average
attack success rate (ASR) from 90.2% to 25.13% while maintaining high accuracy
on clean data and outperforming state-of-the-art defenses. Our code are
available in
https://github.com/CAU-ISS-Lab/Backdoor-Attack-Defense-LLMs/tree/main/SLIP.

</details>


### [24] [Anti-Tamper Protection for Unauthorized Individual Image Generation](https://arxiv.org/abs/2508.06325)
*Zelin Li,Ruohan Zong,Yifan Liu,Ruichen Yao,Yaokun Liu,Yang Zhang,Dong Wang*

Main category: cs.CR

TL;DR: 论文提出了一种名为Anti-Tamper Perturbation (ATP)的新方法，通过在扰动中引入防篡改机制，结合保护扰动和授权扰动，有效防御伪造攻击并检测净化篡改。


<details>
  <summary>Details</summary>
Motivation: 随着个性化图像生成技术的发展，伪造攻击侵犯肖像权和隐私的问题日益严重，现有保护扰动算法容易被净化技术绕过。

Method: ATP在频域中结合保护扰动和授权扰动，通过掩码指导确保两者互不干扰，授权扰动分布于所有像素以保持对篡改的敏感性。

Result: 实验证明ATP在各种攻击场景下均能有效防御伪造攻击。

Conclusion: ATP为保护肖像权和隐私提供了鲁棒的解决方案。

Abstract: With the advancement of personalized image generation technologies, concerns
about forgery attacks that infringe on portrait rights and privacy are growing.
To address these concerns, protection perturbation algorithms have been
developed to disrupt forgery generation. However, the protection algorithms
would become ineffective when forgery attackers apply purification techniques
to bypass the protection. To address this issue, we present a novel approach,
Anti-Tamper Perturbation (ATP). ATP introduces a tamper-proof mechanism within
the perturbation. It consists of protection and authorization perturbations,
where the protection perturbation defends against forgery attacks, while the
authorization perturbation detects purification-based tampering. Both
protection and authorization perturbations are applied in the frequency domain
under the guidance of a mask, ensuring that the protection perturbation does
not disrupt the authorization perturbation. This design also enables the
authorization perturbation to be distributed across all image pixels,
preserving its sensitivity to purification-based tampering. ATP demonstrates
its effectiveness in defending forgery attacks across various attack settings
through extensive experiments, providing a robust solution for protecting
individuals' portrait rights and privacy. Our code is available at:
https://github.com/Seeyn/Anti-Tamper-Perturbation .

</details>


### [25] [When AIOps Become "AI Oops": Subverting LLM-driven IT Operations via Telemetry Manipulation](https://arxiv.org/abs/2508.06394)
*Dario Pasquini,Evgenios M. Kornaropoulos,Giuseppe Ateniese,Omer Akgul,Athanasios Theocharis,Petros Efstathopoulos*

Main category: cs.CR

TL;DR: 论文分析了AIOps的安全风险，提出攻击方法AIOpsDoom和防御机制AIOpsShield。


<details>
  <summary>Details</summary>
Motivation: AIOps的自动化能力可能被攻击者利用，导致系统完整性受损，因此需要研究其安全性。

Method: 通过AIOpsDoom攻击方法，利用错误诱导请求和对抗性奖励篡改，操纵AIOps代理行为。

Result: 实验证明AIOpsShield能有效防御攻击且不影响代理性能。

Conclusion: AIOps成为新兴攻击向量，亟需安全设计。

Abstract: AI for IT Operations (AIOps) is transforming how organizations manage complex
software systems by automating anomaly detection, incident diagnosis, and
remediation. Modern AIOps solutions increasingly rely on autonomous LLM-based
agents to interpret telemetry data and take corrective actions with minimal
human intervention, promising faster response times and operational cost
savings.
  In this work, we perform the first security analysis of AIOps solutions,
showing that, once again, AI-driven automation comes with a profound security
cost. We demonstrate that adversaries can manipulate system telemetry to
mislead AIOps agents into taking actions that compromise the integrity of the
infrastructure they manage. We introduce techniques to reliably inject
telemetry data using error-inducing requests that influence agent behavior
through a form of adversarial reward-hacking; plausible but incorrect system
error interpretations that steer the agent's decision-making. Our attack
methodology, AIOpsDoom, is fully automated--combining reconnaissance, fuzzing,
and LLM-driven adversarial input generation--and operates without any prior
knowledge of the target system.
  To counter this threat, we propose AIOpsShield, a defense mechanism that
sanitizes telemetry data by exploiting its structured nature and the minimal
role of user-generated content. Our experiments show that AIOpsShield reliably
blocks telemetry-based attacks without affecting normal agent performance.
  Ultimately, this work exposes AIOps as an emerging attack vector for system
compromise and underscores the urgent need for security-aware AIOps design.

</details>


### [26] [ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls](https://arxiv.org/abs/2508.06457)
*Sanket Badhe*

Main category: cs.CR

TL;DR: 论文介绍了ScamAgent，一个基于大语言模型（LLM）的多轮对话代理，能够生成高度逼真的诈骗电话脚本，揭示了现有LLM安全防护的不足。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索LLM在多轮对话中被滥用的潜在风险，尤其是用于生成诈骗脚本的威胁。

Method: 方法是通过构建ScamAgent代理，利用对话记忆和动态适应性生成诈骗脚本，并测试现有安全防护措施的有效性。

Result: 结果显示，现有的LLM安全防护（如拒绝机制和内容过滤）对代理级威胁无效，诈骗脚本甚至可以转化为逼真的语音通话。

Conclusion: 结论强调了多轮对话安全审计、代理级控制框架以及检测生成式AI驱动的对话欺骗的新方法的紧迫性。

Abstract: Large Language Models (LLMs) have demonstrated impressive fluency and
reasoning capabilities, but their potential for misuse has raised growing
concern. In this paper, we present ScamAgent, an autonomous multi-turn agent
built on top of LLMs, capable of generating highly realistic scam call scripts
that simulate real-world fraud scenarios. Unlike prior work focused on
single-shot prompt misuse, ScamAgent maintains dialogue memory, adapts
dynamically to simulated user responses, and employs deceptive persuasion
strategies across conversational turns. We show that current LLM safety
guardrails, including refusal mechanisms and content filters, are ineffective
against such agent-based threats. Even models with strong prompt-level
safeguards can be bypassed when prompts are decomposed, disguised, or delivered
incrementally within an agent framework. We further demonstrate the
transformation of scam scripts into lifelike voice calls using modern
text-to-speech systems, completing a fully automated scam pipeline. Our
findings highlight an urgent need for multi-turn safety auditing, agent-level
control frameworks, and new methods to detect and disrupt conversational
deception powered by generative AI.

</details>


### [27] [Voting-Based Semi-Parallel Proof-of-Work Protocol](https://arxiv.org/abs/2508.06489)
*Mustafa Doger,Sennur Ulukus*

Main category: cs.CR

TL;DR: 论文提出了一种基于投票的半并行PoW协议，优于Nakamoto共识和现有并行PoW协议，解决了激励攻击问题。


<details>
  <summary>Details</summary>
Motivation: 现有并行PoW协议在安全性、交易吞吐量和确认延迟方面存在不足，且易受激励攻击。

Method: 提出投票式半并行PoW协议，结合理论分析和MDP模型验证其抗攻击能力。

Result: 新协议在通信开销、吞吐量、激励兼容性和交易费用分配上表现更优。

Conclusion: 半并行PoW协议在实践和安全性上均优于现有方案。

Abstract: Parallel Proof-of-Work (PoW) protocols are suggested to improve the safety
guarantees, transaction throughput and confirmation latencies of Nakamoto
consensus. In this work, we first consider the existing parallel PoW protocols
and develop hard-coded incentive attack structures. Our theoretical results and
simulations show that the existing parallel PoW protocols are more vulnerable
to incentive attacks than the Nakamoto consensus, e.g., attacks have smaller
profitability threshold and they result in higher relative rewards. Next, we
introduce a voting-based semi-parallel PoW protocol that outperforms both
Nakamoto consensus and the existing parallel PoW protocols from most practical
perspectives such as communication overheads, throughput, transaction
conflicts, incentive compatibility of the protocol as well as a fair
distribution of transaction fees among the voters and the leaders. We use
state-of-the-art analysis to evaluate the consistency of the protocol and
consider Markov decision process (MDP) models to substantiate our claims about
the resilience of our protocol against incentive attacks.

</details>
