<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 27]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Blockchain-Based Decentralized Domain Name System](https://arxiv.org/abs/2508.05655)
*Guang Yang,Peter Trinh,Alma Nkemla,Amuru Serikyaku,Edward Tatchim,Osman Sharaf*

Main category: cs.CR

TL;DR: 论文提出了一种基于区块链的去中心化域名系统（DDNS），以解决传统DNS的漏洞，如中毒攻击和中心化问题。


<details>
  <summary>Details</summary>
Motivation: 当前DNS基础设施存在中毒攻击、审查机制和中心化故障点等问题，威胁互联网自由与安全，亟需替代方案。

Method: 设计了一种专用PoW区块链，结合IPFS分布式存储和加密原语，实现零信任验证。

Result: 系统实现了15秒域名记录传播，支持20种DNS记录类型，提供免费.ddns域名，并展示了高扩展性和抗操纵能力。

Conclusion: DDNS系统在性能和安全性上优于传统DNS，具备实际部署潜力。

Abstract: The current Domain Name System (DNS) infrastructure faces critical
vulnerabilities including poisoning attacks, censorship mechanisms, and
centralized points of failure that compromise internet freedom and security.
Recent incidents such as DNS poisoning attacks on ISP customers highlight the
urgent need for resilient alternatives. This paper presents a novel
blockchain-based Decentralized Domain Name System (DDNS). We designed a
specialized Proof-of-Work blockchain to maximize support for DNS-related
protocols and achieve node decentralization. The system integrates our
blockchain with IPFS for distributed storage, implements cryptographic
primitives for end-to-end trust signatures, and achieves Never Trust, Always
Verify zero-trust verification. Our implementation achieves 15-second domain
record propagation times, supports 20 standard DNS record types, and provides
perpetual free .ddns domains. The system has been deployed across distributed
infrastructure in San Jose, Los Angeles, and Orange County, demonstrating
practical scalability and resistance to traditional DNS manipulation
techniques. Performance evaluation shows the system can handle up to Max Theor.
TPS 1,111.1 tx/s (minimal transactions) and Max Theor. TPS 266.7 tx/s (regular
transactions) for domain operations while maintaining sub-second query
resolution through intelligent caching mechanisms.

</details>


### [2] [Universally Unfiltered and Unseen:Input-Agnostic Multimodal Jailbreaks against Text-to-Image Model Safeguards](https://arxiv.org/abs/2508.05658)
*Song Yan,Hui Wei,Jinlong Fei,Guoliang Yang,Zhengyu Zhao,Zheng Wamg*

Main category: cs.CR

TL;DR: 本文提出了一种名为U3-Attack的多模态越狱攻击方法，旨在绕过文本到图像（T2I）模型的安全检查器和提示过滤器，解决了现有方法可扩展性差和优化耗时的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态越狱攻击方法局限于特定提示和图像的扰动，缺乏通用性和效率。本文旨在开发一种更高效、通用的攻击方法。

Method: U3-Attack通过在图像背景上优化对抗性补丁，以及从敏感词生成安全的改写集合，以绕过安全检查器和提示过滤器。

Result: 实验表明，U3-Attack在开源和商业T2I模型上表现优异，例如在Runway-inpainting模型上，其成功率比现有最佳方法高4倍。

Conclusion: U3-Attack是一种高效且通用的多模态越狱攻击方法，能够有效绕过T2I模型的安全防护措施。

Abstract: Various (text) prompt filters and (image) safety checkers have been
implemented to mitigate the misuse of Text-to-Image (T2I) models in creating
Not-Safe-For-Work (NSFW) content.In order to expose potential security
vulnerabilities of such safeguards, multimodal jailbreaks have been
studied.However, existing jailbreaks are limited to prompt-specific and
image-specific perturbations, which suffer from poor scalability and
time-consuming optimization.To address these limitations, we propose
Universally Unfiltered and Unseen (U3)-Attack, a multimodal jailbreak attack
method against T2I safeguards.Specifically, U3-Attack optimizes an adversarial
patch on the image background to universally bypass safety checkers and
optimizes a safe paraphrase set from a sensitive word to universally bypass
prompt filters while eliminating redundant computations.Extensive experimental
results demonstrate the superiority of our U3-Attack on both open-source and
commercial T2I models.For example, on the commercial Runway-inpainting model
with both prompt filter and safety checker, our U3-Attack achieves $~4\times$
higher success rates than the state-of-the-art multimodal jailbreak attack,
MMA-Diffusion.Content Warning: This paper includes examples of NSFW content.

</details>


### [3] [Can LLMs effectively provide game-theoretic-based scenarios for cybersecurity?](https://arxiv.org/abs/2508.05670)
*Daniele Proverbio,Alessio Buscemi,Alessandro Di Stefano,The Anh Han,German Castignani,Pietro Liò*

Main category: cs.CR

TL;DR: 研究探讨了经典博弈论框架是否能有效捕捉LLM驱动行为，发现LLM在博弈中的表现受语言选择影响，需谨慎应用于网络安全。


<details>
  <summary>Details</summary>
Motivation: 探究LLM在博弈论框架中的行为表现，以评估其在网络安全中的适用性。

Method: 使用可复现的博弈论框架测试LLM在零和博弈和囚徒困境中的行为，涉及多语言评估。

Result: LLM的博弈结果受语言和代理特性影响，表现出跨语言不稳定性。

Conclusion: LLM在网络安全中应用需深入研究，避免语言和文化差异导致的行为偏差。

Abstract: Game theory has long served as a foundational tool in cybersecurity to test,
predict, and design strategic interactions between attackers and defenders. The
recent advent of Large Language Models (LLMs) offers new tools and challenges
for the security of computer systems; In this work, we investigate whether
classical game-theoretic frameworks can effectively capture the behaviours of
LLM-driven actors and bots. Using a reproducible framework for game-theoretic
LLM agents, we investigate two canonical scenarios -- the one-shot zero-sum
game and the dynamic Prisoner's Dilemma -- and we test whether LLMs converge to
expected outcomes or exhibit deviations due to embedded biases. Our experiments
involve four state-of-the-art LLMs and span five natural languages, English,
French, Arabic, Vietnamese, and Mandarin Chinese, to assess linguistic
sensitivity. For both games, we observe that the final payoffs are influenced
by agents characteristics such as personality traits or knowledge of repeated
rounds. Moreover, we uncover an unexpected sensitivity of the final payoffs to
the choice of languages, which should warn against indiscriminate application
of LLMs in cybersecurity applications and call for in-depth studies, as LLMs
may behave differently when deployed in different countries. We also employ
quantitative metrics to evaluate the internal consistency and cross-language
stability of LLM agents, to help guide the selection of the most stable LLMs
and optimising models for secure applications.

</details>


### [4] [DINA: A Dual Defense Framework Against Internal Noise and External Attacks in Natural Language Processing](https://arxiv.org/abs/2508.05671)
*Ko-Wei Chuang,Hen-Hsen Huang,Tsai-Yen Li*

Main category: cs.CR

TL;DR: 论文提出DINA框架，用于同时防御NLP系统中的内部标签噪声和外部对抗攻击，显著提升模型鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和生成式AI在客服和内容审核中的应用增加，内外部的对抗威胁日益突出，需系统解决。

Method: 结合计算机视觉中的噪声标签学习方法和对抗训练，提出DINA框架，统一防御内部标签破坏和外部对抗扰动。

Result: 在真实在线游戏数据集上的实验表明，DINA显著优于基线模型，提升了鲁棒性和准确性。

Conclusion: 研究强调了双威胁防御的必要性，并为实际对抗场景中的NLP系统保护提供了实用策略，对公平和负责任的AI部署有广泛意义。

Abstract: As large language models (LLMs) and generative AI become increasingly
integrated into customer service and moderation applications, adversarial
threats emerge from both external manipulations and internal label corruption.
In this work, we identify and systematically address these dual adversarial
threats by introducing DINA (Dual Defense Against Internal Noise and
Adversarial Attacks), a novel unified framework tailored specifically for NLP.
Our approach adapts advanced noisy-label learning methods from computer vision
and integrates them with adversarial training to simultaneously mitigate
internal label sabotage and external adversarial perturbations. Extensive
experiments conducted on a real-world dataset from an online gaming service
demonstrate that DINA significantly improves model robustness and accuracy
compared to baseline models. Our findings not only highlight the critical
necessity of dual-threat defenses but also offer practical strategies for
safeguarding NLP systems in realistic adversarial scenarios, underscoring
broader implications for fair and responsible AI deployment.

</details>


### [5] [Towards Effective Offensive Security LLM Agents: Hyperparameter Tuning, LLM as a Judge, and a Lightweight CTF Benchmark](https://arxiv.org/abs/2508.05674)
*Minghao Shao,Nanda Rani,Kimberly Milner,Haoran Xi,Meet Udeshi,Saksham Aggarwal,Venkata Sai Charan Putrevu,Sandeep Kumar Shukla,Prashanth Krishnamurthy,Farshad Khorrami,Ramesh Karri,Muhammad Shafique*

Main category: cs.CR

TL;DR: 论文探讨了LLM代理系统在自动化进攻性安全任务（如CTF挑战）中的成功因素，提出了CTFJudge框架和CTF Competency Index（CCI）指标，研究了LLM超参数对性能的影响，并发布了CTFTiny基准测试集。


<details>
  <summary>Details</summary>
Motivation: 研究LLM代理系统在自动化进攻性安全任务中的表现，尤其是CTF挑战，以提升代理效率和准确性。

Method: 1. 提出CTFJudge框架，利用LLM作为裁判分析代理轨迹；2. 设计CCI指标评估部分正确性；3. 研究LLM超参数（温度、top-p、最大令牌长度）对性能的影响；4. 发布CTFTiny基准测试集。

Result: 确定了多代理协调的最佳设置，为未来LLM代理在网络安全领域的研究奠定了基础。

Conclusion: 论文为构建高效的LLM进攻性安全代理提供了详细方法，并开源了CTFTiny和CTFJudge工具。

Abstract: Recent advances in LLM agentic systems have improved the automation of
offensive security tasks, particularly for Capture the Flag (CTF) challenges.
We systematically investigate the key factors that drive agent success and
provide a detailed recipe for building effective LLM-based offensive security
agents. First, we present CTFJudge, a framework leveraging LLM as a judge to
analyze agent trajectories and provide granular evaluation across CTF solving
steps. Second, we propose a novel metric, CTF Competency Index (CCI) for
partial correctness, revealing how closely agent solutions align with
human-crafted gold standards. Third, we examine how LLM hyperparameters, namely
temperature, top-p, and maximum token length, influence agent performance and
automated cybersecurity task planning. For rapid evaluation, we present
CTFTiny, a curated benchmark of 50 representative CTF challenges across binary
exploitation, web, reverse engineering, forensics, and cryptography. Our
findings identify optimal multi-agent coordination settings and lay the
groundwork for future LLM agent research in cybersecurity. We make CTFTiny open
source to public https://github.com/NYU-LLM-CTF/CTFTiny along with CTFJudge on
https://github.com/NYU-LLM-CTF/CTFJudge.

</details>


### [6] [Principle-Guided Verilog Optimization: IP-Safe Knowledge Transfer via Local-Cloud Collaboration](https://arxiv.org/abs/2508.05675)
*Jing Wang,Zheng Li,Lei Li,Fan He,Liyu Lin,Yao Lai,Yan Li,Xiaoyang Zeng,Yufeng Guo*

Main category: cs.CR

TL;DR: 提出了一种保护IP的边缘-云协作框架，结合本地小型LLM和云端强大LLM，显著提升RTL代码优化成功率。


<details>
  <summary>Details</summary>
Motivation: 解决云端LLM优化RTL代码时的IP泄露风险，同时利用其强大能力。

Method: 本地小型LLM分析代码对生成设计原则，云端LLM基于抽象原则优化代码，确保IP安全。

Result: 框架优化成功率显著提升（如功耗优化达66.67%），优于单独使用云端LLM或商业模型。

Conclusion: 该框架在性能提升与IP保护间取得平衡，为安全硬件设计优化提供新范式。

Abstract: Recent years have witnessed growing interest in adopting large language
models (LLMs) for Register Transfer Level (RTL) code optimization. While
powerful cloud-based LLMs offer superior optimization capabilities, they pose
unacceptable intellectual property (IP) leakage risks when processing
proprietary hardware designs. In this paper, we propose a new scenario where
Verilog code must be optimized for specific attributes without leaking
sensitive IP information. We introduce the first IP-preserving edge-cloud
collaborative framework that leverages the benefits of both paradigms. Our
approach employs local small LLMs (e.g., Qwen-2.5-Coder-7B) to perform secure
comparative analysis between paired high-quality target designs and novice
draft codes, yielding general design principles that summarize key insights for
improvements. These principles are then used to query stronger cloud LLMs
(e.g., Deepseek-V3) for targeted code improvement, ensuring that only
abstracted and IP-safe guidance reaches external services. Our experimental
results demonstrate that the framework achieves significantly higher
optimization success rates compared to baseline methods. For example, combining
Qwen-2.5-Coder-7B and Deepseek-V3 achieves a 66.67\% optimization success rate
for power utilization, outperforming Deepseek-V3 alone (49.81\%) and even
commercial models like GPT-4o (55.81\%). Further investigation of local and
cloud LLM combinations reveals that different model pairings exhibit varying
strengths for specific optimization objectives, with interesting trends
emerging when varying the number of comparative code pairs. Our work
establishes a new paradigm for secure hardware design optimization that
balances performance gains with IP protection.

</details>


### [7] [Adversarial Attacks on Reinforcement Learning-based Medical Questionnaire Systems: Input-level Perturbation Strategies and Medical Constraint Validation](https://arxiv.org/abs/2508.05677)
*Peizhuo Liu*

Main category: cs.CR

TL;DR: 本文研究了基于强化学习的医疗问卷系统的安全性，通过对抗攻击方法评估其脆弱性，并开发了医学验证框架以确保攻击样本的临床合理性。


<details>
  <summary>Details</summary>
Motivation: 尽管RL医疗问卷系统潜力巨大，但其安全性和鲁棒性尚未解决，需评估其对抗攻击的脆弱性。

Method: 将诊断过程建模为MDP，实施六种对抗攻击方法，并开发包含247项医学约束的验证框架。

Result: 在NHIS数据集上，攻击成功率达33.08%-64.70%，显示系统存在显著漏洞。

Conclusion: 即使输入受严格医学约束，RL医疗问卷系统仍易受对抗攻击影响。

Abstract: RL-based medical questionnaire systems have shown great potential in medical
scenarios. However, their safety and robustness remain unresolved. This study
performs a comprehensive evaluation on adversarial attack methods to identify
and analyze their potential vulnerabilities. We formulate the diagnosis process
as a Markov Decision Process (MDP), where the state is the patient responses
and unasked questions, and the action is either to ask a question or to make a
diagnosis. We implemented six prevailing major attack methods, including the
Fast Gradient Signed Method (FGSM), Projected Gradient Descent (PGD), Carlini &
Wagner Attack (C&W) attack, Basic Iterative Method (BIM), DeepFool, and
AutoAttack, with seven epsilon values each. To ensure the generated adversarial
examples remain clinically plausible, we developed a comprehensive medical
validation framework consisting of 247 medical constraints, including
physiological bounds, symptom correlations, and conditional medical
constraints. We achieved a 97.6% success rate in generating clinically
plausible adversarial samples. We performed our experiment on the National
Health Interview Survey (NHIS) dataset (https://www.cdc.gov/nchs/nhis/), which
consists of 182,630 samples, to predict the participant's 4-year mortality
rate. We evaluated our attacks on the AdaptiveFS framework proposed in
arXiv:2004.00994. Our results show that adversarial attacks could significantly
impact the diagnostic accuracy, with attack success rates ranging from 33.08%
(FGSM) to 64.70% (AutoAttack). Our work has demonstrated that even under strict
medical constraints on the input, such RL-based medical questionnaire systems
still show significant vulnerabilities.

</details>


### [8] [Selection-Based Vulnerabilities: Clean-Label Backdoor Attacks in Active Learning](https://arxiv.org/abs/2508.05681)
*Yuhan Zhi,Longtian Wang,Xiaofei Xie,Chao Shen,Qiang Hu,Xiaohong Guan*

Main category: cs.CR

TL;DR: ALA框架首次利用获取函数作为攻击面，揭示主动学习的弱点，通过优化毒化输入实现高成功率攻击。


<details>
  <summary>Details</summary>
Motivation: 研究主动学习（AL）的安全性，揭示获取函数可能被利用的潜在风险。

Method: 提出ALA框架，优化毒化输入以提高其被获取函数选中的概率。

Result: 实验显示攻击成功率高达94%，且在低毒化预算下仍有效。

Conclusion: 主动学习需谨慎部署，获取函数易被利用。

Abstract: Active learning(AL), which serves as the representative label-efficient
learning paradigm, has been widely applied in resource-constrained scenarios.
The achievement of AL is attributed to acquisition functions, which are
designed for identifying the most important data to label. Despite this
success, one question remains unanswered: is AL safe? In this work, we
introduce ALA, a practical and the first framework to utilize the acquisition
function as the poisoning attack surface to reveal the weakness of active
learning. Specifically, ALA optimizes imperceptibly poisoned inputs to exhibit
high uncertainty scores, increasing their probability of being selected by
acquisition functions. To evaluate ALA, we conduct extensive experiments across
three datasets, three acquisition functions, and two types of clean-label
backdoor triggers. Results show that our attack can achieve high success rates
(up to 94%) even under low poisoning budgets (0.5%-1.0%) while preserving model
utility and remaining undetectable to human annotators. Our findings remind
active learning users: acquisition functions can be easily exploited, and
active learning should be deployed with caution in trusted data scenarios.

</details>


### [9] [MM-FusionNet: Context-Aware Dynamic Fusion for Multi-modal Fake News Detection with Large Vision-Language Models](https://arxiv.org/abs/2508.05684)
*Junhao He,Tianyu Liu,Jingyuan Zhao,Benjamin Turner*

Main category: cs.CR

TL;DR: MM-FusionNet提出了一种基于大型视觉语言模型的多模态假新闻检测框架，通过动态融合模块自适应学习文本和视觉特征的重要性权重，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 多模态假新闻对社会信任和稳定构成威胁，传统文本检测方法因文本与图像的欺骗性交互而效果有限，需要更有效的多模态融合方法。

Method: 提出MM-FusionNet框架，采用Context-Aware Dynamic Fusion Module（CADFM），通过双向跨模态注意力和动态模态门控网络自适应融合文本和视觉特征。

Result: 在包含8万样本的大规模数据集上，MM-FusionNet达到0.938的F1分数，优于现有多模态基线约0.5%，且接近人类水平。

Conclusion: MM-FusionNet通过动态权重分配和模态鲁棒性，展示了实际应用中的高效性和可解释性，为假新闻检测提供了新思路。

Abstract: The proliferation of multi-modal fake news on social media poses a
significant threat to public trust and social stability. Traditional detection
methods, primarily text-based, often fall short due to the deceptive interplay
between misleading text and images. While Large Vision-Language Models (LVLMs)
offer promising avenues for multi-modal understanding, effectively fusing
diverse modal information, especially when their importance is imbalanced or
contradictory, remains a critical challenge. This paper introduces
MM-FusionNet, an innovative framework leveraging LVLMs for robust multi-modal
fake news detection. Our core contribution is the Context-Aware Dynamic Fusion
Module (CADFM), which employs bi-directional cross-modal attention and a novel
dynamic modal gating network. This mechanism adaptively learns and assigns
importance weights to textual and visual features based on their contextual
relevance, enabling intelligent prioritization of information. Evaluated on the
large-scale Multi-modal Fake News Dataset (LMFND) comprising 80,000 samples,
MM-FusionNet achieves a state-of-the-art F1-score of 0.938, surpassing existing
multi-modal baselines by approximately 0.5% and significantly outperforming
single-modal approaches. Further analysis demonstrates the model's dynamic
weighting capabilities, its robustness to modality perturbations, and
performance remarkably close to human-level, underscoring its practical
efficacy and interpretability for real-world fake news detection.

</details>


### [10] [Leveraging large language models for SQL behavior-based database intrusion detection](https://arxiv.org/abs/2508.05690)
*Meital Shlezinger,Shay Akirav,Lei Zhou,Liang Guo,Avi Kessel,Guoliang Li*

Main category: cs.CR

TL;DR: 本文提出了一种基于DistilBERT的两层异常检测方法，结合无监督和监督学习技术，用于识别SQL数据库中的异常行为。


<details>
  <summary>Details</summary>
Motivation: 数据库异常访问行为（如内部和外部攻击）日益增多，现有方法缺乏细粒度检测能力，容易误判或漏判异常行为。

Method: 使用无监督的集成异常检测器标记异常查询，并结合监督学习的微调Transformer模型（DistilBERT）进行高精度内部攻击检测。

Result: 该方法能有效识别异常行为，减少数据标注需求，为数据库系统提供更高级别的安全保障。

Conclusion: 该方法为保护关键数据库系统免受复杂威胁提供了有效解决方案。

Abstract: Database systems are extensively used to store critical data across various
domains. However, the frequency of abnormal database access behaviors, such as
database intrusion by internal and external attacks, continues to rise.
Internal masqueraders often have greater organizational knowledge, making it
easier to mimic employee behavior effectively. In contrast, external
masqueraders may behave differently due to their lack of familiarity with the
organization. Current approaches lack the granularity needed to detect
anomalies at the operational level, frequently misclassifying entire sequences
of operations as anomalies, even though most operations are likely to represent
normal behavior. On the other hand, some anomalous behaviors often resemble
normal activities, making them difficult for existing detection methods to
identify. This paper introduces a two-tiered anomaly detection approach for
Structured Query Language (SQL) using the Bidirectional Encoder Representations
from Transformers (BERT) model, specifically DistilBERT, a more efficient,
pre-trained version. Our method combines both unsupervised and supervised
machine learning techniques to accurately identify anomalous activities while
minimizing the need for data labeling. First, the unsupervised method uses
ensemble anomaly detectors that flag embedding vectors distant from learned
normal patterns of typical user behavior across the database (out-of-scope
queries). Second, the supervised method uses fine-tuned transformer-based
models to detect internal attacks with high precision (in-scope queries), using
role-labeled classification, even on limited labeled SQL data. Our findings
make a significant contribution by providing an effective solution for
safeguarding critical database systems from sophisticated threats.

</details>


### [11] [AuthPrint: Fingerprinting Generative Models Against Malicious Model Providers](https://arxiv.org/abs/2508.05691)
*Kai Yao,Marc Juarez*

Main category: cs.CR

TL;DR: 论文提出了一种在对抗性环境下验证生成模型输出来源的方法，通过提取模型输出空间的秘密指纹并训练验证器，实现了高准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 生成模型在高风险领域应用广泛，但缺乏验证输出来源的机制，尤其是在模型提供者可能对抗的情况下。

Method: 利用可信验证器提取模型输出空间的秘密指纹，并训练模型预测和验证这些指纹。

Result: 在GAN和扩散模型上实现了接近零的FPR@95%TPR，即使对架构和训练数据进行小修改仍保持鲁棒性。

Conclusion: 该方法在对抗性环境下有效验证生成模型输出的来源，具有实际应用潜力。

Abstract: Generative models are increasingly adopted in high-stakes domains, yet
current deployments offer no mechanisms to verify the origin of model outputs.
We address this gap by extending model fingerprinting techniques beyond the
traditional collaborative setting to one where the model provider may act
adversarially. To our knowledge, this is the first work to evaluate
fingerprinting for provenance attribution under such a threat model. The
methods rely on a trusted verifier that extracts secret fingerprints from the
model's output space, unknown to the provider, and trains a model to predict
and verify them. Our empirical evaluation shows that our methods achieve
near-zero FPR@95%TPR for instances of GAN and diffusion models, even when
tested on small modifications to the original architecture and training data.
Moreover, the methods remain robust against adversarial attacks that actively
modify the outputs to bypass detection. Source codes are available at
https://github.com/PSMLab/authprint.

</details>


### [12] [DMFI: Dual-Modality Fine-Tuning and Inference Framework for LLM-Based Insider Threat Detection](https://arxiv.org/abs/2508.05694)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Guanggang Geng,Zhiying Li,Jian Weng*

Main category: cs.CR

TL;DR: DMFI是一个双模态框架，结合语义推理和行为感知微调，用于检测内部威胁，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统模型难以捕捉语义意图和复杂行为动态，现有LLM解决方案在提示适应性和模态覆盖方面有限。

Method: DMFI将原始日志转换为语义视图和行为抽象，使用两个LoRA增强的LLM独立微调，并通过MLP模块融合输出。

Result: 在CERT r4.2和r5.2数据集上，DMFI在检测准确率上优于现有方法。

Conclusion: DMFI结合LLM的语义推理能力和结构化行为建模，为内部威胁检测提供了可扩展且有效的解决方案。

Abstract: Insider threat detection (ITD) poses a persistent and high-impact challenge
in cybersecurity due to the subtle, long-term, and context-dependent nature of
malicious insider behaviors. Traditional models often struggle to capture
semantic intent and complex behavior dynamics, while existing LLM-based
solutions face limitations in prompt adaptability and modality coverage. To
bridge this gap, we propose DMFI, a dual-modality framework that integrates
semantic inference with behavior-aware fine-tuning. DMFI converts raw logs into
two structured views: (1) a semantic view that processes content-rich artifacts
(e.g., emails, https) using instruction-formatted prompts; and (2) a behavioral
abstraction, constructed via a 4W-guided (When-Where-What-Which) transformation
to encode contextual action sequences. Two LoRA-enhanced LLMs are fine-tuned
independently, and their outputs are fused via a lightweight MLP-based decision
module. We further introduce DMFI-B, a discriminative adaptation strategy that
separates normal and abnormal behavior representations, improving robustness
under severe class imbalance. Experiments on CERT r4.2 and r5.2 datasets
demonstrate that DMFI outperforms state-of-the-art methods in detection
accuracy. Our approach combines the semantic reasoning power of LLMs with
structured behavior modeling, offering a scalable and effective solution for
real-world insider threat detection. Our work demonstrates the effectiveness of
combining LLM reasoning with structured behavioral modeling, offering a
scalable and deployable solution for modern insider threat detection.

</details>


### [13] [MambaITD: An Efficient Cross-Modal Mamba Network for Insider Threat Detection](https://arxiv.org/abs/2508.05695)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Zhiying Li,Guanggang Geng,Jian Weng*

Main category: cs.CR

TL;DR: 本文提出了一种基于Mamba状态空间模型和跨模态自适应融合的新框架MambaITD，用于解决企业内部威胁检测中的动态特征建模、计算效率和实时性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法因动态特征建模不足、计算效率低和跨模态信息孤岛问题，无法有效应对内部威胁。

Method: 通过多源日志预处理、Mamba编码器建模长程依赖、门控特征融合机制和自适应阈值优化方法，动态调整决策阈值。

Result: MambaITD在建模效率和特征融合能力上优于传统方法，特别是基于Transformer的方法。

Conclusion: MambaITD为内部威胁检测提供了更有效的解决方案。

Abstract: Enterprises are facing increasing risks of insider threats, while existing
detection methods are unable to effectively address these challenges due to
reasons such as insufficient temporal dynamic feature modeling, computational
efficiency and real-time bottlenecks and cross-modal information island
problem. This paper proposes a new insider threat detection framework MambaITD
based on the Mamba state space model and cross-modal adaptive fusion. First,
the multi-source log preprocessing module aligns heterogeneous data through
behavioral sequence encoding, interval smoothing, and statistical feature
extraction. Second, the Mamba encoder models long-range dependencies in
behavioral and interval sequences, and combines the sequence and statistical
information dynamically in combination with the gated feature fusion mechanism.
Finally, we propose an adaptive threshold optimization method based on
maximizing inter-class variance, which dynamically adjusts the decision
threshold by analyzing the probability distribution, effectively identifies
anomalies, and alleviates class imbalance and concept drift. Compared with
traditional methods, MambaITD shows significant advantages in modeling
efficiency and feature fusion capabilities, outperforming Transformer-based
methods, and provides a more effective solution for insider threat detection.

</details>


### [14] [Log2Sig: Frequency-Aware Insider Threat Detection via Multivariate Behavioral Signal Decomposition](https://arxiv.org/abs/2508.05696)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Zhiying Li,Guanggang Geng*

Main category: cs.CR

TL;DR: Log2Sig是一个新颖的异常检测框架，通过将用户日志转化为多变量行为频率信号，并利用多尺度分解和双视图表示，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将系统日志建模为扁平事件序列，无法捕捉用户行为的多尺度动态和扰动模式，导致检测效果受限。

Method: Log2Sig通过MVMD提取多尺度行为波动，结合Mamba编码器捕捉长期依赖，并将频率信号与行为序列融合，构建综合用户行为特征。

Result: 在CERT r4.2和r5.2数据集上，Log2Sig在准确率和F1分数上均显著优于现有方法。

Conclusion: Log2Sig通过多尺度分析和双视图融合，有效解决了现有方法的局限性，为内部威胁检测提供了新思路。

Abstract: Insider threat detection presents a significant challenge due to the
deceptive nature of malicious behaviors, which often resemble legitimate user
operations. However, existing approaches typically model system logs as flat
event sequences, thereby failing to capture the inherent frequency dynamics and
multiscale disturbance patterns embedded in user behavior. To address these
limitations, we propose Log2Sig, a robust anomaly detection framework that
transforms user logs into multivariate behavioral frequency signals,
introducing a novel representation of user behavior. Log2Sig employs
Multivariate Variational Mode Decomposition (MVMD) to extract Intrinsic Mode
Functions (IMFs), which reveal behavioral fluctuations across multiple temporal
scales. Based on this, the model further performs joint modeling of behavioral
sequences and frequency-decomposed signals: the daily behavior sequences are
encoded using a Mamba-based temporal encoder to capture long-term dependencies,
while the corresponding frequency components are linearly projected to match
the encoder's output dimension. These dual-view representations are then fused
to construct a comprehensive user behavior profile, which is fed into a
multilayer perceptron for precise anomaly detection. Experimental results on
the CERT r4.2 and r5.2 datasets demonstrate that Log2Sig significantly
outperforms state-of-the-art baselines in both accuracy and F1 score.

</details>


### [15] [System Security Framework for 5G Advanced /6G IoT Integrated Terrestrial Network-Non-Terrestrial Network (TN-NTN) with AI-Enabled Cloud Security](https://arxiv.org/abs/2508.05707)
*Sasa Maric,Rasil Baidar,Robert Abbas,Sam Reisenfeld*

Main category: cs.CR

TL;DR: 本文提出了一种基于AI原生云安全的新系统级安全框架，用于5G Advanced/6G物联网集成的TN-NTN架构，解决异构网络的实时威胁检测和智能策略执行问题。


<details>
  <summary>Details</summary>
Motivation: 由于TN-NTN架构的异构性、规模和分布式特性，新的安全挑战出现，需要结合AI和云技术提升安全性。

Method: 利用AI原生云平台实现实时威胁检测、安全自动化和智能策略执行，并通过NTN卫星接入功能增强安全性。

Result: 提出了一个全面的AI驱动的云安全框架，支持零信任原则、联邦学习、安全编排和分层安全架构。

Conclusion: 未来5G Advanced/6G物联网网络应实施AI驱动的卫星NTN，以增强安全性和对抗性威胁的韧性。

Abstract: The integration of Terrestrial Networks (TN) and Non-Terrestrial Networks
(NTN), including 5G Advanced/6G and the Internet of Things (IoT) technologies,
using Low Earth Orbit (LEO) satellites, high-altitude platforms (HAPS), and
Unmanned Aerial Vehicles (UAVs), is redefining the landscape of global
connectivity. This paper introduces a new system-level security framework for
5G Advanced/6G IoT-integrated TN-NTN architectures with AI-native-enabled cloud
security. Due to the heterogeneity, scale, and distributed nature of these
networks, new security challenges have emerged. Leveraging AI-native cloud
platforms offers powerful capabilities for real-time threat detection, security
automation, and intelligent policy enforcement. The NTN satellite access
function enhances security for discontinuous coverage via satellite
connections. In addition, this paper explores the security risks associated
with integrated 5G Advanced/6G IoT TN-NTN systems, including full network
segmentation, network slicing, and the cloudification of the RAN and core. We
present a comprehensive AI-enabled cloud security framework and conclude with
proposals for implementing AI-powered, satellite-based NTN within future 5G
Advanced/6G IoT networks. Our approach emphasizes zero-trust principles,
federated learning, secure orchestration, a layered security framework, and
resilience against adversarial threats.

</details>


### [16] [On Digital Twins in Defence: Overview and Applications](https://arxiv.org/abs/2508.05717)
*Marco Giberna,Holger Voos,Paulo Tavares,João Nunes,Tobias Sorg,Andrea Masini,Jose Luis Sanchez-Lopez*

Main category: cs.CR

TL;DR: 本文探讨了数字孪生技术在国防领域的应用，提出了标准化框架，并分析了实施中的挑战与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 数字孪生技术因其在实时监控、优化和模拟方面的潜力，在国防领域具有广泛应用前景。

Method: 通过文献分析、行业实践、政府策略及利益相关者调查，研究数字孪生在国防中的应用与挑战。

Result: 数字孪生可提升操作性能、预测能力和系统运行时间，但需解决标准化和互操作性等问题。

Conclusion: 未来需加强框架建设和跨学科合作，以充分发挥数字孪生在国防领域的潜力。

Abstract: Digital twin technology has gained increasing attention across various
sectors due to its ability to create virtual replicas of physical systems,
enabling real-time monitoring, optimization, and simulation. This paper
explores the integration of digital twins within defence applications, focusing
on key use cases ranging from system design and development, operational
planning and training, to mission execution and debriefing. By examining the
application of digital twin technologies across defense platforms, we highlight
their key advantages such as enhanced operational performance, predictive
capabilities, and increased system uptime. Additionally, we introduce a novel
characterization framework for digital twins that aims to standardize and unify
their application across different defence domains to facilitate
interoperability. Thereafter, we discuss the main challenges, gaps and
limitations in implementing and adopting digital twins within defence
organizations by analyzing a combination of scientific literature, current
industry practices, governmental strategies, and the findings from a
comprehensive survey of industrial stakeholders and ministries of defense.
Finally, we outline future research directions and development opportunities,
emphasizing the need for robust frameworks and interdisciplinary collaborations
to fully realize the potential of digital twins in the defence sector.

</details>


### [17] [Secure and Scalable Blockchain Voting: A Comparative Framework and the Role of Large Language Models](https://arxiv.org/abs/2508.05865)
*Kiana Kiashemshaki,Elvis Nnaemeka Chukwuani,Mohammad Jalili Torkamani,Negin Mahmoudi*

Main category: cs.CR

TL;DR: 论文提出了一种分析区块链电子投票系统的框架，探讨了共识机制和密码协议的优化策略，并引入大语言模型（LLM）提升智能合约生成与验证。


<details>
  <summary>Details</summary>
Motivation: 区块链技术为电子投票系统提供了透明、去中心化和安全的潜力，但实际应用中仍面临扩展性、计算需求和隐私问题。

Method: 通过比较分析区块链架构、共识机制和密码协议，提出混合共识、轻量级密码学和去中心化身份管理等优化策略，并探索LLM在智能合约生成和异常检测中的应用。

Result: 研究为设计安全、可扩展且智能的区块链电子投票系统提供了基础，支持国家级部署。

Conclusion: 论文为构建端到端区块链电子投票原型提供了框架，结合LLM和模拟分析，推动实际应用。

Abstract: Blockchain technology offers a promising foundation for modernizing E-Voting
systems by enhancing transparency, decentralization, and security. Yet,
real-world adoption remains limited due to persistent challenges such as
scalability constraints, high computational demands, and complex privacy
requirements. This paper presents a comparative framework for analyzing
blockchain-based E-Voting architectures, consensus mechanisms, and
cryptographic protocols. We examine the limitations of prevalent models like
Proof of Work, Proof of Stake, and Delegated Proof of Stake, and propose
optimization strategies that include hybrid consensus, lightweight
cryptography, and decentralized identity management. Additionally, we explore
the novel role of Large Language Models (LLMs) in smart contract generation,
anomaly detection, and user interaction. Our findings offer a foundation for
designing secure, scalable, and intelligent blockchain-based E-Voting systems
suitable for national-scale deployment. This work lays the groundwork for
building an end-to-end blockchain E-Voting prototype enhanced by LLM-guided
smart contract generation and validation, supported by a systematic framework
and simulation-based analysis.

</details>


### [18] [Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System](https://arxiv.org/abs/2508.06059)
*Haorui He,Yupeng Li,Bin Benjamin Zhu,Dacheng Wen,Reynold Cheng,Francis C. M. Lau*

Main category: cs.CR

TL;DR: Fact2Fiction是一种针对基于LLM的事实核查系统的首个投毒攻击框架，通过模仿分解策略和利用系统生成的解释来破坏子声明验证，攻击成功率比现有方法高8.9%--21.2%。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的事实核查系统安全性不足，容易被攻击者利用放大虚假信息，因此需要研究其安全弱点。

Method: Fact2Fiction模仿事实核查系统的分解策略，并利用系统生成的解释来制作恶意证据，破坏子声明验证过程。

Result: 实验表明，Fact2Fiction在不同投毒预算下，攻击成功率比现有方法高8.9%--21.2%。

Conclusion: Fact2Fiction揭示了当前事实核查系统的安全漏洞，强调了防御措施的必要性。

Abstract: State-of-the-art fact-checking systems combat misinformation at scale by
employing autonomous LLM-based agents to decompose complex claims into smaller
sub-claims, verify each sub-claim individually, and aggregate the partial
results to produce verdicts with justifications (explanatory rationales for the
verdicts). The security of these systems is crucial, as compromised
fact-checkers, which tend to be easily underexplored, can amplify
misinformation. This work introduces Fact2Fiction, the first poisoning attack
framework targeting such agentic fact-checking systems. Fact2Fiction mirrors
the decomposition strategy and exploits system-generated justifications to
craft tailored malicious evidences that compromise sub-claim verification.
Extensive experiments demonstrate that Fact2Fiction achieves 8.9\%--21.2\%
higher attack success rates than state-of-the-art attacks across various
poisoning budgets. Fact2Fiction exposes security weaknesses in current
fact-checking systems and highlights the need for defensive countermeasures.

</details>


### [19] [A Game-Theoretic Foundation for Bitcoin's Price: A Security-Utility Equilibrium](https://arxiv.org/abs/2508.06071)
*Liang Chen*

Main category: cs.CR

TL;DR: 本文提出了一种结构化的博弈论模型（RESUNE）来评估比特币等去中心化数字资产的价值，通过市场均衡价格和哈希率的关系内生决定网络安全，并证明了均衡的存在性。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖投机信念评估数字资产价值，本文旨在通过理性预期和安全-效用纳什均衡（RESUNE）提供更结构化的框架。

Method: 采用RESUNE框架，结合自由进入挖矿模型和全局博弈模型，分析价格、哈希率和网络安全的关系。

Result: 证明了RESUNE的存在性，并预测价格对需求的直接影响需超过其对安全的反馈效应才能稳定。模型还解释了比特币价格对哈希率的单向因果关系。

Conclusion: 该模型为数字资产价值提供了结构化分析工具，并提出了可测试的预测，如协议减半对哈希率和价格的影响。

Abstract: This paper introduces a structural game-theoretic model to value
decentralized digital assets like Bitcoin. Instead of relying on speculative
beliefs, it frames the asset's price within a Rational-Expectations
Security-Utility Nash Equilibrium (RESUNE). This equilibrium is a fixed point
where the market-clearing price dictates the hash rate through a free-entry
mining model, which in turn endogenously sets the network's security. The
security, defined as one minus the probability of a 51% attack, is determined
via a global games model of attacker coordination, providing a unique and
continuous security function. We prove the existence of a RESUNE and offer
conditions for its uniqueness and stability. The model predicts that the
stabilizing direct effect of price on demand must outweigh the potentially
destabilizing feedback from price to security. The framework generates testable
predictions, such as a protocol halving causing a contraction in both hash rate
and price. A structural Vector Autoregression (VAR) model is proposed to test
this mechanism. The model decomposes Bitcoin's value into transactional
utility, security, and speculative components and explains the observed
unidirectional causality from price to hash rate.

</details>


### [20] [ProvX: Generating Counterfactual-Driven Attack Explanations for Provenance-Based Detection](https://arxiv.org/abs/2508.06073)
*Weiheng Wu,Wei Qiao,Teng Li,Yebo Feng,Zhuo Ma,Jianfeng Ma,Yang Liu*

Main category: cs.CR

TL;DR: ProvX是一个解释框架，用于解释基于图神经网络的入侵检测系统，通过反事实逻辑和优化任务提供可验证的解释，并提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决GNN模型的黑盒问题，为安全分析师提供可验证的解释和攻击相关证据。

Method: 引入反事实解释逻辑，将离散搜索问题转化为连续优化任务，并结合阶段性固化策略。

Result: 实验显示ProvX能定位关键图结构，解释必要性达51.59%，优于现有方法，并能指导模型优化。

Conclusion: ProvX有效解决了GNN模型的解释性问题，并展示了其在闭环检测-解释-反馈框架中的潜力。

Abstract: Provenance graph-based intrusion detection systems are deployed on hosts to
defend against increasingly severe Advanced Persistent Threat. Using Graph
Neural Networks to detect these threats has become a research focus and has
demonstrated exceptional performance. However, the widespread adoption of
GNN-based security models is limited by their inherent black-box nature, as
they fail to provide security analysts with any verifiable explanations for
model predictions or any evidence regarding the model's judgment in relation to
real-world attacks. To address this challenge, we propose ProvX, an effective
explanation framework for exlaining GNN-based security models on provenance
graphs. ProvX introduces counterfactual explanation logic, seeking the minimal
structural subset within a graph predicted as malicious that, when perturbed,
can subvert the model's original prediction. We innovatively transform the
discrete search problem of finding this critical subgraph into a continuous
optimization task guided by a dual objective of prediction flipping and
distance minimization. Furthermore, a Staged Solidification strategy is
incorporated to enhance the precision and stability of the explanations. We
conducted extensive evaluations of ProvX on authoritative datasets. The
experimental results demonstrate that ProvX can locate critical graph
structures that are highly relevant to real-world attacks and achieves an
average explanation necessity of 51.59\%, with these metrics outperforming
current SOTA explainers. Furthermore, we explore and provide a preliminary
validation of a closed-loop Detection-Explanation-Feedback enhancement
framework, demonstrating through experiments that the explanation results from
ProvX can guide model optimization, effectively enhancing its robustness
against adversarial attacks.

</details>


### [21] [Adaptive Backtracking for Privacy Protection in Large Language Models](https://arxiv.org/abs/2508.06087)
*Zhihao Yao,Yuxuan Gu,Xiachong Feng,Weitao Ma,Bo Li,Xiaocheng Feng*

Main category: cs.CR

TL;DR: 论文提出了一种面向企业的隐私保护方法ABack，解决了现有方法导致模型性能下降和缺乏公开数据集的问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前隐私保护研究主要关注用户隐私，忽视了企业数据泄露风险，尤其是在检索增强生成范式下。

Method: 提出ABack机制，利用隐藏状态模型定位泄露意图并安全重写输出；构建PriGenQA数据集用于评估。

Result: ABack在对抗强适应性攻击者时，隐私效用分数提升15%，避免了性能折衷。

Conclusion: ABack有效解决了企业隐私保护问题，同时避免了性能损失。

Abstract: The preservation of privacy has emerged as a critical topic in the era of
artificial intelligence. However, current work focuses on user-oriented
privacy, overlooking severe enterprise data leakage risks exacerbated by the
Retrieval-Augmented Generation paradigm. To address this gap, our paper
introduces a novel objective: enterprise-oriented privacy concerns. Achieving
this objective requires overcoming two fundamental challenges: existing methods
such as data sanitization severely degrade model performance, and the field
lacks public datasets for evaluation. We address these challenges with several
solutions. (1) To prevent performance degradation, we propose ABack, a
training-free mechanism that leverages a Hidden State Model to pinpoint the
origin of a leakage intention and rewrite the output safely. (2) To solve the
lack of datasets, we construct PriGenQA, a new benchmark for enterprise privacy
scenarios in healthcare and finance. To ensure a rigorous evaluation, we move
beyond simple static attacks by developing a powerful adaptive attacker with
Group Relative Policy Optimization. Experiments show that against this superior
adversary, ABack improves the overall privacy utility score by up to 15\% over
strong baselines, avoiding the performance trade-offs of prior methods.

</details>


### [22] [Simulation in Cybersecurity: Understanding Techniques, Applications, and Goals](https://arxiv.org/abs/2508.06106)
*Luca Serena,Gabriele D'Angelo,Stefano Ferretti,Moreno Marzolla*

Main category: cs.CR

TL;DR: 本文综述了网络安全研究中建模与模拟的现状，通过四个维度分类现有研究，分析了不同方法的优缺点，并探讨了最适合模拟研究的网络威胁和建模范式。


<details>
  <summary>Details</summary>
Motivation: 网络安全研究中建模与模拟的应用广泛，但由于应用领域、攻击场景和目标的多样性，难以识别方法论趋势。现有综述多局限于特定技术或领域，缺乏整体视角。

Method: 通过四个维度（应用领域、网络威胁类型、模拟技术、模拟目标）对现有研究进行分类，并分析不同方法的优缺点。

Result: 确定了最适合模拟研究的网络威胁类型，并分析了不同建模范式在特定网络安全挑战中的适用性。

Conclusion: 综述为网络安全模拟研究提供了全面的方法论指导，帮助研究者选择适合的建模技术和模拟目标。

Abstract: Modeling and simulation are widely used in cybersecurity research to assess
cyber threats, evaluate defense mechanisms, and analyze vulnerabilities.
However, the diversity of application areas, the variety of cyberattacks
scenarios, and the differing objectives of these simulations makes it difficult
to identify methodological trends. Existing reviews often focus on specific
modeling techniques or application domains, making it challenging to analyze
the field as a whole. To address these limitations, we present a comprehensive
review of the current state of the art, classifying the selected papers based
on four dimensions: the application domain, the types of cyber threats
represented, the simulation techniques employed, and the primary goals of the
simulation. The review discusses the strengths and limitations of different
approaches, identifies which cyber threats are the most suited for
simulation-based investigations, and analyzes which modeling paradigms are most
appropriate for specific cybersecurity challenges.

</details>


### [23] [SLIP: Soft Label Mechanism and Key-Extraction-Guided CoT-based Defense Against Instruction Backdoor in APIs](https://arxiv.org/abs/2508.06153)
*Zhengxian Wu,Juan Wen,Wanli Peng,Haowei Chang,Yinghan Zhou,Yiming Xue*

Main category: cs.CR

TL;DR: SLIP是一种针对黑盒后门攻击的防御机制，通过关键提取引导的思维链（KCoT）和软标签机制（SLM）有效降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 随着定制化大语言模型（LLM）代理的发展，黑盒后门攻击威胁加剧，现有防御机制难以应对，亟需新的解决方案。

Method: 提出SLIP，结合KCoT提取任务相关关键词，并通过SLM量化关键词与候选答案的语义相关性，排除异常分数以提高可靠性。

Result: 实验表明，SLIP将攻击成功率从90.2%降至25.13%，同时在干净数据上保持高准确率，优于现有防御方法。

Conclusion: SLIP为LLM代理的后门攻击防御提供了高效且可靠的解决方案。

Abstract: With the development of customized large language model (LLM) agents, a new
threat of black-box backdoor attacks has emerged, where malicious instructions
are injected into hidden system prompts. These attacks easily bypass existing
defenses that rely on white-box access, posing a serious security challenge. To
address this, we propose SLIP, a Soft Label mechanism and key-extraction-guided
CoT-based defense against Instruction backdoors in APIs. SLIP is designed based
on two key insights. First, to counteract the model's oversensitivity to
triggers, we propose a Key-extraction-guided Chain-of-Thought (KCoT). Instead
of only considering the single trigger or the input sentence, KCoT prompts the
agent to extract task-relevant key phrases. Second, to guide the LLM toward
correct answers, our proposed Soft Label Mechanism (SLM) prompts the agent to
quantify the semantic correlation between key phrases and candidate answers.
Crucially, to mitigate the influence of residual triggers or misleading content
in phrases extracted by KCoT, which typically causes anomalous scores, SLM
excludes anomalous scores deviating significantly from the mean and
subsequently averages the remaining scores to derive a more reliable semantic
representation. Extensive experiments on classification and question-answer
(QA) tasks demonstrate that SLIP is highly effective, reducing the average
attack success rate (ASR) from 90.2% to 25.13% while maintaining high accuracy
on clean data and outperforming state-of-the-art defenses. Our code are
available in
https://github.com/CAU-ISS-Lab/Backdoor-Attack-Defense-LLMs/tree/main/SLIP.

</details>


### [24] [Anti-Tamper Protection for Unauthorized Individual Image Generation](https://arxiv.org/abs/2508.06325)
*Zelin Li,Ruohan Zong,Yifan Liu,Ruichen Yao,Yaokun Liu,Yang Zhang,Dong Wang*

Main category: cs.CR

TL;DR: 提出了一种名为Anti-Tamper Perturbation (ATP)的新方法，通过结合保护扰动和授权扰动，有效防御伪造攻击并检测净化篡改。


<details>
  <summary>Details</summary>
Motivation: 随着个性化图像生成技术的发展，伪造攻击侵犯肖像权和隐私的问题日益严重，现有保护扰动算法易被净化技术绕过。

Method: ATP在频域中引入保护扰动和授权扰动，保护扰动防御伪造攻击，授权扰动检测净化篡改，并通过掩码确保两者互不干扰。

Result: 实验表明ATP在各种攻击场景下均能有效防御伪造攻击，保护肖像权和隐私。

Conclusion: ATP为防御伪造攻击提供了鲁棒解决方案，代码已开源。

Abstract: With the advancement of personalized image generation technologies, concerns
about forgery attacks that infringe on portrait rights and privacy are growing.
To address these concerns, protection perturbation algorithms have been
developed to disrupt forgery generation. However, the protection algorithms
would become ineffective when forgery attackers apply purification techniques
to bypass the protection. To address this issue, we present a novel approach,
Anti-Tamper Perturbation (ATP). ATP introduces a tamper-proof mechanism within
the perturbation. It consists of protection and authorization perturbations,
where the protection perturbation defends against forgery attacks, while the
authorization perturbation detects purification-based tampering. Both
protection and authorization perturbations are applied in the frequency domain
under the guidance of a mask, ensuring that the protection perturbation does
not disrupt the authorization perturbation. This design also enables the
authorization perturbation to be distributed across all image pixels,
preserving its sensitivity to purification-based tampering. ATP demonstrates
its effectiveness in defending forgery attacks across various attack settings
through extensive experiments, providing a robust solution for protecting
individuals' portrait rights and privacy. Our code is available at:
https://github.com/Seeyn/Anti-Tamper-Perturbation .

</details>


### [25] [When AIOps Become "AI Oops": Subverting LLM-driven IT Operations via Telemetry Manipulation](https://arxiv.org/abs/2508.06394)
*Dario Pasquini,Evgenios M. Kornaropoulos,Giuseppe Ateniese,Omer Akgul,Athanasios Theocharis,Petros Efstathopoulos*

Main category: cs.CR

TL;DR: 本文首次对AIOps解决方案进行安全分析，揭示其易受攻击性，并提出防御机制AIOpsShield。


<details>
  <summary>Details</summary>
Motivation: AIOps通过自动化提升运维效率，但其安全性尚未被充分研究，可能成为新的攻击向量。

Method: 提出AIOpsDoom攻击方法，利用错误诱导请求操纵遥测数据，误导AIOps代理；设计AIOpsShield防御机制，净化遥测数据。

Result: 实验证明AIOpsShield能有效防御攻击且不影响代理性能。

Conclusion: AIOps可能成为系统攻击的新途径，亟需安全设计。

Abstract: AI for IT Operations (AIOps) is transforming how organizations manage complex
software systems by automating anomaly detection, incident diagnosis, and
remediation. Modern AIOps solutions increasingly rely on autonomous LLM-based
agents to interpret telemetry data and take corrective actions with minimal
human intervention, promising faster response times and operational cost
savings.
  In this work, we perform the first security analysis of AIOps solutions,
showing that, once again, AI-driven automation comes with a profound security
cost. We demonstrate that adversaries can manipulate system telemetry to
mislead AIOps agents into taking actions that compromise the integrity of the
infrastructure they manage. We introduce techniques to reliably inject
telemetry data using error-inducing requests that influence agent behavior
through a form of adversarial reward-hacking; plausible but incorrect system
error interpretations that steer the agent's decision-making. Our attack
methodology, AIOpsDoom, is fully automated--combining reconnaissance, fuzzing,
and LLM-driven adversarial input generation--and operates without any prior
knowledge of the target system.
  To counter this threat, we propose AIOpsShield, a defense mechanism that
sanitizes telemetry data by exploiting its structured nature and the minimal
role of user-generated content. Our experiments show that AIOpsShield reliably
blocks telemetry-based attacks without affecting normal agent performance.
  Ultimately, this work exposes AIOps as an emerging attack vector for system
compromise and underscores the urgent need for security-aware AIOps design.

</details>


### [26] [ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls](https://arxiv.org/abs/2508.06457)
*Sanket Badhe*

Main category: cs.CR

TL;DR: ScamAgent是一个基于LLM的多轮对话代理，能够生成高度逼真的诈骗脚本，绕过现有安全机制，凸显了对多轮安全审计的需求。


<details>
  <summary>Details</summary>
Motivation: 研究LLM被滥用的潜在风险，尤其是多轮对话场景下的诈骗行为。

Method: 开发ScamAgent，利用LLM的记忆和动态适应能力生成诈骗脚本，并通过文本转语音技术实现自动化诈骗流程。

Result: 现有LLM安全机制对ScamAgent无效，需开发新的多轮安全审计和控制框架。

Conclusion: 生成式AI驱动的对话欺骗需新的检测和阻断方法，以应对多轮代理威胁。

Abstract: Large Language Models (LLMs) have demonstrated impressive fluency and
reasoning capabilities, but their potential for misuse has raised growing
concern. In this paper, we present ScamAgent, an autonomous multi-turn agent
built on top of LLMs, capable of generating highly realistic scam call scripts
that simulate real-world fraud scenarios. Unlike prior work focused on
single-shot prompt misuse, ScamAgent maintains dialogue memory, adapts
dynamically to simulated user responses, and employs deceptive persuasion
strategies across conversational turns. We show that current LLM safety
guardrails, including refusal mechanisms and content filters, are ineffective
against such agent-based threats. Even models with strong prompt-level
safeguards can be bypassed when prompts are decomposed, disguised, or delivered
incrementally within an agent framework. We further demonstrate the
transformation of scam scripts into lifelike voice calls using modern
text-to-speech systems, completing a fully automated scam pipeline. Our
findings highlight an urgent need for multi-turn safety auditing, agent-level
control frameworks, and new methods to detect and disrupt conversational
deception powered by generative AI.

</details>


### [27] [Voting-Based Semi-Parallel Proof-of-Work Protocol](https://arxiv.org/abs/2508.06489)
*Mustafa Doger,Sennur Ulukus*

Main category: cs.CR

TL;DR: 本文分析了现有并行PoW协议的安全性问题，并提出了一种投票半并行PoW协议，其在多个方面优于Nakamoto共识和现有并行PoW协议。


<details>
  <summary>Details</summary>
Motivation: 研究并行PoW协议的安全性和性能问题，以改进Nakamoto共识的安全性、吞吐量和延迟。

Method: 通过理论分析和模拟评估现有并行PoW协议的脆弱性，并提出一种投票半并行PoW协议，使用MDP模型验证其抗攻击能力。

Result: 现有并行PoW协议更容易受到激励攻击，而新协议在通信开销、吞吐量、激励兼容性和费用分配等方面表现更优。

Conclusion: 投票半并行PoW协议在安全性和性能上优于现有方案，具有更高的实用价值。

Abstract: Parallel Proof-of-Work (PoW) protocols are suggested to improve the safety
guarantees, transaction throughput and confirmation latencies of Nakamoto
consensus. In this work, we first consider the existing parallel PoW protocols
and develop hard-coded incentive attack structures. Our theoretical results and
simulations show that the existing parallel PoW protocols are more vulnerable
to incentive attacks than the Nakamoto consensus, e.g., attacks have smaller
profitability threshold and they result in higher relative rewards. Next, we
introduce a voting-based semi-parallel PoW protocol that outperforms both
Nakamoto consensus and the existing parallel PoW protocols from most practical
perspectives such as communication overheads, throughput, transaction
conflicts, incentive compatibility of the protocol as well as a fair
distribution of transaction fees among the voters and the leaders. We use
state-of-the-art analysis to evaluate the consistency of the protocol and
consider Markov decision process (MDP) models to substantiate our claims about
the resilience of our protocol against incentive attacks.

</details>
