<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 34]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Sell Data to AI Algorithms Without Revealing It: Secure Data Valuation and Sharing via Homomorphic Encryption](https://arxiv.org/abs/2512.06033)
*Michael Yang,Ruijiang Gao,Zhiqiang,Zheng*

Main category: cs.CR

TL;DR: TIP协议通过同态加密和梯度影响函数，让买家在不解密原始数据的情况下评估外部数据集对AI模型的效用价值，解决了数据市场的价值-隐私困境。


<details>
  <summary>Details</summary>
Motivation: AI发展面临数据市场的根本性摩擦：价值-隐私困境（Arrow信息悖论）。买家无法在不检查数据的情况下验证数据集效用，但检查又会暴露数据隐私，这阻碍了数据市场的有效运作。

Method: 提出可信影响协议(TIP)，结合同态加密和基于梯度的影响函数，使买家能够针对特定AI模型对数据点进行精确的盲评分。针对大语言模型的可扩展性，采用低秩梯度投影降低计算开销，同时保持与明文基准的接近完美保真度。

Result: 在医疗和生成式AI领域的实证模拟验证了框架的经济潜力：加密估值信号与实现的临床效用高度相关，并揭示了预训练语料库中数据价值的重尾分布——少数文本驱动能力而多数文本降低能力。

Conclusion: 这些发现挑战了现行的固定费率补偿模式，为建立基于功绩的、安全的数据经济提供了可扩展的技术基础，解决了数据市场的价值-隐私困境。

Abstract: The rapid expansion of Artificial Intelligence is hindered by a fundamental friction in data markets: the value-privacy dilemma, where buyers cannot verify a dataset's utility without inspection, yet inspection may expose the data (Arrow's Information Paradox). We resolve this challenge by introducing the Trustworthy Influence Protocol (TIP), a privacy-preserving framework that enables prospective buyers to quantify the utility of external data without ever decrypting the raw assets. By integrating Homomorphic Encryption with gradient-based influence functions, our approach allows for the precise, blinded scoring of data points against a buyer's specific AI model. To ensure scalability for Large Language Models (LLMs), we employ low-rank gradient projections that reduce computational overhead while maintaining near-perfect fidelity to plaintext baselines, as demonstrated across BERT and GPT-2 architectures. Empirical simulations in healthcare and generative AI domains validate the framework's economic potential: we show that encrypted valuation signals achieve a high correlation with realized clinical utility and reveal a heavy-tailed distribution of data value in pre-training corpora where a minority of texts drive capability while the majority degrades it. These findings challenge prevailing flat-rate compensation models and offer a scalable technical foundation for a meritocratic, secure data economy.

</details>


### [2] [The Road of Adaptive AI for Precision in Cybersecurity](https://arxiv.org/abs/2512.06048)
*Sahil Garg*

Main category: cs.CR

TL;DR: 论文分享了在网络安全领域设计、构建和运营生产级GenAI管道的实践经验，重点探讨了持续适应机制以应对不断变化的知识库、工具和威胁。


<details>
  <summary>Details</summary>
Motivation: 网络安全日益复杂，为AI研究和实践带来独特挑战与机遇。作者希望通过分享实际部署经验，为AI从业者和行业利益相关者提供可操作的指导，帮助他们在网络安全GenAI前沿领域导航。

Method: 基于真实世界部署经验，提出利用检索级和模型级适应机制的实用指南和最佳实践，探讨不同适应机制在端到端系统中的互补作用。

Result: 提供了从实际部署中提炼的实用指导，提出了利用检索和模型级适应的最佳实践，并强调了使GenAI在网络安全防御中更鲁棒、精确和可审计的开放研究方向。

Conclusion: 网络安全中的GenAI需要持续适应机制来应对快速变化的环境，检索级和模型级适应机制在端到端系统中相互补充，未来需要进一步研究使GenAI在网络安全防御中更鲁棒、精确和可审计。

Abstract: Cybersecurity's evolving complexity presents unique challenges and opportunities for AI research and practice. This paper shares key lessons and insights from designing, building, and operating production-grade GenAI pipelines in cybersecurity, with a focus on the continual adaptation required to keep pace with ever-shifting knowledge bases, tooling, and threats. Our goal is to provide an actionable perspective for AI practitioners and industry stakeholders navigating the frontier of GenAI for cybersecurity, with particular attention to how different adaptation mechanisms complement each other in end-to-end systems. We present practical guidance derived from real-world deployments, propose best practices for leveraging retrieval- and model-level adaptation, and highlight open research directions for making GenAI more robust, precise, and auditable in cyber defense.

</details>


### [3] [Sift or Get Off the PoC: Applying Information Retrieval to Vulnerability Research with SiftRank](https://arxiv.org/abs/2512.06155)
*Caleb Gross*

Main category: cs.CR

TL;DR: SiftRank：利用LLM作为通用排序器，通过列表式排序、拐点收敛检测和迭代精炼，实现O(n)复杂度的安全研究优先级排序算法，可直接处理数千个项目。


<details>
  <summary>Details</summary>
Motivation: 安全研究面临资源限制和优先级排序问题，攻击面太广而分析资源有限。最有效的安全研究人员通常擅长直观选择攻击面的哪个部分进行调查。本文旨在将选择最有前景选项的问题重新定义为信息检索问题。

Method: 提出SiftRank排序算法，采用三种关键机制：1) 使用LLM对小批量（约10个项目）进行列表式排序；2) 基于拐点的收敛检测，当分数分布稳定时自适应终止排序；3) 迭代精炼，逐步将排序努力集中在最相关文档上。算法直接处理数千个项目，每个文档在多个随机批次中评估以减轻LLM判断不一致性。

Result: 在N-day漏洞分析中成功应用，从剥离二进制固件补丁中的2,197个更改函数中识别出漏洞修复函数，耗时99秒，推理成本0.82美元。算法无需专门的检索步骤，可直接处理大规模数据集。

Conclusion: SiftRank为受手动分析限制的安全优先级排序问题提供了可扩展解决方案，仅需标准LLM API访问，无需专用基础设施、嵌入或领域特定微调。该方法将安全研究中的优先级排序问题重新定义为信息检索问题，并通过LLM驱动的排序算法有效解决。

Abstract: Security research is fundamentally a problem of resource constraint and consequent prioritization. There is simply too much attack surface and too little time and energy to spend analyzing it all. The most effective security researchers are often those who are most skilled at intuitively deciding which part of an expansive attack surface to investigate. We demonstrate that this problem of selecting the most promising option from among many possibilities can be reframed as an information retrieval problem, and solved using document ranking techniques with LLMs performing the heavy lifting as general-purpose rankers. We present SiftRank, a ranking algorithm achieving O(n) complexity through three key mechanisms: listwise ranking using an LLM to order documents in small batches of approximately 10 items at a time; inflection-based convergence detection that adaptively terminates ranking when score distributions have stabilized; and iterative refinement that progressively focuses ranking effort on the most relevant documents. Unlike existing reranking approaches that require a separate first-stage retrieval step to narrow datasets to approximately 100 candidates, SiftRank operates directly on thousands of items, with each document evaluated across multiple randomized batches to mitigate inconsistent judgments by an LLM. We demonstrate practical effectiveness on N-day vulnerability analysis, successfully identifying a vulnerability-fixing function among 2,197 changed functions in a stripped binary firmware patch within 99 seconds at an inference cost of $0.82. Our approach enables scalable security prioritization for problems that are generally constrained by manual analysis, requiring only standard LLM API access without specialized infrastructure, embedding, or domain-specific fine-tuning. An open-source implementation of SiftRank may be found at https://github.com/noperator/siftrank.

</details>


### [4] [DEFEND: Poisoned Model Detection and Malicious Client Exclusion Mechanism for Secure Federated Learning-based Road Condition Classification](https://arxiv.org/abs/2512.06172)
*Sheng Liu,Panos Papadimitratos*

Main category: cs.CR

TL;DR: DEFEND：一种针对联邦学习中目标标签翻转攻击的防御机制，通过神经元幅度分析和GMM聚类检测恶意模型，排除恶意客户端，在攻击下保持与无攻击场景相同的性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在智能交通系统（ITS）中应用广泛，特别是道路状况分类任务。然而，开放协作使系统易受目标标签翻转攻击威胁，现有防御措施无法在攻击下维持模型性能，因为缺乏针对TLFA的检测机制且未排除恶意客户端。

Method: 提出DEFEND防御机制：1）基于神经元幅度分析的污染模型检测策略，识别攻击目标；2）使用高斯混合模型聚类分析；3）每轮丢弃污染模型贡献；4）自适应调整客户端评分，最终排除恶意客户端。

Result: 在多种FL-RCC模型和任务上的广泛评估显示，DEFEND能有效抵御TLFA，性能至少比7个基线防御方法提升15.78%，在攻击下能达到与无攻击场景相同的性能水平。

Conclusion: DEFEND通过针对性的污染模型检测和恶意客户端排除机制，填补了现有防御措施的研究空白，能在目标标签翻转攻击下保持联邦学习系统的安全性和性能。

Abstract: Federated Learning (FL) has drawn the attention of the Intelligent Transportation Systems (ITS) community. FL can train various models for ITS tasks, notably camera-based Road Condition Classification (RCC), in a privacy-preserving collaborative way. However, opening up to collaboration also opens FL-based RCC systems to adversaries, i.e., misbehaving participants that can launch Targeted Label-Flipping Attacks (TLFAs) and threaten transportation safety. Adversaries mounting TLFAs poison training data to misguide model predictions, from an actual source class (e.g., wet road) to a wrongly perceived target class (e.g., dry road). Existing countermeasures against poisoning attacks cannot maintain model performance under TLFAs close to the performance level in attack-free scenarios, because they lack specific model misbehavior detection for TLFAs and neglect client exclusion after the detection. To close this research gap, we propose DEFEND, which includes a poisoned model detection strategy that leverages neuron-wise magnitude analysis for attack goal identification and Gaussian Mixture Model (GMM)-based clustering. DEFEND discards poisoned model contributions in each round and adapts accordingly client ratings, eventually excluding malicious clients. Extensive evaluation involving various FL-RCC models and tasks shows that DEFEND can thwart TLFAs and outperform seven baseline countermeasures, with at least 15.78% improvement, with DEFEND remarkably achieving under attack the same performance as in attack-free scenarios.

</details>


### [5] [Privacy Loss of Noise Perturbation via Concentration Analysis of A Product Measure](https://arxiv.org/abs/2512.06253)
*Shuainan Liu,Tianxi Ji,Zhongshuo Fang,Lu Wei,Pan Li*

Main category: cs.CR

TL;DR: 提出一种新的球形对称噪声生成方案，通过几何视角分析隐私损失，在相同(ε,δ)-差分隐私保证下，高维空间中比经典高斯噪声具有更小的期望噪声幅度，显著提高效用。


<details>
  <summary>Details</summary>
Motivation: 传统高斯噪声机制在高维空间中噪声幅度较大，影响查询结果的效用。作者观察到扰动噪声通常具有球形对称性，希望利用这一特性设计更高效的噪声生成方案。

Method: 提出基于球形对称性的噪声生成方案，将噪声分解为半径随机变量（控制幅度）和方向随机变量（控制角度）的乘积测度。从几何视角分析隐私损失，推导乘积测度的闭式矩界来证明(ε,δ)-差分隐私。

Result: 在相同(ε,δ)-差分隐私保证下，新机制在高维空间中比经典高斯噪声具有更小的期望噪声幅度。通过高维空间中的凸和非凸经验风险最小化问题验证了该方法的有效性。

Conclusion: 利用球形对称性设计的乘积噪声机制在保持相同隐私保护水平的同时，显著降低了高维空间中的噪声幅度，提高了差分隐私机制的效用。

Abstract: Noise perturbation is one of the most fundamental approaches for achieving $(ε,δ)$-differential privacy (DP) guarantees when releasing the result of a query or function $f(\cdot)\in\mathbb{R}^M$ evaluated on a sensitive dataset $\mathbf{x}$. In this approach, calibrated noise $\mathbf{n}\in\mathbb{R}^M$ is used to obscure the difference vector $f(\mathbf{x})-f(\mathbf{x}')$, where $\mathbf{x}'$ is known as a neighboring dataset. A DP guarantee is obtained by studying the tail probability bound of a privacy loss random variable (PLRV), defined as the Radon-Nikodym derivative between two distributions. When $\mathbf{n}$ follows a multivariate Gaussian distribution, the PLRV is characterized as a specific univariate Gaussian. In this paper, we propose a novel scheme to generate $\mathbf{n}$ by leveraging the fact that the perturbation noise is typically spherically symmetric (i.e., the distribution is rotationally invariant around the origin). The new noise generation scheme allows us to investigate the privacy loss from a geometric perspective and express the resulting PLRV using a product measure, $W\times U$; measure $W$ is related to a radius random variable controlling the magnitude of $\mathbf{n}$, while measure $U$ involves a directional random variable governing the angle between $\mathbf{n}$ and the difference $f(\mathbf{x})-f(\mathbf{x}')$. We derive a closed-form moment bound on the product measure to prove $(ε,δ)$-DP. Under the same $(ε,δ)$-DP guarantee, our mechanism yields a smaller expected noise magnitude than the classic Gaussian noise in high dimensions, thereby significantly improving the utility of the noisy result $f(\mathbf{x})+\mathbf{n}$. To validate this, we consider convex and non-convex empirical risk minimization (ERM) problems in high dimensional space and apply the proposed product noise to achieve privacy.

</details>


### [6] [JEEVHITAA -- An End-to-End HCAI System to Support Collective Care](https://arxiv.org/abs/2512.06364)
*Shyama Sastha Krishnamoorthy Srinivasan,Harsh Pala,Mohan Kumar,Pushpendra Singh*

Main category: cs.CR

TL;DR: JEEVHITAA是一个面向医疗护理网络的移动健康平台，提供基于角色的上下文感知共享、可验证信息流和端到端加密，集成了检索增强LLM管道用于生成结构化摘要和健康内容验证。


<details>
  <summary>Details</summary>
Motivation: 当前移动健康平台主要是个人中心化的，缺乏协调、可审计的多参与者工作流程原语。然而，全球许多医疗环境中，健康决策是由多参与者护理网络而非单个用户执行的。

Method: 开发了Android/Flutter系统，通过Google Health Connect和BLE连接器获取平台和设备数据，构建多层用户档案，实施细粒度、有时间限制的访问控制。数据端到端加密，集成了检索增强LLM管道用于生成结构化摘要、高级洞察和健康内容验证。

Result: 系统架构、连接器抽象和安全原语已实现，通过合成、本体驱动的模拟和供应商兼容性测试评估了鲁棒性和兼容性。计划进行纵向实际部署以测量系统性能、访问控制正确性和关系感知可信度支持的实际效果。

Conclusion: JEEVHITAA为多参与者护理网络提供了上下文感知、基于角色的共享和可验证信息流解决方案，填补了当前移动健康平台的空白，支持协调的医疗决策制定。

Abstract: Current mobile health platforms are predominantly individual-centric and lack the necessary primitives for coordinated, auditable, multi-actor workflows. However, in many settings worldwide, health decisions are enacted by multi-actor care networks rather than single users. We present JEEVHITAA, an Android/Flutter system that provides context-sensitive, role-aware sharing and verifiable information flows for care circles. JEEVHITAA ingests platform and device data (via Google Health Connect and BLE connectors), constructs multi-layer user profiles from sensor streams and tiered onboarding, and enforces fine-grained, time-bounded access control across permissioned care graphs. Data are end-to-end encrypted in local stores and during peer sync (Firebase), and provisions are made for document capture by camera or upload as PDF. An integrated retrieval-augmented LLM pipeline (i) produces structured, role-targeted summaries and action plans, (ii) enables users to gather advanced insights on health reports, and (iii) performs evidence-grounded user-relevant verification of arbitrary health content, returning provenance, confidence scores, and source citations. We describe the system architecture, connector abstractions, and security primitives, and evaluate robustness and compatibility using synthetic, ontology-driven simulations and vendor compatibility tests. Finally, we outline plans for longitudinal in-the-wild deployments to measure system performance, the correctness of access control, and the real-world effectiveness of relationship-aware credibility support.

</details>


### [7] [Beyond Model Jailbreak: Systematic Dissection of the "Ten DeadlySins" in Embodied Intelligence](https://arxiv.org/abs/2512.06387)
*Yuhang Huang,Junchao Li,Boyang Ma,Xuelong Dai,Minghui Xu,Kaidi Xu,Yue Zhang,Jianping Wang,Xiuzhen Cheng*

Main category: cs.CR

TL;DR: 本文首次对Unitree Go2四足机器人平台进行全面的安全分析，发现了十个跨层安全漏洞，揭示了具身AI系统在无线配置、核心模块和外部接口等多个层面的系统性弱点。


<details>
  <summary>Details</summary>
Motivation: 虽然语言模型越狱问题已引起广泛关注，但具身AI系统的整体安全堆栈（包括物理感知、移动性和云连接应用）尚未得到充分研究。本文旨在填补这一空白，对具身AI平台进行全面的安全分析。

Method: 采用多维度分析方法：BLE嗅探、流量拦截、APK逆向工程、云API测试和硬件探测。研究聚焦于Unitree Go2平台的三个架构层：无线配置层、核心模块层和外部接口层。

Result: 发现了十个跨层安全漏洞，包括硬编码密钥、可预测的握手令牌、WiFi凭据泄露、TLS验证缺失、静态SSH密码、多语言安全绕过行为、不安全的本地中继通道、弱绑定逻辑和无限制固件访问。攻击者可利用这些漏洞劫持设备、注入任意命令、提取敏感信息或获得完全物理控制。

Conclusion: 保护具身AI系统远不止对齐模型本身，需要在整个软硬件生态系统中建立全面的安全防护。本文提出了系统级经验教训和建议，以构建在整个软件硬件生态系统中保持稳健的具身平台。

Abstract: Embodied AI systems integrate language models with real world sensing, mobility, and cloud connected mobile apps. Yet while model jailbreaks have drawn significant attention, the broader system stack of embodied intelligence remains largely unexplored. In this work, we conduct the first holistic security analysis of the Unitree Go2 platform and uncover ten cross layer vulnerabilities the "Ten Sins of Embodied AI Security." Using BLE sniffing, traffic interception, APK reverse engineering, cloud API testing, and hardware probing, we identify systemic weaknesses across three architectural layers: wireless provisioning, core modules, and external interfaces. These include hard coded keys, predictable handshake tokens, WiFi credential leakage, missing TLS validation, static SSH password, multilingual safety bypass behavior, insecure local relay channels, weak binding logic, and unrestricted firmware access. Together, they allow adversaries to hijack devices, inject arbitrary commands, extract sensitive information, or gain full physical control.Our findings show that securing embodied AI requires far more than aligning the model itself. We conclude with system level lessons learned and recommendations for building embodied platforms that remain robust across their entire software hardware ecosystem.

</details>


### [8] [Web Technologies Security in the AI Era: A Survey of CDN-Enhanced Defenses](https://arxiv.org/abs/2512.06390)
*Mehrab Hosain,Sabbir Alom Shuvo,Matthew Ogbe,Md Shah Jalal Mazumder,Yead Rahman,Md Azizul Hakim,Anukul Pandey*

Main category: cs.CR

TL;DR: 这篇综述论文系统分析了边缘计算中AI增强的网络安全防御技术，包括WAF、DDoS防护、机器人管理和API安全，提出了威胁分类、评估指标和部署指南，并指出了AI模型滥用、投毒等新风险。


<details>
  <summary>Details</summary>
Motivation: 现代网络应用面临日益复杂的自动化AI攻击，而内容分发网络和边缘计算作为最接近用户和机器人的防御点，需要部署机器学习驱动的检测、限流和隔离机制来应对这些威胁。

Method: 采用系统性综述方法，分析边缘部署的AI增强防御技术：基于异常和行为的WAF/WAAP、自适应DDoS检测与缓解、抗人类模仿的机器人管理、API发现与加密流量异常分析，并建立威胁分类、评估指标和部署指南。

Result: 边缘中心的AI技术显著改善了检测和缓解时间，减少了数据移动并增强了合规性，但同时也引入了模型滥用、投毒和治理方面的新风险。

Conclusion: 边缘AI防御在提升网络安全效果的同时，需要关注可解释AI、对抗鲁棒性和自主多智能体防御等研究方向，并建立相应的治理框架来应对新风险。

Abstract: The modern web stack, which is dominated by browser-based applications and API-first backends, now operates under an adversarial equilibrium where automated, AI-assisted attacks evolve continuously. Content Delivery Networks (CDNs) and edge computing place programmable defenses closest to users and bots, making them natural enforcement points for machine-learning (ML) driven inspection, throttling, and isolation. This survey synthesizes the landscape of AI-enhanced defenses deployed at the edge: (i) anomaly- and behavior-based Web Application Firewalls (WAFs) within broader Web Application and API Protection (WAAP), (ii) adaptive DDoS detection and mitigation, (iii) bot management that resists human-mimicry, and (iv) API discovery, positive security modeling, and encrypted-traffic anomaly analysis. We add a systematic survey method, a threat taxonomy mapped to edge-observable signals, evaluation metrics, deployment playbooks, and governance guidance. We conclude with a research agenda spanning XAI, adversarial robustness, and autonomous multi-agent defense. Our findings indicate that edge-centric AI measurably improves time-to-detect and time-to-mitigate while reducing data movement and enhancing compliance, yet introduces new risks around model abuse, poisoning, and governance.

</details>


### [9] [AgenticCyber: A GenAI-Powered Multi-Agent System for Multimodal Threat Detection and Adaptive Response in Cybersecurity](https://arxiv.org/abs/2512.06396)
*Shovan Roy*

Main category: cs.CR

TL;DR: AgenticCyber是一个基于生成式AI的多智能体系统，通过协调专门智能体同时监控云日志、监控视频和环境音频，实现实时威胁检测和响应，在分布式环境中提供可扩展的主动网络安全架构。


<details>
  <summary>Details</summary>
Motivation: 分布式环境中网络威胁日益复杂，需要能够跨多模态数据流进行实时检测和响应的先进框架，以克服传统孤立安全技术的局限性。

Method: 使用生成式AI驱动的多智能体系统，协调专门智能体监控云日志、监控视频和环境音频。结合Google的Gemini多模态语言模型和LangChain进行智能体编排，实现跨模态推理和自适应安全态势管理。

Result: 在威胁检测上达到96.2%的F1分数，响应延迟降至420毫秒，平均响应时间减少65%。在AWS CloudTrail日志、UCF-Crime视频帧和UrbanSound8K音频片段等基准数据集上表现优于标准入侵检测系统。

Conclusion: 该工作提出了一种可扩展、模块化的主动网络安全架构，适用于企业网络和物联网生态系统，通过跨模态推理和自动修复克服了传统孤立安全技术的限制。

Abstract: The increasing complexity of cyber threats in distributed environments demands advanced frameworks for real-time detection and response across multimodal data streams. This paper introduces AgenticCyber, a generative AI powered multi-agent system that orchestrates specialized agents to monitor cloud logs, surveillance videos, and environmental audio concurrently. The solution achieves 96.2% F1-score in threat detection, reduces response latency to 420 ms, and enables adaptive security posture management using multimodal language models like Google's Gemini coupled with LangChain for agent orchestration. Benchmark datasets, such as AWS CloudTrail logs, UCF-Crime video frames, and UrbanSound8K audio clips, show greater performance over standard intrusion detection systems, reducing mean time to respond (MTTR) by 65% and improving situational awareness. This work introduces a scalable, modular proactive cybersecurity architecture for enterprise networks and IoT ecosystems that overcomes siloed security technologies with cross-modal reasoning and automated remediation.

</details>


### [10] [KyFrog: A High-Security LWE-Based KEM Inspired by ML-KEM](https://arxiv.org/abs/2512.06411)
*Victor Duarte Melo,Willian J. Buchanan*

Main category: cs.CR

TL;DR: KyFrog是一个基于LWE的密钥封装机制，采用大维度(n=1024)和小模数(q=1103)设计，以牺牲密文大小(约0.5MB)为代价获得高安全性(约2^325经典和量子安全)，公钥和私钥大小与ML-KEM相当。


<details>
  <summary>Details</summary>
Motivation: 探索与现有方案不同的设计点：现有LWE方案通常追求较小的公钥和密文大小，而KyFrog则通过牺牲密文大小来获得更高的安全性边界，为实际应用提供另一种安全性与效率的权衡选择。

Method: 采用大维度(n=1024)和小模数(q=1103)的LWE结构，配合窄误差分布(标准差σ_s=σ_e=1.4)，使用Lattice Estimator工具评估安全性，针对最先进的格攻击提供约2^325的经典和量子安全。

Result: KyFrog实现了约2^325的经典和量子安全，密文大小约0.5MB，公钥和私钥大小与ML-KEM标准相当。所有代码和数据已作为开源软件发布，包含完整的C++23实现和实验脚本。

Conclusion: KyFrog展示了通过接受较大的密文大小可以获得显著更高的安全性边界，为后量子密码学提供了与ML-KEM不同的设计权衡方案，特别是在需要高安全性保证的应用场景中具有价值。

Abstract: KyFrog is a conservative Learning-with-Errors (LWE) key-encapsulation mechanism designed to explore an alternative operating point compared to schemes with relatively small public keys and ciphertexts. KyFrog uses a larger dimension ($n = 1024$) and a small prime modulus $q = 1103$, together with narrow error distributions with standard deviations $σ_s = σ_e = 1.4$, to target approximately $2^{325}$ classical and quantum security against state-of-the-art lattice attacks under standard cost models, as estimated using the Lattice Estimator. The price paid for this security margin is an extremely large KEM ciphertext (about 0.5 MiB), while public and secret keys remain in the same ballpark as ML-KEM. We describe the design rationale, parameter search methodology, and implementation details of KyFrog, and we compare its asymptotic security and concrete parameter sizes with the ML-KEM standard. All code and data for this work are released as free and open-source software, with the full C++23 implementation and experimental scripts available at: https://github.com/victormeloasm/kyfrog

</details>


### [11] [Formalisation of Security for Federated Learning with DP and Attacker Advantage in IIIf for Satellite Swarms -- Extended Version](https://arxiv.org/abs/2512.06467)
*Florian Kammüller*

Main category: cs.CR

TL;DR: 本文扩展了联邦学习分布式动态系统的差分隐私形式化定义，并将其与攻击者优势概念关联，在Isabelle证明助手中进行机器验证，以卫星群系统作为案例研究。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在分布式应用（如卫星群）中能有效应用，但存在数据从梯度泄漏（DLG）攻击。现有防御措施缺乏理论基础和安全性的严格评估。

Method: 在Isabelle Insider和基础设施框架（IIIf）内扩展联邦学习分布式动态系统的差分隐私形式化定义，将差分隐私与攻击者优势概念关联，并进行机器验证。

Result: 建立了联邦学习分布式动态系统的形式化差分隐私框架，提供了理论基础和安全性评估方法，以卫星群系统作为验证案例。

Conclusion: 通过形式化方法和机器验证，为联邦学习系统提供了更严格的差分隐私安全基础，有助于防御数据从梯度泄漏攻击。

Abstract: In distributed applications, like swarms of satellites, machine learning can be efficiently applied even on small devices by using Federated Learning (FL). This allows to reduce the learning complexity by transmitting only updates to the general model in the server in the form of differences in stochastic gradient descent. FL naturally supports differential privacy but new attacks, so called Data Leakage from Gradient (DLG) have been discovered recently. There has been work on defenses against DLG but there is a lack of foundation and rigorous evaluation of their security. In the current work, we extend existing work on a formal notion of Differential Privacy for Federated Learning distributed dynamic systems and relate it to the notion of the attacker advantage. This formalisation is carried out within the Isabelle Insider and Infrastructure framework (IIIf) allowing the machine supported verification of theory and applications within the proof assistant Isabelle. Satellite swarm systems are used as a motivating use case but also as a validation case study.

</details>


### [12] [PDRIMA: A Policy-Driven Runtime Integrity Measurement and Attestation Approach for ARM TrustZone-based TEE](https://arxiv.org/abs/2512.06500)
*Jingkai Mao,Xiaolin Chang*

Main category: cs.CR

TL;DR: PDRIMA是一个针对TrustZone TEE的运行时完整性保护框架，通过策略驱动的测量、评估和远程证明来防御TEE攻击


<details>
  <summary>Details</summary>
Motivation: 现有防御主要关注安全启动或REE侧监控，对TEE运行时完整性缺乏可见性，导致TrustZone设备容易受到持久性TEE攻击

Method: 系统分析TEE攻击面，引入两个TEE内子系统：安全监控代理(SMA)执行策略驱动的测量、评估、日志记录和定时重新测量；远程证明代理(RAA)聚合防篡改证据并提供远程证明协议

Result: 分析了PDRIMA的安全性，在Raspberry Pi 3B+上基于OP-TEE实现了原型，评估了性能开销表明其实用性

Conclusion: PDRIMA为TrustZone TEE提供了有效的运行时完整性保护机制，能够防御已识别的攻击面，具有实际部署的可行性

Abstract: Trusted Execution Environments (TEEs) such as ARM TrustZone are widely used in IoT and embedded devices to protect sensitive code and data. However, most existing defenses focus on secure boot or REE-side monitoring and provide little visibility into the runtime integrity of the TEE. This leaves TrustZone-based devices exposed to persistent TEE compromises. We propose Policy-Driven Runtime Integrity Measurement and Attestation (PDRIMA), a runtime integrity protection approach for TrustZone-based TEEs. PDRIMA systematically analyzes TEE attack surfaces and introduces two in-TEE subsystems: a Secure Monitor Agent (SMA) that performs policy-driven measurement, appraisal, logging, and time-based re-measurement over the TEE kernel, static components, user-TAs, and security-critical system calls; and a Remote Attestation Agent (RAA) that aggregates tamper-evident evidence and exposes a remote attestation protocol for verifying. We analyze PDRIMA's security against identified attack surfaces, implement a prototype on OP-TEE for Raspberry Pi 3B+, and evaluate its performance overhead to indicate its practicability.

</details>


### [13] [BEACON: A Unified Behavioral-Tactical Framework for Explainable Cybercrime Analysis with Large Language Models](https://arxiv.org/abs/2512.06555)
*Arush Sachdeva,Rajendraprasad Saravanan,Gargi Sarkar,Kavita Vemuri,Sandeep Kumar Shukla*

Main category: cs.CR

TL;DR: BEACON是一个统一的双维度框架，将行为心理学与网络犯罪战术生命周期相结合，通过微调大语言模型实现心理和战术维度的多标签分类及可解释分析，在真实网络犯罪叙事数据集上取得了20%的分类准确率提升。


<details>
  <summary>Details</summary>
Motivation: 当前网络犯罪分析框架主要关注技术操作层面，忽视了犯罪者利用人类认知偏差进行心理操纵的重要维度，这限制了网络犯罪分析的全面性和有效性。

Method: 提出BEACON框架：1）基于前景理论和Cialdini说服原则定义6种心理操纵类别；2）建立14阶段的网络犯罪战术生命周期；3）使用参数高效学习方法微调大语言模型，实现心理和战术维度的多标签分类并生成可解释说明。

Result: 在真实世界和合成增强的网络犯罪叙事数据集上实验表明：整体分类准确率比基础模型提升20%，推理质量（通过ROUGE和BERTScore衡量）也有显著提升。

Conclusion: BEACON框架能够将非结构化的受害者叙述自动分解为结构化的行为和操作情报，支持改进网络犯罪调查、案件关联和主动诈骗检测，为网络犯罪分析提供了更全面的视角。

Abstract: Cybercrime increasingly exploits human cognitive biases in addition to technical vulnerabilities, yet most existing analytical frameworks focus primarily on operational aspects and overlook psychological manipulation. This paper proposes BEACON, a unified dual-dimension framework that integrates behavioral psychology with the tactical lifecycle of cybercrime to enable structured, interpretable, and scalable analysis of cybercrime. We formalize six psychologically grounded manipulation categories derived from Prospect Theory and Cialdini's principles of persuasion, alongside a fourteen-stage cybercrime tactical lifecycle spanning reconnaissance to final impact. A single large language model is fine-tuned using parameter-efficient learning to perform joint multi-label classification across both psychological and tactical dimensions while simultaneously generating human-interpretable explanations. Experiments conducted on a curated dataset of real-world and synthetically augmented cybercrime narratives demonstrate a 20 percent improvement in overall classification accuracy over the base model, along with substantial gains in reasoning quality measured using ROUGE and BERTScore. The proposed system enables automated decomposition of unstructured victim narratives into structured behavioral and operational intelligence, supporting improved cybercrime investigation, case linkage, and proactive scam detection.

</details>


### [14] [Securing the Model Context Protocol: Defending LLMs Against Tool Poisoning and Adversarial Attacks](https://arxiv.org/abs/2512.06556)
*Saeid Jamshidi,Kawser Wazed Nafi,Arghavan Moradi Dakhel,Negar Shahabi,Foutse Khomh,Naser Ezzati-Jivan*

Main category: cs.CR

TL;DR: 本文分析了MCP（模型上下文协议）的安全漏洞，提出了针对工具描述符语义攻击的三类威胁，并设计了一个包含签名验证、语义审查和运行时防护的三层安全框架。


<details>
  <summary>Details</summary>
Motivation: MCP使大语言模型能够通过结构化描述符集成外部工具，增强了决策自主性，但这种自主性带来了被忽视的安全漏洞。现有防御主要关注提示注入攻击，未能解决工具元数据中嵌入的威胁，导致MCP系统面临语义操纵风险。

Method: 提出了一个三层安全框架：1）基于RSA的清单签名确保描述符完整性；2）LLM-on-LLM语义审查检测可疑工具定义；3）轻量级启发式护栏在运行时阻止异常工具行为。在GPT-4、DeepSeek和Llama-3.5上评估了八种提示策略。

Result: 安全性能因模型架构和推理方法差异显著：GPT-4阻止约71%的不安全工具调用，平衡了延迟和安全性；DeepSeek对Shadowing攻击最具弹性但延迟更高；Llama-3.5最快但最不鲁棒。框架无需模型微调或内部修改即可降低不安全工具调用率。

Conclusion: MCP系统的安全需要专门针对语义攻击的防御措施。提出的三层框架有效降低了不安全工具调用风险，但安全性能高度依赖于底层LLM的架构和推理能力，需要在延迟、安全性和模型选择之间权衡。

Abstract: The Model Context Protocol (MCP) enables Large Language Models to integrate external tools through structured descriptors, increasing autonomy in decision-making, task execution, and multi-agent workflows. However, this autonomy creates a largely overlooked security gap. Existing defenses focus on prompt-injection attacks and fail to address threats embedded in tool metadata, leaving MCP-based systems exposed to semantic manipulation. This work analyzes three classes of semantic attacks on MCP-integrated systems: (1) Tool Poisoning, where adversarial instructions are hidden in tool descriptors; (2) Shadowing, where trusted tools are indirectly compromised through contaminated shared context; and (3) Rug Pulls, where descriptors are altered after approval to subvert behavior. To counter these threats, we introduce a layered security framework with three components: RSA-based manifest signing to enforce descriptor integrity, LLM-on-LLM semantic vetting to detect suspicious tool definitions, and lightweight heuristic guardrails that block anomalous tool behavior at runtime. Through evaluation of GPT-4, DeepSeek, and Llama-3.5 across eight prompting strategies, we find that security performance varies widely by model architecture and reasoning method. GPT-4 blocks about 71 percent of unsafe tool calls, balancing latency and safety. DeepSeek shows the highest resilience to Shadowing attacks but with greater latency, while Llama-3.5 is fastest but least robust. Our results show that the proposed framework reduces unsafe tool invocation rates without model fine-tuning or internal modification.

</details>


### [15] [Characterizing Large-Scale Adversarial Activities Through Large-Scale Honey-Nets](https://arxiv.org/abs/2512.06557)
*Tonia Haikal,Eman Hammad,Shereen Ismail*

Main category: cs.CR

TL;DR: 本文通过部署HoneyTrap自适应蜜罐框架，对攻击者行为进行纵向分析，收集了超过6030万事件，发现HTTP/HTTPS和SSH是最主要攻击目标，同时识别了Minecraft等非常规服务的攻击模式。


<details>
  <summary>Details</summary>
Motivation: 随着网络威胁日益复杂，特别是针对关键基础设施和物联网生态系统的攻击，需要新的方法来表征攻击者策略，以更好地理解和防御这些威胁。

Method: 使用HoneyTrap自适应蜜罐框架，在分布式节点部署模拟脆弱服务，收集恶意流量。采用Apache Parquet格式处理数据（5.8-9.3倍压缩，7.2倍查询加速），并通过ASN增强和SHA-256伪匿名化保护隐私。

Result: 在24天观测期内收集6030万事件：1) HTTP/HTTPS（端口80/443）为主要目标，超过800万连接尝试，日峰值超170万事件；2) SSH（端口22）遭受460万次暴力破解攻击；3) Minecraft（25565）和SMB（445）等非常规服务也受攻击，Minecraft日均约11.8万次尝试且与其他端口攻击峰值相关。

Conclusion: HoneyTrap框架有效捕获了攻击者行为模式，揭示了HTTP/HTTPS和SSH是主要攻击目标，同时识别了非常规服务的攻击相关性，为网络安全防御提供了重要洞察。

Abstract: The increasing sophistication of cyber threats demands novel approaches to characterize adversarial strategies, particularly those targeting critical infrastructure and IoT ecosystems. This paper presents a longitudinal analysis of attacker behavior using HoneyTrap, an adaptive honeypot framework deployed across geographically distributed nodes to emulate vulnerable services and safely capture malicious traffic. Over a 24 day observation window, more than 60.3 million events were collected. To enable scalable analytics, raw JSON logs were transformed into Apache Parquet, achieving 5.8 - 9.3x compression and 7.2x faster queries, while ASN enrichment and salted SHA-256 pseudonymization added network intelligence and privacy preservation.
  Our analysis reveals three key findings: (1) The majority of traffic targeted HTTP and HTTPS services (ports 80 and 443), with more than 8 million connection attempts and daily peaks exceeding 1.7 million events. (2) SSH (port 22) was frequently subject to brute-force attacks, with over 4.6 million attempts. (3) Less common services like Minecraft (25565) and SMB (445) were also targeted, with Minecraft receiving about 118,000 daily attempts that often coincided with spikes on other ports.

</details>


### [16] [OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation](https://arxiv.org/abs/2512.06589)
*Xiaojun Jia,Jie Liao,Qi Guo,Teng Ma,Simeng Qin,Ranjie Duan,Tianlin Li,Yihao Huang,Zhitao Zeng,Dongxian Wu,Yiming Li,Wenqi Ren,Xiaochun Cao,Yang Liu*

Main category: cs.CR

TL;DR: OmniSafeBench-MM是一个全面的多模态越狱攻击-防御评估工具箱，包含13种攻击方法、15种防御策略和多样化数据集，提供三维评估协议来衡量危害性、意图对齐和响应细节。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型存在安全漏洞，容易被越狱攻击绕过安全对齐，而现有基准测试如JailBreakV-28K等存在局限性：攻击场景有限、缺乏标准化防御评估、没有统一可复现的工具箱。

Method: 构建包含13种代表性攻击方法、15种防御策略的综合工具箱，数据集涵盖9个主要风险领域和50个细粒度类别，采用咨询式、命令式和陈述式查询类型，建立三维评估协议（危害性分级、意图对齐、响应细节）。

Result: 在10个开源和8个闭源MLLMs上进行了广泛实验，揭示了它们对多模态越狱攻击的脆弱性，提供了标准化、可复现的研究平台。

Conclusion: OmniSafeBench-MM通过统一数据、方法和评估，为多模态越狱攻击-防御研究提供了标准化基础，有助于未来安全研究的发展。

Abstract: Recent advances in multi-modal large language models (MLLMs) have enabled unified perception-reasoning capabilities, yet these systems remain highly vulnerable to jailbreak attacks that bypass safety alignment and induce harmful behaviors. Existing benchmarks such as JailBreakV-28K, MM-SafetyBench, and HADES provide valuable insights into multi-modal vulnerabilities, but they typically focus on limited attack scenarios, lack standardized defense evaluation, and offer no unified, reproducible toolbox. To address these gaps, we introduce OmniSafeBench-MM, which is a comprehensive toolbox for multi-modal jailbreak attack-defense evaluation. OmniSafeBench-MM integrates 13 representative attack methods, 15 defense strategies, and a diverse dataset spanning 9 major risk domains and 50 fine-grained categories, structured across consultative, imperative, and declarative inquiry types to reflect realistic user intentions. Beyond data coverage, it establishes a three-dimensional evaluation protocol measuring (1) harmfulness, distinguished by a granular, multi-level scale ranging from low-impact individual harm to catastrophic societal threats, (2) intent alignment between responses and queries, and (3) response detail level, enabling nuanced safety-utility analysis. We conduct extensive experiments on 10 open-source and 8 closed-source MLLMs to reveal their vulnerability to multi-modal jailbreak. By unifying data, methodology, and evaluation into an open-source, reproducible platform, OmniSafeBench-MM provides a standardized foundation for future research. The code is released at https://github.com/jiaxiaojunQAQ/OmniSafeBench-MM.

</details>


### [17] [The Evolution of Agentic AI in Cybersecurity: From Single LLM Reasoners to Multi-Agent Systems and Autonomous Pipelines](https://arxiv.org/abs/2512.06659)
*Vaishali Vinay*

Main category: cs.CR

TL;DR: 这篇论文提出了网络安全领域智能体AI的五代分类法，追踪了从文本推理到多智能体协作框架的演进，比较了推理深度、工具使用、记忆等维度，并总结了评估基准和未解决挑战。


<details>
  <summary>Details</summary>
Motivation: 网络安全运营中心需要多步推理、工具驱动分析和快速决策，但单个大语言模型在真实SOC环境中存在不足，需要可重现、可追溯的工作流程。因此需要研究智能体AI在网络安全中的架构演进。

Method: 提出五代分类法：从纯文本LLM推理器到工具增强智能体、分布式多智能体系统、模式绑定工具生态系统，再到半自主调查流水线。通过核心维度（推理深度、工具使用、记忆、可重现性、安全性）进行比较分析。

Result: 建立了网络安全智能体AI的系统分类框架，识别了各代系统的能力演进和风险变化，综合了新兴的评估基准，为领域发展提供了结构化视角。

Conclusion: 智能体AI正在网络安全领域快速演进，但面临响应验证、工具使用正确性、多智能体协调、长时程推理和高影响行动保障等挑战，需要确保其安全可靠部署。

Abstract: Cybersecurity has become one of the earliest adopters of agentic AI, as security operations centers increasingly rely on multi-step reasoning, tool-driven analysis, and rapid decision-making under pressure. While individual large language models can summarize alerts or interpret unstructured reports, they fall short in real SOC environments that require grounded data access, reproducibility, and accountable workflows. In response, the field has seen a rapid architectural evolution from single-model helpers toward tool-augmented agents, distributed multi-agent systems, schema-bound tool ecosystems, and early explorations of semi-autonomous investigative pipelines. This survey presents a five-generation taxonomy of agentic AI in cybersecurity. It traces how capabilities and risks change as systems advance from text-only LLM reasoners to multi-agent collaboration frameworks and constrained-autonomy pipelines. We compare these generations across core dimensions - reasoning depth, tool use, memory, reproducibility, and safety. In addition, we also synthesize emerging benchmarks used to evaluate cyber-oriented agents. Finally, we outline the unresolved challenges that accompany this evolution, such as response validation, tool-use correctness, multi-agent coordination, long-horizon reasoning, and safeguards for high-impact actions. Collectively, this work provides a structured perspective on how agentic AI is taking shape within cybersecurity and what is required to ensure its safe and reliable deployment.

</details>


### [18] [Towards Small Language Models for Security Query Generation in SOC Workflows](https://arxiv.org/abs/2512.06660)
*Saleha Muzammil,Rahul Reddy,Vishal Kamalakrishnan,Hadi Ahmadi,Wajih Ul Hassan*

Main category: cs.CR

TL;DR: 本文提出一个三旋钮框架，使用小型语言模型实现自然语言到Kusto查询语言的准确翻译，在安全运营中达到高精度且成本效益显著。


<details>
  <summary>Details</summary>
Motivation: 安全运营中心分析师需要查询海量遥测数据，但编写正确的Kusto查询语言需要专业知识，这成为安全团队规模扩展的瓶颈。需要探索小型语言模型是否能提供准确、经济高效的自然语言到KQL翻译方案。

Method: 提出三旋钮框架：1) 轻量级检索和错误感知提示，解决常见解析失败问题；2) LoRA微调与推理蒸馏，为NLQ-KQL对添加简要思维链解释；3) 两阶段架构，使用SLM生成候选查询，低成本LLM进行模式感知的细化和选择。

Result: 在微软NL2KQL Defender评估数据集上，两阶段方法达到0.987语法准确率和0.906语义准确率。在Microsoft Sentinel数据上达到0.964语法准确率和0.831语义准确率，相比GPT-5降低10倍令牌成本。

Conclusion: 小型语言模型可以作为安全运营中自然语言查询的实用、可扩展基础，在保持高准确性的同时显著降低成本。

Abstract: Analysts in Security Operations Centers routinely query massive telemetry streams using Kusto Query Language (KQL). Writing correct KQL requires specialized expertise, and this dependency creates a bottleneck as security teams scale. This paper investigates whether Small Language Models (SLMs) can enable accurate, cost-effective natural-language-to-KQL translation for enterprise security. We propose a three-knob framework targeting prompting, fine-tuning, and architecture design. First, we adapt existing NL2KQL framework for SLMs with lightweight retrieval and introduce error-aware prompting that addresses common parser failures without increasing token count. Second, we apply LoRA fine-tuning with rationale distillation, augmenting each NLQ-KQL pair with a brief chain-of-thought explanation to transfer reasoning from a teacher model while keeping the SLM compact. Third, we propose a two-stage architecture that uses an SLM for candidate generation and a low-cost LLM judge for schema-aware refinement and selection. We evaluate nine models (five SLMs and four LLMs) across syntax correctness, semantic accuracy, table selection, and filter precision, alongside latency and token cost. On Microsoft's NL2KQL Defender Evaluation dataset, our two-stage approach achieves 0.987 syntax and 0.906 semantic accuracy. We further demonstrate generalizability on Microsoft Sentinel data, reaching 0.964 syntax and 0.831 semantic accuracy. These results come at up to 10x lower token cost than GPT-5, establishing SLMs as a practical, scalable foundation for natural-language querying in security operations.

</details>


### [19] [Look Twice before You Leap: A Rational Agent Framework for Localized Adversarial Anonymization](https://arxiv.org/abs/2512.06713)
*Donghang Duan,Xu Zheng*

Main category: cs.CR

TL;DR: 论文提出RLAA框架解决本地小模型文本匿名化中的隐私-效用权衡问题，通过引入仲裁者机制防止贪婪策略导致的效用崩溃，实现完全本地化的隐私保护。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的文本匿名化框架依赖远程API服务，存在"隐私悖论"：用户必须向不可信的第三方披露数据以实现隐私保护。直接将现有框架迁移到本地小模型会导致效用灾难性崩溃。

Method: 提出Rational Localized Adversarial Anonymization (RLAA)框架，采用Attacker-Arbitrator-Anonymizer (A-A-A)架构。将匿名化过程建模为边际隐私增益(MPG)和边际效用成本(MUC)的权衡，引入仲裁者作为理性守门员，验证攻击者推理，过滤对隐私保护益处微小的反馈，强制执行理性早停准则。

Result: 在不同数据集上的广泛实验表明，RLAA实现了最佳的隐私-效用权衡，在某些情况下甚至基于帕累托原则优于现有最优方法。

Conclusion: RLAA框架解决了本地小模型文本匿名化中的效用崩溃问题，通过理性仲裁机制实现了完全本地化的隐私保护，在隐私和效用之间取得了更好的平衡。

Abstract: Current LLM-based text anonymization frameworks usually rely on remote API services from powerful LLMs, which creates an inherent "privacy paradox": users must somehow disclose data to untrusted third parties for superior privacy preservation. Moreover, directly migrating these frameworks to local small-scale models (LSMs) offers a suboptimal solution with catastrophic collapse in utility based on our core findings. Our work argues that this failure stems not merely from the capability deficits of LSMs, but from the inherent irrationality of the greedy adversarial strategies employed by current state-of-the-art (SoTA) methods. We model the anonymization process as a trade-off between Marginal Privacy Gain (MPG) and Marginal Utility Cost (MUC), and demonstrate that greedy strategies inevitably drift into an irrational state. To address this, we propose Rational Localized Adversarial Anonymization (RLAA), a fully localized and training-free framework featuring an Attacker-Arbitrator-Anonymizer (A-A-A) architecture. RLAA introduces an arbitrator that acts as a rationality gatekeeper, validating the attacker's inference to filter out feedback providing negligible benefits on privacy preservation. This mechanism enforces a rational early-stopping criterion, and systematically prevents utility collapse. Extensive experiments on different datasets demonstrate that RLAA achieves the best privacy-utility trade-off, and in some cases even outperforms SoTA on the Pareto principle. Our code and datasets will be released upon acceptance.

</details>


### [20] [PrivLLMSwarm: Privacy-Preserving LLM-Driven UAV Swarms for Secure IoT Surveillance](https://arxiv.org/abs/2512.06747)
*Jifar Wakuma Ayana,Huang Qiming*

Main category: cs.CR

TL;DR: PrivLLMSwarm：基于安全多方计算的隐私保护框架，用于无人机群在物联网环境中的安全LLM推理和协调


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的无人机系统以明文处理敏感操作数据，面临隐私和安全风险，需要保护无人机群协调中的隐私

Method: 采用安全多方计算（MPC）进行安全LLM推理，包含MPC优化的transformer组件和非线性激活函数的高效近似，通过强化学习在仿真中微调GPT指令生成器

Result: 在城市规模仿真中实现高语义准确性、低加密推理延迟和稳健的编队控制，相比差分隐私、联邦学习和明文基线提供更好的隐私-效用平衡

Conclusion: PrivLLMSwarm为隐私敏感的物联网应用（如智慧城市监控和应急响应）中的安全LLM驱动无人机群建立了实用基础

Abstract: Large Language Models (LLMs) are emerging as powerful enablers for autonomous reasoning and natural-language coordination in unmanned aerial vehicle (UAV) swarms operating within Internet of Things (IoT) environments. However, existing LLM-driven UAV systems process sensitive operational data in plaintext, exposing them to privacy and security risks. This work introduces PrivLLMSwarm, a privacy-preserving framework that performs secure LLM inference for UAV swarm coordination through Secure Multi-Party Computation (MPC). The framework incorporates MPC-optimized transformer components with efficient approximations of nonlinear activations, enabling practical encrypted inference on resource-constrained aerial platforms. A fine-tuned GPT-based command generator, enhanced through reinforcement learning in simulation, provides reliable instructions while maintaining confidentiality. Experimental evaluation in urban-scale simulations demonstrates that PrivLLMSwarm achieves high semantic accuracy, low encrypted inference latency, and robust formation control under privacy constraints. Comparative analysis shows PrivLLMSwarm offers a superior privacy-utility balance compared to differential privacy, federated learning, and plaintext baselines. To support reproducibility, the full implementation including source code, MPC components, and a synthetic dataset is publicly available. PrivLLMSwarm establishes a practical foundation for secure, LLM-enabled UAV swarms in privacy-sensitive IoT applications including smart-city monitoring and emergency response.

</details>


### [21] [From Description to Score: Can LLMs Quantify Vulnerabilities?](https://arxiv.org/abs/2512.06781)
*Sima Jafarikhah,Daniel Thompson,Eva Deans,Hossein Siadati,Yi Liu*

Main category: cs.CR

TL;DR: 该研究评估了通用大语言模型（ChatGPT、Llama、Grok、DeepSeek、Gemini）在自动化CVE漏洞评分方面的表现，发现LLMs在某些指标上显著优于基线，但性能因模型和CVSS指标而异，且CVE描述的模糊性限制了自动化效果。


<details>
  <summary>Details</summary>
Motivation: 手动漏洞评分（如CVSS评分）是资源密集型且受主观影响的过程。随着CVE积压不断增加，需要探索自动化解决方案来减轻人工负担并提高评分一致性。

Method: 研究分析了31,000多个近期CVE条目，使用五种通用大语言模型（ChatGPT、Llama、Grok、DeepSeek、Gemini）进行自动化CVSS评分。比较了模型在不同CVSS指标上的表现，并尝试了集成元分类器方法。

Result: LLMs在某些指标（如可用性影响）上显著优于基线，但在其他指标（如攻击复杂度）上提升有限。ChatGPT-5获得最高精度。模型倾向于对相同的CVE产生错误分类，集成方法仅带来边际改进。CVE描述缺乏关键上下文或存在模糊表述是主要限制因素。

Conclusion: 需要改进漏洞描述的清晰度和上下文完整性，以支持更可靠的自动化推理。虽然LLMs在自动化漏洞评分方面显示出潜力，但当前限制表明需要结合更好的数据质量和可能的混合方法来解决CVE积压问题。

Abstract: Manual vulnerability scoring, such as assigning Common Vulnerability Scoring System (CVSS) scores, is a resource-intensive process that is often influenced by subjective interpretation. This study investigates the potential of general-purpose large language models (LLMs), namely ChatGPT, Llama, Grok, DeepSeek, and Gemini, to automate this process by analyzing over 31{,}000 recent Common Vulnerabilities and Exposures (CVE) entries. The results show that LLMs substantially outperform the baseline on certain metrics (e.g., \textit{Availability Impact}), while offering more modest gains on others (e.g., \textit{Attack Complexity}). Moreover, model performance varies across both LLM families and individual CVSS metrics, with ChatGPT-5 attaining the highest precision. Our analysis reveals that LLMs tend to misclassify many of the same CVEs, and ensemble-based meta-classifiers only marginally improve performance. Further examination shows that CVE descriptions often lack critical context or contain ambiguous phrasing, which contributes to systematic misclassifications. These findings underscore the importance of enhancing vulnerability descriptions and incorporating richer contextual details to support more reliable automated reasoning and alleviate the growing backlog of CVEs awaiting triage.

</details>


### [22] [CKG-LLM: LLM-Assisted Detection of Smart Contract Access Control Vulnerabilities Based on Knowledge Graphs](https://arxiv.org/abs/2512.06846)
*Xiaoqi Li,Hailu Kuang,Wenkai Li,Zongwei Li,Shipeng Ye*

Main category: cs.CR

TL;DR: CKG-LLM：基于知识图谱和LLM的智能合约访问控制漏洞检测框架


<details>
  <summary>Details</summary>
Motivation: 传统智能合约分析方法（如AST、CFG、SSA）在捕捉语义结构和控制逻辑方面存在局限，而知识图谱能提供更丰富的中间抽象表示，支持图查询语言识别违规元素。

Method: 提出CKG-LLM框架，利用大语言模型的推理和代码生成能力，将自然语言漏洞模式转换为可执行的合约知识图谱查询，自动定位易受攻击的代码元素。

Result: 实验评估表明，CKG-LLM在检测访问控制漏洞方面优于现有工具。

Conclusion: CKG-LLM展示了知识图谱与LLM结合在智能合约安全分析中的有效性，并讨论了未来研究方向。

Abstract: Traditional approaches for smart contract analysis often rely on intermediate representations such as abstract syntax trees, control-flow graphs, or static single assignment form. However, these methods face limitations in capturing both semantic structures and control logic. Knowledge graphs, by contrast, offer a structured representation of entities and relations, enabling richer intermediate abstractions of contract code and supporting the use of graph query languages to identify rule-violating elements. This paper presents CKG-LLM, a framework for detecting access-control vulnerabilities in smart contracts. Leveraging the reasoning and code generation capabilities of large language models, CKG-LLM translates natural-language vulnerability patterns into executable queries over contract knowledge graphs to automatically locate vulnerable code elements. Experimental evaluation demonstrates that CKG-LLM achieves superior performance in detecting access-control vulnerabilities compared to existing tools. Finally, we discuss potential extensions of CKG-LLM as part of future research directions.

</details>


### [23] [Patronus: Identifying and Mitigating Transferable Backdoors in Pre-trained Language Models](https://arxiv.org/abs/2512.06899)
*Tianhang Zhao,Wei Du,Haodong Zhao,Sufeng Duan,Gongshen Liu*

Main category: cs.CR

TL;DR: Patronus是一个防御预训练语言模型后门攻击的新框架，利用触发器在输入侧的不变性来应对下游任务微调导致的参数变化，通过多触发器对比搜索算法和双阶段缓解策略，在15个PLM和10个任务上实现了≥98.7%的后门检测召回率。


<details>
  <summary>Details</summary>
Motivation: 可迁移后门对预训练语言模型供应链构成严重威胁，现有防御方法主要依赖输出特征空间中的异常检测，但存在关键缺陷：下游任务微调会修改模型参数，改变输出分布，使预先计算的防御失效。

Method: 提出Patronus框架，利用触发器在输入侧对参数变化的不变性。采用多触发器对比搜索算法，将基于梯度的优化与对比学习目标有效结合。使用双阶段缓解策略：实时输入监控和通过对抗训练的模型净化。

Result: 在15个预训练语言模型和10个任务上的广泛实验表明，Patronus实现了≥98.7%的后门检测召回率，并将攻击成功率降低到干净设置水平，在所有设置中显著优于所有最先进的基线方法。

Conclusion: Patronus通过输入侧不变性和创新的优化算法，有效解决了下游任务微调导致的防御失效问题，为预训练语言模型供应链安全提供了强大的防御框架。

Abstract: Transferable backdoors pose a severe threat to the Pre-trained Language Models (PLMs) supply chain, yet defensive research remains nascent, primarily relying on detecting anomalies in the output feature space. We identify a critical flaw that fine-tuning on downstream tasks inevitably modifies model parameters, shifting the output distribution and rendering pre-computed defense ineffective. To address this, we propose Patronus, a novel framework that use input-side invariance of triggers against parameter shifts. To overcome the convergence challenges of discrete text optimization, Patronus introduces a multi-trigger contrastive search algorithm that effectively bridges gradient-based optimization with contrastive learning objectives. Furthermore, we employ a dual-stage mitigation strategy combining real-time input monitoring with model purification via adversarial training. Extensive experiments across 15 PLMs and 10 tasks demonstrate that Patronus achieves $\geq98.7\%$ backdoor detection recall and reduce attack success rates to clean settings, significantly outperforming all state-of-the-art baselines in all settings. Code is available at https://github.com/zth855/Patronus.

</details>


### [24] [SoK: Trust-Authorization Mismatch in LLM Agent Interactions](https://arxiv.org/abs/2512.06914)
*Guanquan Shi,Haohua Du,Zhiqiang Wang,Xiaoyu Liang,Weiwenpei Liu,Song Bian,Zhenyu Guan*

Main category: cs.CR

TL;DR: 该论文提出了一个统一的形式化框架来分析AI代理交互安全，重点关注信任评估与授权策略之间的不匹配问题，并基于此模型对现有攻击和防御进行分类。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型发展为能够与外部世界交互的自主代理，传统的确定性安全机制无法应对基于概率推理的AI代理，学术界对这一新兴领域缺乏系统性分析框架。

Method: 引入一个以信任-授权差距为核心的新型风险分析模型，以此作为统一视角，对现有看似孤立的攻击和防御实现路径进行调查和分类。

Result: 该框架不仅统一了该领域的研究，还识别出关键的研究空白，为构建鲁棒可信的代理和动态授权机制提供了系统性研究方向。

Conclusion: 论文为解决AI代理交互安全挑战提供了统一的形式化视角，通过信任-授权差距模型为未来的研究和安全机制设计奠定了基础。

Abstract: Large Language Models (LLMs) are rapidly evolving into autonomous agents capable of interacting with the external world, significantly expanding their capabilities through standardized interaction protocols. However, this paradigm revives the classic cybersecurity challenges of agency and authorization in a novel and volatile context. As decision-making shifts from deterministic code logic to probabilistic inference driven by natural language, traditional security mechanisms designed for deterministic behavior fail. It is fundamentally challenging to establish trust for unpredictable AI agents and to enforce the Principle of Least Privilege (PoLP) when instructions are ambiguous. Despite the escalating threat landscape, the academic community's understanding of this emerging domain remains fragmented, lacking a systematic framework to analyze its root causes. This paper provides a unifying formal lens for agent-interaction security.
  We observed that most security threats in this domain stem from a fundamental mismatch between trust evaluation and authorization policies. We introduce a novel risk analysis model centered on this trust-authorization gap. Using this model as a unifying lens, we survey and classify the implementation paths of existing, often seemingly isolated, attacks and defenses. This new framework not only unifies the field but also allows us to identify critical research gaps. Finally, we leverage our analysis to suggest a systematic research direction toward building robust, trusted agents and dynamic authorization mechanisms.

</details>


### [25] [A Comprehensive Study of Supervised Machine Learning Models for Zero-Day Attack Detection: Analyzing Performance on Imbalanced Data](https://arxiv.org/abs/2512.07030)
*Zahra Lotfi,Mostafa Lotfi*

Main category: cs.CR

TL;DR: 该研究评估了五种监督学习模型检测零日攻击的性能，提出结合网格搜索、降维和过采样的框架处理数据不平衡问题，发现随机森林性能最佳但处理时间长，最终选择XGBoost作为快速准确的零日攻击检测模型。


<details>
  <summary>Details</summary>
Motivation: 零日攻击因其未知模式难以被传统安全系统检测，现有监督学习模型在训练阶段未见过此类攻击，导致对未知攻击检测效率低下。研究旨在解决监督模型在零日攻击检测中的性能问题，并处理实际应用中常见的数据不平衡问题。

Method: 研究评估了五种监督学习模型，提出包含网格搜索、降维和过采样方法的框架来处理数据不平衡问题。使用高度不平衡的数据集模拟真实攻击检测场景，在测试阶段仅暴露零日攻击，确保模型未在训练阶段见过这些攻击模式。

Result: 随机森林在过采样和非过采样条件下均表现最佳，但处理时间较长。XGBoost在保持高准确率的同时具有更快的处理速度，因此被选为检测零日攻击的最佳模型。研究还比较了过采样方法对模型性能的影响。

Conclusion: 监督学习模型可以通过适当的框架改进来有效检测零日攻击，其中XGBoost在准确性和处理速度之间取得了最佳平衡，适合实际部署。过采样方法有助于改善模型在数据不平衡情况下的性能。

Abstract: Among the various types of cyberattacks, identifying zero-day attacks is problematic because they are unknown to security systems as their pattern and characteristics do not match known blacklisted attacks. There are many Machine Learning (ML) models designed to analyze and detect network attacks, especially using supervised models. However, these models are designed to classify samples (normal and attacks) based on the patterns they learn during the training phase, so they perform inefficiently on unseen attacks. This research addresses this issue by evaluating five different supervised models to assess their performance and execution time in predicting zero-day attacks and find out which model performs accurately and quickly. The goal is to improve the performance of these supervised models by not only proposing a framework that applies grid search, dimensionality reduction and oversampling methods to overcome the imbalance problem, but also comparing the effectiveness of oversampling on ml model metrics, in particular the accuracy. To emulate attack detection in real life, this research applies a highly imbalanced data set and only exposes the classifiers to zero-day attacks during the testing phase, so the models are not trained to flag the zero-day attacks. Our results show that Random Forest (RF) performs best under both oversampling and non-oversampling conditions, this increased effectiveness comes at the cost of longer processing times. Therefore, we selected XG Boost (XGB) as the top model due to its fast and highly accurate performance in detecting zero-day attacks.

</details>


### [26] [Managed TLS Under Migration: Authentication Authority Across CDN and Hosting Transitions](https://arxiv.org/abs/2512.07033)
*Daniyal Ganiuly,Nurzhau Bolatbek,Assel Smaiyl*

Main category: cs.CR

TL;DR: 研究发现托管TLS平台在域名迁移后仍继续使用原证书直到过期，导致认证权限与DNS控制分离，存在多个环境可认证同一域名的安全窗口。


<details>
  <summary>Details</summary>
Motivation: 托管TLS简化了HTTPS部署，但将私钥和证书控制权从域名所有者转移到平台。这种控制权转移在提供商切换时的安全影响尚未充分研究，需要了解平台在域名迁移后的证书管理行为。

Method: 建立受控测量环境，监测多个托管TLS平台在域名迁移后的行为。观察每个平台在委托期间颁发的证书的整个剩余生命周期，监控DNS解析器将流量导向新基础设施后的平台响应。

Result: 所有平台在域名迁移后都继续使用原证书直到过期，没有平台撤销、替换或停用证书，委托结束后也没有颁发新证书。直接连接到原平台仍能完成TLS握手，证明认证能力独立于DNS状态持续存在。

Conclusion: 托管TLS平台在委托期间颁发的证书在整个生命周期内都保持认证权限，导致DNS控制与认证材料控制之间存在安全窗口。随着托管TLS采用增加，需要更清晰的密钥退役和证书失效机制，确保认证权限在过渡期间跟随操作权限。

Abstract: Managed TLS has become a common approach for deploying HTTPS, with platforms generating and storing private keys and automating certificate issuance on behalf of domain operators. This model simplifies operational management but shifts control of authentication material from the domain owner to the platform. The implications of this shift during provider transitions remain insufficiently examined. This study investigates how managed TLS platforms behave when a domain is moved away from the platform that originally issued and stored its certificate. A controlled measurement environment was used to monitor multiple platforms after migration. Each platform was observed for the full remaining lifetime of the certificate that had been active during delegation. The measurements show that platforms continue to serve the same certificate until it expires, even after DNS resolvers direct traffic toward new infrastructure. No platform revoked, replaced, or retired the certificate, and no new certificate was issued after delegation ended. Direct connections to the previous platform continued to complete TLS handshakes with the stale certificate, which confirms that authentication capability persisted independently of DNS state. These findings indicate that authentication authority remains with the previous platform for the entire lifetime of certificates issued during the delegation period. The gap between DNS control and control of authentication material introduces a window in which multiple environments can authenticate the same domain. As managed TLS adoption grows, clearer mechanisms for key retirement and certificate invalidation are needed to ensure that the authentication authority follows operational authority during transitions.

</details>


### [27] [Ideal Attribution and Faithful Watermarks for Language Models](https://arxiv.org/abs/2512.07038)
*Min Jae Song,Kameron Shahabi*

Main category: cs.CR

TL;DR: 论文提出了理想归因机制作为字符串归因决策的形式化抽象，以账本为核心记录模型与用户的交互历史，为水印方案提供统一的理论框架和设计路线图。


<details>
  <summary>Details</summary>
Motivation: 当前水印方案缺乏统一的理论框架，其保证通常是零散的概率性陈述。需要建立形式化抽象来清晰描述归因决策，为水印方案提供理论基础和设计指导。

Method: 提出理想归因机制的形式化抽象，核心是账本（append-only log）记录模型与用户的提示-响应交互历史。机制基于账本和明确的选择标准产生确定性决策，作为归因的基准真值。

Result: 建立了水印方案设计的新视角：将水印方案视为理想归因机制的忠实表示。提供了统一语言描述各种方案的保证，能够精确推理未来水印方案的期望特性。

Conclusion: 该框架为水印方案提供了概念清晰的理论基础，明确了理想化设置下可实现的保证，并为实际方案设计提供了路线图，即使当前没有构造能够实现所有期望特性。

Abstract: We introduce ideal attribution mechanisms, a formal abstraction for reasoning about attribution decisions over strings. At the core of this abstraction lies the ledger, an append-only log of the prompt-response interaction history between a model and its user. Each mechanism produces deterministic decisions based on the ledger and an explicit selection criterion, making it well-suited to serve as a ground truth for attribution. We frame the design goal of watermarking schemes as faithful representation of ideal attribution mechanisms. This novel perspective brings conceptual clarity, replacing piecemeal probabilistic statements with a unified language for stating the guarantees of each scheme. It also enables precise reasoning about desiderata for future watermarking schemes, even when no current construction achieves them, since the ideal functionalities are specified first. In this way, the framework provides a roadmap that clarifies which guarantees are attainable in an idealized setting and worth pursuing in practice.

</details>


### [28] [ThinkTrap: Denial-of-Service Attacks against Black-box LLM Services via Infinite Thinking](https://arxiv.org/abs/2512.07086)
*Yunzhe Li,Jianan Wang,Hongzi Zhu,James Lin,Shan Chang,Minyi Guo*

Main category: cs.CR

TL;DR: ThinkTrap：一种针对黑盒LLM服务的拒绝服务攻击框架，通过优化输入空间诱导模型进入过长或无限生成循环


<details>
  <summary>Details</summary>
Motivation: 随着LLM作为云服务部署，出现了通过无界推理进行DoS攻击的新威胁。攻击者可以设计特殊输入使模型进入过长或无限生成循环，耗尽计算资源。现有防御措施采用闭源黑盒设置，但仍需研究黑盒环境下的攻击方法。

Method: 提出ThinkTrap框架：1）将离散token映射到连续嵌入空间；2）利用输入稀疏性在低维子空间中进行高效黑盒优化；3）识别能诱导扩展或非终止生成的对抗提示，实现最小token开销的DoS攻击。

Result: 在多个商业闭源LLM服务上评估，即使在严格的请求频率限制下（通常10 RPM），攻击可将服务吞吐量降低至原始容量的1%，在某些情况下甚至导致完全服务故障。

Conclusion: ThinkTrap证明了即使在黑盒环境下，通过输入空间优化也能有效实施针对LLM服务的DoS攻击，揭示了当前LLM部署模型的安全漏洞，需要更强大的防御机制。

Abstract: Large Language Models (LLMs) have become foundational components in a wide range of applications, including natural language understanding and generation, embodied intelligence, and scientific discovery. As their computational requirements continue to grow, these models are increasingly deployed as cloud-based services, allowing users to access powerful LLMs via the Internet. However, this deployment model introduces a new class of threat: denial-of-service (DoS) attacks via unbounded reasoning, where adversaries craft specially designed inputs that cause the model to enter excessively long or infinite generation loops. These attacks can exhaust backend compute resources, degrading or denying service to legitimate users. To mitigate such risks, many LLM providers adopt a closed-source, black-box setting to obscure model internals. In this paper, we propose ThinkTrap, a novel input-space optimization framework for DoS attacks against LLM services even in black-box environments. The core idea of ThinkTrap is to first map discrete tokens into a continuous embedding space, then undertake efficient black-box optimization in a low-dimensional subspace exploiting input sparsity. The goal of this optimization is to identify adversarial prompts that induce extended or non-terminating generation across several state-of-the-art LLMs, achieving DoS with minimal token overhead. We evaluate the proposed attack across multiple commercial, closed-source LLM services. Our results demonstrate that, even far under the restrictive request frequency limits commonly enforced by these platforms, typically capped at ten requests per minute (10 RPM), the attack can degrade service throughput to as low as 1% of its original capacity, and in some cases, induce complete service failure.

</details>


### [29] [Breaking ECDSA with Electromagnetic Side-Channel Attacks: Challenges and Practicality on Modern Smartphones](https://arxiv.org/abs/2512.07292)
*Felix Oberhansl,Marc Schink,Nisha Jacob Kabakci,Michael Gruber,Dominik Klein,Sven Freud,Tobias Damm,Michael Hartmeier,Ivan Gavrilan,Silvan Streit,Jonas Stappenbeck,Andreas Seelos Zankl*

Main category: cs.CR

TL;DR: 现代智能手机对物理侧信道攻击（SCA）的脆弱性研究不足，本文通过针对现代SoC的新攻击方法，在Raspberry Pi 4和Fairphone 4上成功恢复ECDSA密钥，证明现有防护措施不足，强调智能手机需要独立认证的安全元件。


<details>
  <summary>Details</summary>
Motivation: 智能手机处理敏感任务（如支付、身份认证），但现代智能手机对物理侧信道攻击的脆弱性研究不足，特别是2019年后的硬件。随着欧盟数字身份钱包等关键应用的发展，评估现代SoC平台对电磁侧信道攻击的抵抗力至关重要。

Method: 使用针对现代SoC的新攻击方法，在Raspberry Pi 4（Broadcom BCM2711 SoC）和Fairphone 4（Snapdragon 750G 5G SoC）上进行电磁侧信道分析。采用Alam等人的Nonce@Once攻击方法，针对OpenSSL和libgcrypt实现ECDSA密钥恢复。通过案例研究分析硬件和软件堆栈对攻击可行性的影响。

Result: 成功从OpenSSL恢复ECDSA密钥，证明libgcrypt的防护措施不能完全缓解攻击。现代SoC的复杂性（异构处理器集群、10nm以下工艺、2GHz以上频率）并未阻止电磁侧信道攻击。Android加密实现存在弱点，需要独立认证的安全元件。

Conclusion: 现代智能手机仍然容易受到物理侧信道攻击，现有软件防护措施不足。为确保敏感应用（如欧盟数字身份钱包）的安全，所有智能手机都需要配备独立认证的安全元件（SE）。

Abstract: Smartphones handle sensitive tasks such as messaging and payment and may soon support critical electronic identification through initiatives such as the European Digital Identity (EUDI) wallet, currently under development. Yet the susceptibility of modern smartphones to physical side-channel analysis (SCA) is underexplored, with recent work limited to pre-2019 hardware. Since then, smartphone system on chip (SoC) platforms have grown more complex, with heterogeneous processor clusters, sub 10 nm nodes, and frequencies over 2 GHz, potentially complicating SCA. In this paper, we assess the feasibility of electromagnetic (EM) SCA on a Raspberry Pi 4, featuring a Broadcom BCM2711 SoC and a Fairphone 4 featuring a Snapdragon 750G 5G SoC. Using new attack methodologies tailored to modern SoCs, we recover ECDSA secrets from OpenSSL by mounting the Nonce@Once attack of Alam et al. (Euro S&P 2021) and show that the libgcrypt countermeasure does not fully mitigate it. We present case studies illustrating how hardware and software stacks impact EM SCA feasibility. Motivated by use cases such as the EUDI wallet, we survey Android cryptographic implementations and define representative threat models to assess the attack. Our findings show weaknesses in ECDSA software implementations and underscore the need for independently certified secure elements (SEs) in all smartphones.

</details>


### [30] [PrivORL: Differentially Private Synthetic Dataset for Offline Reinforcement Learning](https://arxiv.org/abs/2512.07342)
*Chen Gong,Zheng Liu,Kecen Li,Tianhao Wang*

Main category: cs.CR

TL;DR: PrivORL：首个差分隐私离线强化学习数据集合成方法，使用扩散模型和扩散transformer分别合成状态转移和轨迹，通过DP-SGD保护隐私，并引入好奇心驱动预训练提升数据多样性。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习通过共享预收集数据集训练RL模型，避免了与环境交互的成本，但在导航等关键领域应用时存在隐私泄露风险。需要保护离线RL数据集中的隐私信息。

Method: 1. 使用扩散模型合成状态转移，扩散transformer合成轨迹；2. 采用DP-SGD在敏感数据集上进行微调；3. 引入好奇心驱动预训练，利用好奇心模块反馈提升合成数据多样性。

Result: 在五个敏感离线RL数据集上的实验表明，该方法在DP状态转移和轨迹合成方面均优于基线方法，实现了更好的效用和保真度。

Conclusion: PrivORL是首个差分隐私离线RL数据集合成方法，能够生成多样且接近原始敏感数据集的合成数据，同时保护隐私，为下游分析和研究提供安全的数据集。

Abstract: Recently, offline reinforcement learning (RL) has become a popular RL paradigm. In offline RL, data providers share pre-collected datasets -- either as individual transitions or sequences of transitions forming trajectories -- to enable the training of RL models (also called agents) without direct interaction with the environments. Offline RL saves interactions with environments compared to traditional RL, and has been effective in critical areas, such as navigation tasks. Meanwhile, concerns about privacy leakage from offline RL datasets have emerged.
  To safeguard private information in offline RL datasets, we propose the first differential privacy (DP) offline dataset synthesis method, PrivORL, which leverages a diffusion model and diffusion transformer to synthesize transitions and trajectories, respectively, under DP. The synthetic dataset can then be securely released for downstream analysis and research. PrivORL adopts the popular approach of pre-training a synthesizer on public datasets, and then fine-tuning on sensitive datasets using DP Stochastic Gradient Descent (DP-SGD). Additionally, PrivORL introduces curiosity-driven pre-training, which uses feedback from the curiosity module to diversify the synthetic dataset and thus can generate diverse synthetic transitions and trajectories that closely resemble the sensitive dataset. Extensive experiments on five sensitive offline RL datasets show that our method achieves better utility and fidelity in both DP transition and trajectory synthesis compared to baselines. The replication package is available at the GitHub repository.

</details>


### [31] [Amulet: Fast TEE-Shielded Inference for On-Device Model Protection](https://arxiv.org/abs/2512.07495)
*Zikai Mao,Lingchen Zhao,Lei Xu,Wentao Dong,Shenyi Zhang,Cong Wang,Qian Wang*

Main category: cs.CR

TL;DR: Amulet是一个快速TEE保护的设备端推理框架，通过混淆神经网络模型，使其能安全存储在非可信内存中，大幅减少TEE交互次数，显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 设备端机器学习面临模型隐私安全风险，现有TEE保护方案因可信内存有限需要频繁分区加载模型，导致推理延迟大幅增加，需要更高效的解决方案。

Method: 提出针对常见神经网络架构的混淆方法，在TEE中对模型进行混淆转换，使整个模型能安全存储在非可信内存中，利用GPU加速推理，仅需两次最小开销的TEE交互。

Result: 推理延迟仅为未保护模型的2.8-4.8倍，精度损失可忽略，比完全在TEE内执行的基线方法快8-9倍，比最先进的混淆方法快约2.2倍。

Conclusion: Amulet通过创新的混淆技术实现了高效安全的设备端模型保护，在保持模型隐私的同时大幅提升了推理性能，为设备端ML安全提供了实用解决方案。

Abstract: On-device machine learning (ML) introduces new security concerns about model privacy. Storing valuable trained ML models on user devices exposes them to potential extraction by adversaries. The current mainstream solution for on-device model protection is storing the weights and conducting inference within Trusted Execution Environments (TEEs). However, due to limited trusted memory that cannot accommodate the whole model, most existing approaches employ a partitioning strategy, dividing a model into multiple slices that are loaded into the TEE sequentially. This frequent interaction between untrusted and trusted worlds dramatically increases inference latency, sometimes by orders of magnitude. In this paper, we propose Amulet, a fast TEE-shielded on-device inference framework for ML model protection. Amulet incorporates a suite of obfuscation methods specifically designed for common neural network architectures. After obfuscation by the TEE, the entire transformed model can be securely stored in untrusted memory, allowing the inference process to execute directly in untrusted memory with GPU acceleration. For each inference request, only two rounds of minimal-overhead interaction between untrusted and trusted memory are required to process input samples and output results. We also provide theoretical proof from an information-theoretic perspective that the obfuscated model does not leak information about the original weights. We comprehensively evaluated Amulet using diverse model architectures ranging from ResNet-18 to GPT-2. Our approach incurs inference latency only 2.8-4.8x that of unprotected models with negligible accuracy loss, achieving an 8-9x speedup over baseline methods that execute inference entirely within TEEs, and performing approximately 2.2x faster than the state-of-the-art obfuscation-based method.

</details>


### [32] [VulnLLM-R: Specialized Reasoning LLM with Agent Scaffold for Vulnerability Detection](https://arxiv.org/abs/2512.07533)
*Yuzhou Nie,Hongwei Li,Chengquan Guo,Ruizhe Jiang,Zhun Wang,Bo Li,Dawn Song,Wenbo Guo*

Main category: cs.CR

TL;DR: VulnLLM-R是首个专门用于漏洞检测的推理大语言模型，通过程序状态推理而非简单模式匹配来提升泛化能力，在7B参数规模下超越现有静态分析工具和商业大模型。


<details>
  <summary>Details</summary>
Motivation: 现有推理大模型要么规模过大、闭源，要么在漏洞检测方面性能有限。需要专门化的推理模型来提升漏洞检测的泛化能力，避免学习捷径。

Method: 提出新颖的训练方法：专门化数据选择、推理数据生成、推理数据过滤与修正、测试阶段优化。基于此方法训练了一个70亿参数的推理模型。

Result: 在Python、C/C++、Java的SOTA数据集上，VulnLLM-R在效果和效率上均优于现有静态分析工具及开源/商业大模型。构建的智能体在真实项目中超越CodeQL和AFL++，并在活跃仓库中发现零日漏洞。

Conclusion: 这是首个使用专门化推理模型驱动的AI智能体实现真实世界项目级漏洞检测的开创性工作，为AI驱动的安全分析提供了新方向。

Abstract: We propose VulnLLM-R, the~\emph{first specialized reasoning LLM} for vulnerability detection. Our key insight is that LLMs can reason about program states and analyze the potential vulnerabilities, rather than simple pattern matching. This can improve the model's generalizability and prevent learning shortcuts. However, SOTA reasoning LLMs are typically ultra-large, closed-source, or have limited performance in vulnerability detection. To address this, we propose a novel training recipe with specialized data selection, reasoning data generation, reasoning data filtering and correction, and testing-phase optimization. Using our proposed methodology, we train a reasoning model with seven billion parameters. Through extensive experiments on SOTA datasets across Python, C/C++, and Java, we show that VulnLLM-R has superior effectiveness and efficiency than SOTA static analysis tools and both open-source and commercial large reasoning models. We further conduct a detailed ablation study to validate the key designs in our training recipe. Finally, we construct an agent scaffold around our model and show that it outperforms CodeQL and AFL++ in real-world projects. Our agent further discovers a set of zero-day vulnerabilities in actively maintained repositories. This work represents a pioneering effort to enable real-world, project-level vulnerability detection using AI agents powered by specialized reasoning models. The code is available at~\href{https://github.com/ucsb-mlsec/VulnLLM-R}{github}.

</details>


### [33] [Privacy Practices of Browser Agents](https://arxiv.org/abs/2512.07725)
*Alisha Ukani,Hamed Haddadi,Ali Shahin Shamsabadi,Peter Snyder*

Main category: cs.CR

TL;DR: 对8个主流浏览器代理进行系统性隐私风险评估，发现30个漏洞，包括隐私功能禁用和敏感信息自动填充等问题


<details>
  <summary>Details</summary>
Motivation: 浏览器代理利用大语言模型自动化网页浏览，其强大的自动化能力也带来了高风险。这些工具处理的任务类型和托管的信息意味着漏洞可能导致严重的隐私危害

Method: 提出包含5大因素（共15个具体测量指标）的隐私风险评估框架：1)代理组件漏洞 2)网站行为防护 3)跨站追踪防护 4)隐私影响提示响应 5)个人信息泄露检测。将该框架应用于8个浏览器代理

Result: 在8个浏览器代理中发现30个漏洞，包括浏览器隐私功能被禁用、敏感个人信息在表单字段中"自动填充"等问题。已负责任地披露发现，并计划发布数据集和其他成果

Conclusion: 浏览器代理存在显著的隐私风险，需要更严格的隐私保护措施。提出的评估框架能有效识别这些风险，为改进浏览器代理的隐私安全性提供了基础

Abstract: This paper presents a systematic evaluation of the privacy behaviors and attributes of eight recent, popular browser agents. Browser agents are software that automate Web browsing using large language models and ancillary tooling. However, the automated capabilities that make browser agents powerful also make them high-risk points of failure. Both the kinds of tasks browser agents are designed to execute, along with the kinds of information browser agents are entrusted with to fulfill those tasks, mean that vulnerabilities in these tools can result in enormous privacy harm.
  This work presents a framework of five broad factors (totaling 15 distinct measurements) to measure the privacy risks in browser agents. Our framework assesses i. vulnerabilities in the browser agent's components, ii. how the browser agent protects against website behaviors, iii. whether the browser agent prevents cross-site tracking, iv. how the agent responds to privacy-affecting prompts, and v. whether the tool leaks personal information to sites. We apply our framework to eight browser agents and identify 30 vulnerabilities, ranging from disabled browser privacy features to "autocompleting" sensitive personal information in form fields. We have responsibly disclosed our findings, and plan to release our dataset and other artifacts.

</details>


### [34] [An Adaptive Multi-Layered Honeynet Architecture for Threat Behavior Analysis via Deep Learning](https://arxiv.org/abs/2512.07827)
*Lukas Johannes Möller*

Main category: cs.CR

TL;DR: ADLAH是一个自适应深度学习异常检测蜜网系统，通过强化学习实时决策何时将低交互传感器节点升级到高交互蜜罐，实现成本高效的威胁情报收集和自动化僵尸网络攻击链分析。


<details>
  <summary>Details</summary>
Motivation: 网络威胁日益复杂多样，传统静态蜜罐已无法满足需求，需要自适应、智能驱动的欺骗技术来获取高质量威胁情报，同时控制成本。

Method: 提出端到端AI驱动欺骗平台架构，核心是强化学习代理实时决策会话升级时机：从低交互传感器节点动态升级到高交互蜜罐。系统还包括自动化攻击链提取、聚类和版本管理功能。

Result: 开发了核心决策机制的功能原型，证明了技术可行性。由于缺乏真实大规模数据，未进行现场规模验证，但详细分析了设计权衡和限制，并提供了严格的规模化实证评估路线图。

Conclusion: ADLAH架构为实现成本高效的高价值对手行为捕获、系统化僵尸网络版本管理和可操作威胁情报生产提供了实用路径，特别适合处理以自动化流量为主的暴露服务场景。

Abstract: The escalating sophistication and variety of cyber threats have rendered static honeypots inadequate, necessitating adaptive, intelligence-driven deception. In this work, ADLAH is introduced: an Adaptive Deep Learning Anomaly Detection Honeynet designed to maximize high-fidelity threat intelligence while minimizing cost through autonomous orchestration of infrastructure. The principal contribution is offered as an end-to-end architectural blueprint and vision for an AI-driven deception platform. Feasibility is evidenced by a functional prototype of the central decision mechanism, in which a reinforcement learning (RL) agent determines, in real time, when sessions should be escalated from low-interaction sensor nodes to dynamically provisioned, high-interaction honeypots. Because sufficient live data were unavailable, field-scale validation is not claimed; instead, design trade-offs and limitations are detailed, and a rigorous roadmap toward empirical evaluation at scale is provided. Beyond selective escalation and anomaly detection, the architecture pursues automated extraction, clustering, and versioning of bot attack chains, a core capability motivated by the empirical observation that exposed services are dominated by automated traffic. Together, these elements delineate a practical path toward cost-efficient capture of high-value adversary behavior, systematic bot versioning, and the production of actionable threat intelligence.

</details>
