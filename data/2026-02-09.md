<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 30]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Multi-Agent-Driven Cognitive Secure Communications in Satellite-Terrestrial Networks](https://arxiv.org/abs/2602.06048)
*Yujie Ling,Zan Li,Lei Guan,Zheng Zhang,Shengyu Zhang,Tony Q. S. Quek*

Main category: cs.CR

TL;DR: 该论文提出了一种基于多智能体协调的认知安全通信框架，用于卫星-地面网络，通过频谱调度和对抗性保护来增强安全性，同时降低功耗。


<details>
  <summary>Details</summary>
Motivation: 卫星-地面网络面临恶意窃听者的严重威胁，这些窃听者具有非合作行为和智能攻击能力，会危及用户的隐私信息安全。需要一种能够同时保证可靠传输和增强安全性的解决方案。

Method: 提出两层协调防御系统：1) 基础层采用多智能体协调调度确定卫星操作矩阵和频隙占用矩阵，缓解频谱拥塞；2) 保护层利用生成对抗网络产生对抗矩阵，并采用学习辅助的功率控制设置真实和对抗信号功率，主动降低窃听者的推断能力。

Result: 仿真结果表明，该方法在认知安全通信场景下，相比基准方法在增强安全性能和降低功耗方面表现更优。

Conclusion: 提出的多智能体驱动的认知安全通信框架能有效协调频谱调度和保护，在保证可靠传输的同时增强卫星-地面网络的安全性，为应对智能窃听威胁提供了有效解决方案。

Abstract: Satellite-terrestrial networks (STNs) have emerged as a promising architecture for providing seamless wireless coverage and connectivity for multiple users. However, potential malicious eavesdroppers pose a serious threat to the private information via STNs due to their non-cooperative behavior and ability to launch intelligent attacks. To address this challenge, we propose a cognitive secure communication framework driven by multiple agents that coordinates spectrum scheduling and protection through real-time sensing, thereby disrupting the judgment of eavesdroppers while preserving reliable data transmission. On this basis, we formulate an optimization problem to maximize the secrecy probability of legitimate users, subject to a reliable transmission probability threshold. To tackle this problem, we propose a two-layer coordinated defense system. First, we develop a foundation layer based on multi-agent coordination schedule to determine the satellite operation matrix and the frequency slot occupation matrices, aiming to mitigate spectrum congestion and enhance transmission reliability. Then, we exploit generative adversarial networks to produce adversarial matrices, and employ learning-aided power control to set real and adversarial signal powers for protection layer, which actively degrades the inference capability of eavesdroppers. Simulation results demonstrate that the proposed method outperforms benchmark methods in terms of enhancing security performance and reducing power overhead for STNs in the cognitive secure communication scenario.

</details>


### [2] [Know Your Scientist: KYC as Biosecurity Infrastructure](https://arxiv.org/abs/2602.06172)
*Jonathan Feldman,Tal Feldman,Annie I Anton*

Main category: cs.CR

TL;DR: 提出基于KYC（了解你的客户）的三层生物AI安全框架，将治理重点从内容审查转向用户验证和监控，以应对蛋白质设计工具的双重用途风险


<details>
  <summary>Details</summary>
Motivation: 当前用于蛋白质设计和结构预测的生物AI工具快速发展，带来了现有安全措施无法充分应对的双重用途风险。现有的模型级限制措施（关键词过滤、输出筛查、基于内容的访问拒绝）从根本上不适合生物学领域，因为可靠的功能预测仍然难以实现，新颖威胁会通过设计规避检测。

Method: 提出受金融领域反洗钱实践启发的三层KYC框架：第一层利用研究机构作为信任锚点，为附属研究人员提供担保并承担审查责任；第二层通过序列同源性搜索和功能注释进行输出筛查；第三层监控行为模式以检测与声明研究目的不一致的异常情况。

Result: 该分层方法为合法研究人员保留了访问权限，同时通过机构问责和可追溯性提高了滥用成本。该框架可以立即使用现有机构基础设施实施，无需新的立法或监管授权。

Conclusion: KYC框架将生物AI治理从内容检查转向用户验证和监控，能够更好地应对蛋白质设计工具的双重用途风险，同时保持科学研究的可访问性。

Abstract: Biological AI tools for protein design and structure prediction are advancing rapidly, creating dual-use risks that existing safeguards cannot adequately address. Current model-level restrictions, including keyword filtering, output screening, and content-based access denials, are fundamentally ill-suited to biology, where reliable function prediction remains beyond reach and novel threats evade detection by design. We propose a three-tier Know Your Customer (KYC) framework, inspired by anti-money laundering (AML) practices in the financial sector, that shifts governance from content inspection to user verification and monitoring. Tier I leverages research institutions as trust anchors to vouch for affiliated researchers and assume responsibility for vetting. Tier II applies output screening through sequence homology searches and functional annotation. Tier III monitors behavioral patterns to detect anomalies inconsistent with declared research purposes. This layered approach preserves access for legitimate researchers while raising the cost of misuse through institutional accountability and traceability. The framework can be implemented immediately using existing institutional infrastructure, requiring no new legislation or regulatory mandates.

</details>


### [3] [Identifying Adversary Tactics and Techniques in Malware Binaries with an LLM Agent](https://arxiv.org/abs/2602.06325)
*Zhou Xuan,Xiangzhe Xu,Mingwei Zheng,Louis Zheng-Hua Tan,Jinyao Guo,Tiantai Zhang,Le Yu,Chengpeng Wang,Xiangyu Zhang*

Main category: cs.CR

TL;DR: TTPDetect：首个基于LLM代理的恶意软件二进制文件TTP识别系统，通过检索增强和推理对齐，在函数级TTP识别上达到93%以上的准确率。


<details>
  <summary>Details</summary>
Motivation: 现实中的恶意软件二进制文件通常被剥离符号、包含大量函数，且恶意行为分布在多个代码区域，使得TTP（战术、技术和程序）归因变得困难。现有LLM直接应用面临分析入口点识别、部分可观测性下的推理以及与TTP特定决策逻辑不对齐等挑战。

Method: TTPDetect结合密集检索与基于LLM的神经检索来缩小分析入口点空间，采用函数级分析代理，包含按需增量上下文检索的Context Explorer和实现推理时对齐的TTP-Specific Reasoning Guideline。构建了跨不同恶意软件家族和平台的TTP标注数据集。

Result: 在函数级TTP识别上达到93.25%的精确率和93.81%的召回率，分别比基线方法提升10.38%和18.78%。在真实恶意软件样本上，TTPDetect以87.37%的精确率识别TTP。对于有专家报告记录的恶意软件，恢复了85.7%的已记录TTP，并平均每个恶意软件发现了10.5个先前未报告的TTP。

Conclusion: TTPDetect是首个用于剥离符号恶意软件二进制文件中TTP识别的LLM代理系统，通过检索增强和推理对齐有效解决了现有挑战，在TTP识别方面表现出色，并能发现先前未记录的TTP，为安全分析和威胁情报提供了有力工具。

Abstract: Understanding TTPs (Tactics, Techniques, and Procedures) in malware binaries is essential for security analysis and threat intelligence, yet remains challenging in practice. Real-world malware binaries are typically stripped of symbols, contain large numbers of functions, and distribute malicious behavior across multiple code regions, making TTP attribution difficult. Recent large language models (LLMs) offer strong code understanding capabilities, but applying them directly to this task faces challenges in identifying analysis entry points, reasoning under partial observability, and misalignment with TTP-specific decision logic. We present TTPDetect, the first LLM agent for recognizing TTPs in stripped malware binaries. TTPDetect combines dense retrieval with LLM-based neural retrieval to narrow the space of analysis entry points. TTPDetect further employs a function-level analyzing agent consisting of a Context Explorer that performs on-demand, incremental context retrieval and a TTP-Specific Reasoning Guideline that achieves inference-time alignment. We build a new dataset that labels decompiled functions with TTPs across diverse malware families and platforms. TTPDetect achieves 93.25% precision and 93.81% recall on function-level TTP recognition, outperforming baselines by 10.38% and 18.78%, respectively. When evaluated on real world malware samples, TTPDetect recognizes TTPs with a precision of 87.37%. For malware with expert-written reports, TTPDetect recovers 85.7% of the documented TTPs and further discovers, on average, 10.5 previously unreported TTPs per malware.

</details>


### [4] [AdFL: In-Browser Federated Learning for Online Advertisement](https://arxiv.org/abs/2602.06336)
*Ahmad Alemari,Pritam Sen,Cristian Borcea*

Main category: cs.CR

TL;DR: AdFL是一个基于浏览器的联邦学习框架，用于学习用户广告偏好，在保护用户隐私的同时实现精准广告投放，实验显示其广告可见性预测AUC可达92.59%。


<details>
  <summary>Details</summary>
Motivation: 随着GDPR等在线隐私法规的出台，在线出版商需要在定向广告收入和用户隐私之间找到平衡。传统方法需要共享用户原始数据，存在隐私风险，而联邦学习可以在不共享原始数据的情况下进行分布式学习。

Method: 提出AdFL框架，在浏览器中运行联邦学习来学习用户广告偏好。该框架利用浏览器标准API，无需客户端安装额外软件，支持任何使用浏览器可用特征（如广告可见性、点击率、页面停留时间等）的模型。服务器端运行在出版商处，协调用户学习过程。

Result: 构建了广告可见性预测的概念验证模型，在日访问量4万的非重叠数据集上测试。实验表明：AdFL能在几毫秒内捕获浏览器中的训练信息；广告可见性预测AUC最高达92.59%；使用差分隐私保护本地模型参数时性能良好，与非差分隐私版本相比仅有适度下降。

Conclusion: AdFL框架证明了在浏览器中实施联邦学习进行广告偏好学习的可行性，能够在保护用户隐私的同时实现有效的广告定向，为在线出版商在隐私法规环境下平衡广告收入和用户隐私提供了解决方案。

Abstract: Since most countries are coming up with online privacy regulations, such as GDPR in the EU, online publishers need to find a balance between revenue from targeted advertisement and user privacy. One way to be able to still show targeted ads, based on user personal and behavioral information, is to employ Federated Learning (FL), which performs distributed learning across users without sharing user raw data with other stakeholders in the publishing ecosystem. This paper presents AdFL, an FL framework that works in the browsers to learn user ad preferences. These preferences are aggregated in a global FL model, which is then used in the browsers to show more relevant ads to users. AdFL can work with any model that uses features available in the browser such as ad viewability, ad click-through, user dwell time on pages, and page content. The AdFL server runs at the publisher and coordinates the learning process for the users who browse pages on the publisher's website. The AdFL prototype does not require the client to install any software, as it is built utilizing standard APIs available on most modern browsers. We built a proof-of-concept model for ad viewability prediction that runs on top of AdFL. We tested AdFL and the model with two non-overlapping datasets from a website with 40K visitors per day. The experiments demonstrate AdFL's feasibility to capture the training information in the browser in a few milliseconds, show that the ad viewability prediction achieves up to 92.59% AUC, and indicate that utilizing differential privacy (DP) to safeguard local model parameters yields adequate performance, with only modest declines in comparison to the non-DP variant.

</details>


### [5] [Zero-Trust Runtime Verification for Agentic Payment Protocols: Mitigating Replay and Context-Binding Failures in AP2](https://arxiv.org/abs/2602.06345)
*Qianlong Lan,Anuj Kaul,Shaun Jones,Stephanie Westrum*

Main category: cs.CR

TL;DR: 提出零信任运行时验证框架，通过动态生成时间绑定nonce来强制执行上下文绑定和一次性消费语义，解决AP2授权协议在AI代理支付系统中的运行时安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 自主AI代理执行商业交易的需求推动了基于授权的支付协议（如UCP和AP2）的采用。这些协议使用加密授权取代交互式会话授权，支持异步自主执行。虽然AP2在规范层面提供了签名验证、显式绑定和过期语义等保证，但实际代理执行中的重试、并发和编排等运行时行为对授权使用的隐含假设提出了挑战。

Method: 提出零信任运行时验证框架，使用动态生成的时间绑定nonce来强制执行显式上下文绑定和一次性消费授权语义。该框架确保授权决策在运行时评估，而不是基于静态签发属性进行假设。

Result: 通过高并发下的模拟评估表明，上下文感知绑定和一次性消费强制执行解决了不同且互补的攻击类别，两者都需要防止重放和上下文重定向攻击。该框架缓解了所有评估的攻击，同时在高达10,000 TPS的吞吐量水平下保持约3.8ms的稳定验证延迟。所需的运行时状态受峰值并发限制而非累积交易历史，表明可以实现最小且可预测的开销。

Conclusion: 提出的零信任运行时验证框架通过强制执行上下文绑定和一次性消费语义，有效解决了AP2授权生命周期中的运行时安全漏洞，为基于代理的支付系统提供了稳健的运行时安全保障，且开销可控。

Abstract: The deployment of autonomous AI agents capable of executing commercial transactions has motivated the adoption of mandate-based payment authorization protocols, including the Universal Commerce Protocol (UCP) and the Agent Payments Protocol (AP2). These protocols replace interactive, session-based authorization with cryptographically issued mandates, enabling asynchronous and autonomous execution. While AP2 provides specification-level guarantees through signature verification, explicit binding, and expiration semantics, real-world agentic execution introduces runtime behaviors such as retries, concurrency, and orchestration that challenge implicit assumptions about mandate usage.
  In this work, we present a security analysis of the AP2 mandate lifecycle and identify enforcement gaps that arise during runtime in agent-based payment systems. We propose a zero-trust runtime verification framework that enforces explicit context binding and consume-once mandate semantics using dynamically generated, time-bound nonces, ensuring that authorization decisions are evaluated at execution time rather than assumed from static issuance properties.
  Through simulation-based evaluation under high concurrency, we show that context-aware binding and consume-once enforcement address distinct and complementary attack classes, and that both are required to prevent replay and context-redirect attacks. The proposed framework mitigates all evaluated attacks while maintaining stable verification latency of approximately 3.8~ms at throughput levels up to 10{,}000 transactions per second. We further demonstrate that the required runtime state is bounded by peak concurrency rather than cumulative transaction history, indicating that robust runtime security for agentic payment execution can be achieved with minimal and predictable overhead.

</details>


### [6] [Empirical Analysis of Adversarial Robustness and Explainability Drift in Cybersecurity Classifiers](https://arxiv.org/abs/2602.06395)
*Mona Rajhans,Vishal Khawarey*

Main category: cs.CR

TL;DR: 该论文实证研究了网络安全领域中机器学习模型对抗性攻击的鲁棒性和可解释性漂移问题，提出了鲁棒性指数(RI)作为量化指标，并通过实验验证对抗训练能提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在网络安全应用中日益普及，但容易受到对抗性扰动攻击，这些微小但有意的输入修改会降低检测准确性和可解释性。需要研究对抗性鲁棒性与可解释性漂移之间的关系，以设计更可信的AI驱动网络安全系统。

Method: 在两个网络安全领域（钓鱼URL分类和网络入侵检测）进行实证研究，使用L∞有界的FGSM和PGD扰动方法评估模型准确性影响。引入鲁棒性指数(RI)作为量化指标（准确率-扰动曲线下面积）。采用基于梯度的特征敏感性和SHAP归因漂移分析来识别最易受对抗性操纵的输入特征。

Result: 在Phishing Websites和UNSW NB15数据集上的实验显示一致的鲁棒性趋势：对抗训练能将RI提升高达9%，同时保持干净数据的准确性。分析揭示了哪些输入特征最易受对抗性操纵，以及鲁棒性与可解释性退化之间的耦合关系。

Conclusion: 研究发现对抗性鲁棒性与可解释性退化之间存在紧密耦合关系，强调了在可信AI驱动网络安全系统设计中定量评估的重要性。对抗训练能有效提升模型鲁棒性而不损害干净数据性能。

Abstract: Machine learning (ML) models are increasingly deployed in cybersecurity applications such as phishing detection and network intrusion prevention. However, these models remain vulnerable to adversarial perturbations small, deliberate input modifications that can degrade detection accuracy and compromise interpretability. This paper presents an empirical study of adversarial robustness and explainability drift across two cybersecurity domains phishing URL classification and network intrusion detection. We evaluate the impact of L (infinity) bounded Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) perturbations on model accuracy and introduce a quantitative metric, the Robustness Index (RI), defined as the area under the accuracy perturbation curve. Gradient based feature sensitivity and SHAP based attribution drift analyses reveal which input features are most susceptible to adversarial manipulation. Experiments on the Phishing Websites and UNSW NB15 datasets show consistent robustness trends, with adversarial training improving RI by up to 9 percent while maintaining clean-data accuracy. These findings highlight the coupling between robustness and interpretability degradation and underscore the importance of quantitative evaluation in the design of trustworthy, AI-driven cybersecurity systems.

</details>


### [7] [VENOMREC: Cross-Modal Interactive Poisoning for Targeted Promotion in Multimodal LLM Recommender Systems](https://arxiv.org/abs/2602.06409)
*Guowei Guan,Yurong Hao,Jiaming Zhang,Tiantong Wu,Fuyao Zhang,Tianxiang Chen,Longtao Huang,Cyril Leung,Wei Yang Bryan Lim*

Main category: cs.CR

TL;DR: 论文提出VENOMREC攻击方法，通过跨模态协同投毒来操纵多模态推荐系统，利用暴露对齐和跨模态交互扰动技术，在保持推荐效用的同时显著提升攻击效果。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型推动推荐系统向基于内容的检索和排序发展，虽然跨模态共识能缓解传统投毒攻击，但也引入了新的攻击面——同步多模态投毒可以在微调期间稳定地操纵融合表示。

Method: 提出VENOMREC方法：1) 暴露对齐：识别联合嵌入空间中的高暴露区域；2) 跨模态交互扰动：通过注意力引导的耦合token-patch编辑来制作协同攻击样本。

Result: 在三个真实世界多模态数据集上的实验表明，VENOMREC始终优于强基线方法，平均ER@20达到0.73，比最强基线提升+0.52绝对ER点，同时保持相当的推荐效用。

Conclusion: 跨模态交互投毒是多模态推荐系统的新安全威胁，VENOMREC方法有效展示了这种攻击的可行性，为多模态推荐系统的安全性研究提供了重要启示。

Abstract: Multimodal large language models (MLLMs) are pushing recommender systems (RecSys) toward content-grounded retrieval and ranking via cross-modal fusion. We find that while cross-modal consensus often mitigates conventional poisoning that manipulates interaction logs or perturbs a single modality, it also introduces a new attack surface where synchronised multimodal poisoning can reliably steer fused representations along stable semantic directions during fine-tuning. To characterise this threat, we formalise cross-modal interactive poisoning and propose VENOMREC, which performs Exposure Alignment to identify high-exposure regions in the joint embedding space and Cross-modal Interactive Perturbation to craft attention-guided coupled token-patch edits. Experiments on three real-world multimodal datasets demonstrate that VENOMREC consistently outperforms strong baselines, achieving 0.73 mean ER@20 and improving over the strongest baseline by +0.52 absolute ER points on average, while maintaining comparable recommendation utility.

</details>


### [8] [The Avatar Cache: Enabling On-Demand Security with Morphable Cache Architecture](https://arxiv.org/abs/2602.06433)
*Anubhav Bhatla,Navneet Navneet,Moinuddin Qureshi,Biswabandan Panda*

Main category: cs.CR

TL;DR: Avatar：一种安全可变的末级缓存设计，支持三种模式动态切换，在保持接近传统缓存结构的同时提供强安全保证


<details>
  <summary>Details</summary>
Motivation: 现有安全LLC设计存在显著缺陷：随机化方案（如Mirage）存储开销大（20%），分区方案性能开销大（平均超5%，最高49%），且需要OS支持或面临可扩展性问题，阻碍工业采用。本文探索能否以最小改动实现强LLC安全，仅在需要时启用安全功能。

Method: 提出Avatar缓存架构，支持三种模式动态切换：非安全模式（Avatar-N）、随机化安全模式（Avatar-R）和分区安全模式（Avatar-P）。Avatar-R通过引入额外无效条目并利用高相联度提供强安全保证；Avatar-P同时缓解冲突和占用攻击。设计保持传统组相联缓存结构，便于工业采用。

Result: Avatar-R每10^30年才发生一次组相联驱逐，仅带来1.5%存储开销、2.7%静态功耗增加和0.2%性能下降。Avatar-P仅产生3%性能开销，显著优于先前基于路的分区方案。当不需要安全时，可切换至Avatar-N最大化性能和能效。

Conclusion: Avatar通过最小化改动传统缓存结构，实现了强LLC安全与低开销的平衡，支持按需动态切换安全模式，解决了现有安全LLC设计的工业采用障碍。

Abstract: The sharing of the last-level cache (LLC) among multiple cores makes it vulnerable to cross-core conflict- and occupancy-based attacks. Despite extensive prior work, modern processors still employ non-secure set-associative LLCs. Existing secure LLC designs broadly fall into two categories: (i) randomized and (ii) partitioned. The state-of-the-art randomized design, Mirage, mitigates conflict-based attacks but incurs significant area overhead (20% additional storage) and design complexity. Partitioned LLCs mitigate both conflict- and occupancy-based attacks, but often suffer from large performance overheads (on average over 5% and up to 49%), require OS support in set-based schemes, or face scalability issues in way-based schemes. These factors pose major obstacles to the industrial adoption of secure LLCs. This paper asks whether strong LLC security can be achieved with minimal changes to a conventional set-associative LLC, enabling security only when needed while preserving low performance, power, and area overheads. We propose Avatar, a secure and morphable LLC that supports three modes: non-secure (Avatar-N), randomized secure (Avatar-R), and partitioned secure (Avatar-P), and can switch dynamically between them. Avatar closely resembles a conventional set-associative LLC, facilitating industrial adoption. Avatar-R introduces extra invalid entries and leverages high associativity to provide a strong security guarantee with little capacity loss, achieving only one set-associative eviction per $10^{30}$ years, while incurring 1.5% storage overhead, a 2.7% increase in static power, and a 0.2% slowdown over a 16~MB baseline. Avatar-P mitigates both conflict- and occupancy-based attacks with only a 3% performance overhead, substantially outperforming prior way-based partitioned LLCs. When security is unnecessary, Avatar switches to Avatar-N to maximize performance and energy efficiency.

</details>


### [9] [TrajAD: Trajectory Anomaly Detection for Trustworthy LLM Agents](https://arxiv.org/abs/2602.06443)
*Yibing Liu,Chong Zhang,Zhongyi Han,Hansong Liu,Yong Wang,Yang Yu,Xiaoyan Wang,Yilong Yin*

Main category: cs.CR

TL;DR: 提出轨迹异常检测任务，构建TrajBench数据集，开发TrajAD专用验证器，用于LLM代理执行过程中的异常检测和定位


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理安全措施主要关注静态输入/输出过滤，但确保可靠性需要审计中间执行过程，需要精确的异常检测和定位能力以实现高效的回滚重试

Method: 通过扰动-完成策略构建TrajBench数据集，提出TrajAD专用验证器，使用细粒度过程监督进行训练

Result: 通用LLM即使在零样本提示下也难以识别和定位轨迹异常，而TrajAD方法优于基线，证明专用监督对构建可信代理至关重要

Conclusion: 通用能力不能自动转化为过程可靠性，需要专门的轨迹异常检测方法和细粒度过程监督来构建可信的LLM代理

Abstract: We address the problem of runtime trajectory anomaly detection, a critical capability for enabling trustworthy LLM agents. Current safety measures predominantly focus on static input/output filtering. However, we argue that ensuring LLM agents reliability requires auditing the intermediate execution process. In this work, we formulate the task of Trajectory Anomaly Detection. The goal is not merely detection, but precise error localization. This capability is essential for enabling efficient rollback-and-retry. To achieve this, we construct TrajBench, a dataset synthesized via a perturb-and-complete strategy to cover diverse procedural anomalies. Using this benchmark, we investigate the capability of models in process supervision. We observe that general-purpose LLMs, even with zero-shot prompting, struggle to identify and localize these anomalies. This reveals that generalized capabilities do not automatically translate to process reliability. To address this, we propose TrajAD, a specialized verifier trained with fine-grained process supervision. Our approach outperforms baselines, demonstrating that specialized supervision is essential for building trustworthy agents.

</details>


### [10] [Subgraph Reconstruction Attacks on Graph RAG Deployments with Practical Defenses](https://arxiv.org/abs/2602.06495)
*Minkyoo Song,Jaehan Kim,Myungchul Kang,Hanna Kim,Seungwon Shin,Sooel Son*

Main category: cs.CR

TL;DR: 论文提出GRASP攻击方法，能够从受保护的Graph RAG系统中有效提取知识图谱子图，同时评估防御措施。


<details>
  <summary>Details</summary>
Motivation: Graph RAG系统虽然增强了关系推理能力，但存在隐私泄露风险：攻击者可能重构知识图谱子图，窃取私有知识资产。现有攻击方法在简单安全提示下效果不佳，需要更有效的提取技术。

Method: 提出GRASP攻击方法：1) 将提取重构为上下文处理任务；2) 使用每记录标识符强制格式合规、实例接地的输出，减少幻觉并保留关系细节；3) 使用动量感知调度器多样化目标驱动攻击查询，在严格查询预算内操作。

Result: 在两个真实知识图谱、四个安全对齐LLM和多个Graph RAG框架上，GRASP实现了最强的类型忠实重构，达到82.9 F1分数，而先前方法完全失败。

Conclusion: Graph RAG系统面临严重的隐私泄露风险，GRASP攻击能有效提取知识图谱。论文还提出了两种轻量级防御措施，能在不损失实用性的情况下显著降低重构保真度。

Abstract: Graph-based retrieval-augmented generation (Graph RAG) is increasingly deployed to support LLM applications by augmenting user queries with structured knowledge retrieved from a knowledge graph. While Graph RAG improves relational reasoning, it introduces a largely understudied threat: adversaries can reconstruct subgraphs from a target RAG system's knowledge graph, enabling privacy inference and replication of curated knowledge assets. We show that existing attacks are largely ineffective against Graph RAG even with simple prompt-based safeguards, because these attacks expose explicit exfiltration intent and are therefore easily suppressed by lightweight safe prompts. We identify three technical challenges for practical Graph RAG extraction under realistic safeguards and introduce GRASP, a closed-box, multi-turn subgraph reconstruction attack. GRASP (i) reframes extraction as a context-processing task, (ii) enforces format-compliant, instance-grounded outputs via per-record identifiers to reduce hallucinations and preserve relational details, and (iii) diversifies goal-driven attack queries using a momentum-aware scheduler to operate within strict query budgets. Across two real-world knowledge graphs, four safety-aligned LLMs, and multiple Graph RAG frameworks, GRASP attains the strongest type-faithful reconstruction where prior methods fail, reaching up to 82.9 F1. We further evaluate defenses and propose two lightweight mitigations that substantially reduce reconstruction fidelity without utility loss.

</details>


### [11] [Sequential Auditing for f-Differential Privacy](https://arxiv.org/abs/2602.06518)
*Tim Kutta,Martin Dunsche,Yu Wei,Vassilis Zikas*

Main category: cs.CR

TL;DR: 提出新的差分隐私审计方法，基于输出样本评估算法的隐私保护水平，专注于f-DP概念，自适应确定样本量，支持白盒和黑盒设置。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私审计方法大多是批量处理或针对传统(ε,δ)-DP，需要用户指定样本量，导致样本量过大，特别是在DP-SGD等昂贵训练过程中成本过高。

Method: 开发基于f-DP概念的新型审计器，能够检测整个隐私谱系的违规行为，自适应确定最优样本量，无需用户预先指定，支持白盒和黑盒设置，可在单次运行框架中执行。

Result: 审计器具有统计显著性保证，理论和模拟支持其有效性，显著减少所需样本量，特别适用于DP-SGD等昂贵训练过程。

Conclusion: 提出的f-DP审计方法比传统方法更高效灵活，自适应样本量确定机制解决了现有审计中样本量过大的问题，为差分隐私算法的实际验证提供了实用工具。

Abstract: We present new auditors to assess Differential Privacy (DP) of an algorithm based on output samples. Such empirical auditors are common to check for algorithmic correctness and implementation bugs. Most existing auditors are batch-based or targeted toward the traditional notion of $(\varepsilon,δ)$-DP; typically both. In this work, we shift the focus to the highly expressive privacy concept of $f$-DP, in which the entire privacy behavior is captured by a single tradeoff curve. Our auditors detect violations across the full privacy spectrum with statistical significance guarantees, which are supported by theory and simulations. Most importantly, and in contrast to prior work, our auditors do not require a user-specified sample size as an input. Rather, they adaptively determine a near-optimal number of samples needed to reach a decision, thereby avoiding the excessively large sample sizes common in many auditing studies. This reduction in sampling cost becomes especially beneficial for expensive training procedures such as DP-SGD. Our method supports both whitebox and blackbox settings and can also be executed in single-run frameworks.

</details>


### [12] [Dependable Artificial Intelligence with Reliability and Security (DAIReS): A Unified Syndrome Decoding Approach for Hallucination and Backdoor Trigger Detection](https://arxiv.org/abs/2602.06532)
*Hema Karnam Surendrababu,Nithin Nagaraj*

Main category: cs.CR

TL;DR: 提出基于Syndrome Decoding的统一方法DAIReS，用于检测机器学习系统中的安全漏洞（如后门数据投毒攻击）和可靠性问题（如LLM幻觉），通过将解码方法适配到NLP句子嵌入空间来实现检测。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型（包括大语言模型）存在安全漏洞和可靠性问题。安全方面，模型容易受到后门数据投毒攻击，导致未经授权的模型行为和系统性误分类；可靠性方面，LLM会产生幻觉，导致不可预测的输出和用户风险。需要统一的方法来检测这两类问题。

Method: 提出DAIReS方法，基于Syndrome Decoding（综合征解码）的统一框架。将Syndrome Decoding方法适配到NLP句子嵌入空间，能够：1）在ML训练数据集中区分中毒和非中毒样本；2）通过自引用元解释任务有效检测LLM中的幻觉内容。

Result: 该方法能够同时检测安全违规（后门数据投毒）和可靠性违规（LLM幻觉），提供了一种统一的检测框架。

Conclusion: Syndrome Decoding方法可以有效地统一检测机器学习系统中的安全性和可靠性问题，为构建更可靠的AI系统提供了新思路。

Abstract: Machine Learning (ML) models, including Large Language Models (LLMs), are characterized by a range of system-level attributes such as security and reliability. Recent studies have demonstrated that ML models are vulnerable to multiple forms of security violations, among which backdoor data-poisoning attacks represent a particularly insidious threat, enabling unauthorized model behavior and systematic misclassification. In parallel, deficiencies in model reliability can manifest as hallucinations in LLMs, leading to unpredictable outputs and substantial risks for end users. In this work on Dependable Artificial Intelligence with Reliability and Security (DAIReS), we propose a novel unified approach based on Syndrome Decoding for the detection of both security and reliability violations in learning-based systems. Specifically, we adapt the syndrome decoding approach to the NLP sentence-embedding space, enabling the discrimination of poisoned and non-poisoned samples within ML training datasets. Additionally, the same methodology can effectively detect hallucinated content due to self referential meta explanation tasks in LLMs.

</details>


### [13] [AlertBERT: A noise-robust alert grouping framework for simultaneous cyber attacks](https://arxiv.org/abs/2602.06534)
*Lukas Karner,Max Landauer,Markus Wurzenberger,Florian Skopik*

Main category: cs.CR

TL;DR: AlertBERT：基于自监督学习的网络安全告警分组框架，在噪声环境下优于传统时间分组方法


<details>
  <summary>Details</summary>
Motivation: 传统基于时间的告警分组方法在大规模计算机网络中效果不佳，存在高误报率和并发攻击问题，导致SOC分析师面临告警疲劳，反应时间慢和决策错误

Method: 提出AlertBERT自监督框架，利用掩码语言模型和基于密度的聚类技术，支持实时和取证操作；同时开发了数据增强方法，可灵活控制噪声水平并模拟并发攻击

Result: AlertBERT在生成的数据集上持续优于传统时间分组技术，在识别正确告警组方面实现了更高的准确性

Conclusion: AlertBERT框架能有效解决大规模网络中告警分组问题，减少分析师告警疲劳，提高安全运营效率

Abstract: Automated detection of cyber attacks is a critical capability to counteract the growing volume and sophistication of cyber attacks. However, the high numbers of security alerts issued by intrusion detection systems lead to alert fatigue among analysts working in security operations centres (SOC), which in turn causes slow reaction time and incorrect decision making. Alert grouping, which refers to clustering of security alerts according to their underlying causes, can significantly reduce the number of distinct items analysts have to consider. Unfortunately, conventional time-based alert grouping solutions are unsuitable for large scale computer networks characterised by high levels of false positive alerts and simultaneously occurring attacks. To address these limitations, we propose AlertBERT, a self-supervised framework designed to group alerts from isolated or concurrent attacks in noisy environments. Thereby, our open-source implementation of AlertBERT leverages masked-language-models and density-based clustering to support both real-time or forensic operation. To evaluate our framework, we further introduce a novel data augmentation method that enables flexible control over noise levels and simulates concurrent attack occurrences. Based on the data sets generated through this method, we demonstrate that AlertBERT consistently outperforms conventional time-based grouping techniques, achieving superior accuracy in identifying correct alert groups.

</details>


### [14] [Malicious Agent Skills in the Wild: A Large-Scale Security Empirical Study](https://arxiv.org/abs/2602.06547)
*Yi Liu,Zhihao Chen,Yanjun Zhang,Gelei Deng,Yuekang Li,Jianting Ning,Leo Yu Zhang*

Main category: cs.CR

TL;DR: 首个恶意AI代理技能数据集分析：从98,380个社区技能中确认157个恶意技能，包含632个漏洞，主要分为数据窃取和代理劫持两类攻击


<details>
  <summary>Details</summary>
Motivation: 第三方AI代理技能通过指令文件和可执行代码扩展LLM代理功能，这些技能以用户权限运行并通过社区注册中心分发，但缺乏恶意技能的真实数据集来评估安全威胁

Method: 从两个社区注册中心收集98,380个技能，通过行为验证构建首个标记的恶意代理技能数据集，确认157个恶意技能并分析其漏洞特征和攻击模式

Result: 发现恶意技能平均包含4.03个漏洞，攻击分为数据窃取型（通过供应链技术窃取凭证）和代理劫持型（通过指令操纵破坏代理决策）；单个攻击者占54.1%案例；高级攻击100%使用未公开的"影子功能"

Conclusion: AI代理技能生态系统存在系统性安全风险，需要更严格的安全审查机制；负责任披露使93.6%恶意技能在30天内被移除；发布数据集和分析管道支持未来代理安全研究

Abstract: Third-party agent skills extend LLM-based agents with instruction files and executable code that run on users' machines. Skills execute with user privileges and are distributed through community registries with minimal vetting, but no ground-truth dataset exists to characterize the resulting threats. We construct the first labeled dataset of malicious agent skills by behaviorally verifying 98,380 skills from two community registries, confirming 157 malicious skills with 632 vulnerabilities. These attacks are not incidental. Malicious skills average 4.03 vulnerabilities across a median of three kill chain phases, and the ecosystem has split into two archetypes: Data Thieves that exfiltrate credentials through supply chain techniques, and Agent Hijackers that subvert agent decision-making through instruction manipulation. A single actor accounts for 54.1\% of confirmed cases through templated brand impersonation. Shadow features, capabilities absent from public documentation, appear in 0\% of basic attacks but 100\% of advanced ones; several skills go further by exploiting the AI platform's own hook system and permission flags. Responsible disclosure led to 93.6\% removal within 30 days. We release the dataset and analysis pipeline to support future work on agent skill security.

</details>


### [15] [A Survey of Security Threats and Trust Management in Vehicular Ad Hoc Networks](https://arxiv.org/abs/2602.06608)
*Rezvi Shahariar,Chris Phillips*

Main category: cs.CR

TL;DR: 本文综述了车载自组织网络（VANETs）中最先进的信任模型，将现有模型分为接收方信任评估和发送方信任评估两大类，并比较了它们的优缺点，最后提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统安全方法无法有效应对VANETs中的恶意内部攻击，信任管理在隔离这些攻击中起着关键作用。本文旨在系统回顾、分类和总结现有的信任模型，帮助读者理解不同评估方法的差异。

Method: 首先对现有信任模型进行回顾、分类和总结，然后比较它们的成就。通过文献调查识别出两大类信任模型：接收方信任评估和发送方信任评估。使用序列图比较这两种评估技术。

Result: 识别出VANETs中存在两大类信任模型：接收方信任评估（大多数模型采用）和发送方信任评估（目前只有一个模型）。接收方模型需要每个接收方在消息到达时计算信任，而发送方模型在消息发布时验证信任，接收方无需计算。发送方评估支持快速决策，但存在争议时需要路边单元（RSU）裁决。

Conclusion: 建议未来研究关注VANETs中发送方信任评估的发展，同时强调在部署信任模型时需要考虑实时约束和效率等挑战。

Abstract: This paper presents a survey of state-of-the-art trust models for Vehicular Ad Hoc Networks (VANETs). Trust management plays an essential role in isolating malicious insider attacks in VANETs which traditional security approaches fail to thwart. To this end, many trust models are presented; some of them only address trust management, while others address security and privacy aspects besides trust management. This paper first reviews, classifies, and summarizes state-of-the-art trust models, and then compares their achievements. From this literature survey, our reader will easily identify two broad classes of trust models that exist in literature, differing primarily in their evaluation point. For example, most trust models follow receiver-side trust evaluation and to the best of our knowledge, there is only one trust model for VANETs which evaluates trust at the sender-side unless a dispute arises. In the presence of a dispute, a Roadside Unit (RSU) rules on the validity of an event. In receiver-side trust models, each receiver becomes busy while computing the trust of a sender and its messages upon the messages' arrival. Conversely, in the sender-side class, receivers are free from any kind of computation as the trust is verified at the time the message is announced. Also, vehicles can quickly act on the information, such as taking a detour to an alternate route, as it supports fast decision-making. We provide a comparison between these two evaluation techniques using a sequence diagram. We then conclude the survey by suggesting future work for sender-side evaluation of trust in VANETs. Additionally, the challenges (real-time constraints and efficiency) are emphasized whilst considering the deployment of a trust model in VANETs

</details>


### [16] [HYDRA: Unearthing "Black Swan" Vulnerabilities in LEO Satellite Networks](https://arxiv.org/abs/2602.06612)
*Bintao Yuan,Mingsheng Tang,Binbin Ge,Hongbin Luo,Zijie Yan*

Main category: cs.CR

TL;DR: HYDRA是一个基于超图的动态风险分析框架，用于评估低地球轨道卫星星座的系统性安全风险，通过Hyper-Bridge Centrality指标识别拓扑边缘但结构致命的关键节点。


<details>
  <summary>Details</summary>
Motivation: 随着低地球轨道卫星星座成为关键基础设施，针对它们的攻击日益增多。传统的拓扑中心分析方法无法捕捉由动态负载不平衡和高阶依赖关系引起的系统性风险，这些风险可能将局部故障转化为全网级联故障。

Method: 提出HYDRA框架，核心是新颖的Hyper-Bridge Centrality指标，通过负载冗余比在依赖结构中量化节点关键性。使用真实的StarLink TLE数据和基于人口的重力模型进行广泛模拟验证。

Result: HBC指标始终优于传统指标，识别出超越中介中心性结构破坏潜力的关键故障点。最关键漏洞不在密集连接的卫星核心，而在看似边缘的天地接口，这些是系统的"黑天鹅"节点。

Conclusion: 这项工作将安全范式从连通性转向结构应力，表明保护网络边缘至关重要，需要从根本上重新设计冗余策略。

Abstract: As Low Earth Orbit (LEO) become mega-constellations critical infrastructure, attacks targeting them have grown in number and range. The security analysis of LEO constellations faces a fundamental paradigm gap: traditional topology-centric methods fail to capture systemic risks arising from dynamic load imbalances and high-order dependencies, which can transform localized failures into network-wide cascades. To address this, we propose HYDRA, a hypergraph-based dynamic risk analysis framework. Its core is a novel metric, Hyper-Bridge Centrality (HBC), which quantifies node criticality via a load-to-redundancy ratio within dependency structures. A primary challenge to resilience: the most critical vulnerabilities are not in the densely connected satellite core, but in the seemingly marginal ground-space interfaces. These are the system's "Black Swan" nodes--topologically peripheral yet structurally lethal. We validate this through extensive simulations using realistic StarLink TLE data and population-based gravity model. Experiments demonstrate that HBC consistently outperforms traditional metrics, identifying critical failure points that surpass the structural damage potential of even betweenness centrality. This work shifts the security paradigm from connectivity to structural stress, demonstrating that securing the network edge is paramount and necessitates a fundamental redesign of redundancy strategies.

</details>


### [17] [Confundo: Learning to Generate Robust Poison for Practical RAG Systems](https://arxiv.org/abs/2602.06616)
*Haoyang Hu,Zhejun Jiang,Yueming Lyu,Junyuan Zhang,Yi Liu,Ka-Ho Chow*

Main category: cs.CR

TL;DR: Confundo是一个针对检索增强生成(RAG)系统的学习式投毒攻击框架，通过微调大语言模型作为投毒生成器，在现实RAG系统中实现高效、鲁棒且隐蔽的攻击，同时提供防御方案保护网络内容。


<details>
  <summary>Details</summary>
Motivation: 现有RAG投毒攻击在现实系统中效果严重下降，原因在于忽略了两个现实因素：(1)内容在使用前会被处理，可能分割投毒内容削弱效果；(2)用户查询通常与攻击设计时预期的不完全一致。这导致从业者低估风险并产生虚假安全感。

Method: 提出Confundo框架，通过微调大语言模型作为投毒生成器，学习生成高效、鲁棒且隐蔽的恶意内容。该统一框架支持多种攻击目标，包括操纵事实正确性、诱导偏见观点和触发幻觉。

Result: Confundo在多种数据集和RAG配置中大幅优于各种专门设计的攻击方法，即使在存在防御措施的情况下仍保持高效。同时展示了防御用例，保护网络内容免遭未经授权的RAG系统抓取，且不影响用户体验。

Conclusion: Confundo框架更好地描述了现实RAG系统的威胁，揭示了现有攻击评估的局限性，同时提供了攻击和防御的双重视角，有助于更准确地评估RAG系统的安全风险。

Abstract: Retrieval-augmented generation (RAG) is increasingly deployed in real-world applications, where its reference-grounded design makes outputs appear trustworthy. This trust has spurred research on poisoning attacks that craft malicious content, inject it into knowledge sources, and manipulate RAG responses. However, when evaluated in practical RAG systems, existing attacks suffer from severely degraded effectiveness. This gap stems from two overlooked realities: (i) content is often processed before use, which can fragment the poison and weaken its effect, and (ii) users often do not issue the exact queries anticipated during attack design. These factors can lead practitioners to underestimate risks and develop a false sense of security. To better characterize the threat to practical systems, we present Confundo, a learning-to-poison framework that fine-tunes a large language model as a poison generator to achieve high effectiveness, robustness, and stealthiness. Confundo provides a unified framework supporting multiple attack objectives, demonstrated by manipulating factual correctness, inducing biased opinions, and triggering hallucinations. By addressing these overlooked challenges, Confundo consistently outperforms a wide range of purpose-built attacks across datasets and RAG configurations by large margins, even in the presence of defenses. Beyond exposing vulnerabilities, we also present a defensive use case that protects web content from unauthorized incorporation into RAG systems via scraping, with no impact on user experience.

</details>


### [18] [TrapSuffix: Proactive Defense Against Adversarial Suffixes in Jailbreaking](https://arxiv.org/abs/2602.06630)
*Mengyao Du,Han Fang,Haokai Ma,Gang Yang,Quanjun Yin,Shouling Ji,Ee-Chien Chang*

Main category: cs.CR

TL;DR: TrapSuffix是一种主动防御方法，通过轻量级微调在LLM中植入陷阱行为，使攻击者要么陷入优化陷阱无法生成有效对抗后缀，要么生成带有可追踪指纹的后缀，实现强防御和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 现有防御大多是被动检测可疑后缀，没有利用防御者固有的不对称能力来主动植入秘密和隐藏漏洞。后缀攻击具有无限表面形式，使得防御困难。

Method: 提出TrapSuffix，一种轻量级微调方法，在不改变推理流程的情况下向基础模型注入陷阱对齐行为，通过重塑模型对对抗后缀的响应景观，引导攻击者陷入两难境地。

Result: 在多样化后缀攻击设置中，TrapSuffix将平均攻击成功率降至0.01%以下，平均追踪成功率87.9%，无推理开销，仅需平均15.87MB额外内存，而现有LLM检测防御通常需要1e4MB级别内存。

Conclusion: TrapSuffix提供了一种主动防御范式，通过可控性导向的方法实现强防御和可靠可追溯性，与现有基于过滤的防御自然组合提供互补保护。

Abstract: Suffix-based jailbreak attacks append an adversarial suffix, i.e., a short token sequence, to steer aligned LLMs into unsafe outputs. Since suffixes are free-form text, they admit endlessly many surface forms, making jailbreak mitigation difficult. Most existing defenses depend on passive detection of suspicious suffixes, without leveraging the defender's inherent asymmetric ability to inject secrets and proactively conceal gaps. Motivated by this, we take a controllability-oriented perspective and develop a proactive defense that nudges attackers into a no-win dilemma: either they fall into defender-designed optimization traps and fail to produce an effective adversarial suffix, or they can succeed only by generating adversarial suffixes that carry distinctive, traceable fingerprints. We propose TrapSuffix, a lightweight fine-tuning approach that injects trap-aligned behaviors into the base model without changing the inference pipeline. TrapSuffix channels jailbreak attempts into these two outcomes by reshaping the model's response landscape to adversarial suffixes. Across diverse suffix-based jailbreak settings, TrapSuffix reduces the average attack success rate to below 0.01 percent and achieves an average tracing success rate of 87.9 percent, providing both strong defense and reliable traceability. It introduces no inference-time overhead and incurs negligible memory cost, requiring only 15.87 MB of additional memory on average, whereas state-of-the-art LLM-based detection defenses typically incur memory overheads at the 1e4 MB level, while composing naturally with existing filtering-based defenses for complementary protection.

</details>


### [19] [Jamming Attacks on the Random Access Channel in 5G and B5G Networks](https://arxiv.org/abs/2602.06634)
*Wilfrid Azariah,Yi-Quan Chen,Zhong-Xin You,Ray-Guang Cheng,Shiann-Tsong Sheu,Binbin Chen*

Main category: cs.CR

TL;DR: 本文提出了一种分析模型来预测Msg1干扰攻击对5G/B5G网络RACH性能的影响，并通过OAI开源UE实现干扰攻击器进行空中实验验证。


<details>
  <summary>Details</summary>
Motivation: 随机接入信道（RACH）干扰对5G及未来网络构成严重安全威胁，特别是Msg1干扰攻击能够有效阻断合法用户设备接入，需要建立准确的分析模型来评估其影响。

Method: 1. 提出分析模型预测Msg1干扰攻击对RACH性能的影响；2. 使用OpenAirInterface开源用户设备实现Msg1干扰攻击器；3. 通过空中实验验证分析模型的准确性。

Result: 实验结果表明，低功率和隐蔽的Msg1干扰攻击能够有效阻断5G/B5G系统中合法用户设备的接入，验证了所提分析模型的准确性。

Conclusion: Msg1干扰攻击对5G/B5G网络构成严重安全威胁，所提出的分析模型能够准确预测其影响，为网络防御提供了重要工具。

Abstract: Random Access Channel (RACH) jamming poses a critical security threat to 5G and beyond (B5G) networks. This paper presents an analytical model for predicting the impact of Msg1 jamming attacks on RACH performance. We use the OpenAirInterface (OAI) open-source user equipment (UE) to implement a Msg1 jamming attacker. Over-the-air experiments validate the accuracy of the proposed analytical model. The results show that low-power and stealthy Msg1 jamming can effectively block legitimate UE access in 5G/B5G systems.

</details>


### [20] [Wonderboom -- Efficient, and Censorship-Resilient Signature Aggregation for Million Scale Consensus](https://arxiv.org/abs/2602.06655)
*Zeta Avarikioti,Ray Neiheiser,Krzysztof Pietrzak,Michelle X. Yeo*

Main category: cs.CR

TL;DR: Wonderboom是一个针对以太坊的百万级签名聚合协议，能够在单个以太坊时隙内高效聚合数百万验证者的签名，速度提升32倍，同时提供比现有协议更高的安全性保证。


<details>
  <summary>Details</summary>
Motivation: 以太坊目前有近百万验证者，导致区块最终确认需要约15分钟，这对于许多实际应用来说太慢。现有签名聚合和传播协议存在缺陷，可能被攻击者利用来转移诚实节点和敌对节点之间的权益比例。

Method: 提出了Wonderboom协议，这是第一个百万级聚合协议，能够在单个以太坊时隙内高效聚合数百万验证者的签名。同时开发了首个能够模拟百万级协议规模的仿真工具来评估协议性能。

Result: 即使在最坏情况下，Wonderboom也能在单个以太坊时隙内聚合和验证超过200万个签名，比现有协议快32倍，同时提供更高的安全性保证。

Conclusion: Wonderboom解决了以太坊大规模验证者集带来的性能瓶颈，显著提升了区块最终确认速度，同时增强了协议安全性，对以太坊的实际应用具有重要意义。

Abstract: Over the last years, Ethereum has evolved into a public platform that safeguards the savings of hundreds of millions of people and secures more than $650 billion in assets, placing it among the top 25 stock exchanges worldwide in market capitalization, ahead of Singapore, Mexico, and Thailand. As such, the performance and security of the Ethereum blockchain are not only of theoretical interest, but also carry significant global economic implications. At the time of writing, the Ethereum platform is collectively secured by almost one million validators highlighting its decentralized nature and underlining its economic security guarantees. However, due to this large validator set, the protocol takes around 15 minutes to finalize a block which is prohibitively slow for many real world applications. This delay is largely driven by the cost of aggregating and disseminating signatures across a validator set of this scale. Furthermore, as we show in this paper, the existing protocol that is used to aggregate and disseminate the signatures has several shortcomings that can be exploited by adversaries to shift stake proportion from honest to adversarial nodes. In this paper, we introduce Wonderboom, the first million scale aggregation protocol that can efficiently aggregate the signatures of millions of validators in a single Ethereum slot (x32 faster) while offering higher security guarantees than the state of the art protocol used in Ethereum. Furthermore, to evaluate Wonderboom, we implement the first simulation tool that can simulate such a protocol on the million scale and show that even in the worst case Wonderboom can aggregate and verify more than 2 million signatures within a single Ethereum slot.

</details>


### [21] [Evaluating and Enhancing the Vulnerability Reasoning Capabilities of Large Language Models](https://arxiv.org/abs/2602.06687)
*Li Lu,Yanjie Zhao,Hongzhou Rao,Kechi Zhang,Haoyu Wang*

Main category: cs.CR

TL;DR: 论文提出DAGVul框架，将漏洞推理建模为有向无环图生成任务，通过强化学习与可验证奖励机制提升LLM在漏洞检测中的逻辑一致性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在漏洞检测中存在可靠性问题：模型经常基于幻觉逻辑或表面模式给出正确检测结果，但实际推理过程与根本原因不符。现有基准测试缺乏细粒度评估推理过程的能力。

Method: 1) 构建包含真实漏洞和语义等价代码扰动的基准数据集；2) 提出DAGVul框架，将漏洞推理建模为有向无环图生成任务，明确映射因果依赖关系；3) 引入强化学习与可验证奖励机制，使模型推理轨迹与程序内在逻辑对齐。

Result: 实验显示DAGVul将推理F1分数平均提升18.9%，8B参数版本不仅超越同规模模型，还优于专用大规模推理模型，甚至与Claude-Sonnet-4.5等SOTA模型竞争（75.47% vs. 76.11%）。

Conclusion: DAGVul框架通过结构化推理和强化学习对齐，显著提升了LLM在漏洞检测中的逻辑一致性和可靠性，为漏洞推理任务建立了新的效率标准。

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in vulnerability detection. However, a critical reliability gap persists: models frequently yield correct detection verdicts based on hallucinated logic or superficial patterns that deviate from the actual root cause. This misalignment remains largely obscured because contemporary benchmarks predominantly prioritize coarse-grained classification metrics, lacking the granular ground truth required to evaluate the underlying reasoning process. To bridge this gap, we first construct a benchmark consisting of two datasets: (1) real-world vulnerabilities with expert-curated causal reasoning as ground truth, and (2) semantically equivalent code perturbations for assessing reasoning robustness. Our large-scale empirical study reveals that even state-of-the-art models struggle to maintain logical consistency during semantic code comprehension, exhibiting 12 systematic failure patterns. Addressing these limitations, we propose DAGVul, a novel framework that models vulnerability reasoning as a Directed Acyclic Graph (DAG) generation task. Unlike linear chain-of-thought (CoT), our approach explicitly maps causal dependencies to enforce structural consistency. By further introducing Reinforcement Learning with Verifiable Rewards (RLVR), we align model reasoning trace with program-intrinsic logic. Experimental results demonstrate that our framework improves the reasoning F1-score by an average of 18.9% over all the baselines. Remarkably, our 8B-parameter implementation not only outperforms existing models of comparable scale but also surpasses specialized large-scale reasoning models, including Qwen3-30B-Reasoning and GPT-OSS-20B-High. It is even competitive with state-of-the-art models like Claude-Sonnet-4.5 (75.47% vs. 76.11%), establishing new efficiency in vulnerability reasoning across model scales.

</details>


### [22] [Taipan: A Query-free Transfer-based Multiple Sensitive Attribute Inference Attack Solely from Publicly Released Graphs](https://arxiv.org/abs/2602.06700)
*Ying Song,Balaji Palanisamy*

Main category: cs.CR

TL;DR: Taipan是首个针对图数据的无查询、基于迁移的多敏感属性推理攻击框架，利用图结构内在的信息泄露漏洞，无需模型查询即可从公开图中推断多个敏感属性。


<details>
  <summary>Details</summary>
Motivation: 现有属性推理攻击(AIAs)主要依赖重复模型查询，这在现实场景中不切实际（数据保护法规、查询预算限制、检测风险）。更重要的是，现有方法忽视了图数据发布时固有的多敏感信息泄露漏洞，即仅从公开图结构就能泄露敏感信息。

Method: 提出Taipan框架：1) 分层攻击知识路由：捕获复杂的属性间相关性；2) 提示引导的攻击原型精炼：缓解负迁移和性能退化。还提出了专门针对图多敏感属性推理攻击的系统评估框架。

Result: 在多种真实世界图数据集上的实验表明，Taipan在相同分布设置、异构相似分布和异分布设置（特征维度不匹配）下均能实现强大的攻击性能，即使在严格的差分隐私保证下仍保持有效。

Conclusion: 研究揭示了图数据发布时固有的多敏感信息泄露风险，强调了开发更鲁棒的多属性隐私保护图发布方法和数据共享实践的紧迫需求。

Abstract: Graph-structured data underpin a wide spectrum of modern applications. However, complex graph topologies and homophilic patterns can facilitate attribute inference attacks (AIAs) by enabling sensitive information leakage to propagate across local neighborhoods. Existing AIAs predominantly assume that adversaries can probe sensitive attributes through repeated model queries. Such assumptions are often impractical in real-world settings due to stringent data protection regulations, prohibitive query budgets, and heightened detection risks, especially when inferring multiple sensitive attributes. More critically, this model-centric perspective obscures a pervasive blind spot: \textbf{intrinsic multiple sensitive information leakage arising solely from publicly released graphs.} To exploit this unexplored vulnerability, we introduce a new attack paradigm and propose \textbf{Taipan, the first query-free transfer-based attack framework for multiple sensitive attribute inference attacks on graphs (G-MSAIAs).} Taipan integrates \emph{Hierarchical Attack Knowledge Routing} to capture intricate inter-attribute correlations, and \emph{Prompt-guided Attack Prototype Refinement} to mitigate negative transfer and performance degradation. We further present a systematic evaluation framework tailored to G-MSAIAs. Extensive experiments on diverse real-world graph datasets demonstrate that Taipan consistently achieves strong attack performance across same-distribution settings and heterogeneous similar- and out-of-distribution settings with mismatched feature dimensionalities, and remains effective even under rigorous differential privacy guarantees. Our findings underscore the urgent need for more robust multi-attribute privacy-preserving graph publishing methods and data-sharing practices.

</details>


### [23] [GhostCite: A Large-Scale Analysis of Citation Validity in the Age of Large Language Models](https://arxiv.org/abs/2602.06718)
*Zuyao Xu,Yuqi Qiu,Lu Sun,FaSheng Miao,Fubin Wu,Xinyi Wang,Xiang Li,Haozhe Lu,ZhengZe Zhang,Yuxin Hu,Jialu Li,Jin Luo,Feng Zhang,Rui Luo,Xinran Liu,Yingxian Li,Jiaji Liu*

Main category: cs.CR

TL;DR: LLM生成的"幽灵引用"威胁科学引用可信度，研究发现所有主流LLM都会产生虚假引用，且AI/ML领域论文中1.07%包含无效引用，研究人员和审稿人的验证不足加剧了这一问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在学术写作中的广泛应用，其生成虚假引用（"幽灵引用"）的风险对科学引用的可信度构成系统性威胁，需要量化这一威胁并寻找缓解方案。

Method: 开发CiteVerifier开源框架进行大规模引用验证，通过三个实验：1) 在40个研究领域评估13个SOTA LLM的引用生成；2) 分析2020-2025年56,381篇AI/ML和安全顶会论文中的220万条引用；3) 调查97名研究人员并分析94份有效问卷。

Result: 所有LLM的虚假引用率在14.23%-94.93%之间，存在显著领域差异；AI/ML领域1.07%的论文（604篇）包含无效引用，2025年激增80.9%；41.5%的研究人员不检查直接复制BibTeX，44.4%对可疑引用无行动，76.7%的审稿人不仔细检查引用。

Conclusion: 不可靠的AI工具与研究人员验证不足、同行评审审查不充分相结合，导致虚假引用污染科学记录，需要研究人员、会议和工具开发者共同采取干预措施保护引用完整性。

Abstract: Citations provide the basis for trusting scientific claims; when they are invalid or fabricated, this trust collapses. With the advent of Large Language Models (LLMs), this risk has intensified: LLMs are increasingly used for academic writing, yet their tendency to fabricate citations (``ghost citations'') poses a systemic threat to citation validity.
  To quantify this threat and inform mitigation, we develop CiteVerifier, an open-source framework for large-scale citation verification, and conduct the first comprehensive study of citation validity in the LLM era through three experiments built on it. We benchmark 13 state-of-the-art LLMs on citation generation across 40 research domains, finding that all models hallucinate citations at rates from 14.23\% to 94.93\%, with significant variation across research domains. Moreover, we analyze 2.2 million citations from 56,381 papers published at top-tier AI/ML and Security venues (2020--2025), confirming that 1.07\% of papers contain invalid or fabricated citations (604 papers), with an 80.9\% increase in 2025 alone. Furthermore, we survey 97 researchers and analyze 94 valid responses after removing 3 conflicting samples, revealing a critical ``verification gap'': 41.5\% of researchers copy-paste BibTeX without checking and 44.4\% choose no-action responses when encountering suspicious references; meanwhile, 76.7\% of reviewers do not thoroughly check references and 80.0\% never suspect fake citations. Our findings reveal an accelerating crisis where unreliable AI tools, combined with inadequate human verification by researchers and insufficient peer review scrutiny, enable fabricated citations to contaminate the scientific record. We propose interventions for researchers, venues, and tool developers to protect citation integrity.

</details>


### [24] [Beyond Function-Level Analysis: Context-Aware Reasoning for Inter-Procedural Vulnerability Detection](https://arxiv.org/abs/2602.06751)
*Yikun Li,Ting Zhang,Jieke Shi,Chengran Yang,Junda He,Xin Zhou,Jinfeng Jiang,Huihui Huang,Wen Bin Leow,Yide Yin,Eng Lieh Ouh,Lwin Khin Shar,David Lo*

Main category: cs.CR

TL;DR: CPRVul是一个上下文感知的漏洞检测框架，通过上下文分析和结构化推理，在三个高质量漏洞数据集上显著优于仅函数级别的检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞检测方法大多在函数级别操作，缺乏跨过程上下文信息。然而漏洞的存在和根本原因通常依赖于上下文信息。简单地附加上下文不可靠，因为真实世界的上下文冗长、冗余且嘈杂，会降低代码模型的性能。

Method: CPRVul框架包含两个阶段：1) 上下文分析和选择：构建代码属性图，提取候选上下文，使用LLM生成安全相关配置文件并分配相关性分数，只选择高影响力的上下文元素；2) 结构化推理：整合目标函数、选择的上下文和辅助漏洞元数据生成推理轨迹，用于微调LLMs进行基于推理的漏洞检测。

Result: 在PrimeVul、TitanVul和CleanVul三个数据集上，CPRVul准确率从64.94%到73.76%，显著优于UniXcoder的56.65%到63.68%。在具有挑战性的PrimeVul基准上，准确率从55.17%提升到67.78%（22.9%改进）。消融实验表明，仅原始上下文或仅处理后的上下文都无法带来收益，只有处理后的上下文与结构化推理结合才能获得性能提升。

Conclusion: CPRVul证明了上下文感知漏洞检测的有效性，通过精心选择的上下文与结构化推理的结合，能够显著提升漏洞检测性能。该方法为解决现实世界中复杂漏洞检测问题提供了新的方向。

Abstract: Recent progress in ML and LLMs has improved vulnerability detection, and recent datasets have reduced label noise and unrelated code changes. However, most existing approaches still operate at the function level, where models are asked to predict whether a single function is vulnerable without inter-procedural context. In practice, vulnerability presence and root cause often depend on contextual information. Naively appending such context is not a reliable solution: real-world context is long, redundant, and noisy, and we find that unstructured context frequently degrades the performance of strong fine-tuned code models.
  We present CPRVul, a context-aware vulnerability detection framework that couples Context Profiling and Selection with Structured Reasoning. CPRVul constructs a code property graph, and extracts candidate context. It then uses an LLM to generate security-focused profiles and assign relevance scores, selecting only high-impact contextual elements that fit within the model's context window. In the second phase, CPRVul integrates the target function, the selected context, and auxiliary vulnerability metadata to generate reasoning traces, which are used to fine-tune LLMs for reasoning-based vulnerability detection.
  We evaluate CPRVul on three high-quality vulnerability datasets: PrimeVul, TitanVul, and CleanVul. Across all datasets, CPRVul consistently outperforms function-only baselines, achieving accuracies ranging from 64.94% to 73.76%, compared to 56.65% to 63.68% for UniXcoder. Specifically, on the challenging PrimeVul benchmark, CPRVul achieves 67.78% accuracy, outperforming prior state-of-the-art approaches, improving accuracy from 55.17% to 67.78% (22.9% improvement). Our ablations further show that neither raw context nor processed context alone benefits strong code models; gains emerge only when processed context is paired with structured reasoning.

</details>


### [25] [A Unified Framework for LLM Watermarks](https://arxiv.org/abs/2602.06754)
*Thibaud Gloaguen,Robin Staab,Nikola Jovanović,Martin Vechev*

Main category: cs.CR

TL;DR: 该论文提出了一个统一的、原则性的LLM水印框架，将现有方法归纳为约束优化问题，揭示了质量-多样性-检测能力的权衡，并可用于设计新的水印方案。


<details>
  <summary>Details</summary>
Motivation: 当前LLM水印算法设计分散，缺乏统一的理论框架。不同方法采用自下而上的设计方式，没有通用的原则性表述，限制了水印技术的系统化发展和优化。

Method: 提出一个原则性的约束优化问题框架，将现有水印方案统一为特定约束下的优化问题。该框架允许使用困惑度作为质量代理，并推导出针对特定约束最优的新水印方案。

Result: 实验验证表明，从给定约束推导出的水印方案在该约束下能最大化检测能力。框架成功统一了现有主流水印方法，并揭示了质量-多样性-检测能力之间的权衡关系。

Conclusion: 该研究为LLM水印提供了首个通用原则性框架，不仅统一了现有方法，还为设计针对特定需求的新水印方案提供了理论基础，推动了水印技术的系统化发展。

Abstract: LLM watermarks allow tracing AI-generated texts by inserting a detectable signal into their generated content. Recent works have proposed a wide range of watermarking algorithms, each with distinct designs, usually built using a bottom-up approach. Crucially, there is no general and principled formulation for LLM watermarking.
  In this work, we show that most existing and widely used watermarking schemes can in fact be derived from a principled constrained optimization problem. Our formulation unifies existing watermarking methods and explicitly reveals the constraints that each method optimizes. In particular, it highlights an understudied quality-diversity-power trade-off. At the same time, our framework also provides a principled approach for designing novel watermarking schemes tailored to specific requirements. For instance, it allows us to directly use perplexity as a proxy for quality, and derive new schemes that are optimal with respect to this constraint. Our experimental evaluation validates our framework: watermarking schemes derived from a given constraint consistently maximize detection power with respect to that constraint.

</details>


### [26] [$f$-Differential Privacy Filters: Validity and Approximate Solutions](https://arxiv.org/abs/2602.06756)
*Long Tran,Antti Koskela,Ossi Räisä,Antti Honkela*

Main category: cs.CR

TL;DR: 本文研究完全自适应组合下的隐私损失计算问题，发现基于f-DP的自然隐私过滤器方法存在根本性缺陷，并建立了其有效性的充要条件，同时证明了完全自适应中心极限定理并构建了近似高斯DP过滤器。


<details>
  <summary>Details</summary>
Motivation: 在完全自适应组合（机制选择和隐私参数都依赖于先前输出历史）下准确计算隐私损失是差分隐私的核心挑战。隐私过滤器作为确保全局隐私预算不被超过的停止规则，对于f-DP等最优权衡函数概念是否能在完全自适应交互下有效仍不清楚。

Method: 分析了基于f-DP的自然隐私过滤器方法（组合个体权衡曲线并在超过预设f-DP曲线时停止），研究了其失效原因和条件。证明了完全自适应中心极限定理，并为子采样高斯机制构建了近似高斯DP过滤器。

Result: 发现自然的f-DP过滤器方法存在根本性无效问题，刻画了其失效的时间和原因，建立了该过滤器有效的充要条件。证明了完全自适应中心极限定理，并为子采样高斯机制（采样率q<0.2和q>0.8）构建了近似高斯DP过滤器，比基于Rényi DP的过滤器提供更紧的隐私保证。

Conclusion: f-DP在完全自适应组合下的隐私过滤器设计比预期更复杂，自然方法存在根本缺陷。研究为理解f-DP在自适应设置下的行为提供了理论框架，并开发了实用的近似过滤器，为实际应用提供了更优的隐私保证。

Abstract: Accounting for privacy loss under fully adaptive composition -- where both the choice of mechanisms and their privacy parameters may depend on the entire history of prior outputs -- is a central challenge in differential privacy (DP). In this setting, privacy filters are stopping rules for compositions that ensure a prescribed global privacy budget is not exceeded. It remains unclear whether optimal trade-off-function-based notions, such as $f$-DP, admit valid privacy filters under fully adaptive interaction. We show that the natural approach to defining an $f$-DP filter -- composing individual trade-off curves and stopping when the prescribed $f$-DP curve is crossed -- is fundamentally invalid. We characterise when and why this failure occurs, and establish necessary and sufficient conditions under which the natural filter is valid. Furthermore, we prove a fully adaptive central limit theorem for $f$-DP and construct an approximate Gaussian DP filter for subsampled Gaussian mechanisms at small sampling rates $q<0.2$ and large sampling rates $q>0.8$, yielding tighter privacy guarantees than filters based on Rényi DP in the same setting.

</details>


### [27] ["Tab, Tab, Bug'': Security Pitfalls of Next Edit Suggestions in AI-Integrated IDEs](https://arxiv.org/abs/2602.06759)
*Yunlong Lyu,Yixuan Tang,Peng Chen,Tian Dong,Xinyu Wang,Zhiqiang Dong,Hao Chen*

Main category: cs.CR

TL;DR: 首次系统研究AI集成IDE中Next Edit Suggestions（NES）的安全问题，发现NES因上下文扩展而增加攻击面，易受上下文污染攻击，且开发者对其安全风险认知不足。


<details>
  <summary>Details</summary>
Motivation: 现代AI集成IDE从被动代码补全转向主动的Next Edit Suggestions（NES），但现有研究主要关注独立LLM代码生成的安全问题，忽略了NES在AI集成IDE中可能带来的攻击向量。NES机制尚未深入探索，其安全影响也未完全理解。

Method: 1. 深入剖析NES机制以理解新威胁向量；2. 进行全面的实验室研究评估NES安全影响；3. 对200多名专业开发者进行大规模在线调查，评估实际开发工作流中对NES安全风险的认知。

Result: 1. NES检索显著扩展的上下文（包括不可感知的用户操作和全局代码库检索），增加了攻击面；2. NES易受上下文污染攻击，对事务性编辑和人机交互敏感；3. 开发者普遍缺乏对NES潜在安全风险的认知，需要加强教育和改进安全防护措施。

Conclusion: 这是首个系统研究NES安全性的工作，揭示了NES在AI集成IDE中引入的新安全风险，强调了提高开发者安全意识和改进IDE安全防护措施的必要性。

Abstract: Modern AI-integrated IDEs are shifting from passive code completion to proactive Next Edit Suggestions (NES). Unlike traditional autocompletion, NES is designed to construct a richer context from both recent user interactions and the broader codebase to suggest multi-line, cross-line, or even cross-file modifications. This evolution significantly streamlines the programming workflow into a tab-by-tab interaction and enhances developer productivity. Consequently, NES introduces a more complex context retrieval mechanism and sophisticated interaction patterns. However, existing studies focus almost exclusively on the security implications of standalone LLM-based code generation, ignoring the potential attack vectors posed by NES in modern AI-integrated IDEs. The underlying mechanisms of NES remain under-explored, and their security implications are not yet fully understood.
  In this paper, we conduct the first systematic security study of NES systems. First, we perform an in-depth dissection of the NES mechanisms to understand the newly introduced threat vectors. It is found that NES retrieves a significantly expanded context, including inputs from imperceptible user actions and global codebase retrieval, which increases the attack surfaces. Second, we conduct a comprehensive in-lab study to evaluate the security implications of NES. The evaluation results reveal that NES is susceptible to context poisoning and is sensitive to transactional edits and human-IDE interactions. Third, we perform a large-scale online survey involving over 200 professional developers to assess the perceptions of NES security risks in real-world development workflows. The survey results indicate a general lack of awareness regarding the potential security pitfalls associated with NES, highlighting the need for increased education and improved security countermeasures in AI-integrated IDEs.

</details>


### [28] [Next-generation cyberattack detection with large language models: anomaly analysis across heterogeneous logs](https://arxiv.org/abs/2602.06777)
*Yassine Chagna,Antal Goldschmidt*

Main category: cs.CR

TL;DR: 本文提出LogAtlas框架，使用大语言模型进行异构日志源的异常检测，解决了传统入侵检测系统的高误报率、语义盲点和数据稀缺问题，通过两阶段训练框架实现实时检测。


<details>
  <summary>Details</summary>
Motivation: 传统入侵检测系统存在高误报率、语义盲点和数据稀缺问题，日志数据因敏感性导致干净数据集稀少，需要更有效的异常检测方法。

Method: 提出三方面贡献：1) LogAtlas-Foundation-Sessions和LogAtlas-Defense-Set平衡异构日志数据集；2) 实证基准测试揭示传统指标在安全应用中的误导性；3) 两阶段训练框架（Base-AMAN 3B参数用于日志理解，AMAN 0.5B参数通过知识蒸馏用于实时检测）。

Result: 实现了实际可行性，每个会话推理时间为0.3-0.5秒，每日运营成本低于50美元，展示了LLM在安全异常检测中的实用价值。

Conclusion: 大语言模型可用于异构日志源的异常检测，通过专门的数据集、改进的评估指标和高效的两阶段训练框架，能够解决传统入侵检测系统的局限性，实现实时、低成本的部署。

Abstract: This project explores large language models (LLMs) for anomaly detection across heterogeneous log sources. Traditional intrusion detection systems suffer from high false positive rates, semantic blindness, and data scarcity, as logs are inherently sensitive, making clean datasets rare. We address these challenges through three contributions: (1) LogAtlas-Foundation-Sessions and LogAtlas-Defense-Set, balanced and heterogeneous log datasets with explicit attack annotations and privacy preservation; (2) empirical benchmarking revealing why standard metrics such as F1 and accuracy are misleading for security applications; and (3) a two phase training framework combining log understanding (Base-AMAN, 3B parameters) with real time detection (AMAN, 0.5B parameters via knowledge distillation). Results demonstrate practical feasibility, with inference times of 0.3-0.5 seconds per session and operational costs below 50 USD per day.

</details>


### [29] [Plato's Form: Toward Backdoor Defense-as-a-Service for LLMs with Prototype Representations](https://arxiv.org/abs/2602.06887)
*Chen Chen,Yuchen Sun,Jiaxin Gao,Yanwen Jia,Xueluan Gong,Qian Wang,Kwok-Yan Lam*

Main category: cs.CR

TL;DR: PROTOPURIFY：一种针对大语言模型后门攻击的净化框架，通过参数编辑在最小假设下实现防御，支持后门防御即服务


<details>
  <summary>Details</summary>
Motivation: 现有后门防御方法难以实现后门防御即服务（BDaaS），因为它们需要不现实的辅助信息（如下游干净数据、已知触发器/目标或任务领域细节），并且缺乏跨不同后门模型的可重用、可扩展的净化方法

Method: 首先从干净模型和后门模型对构建后门向量池，将向量聚合成候选原型，通过相似性匹配选择与目标模型最对齐的候选原型；然后通过层间原型对齐识别边界层，通过抑制受影响层中原型对齐组件进行针对性净化，实现细粒度缓解

Result: 在各种大语言模型上的分类和生成任务实验中，PROTOPURIFY在6种不同攻击（包括单触发器、多触发器和无触发器后门设置）中始终优于6种代表性防御方法，将攻击成功率降至10%以下（某些情况下低至1.6%），同时干净效用下降不到3%

Conclusion: PROTOPURIFY作为一种BDaaS就绪的原语，支持可重用性、可定制性、可解释性和运行时效率，对自适应后门变体具有鲁棒性，在非后门模型上表现稳定

Abstract: Large language models (LLMs) are increasingly deployed in security-sensitive applications, yet remain vulnerable to backdoor attacks. However, existing backdoor defenses are difficult to operationalize for Backdoor Defense-as-a-Service (BDaaS), as they require unrealistic side information (e.g., downstream clean data, known triggers/targets, or task domain specifics), and lack reusable, scalable purification across diverse backdoored models. In this paper, we present PROTOPURIFY, a backdoor purification framework via parameter edits under minimal assumptions. PROTOPURIFY first builds a backdoor vector pool from clean and backdoored model pairs, aggregates vectors into candidate prototypes, and selects the most aligned candidate for the target model via similarity matching. PROTOPURIFY then identifies a boundary layer through layer-wise prototype alignment and performs targeted purification by suppressing prototype-aligned components in the affected layers, achieving fine-grained mitigation with minimal impact on benign utility. Designed as a BDaaS-ready primitive, PROTOPURIFY supports reusability, customizability, interpretability, and runtime efficiency. Experiments across various LLMs on both classification and generation tasks show that PROTOPURIFY consistently outperforms 6 representative defenses against 6 diverse attacks, including single-trigger, multi-trigger, and triggerless backdoor settings. PROTOPURIFY reduces ASR to below 10%, and even as low as 1.6% in some cases, while incurring less than a 3% drop in clean utility. PROTOPURIFY further demonstrates robustness against adaptive backdoor variants and stability on non-backdoored models.

</details>


### [30] [TamperBench: Systematically Stress-Testing LLM Safety Under Fine-Tuning and Tampering](https://arxiv.org/abs/2602.06911)
*Saad Hossain,Tom Tseng,Punya Syon Pandey,Samanvay Vajpayee,Matthew Kowal,Nayeema Nonta,Samuel Simko,Stephen Casper,Zhijing Jin,Kellin Pelrine,Sirisha Rambhatla*

Main category: cs.CR

TL;DR: TamperBench：首个评估大语言模型篡改抵抗能力的统一框架，包含攻击库、超参数扫描、安全与效用评估，用于系统评估21个开源LLM的篡改抵抗能力。


<details>
  <summary>Details</summary>
Motivation: 随着开源大语言模型能力增强，提高其对不安全修改（无论是意外还是故意）的篡改抵抗能力变得至关重要，但目前缺乏标准评估方法，不同数据集、指标和篡改配置使得模型间安全性和鲁棒性难以比较。

Method: TamperBench框架包含三个核心组件：(1) 收集最先进的权重空间微调攻击和潜在空间表示攻击库；(2) 通过每个攻击-模型对的系统超参数扫描实现现实对抗评估；(3) 提供安全性和效用评估。框架只需最少额外代码即可指定任何微调配置、对齐阶段防御方法和指标套件。

Result: 使用TamperBench评估了21个开源LLM（包括防御增强变体），在9种篡改威胁下使用标准化安全和能力指标进行评估，发现：训练后处理对篡改抵抗有影响；越狱调优通常是最严重的攻击；Triplet成为领先的对齐阶段防御方法。

Conclusion: TamperBench为系统评估LLM篡改抵抗能力提供了首个统一框架，能够产生可重复的评估结果和新的研究见解，有助于比较不同模型和防御方法的安全性和鲁棒性。

Abstract: As increasingly capable open-weight large language models (LLMs) are deployed, improving their tamper resistance against unsafe modifications, whether accidental or intentional, becomes critical to minimize risks. However, there is no standard approach to evaluate tamper resistance. Varied data sets, metrics, and tampering configurations make it difficult to compare safety, utility, and robustness across different models and defenses. To this end, we introduce TamperBench, the first unified framework to systematically evaluate the tamper resistance of LLMs. TamperBench (i) curates a repository of state-of-the-art weight-space fine-tuning attacks and latent-space representation attacks; (ii) enables realistic adversarial evaluation through systematic hyperparameter sweeps per attack-model pair; and (iii) provides both safety and utility evaluations. TamperBench requires minimal additional code to specify any fine-tuning configuration, alignment-stage defense method, and metric suite while ensuring end-to-end reproducibility. We use TamperBench to evaluate 21 open-weight LLMs, including defense-augmented variants, across nine tampering threats using standardized safety and capability metrics with hyperparameter sweeps per model-attack pair. This yields novel insights, including effects of post-training on tamper resistance, that jailbreak-tuning is typically the most severe attack, and that Triplet emerges as a leading alignment-stage defense. Code is available at: https://github.com/criticalml-uw/TamperBench

</details>
