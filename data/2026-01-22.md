<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 30]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Guardrails for trust, safety, and ethical development and deployment of Large Language Models (LLM)](https://arxiv.org/abs/2601.14298)
*Anjanava Biswas,Wrick Talukdar*

Main category: cs.CR

TL;DR: 提出一种灵活自适应序列机制，结合信任与安全模块，为LLM开发部署提供安全防护框架


<details>
  <summary>Details</summary>
Motivation: LLM在带来革命性AI能力的同时，存在泄露隐私、产生虚假信息、被恶意利用等安全、隐私和伦理风险，亟需建立防护机制确保生成内容的安全、可靠和合乎伦理

Method: 提出Flexible Adaptive Sequencing机制，整合信任与安全模块，为LLM应用实现安全防护措施

Result: 未在摘要中明确说明具体实验结果，但提出了一个可用于实施LLM安全防护的框架机制

Conclusion: 需要建立防护框架来防止LLM的滥用，提出的灵活自适应序列机制为解决LLM安全、隐私和伦理问题提供了可行方案

Abstract: The AI era has ushered in Large Language Models (LLM) to the technological forefront, which has been much of the talk in 2023, and is likely to remain as such for many years to come. LLMs are the AI models that are the power house behind generative AI applications such as ChatGPT. These AI models, fueled by vast amounts of data and computational prowess, have unlocked remarkable capabilities, from human-like text generation to assisting with natural language understanding (NLU) tasks. They have quickly become the foundation upon which countless applications and software services are being built, or at least being augmented with. However, as with any groundbreaking innovations, the rise of LLMs brings forth critical safety, privacy, and ethical concerns. These models are found to have a propensity to leak private information, produce false information, and can be coerced into generating content that can be used for nefarious purposes by bad actors, or even by regular users unknowingly. Implementing safeguards and guardrailing techniques is imperative for applications to ensure that the content generated by LLMs are safe, secure, and ethical. Thus, frameworks to deploy mechanisms that prevent misuse of these models via application implementations is imperative. In this study, wepropose a Flexible Adaptive Sequencing mechanism with trust and safety modules, that can be used to implement safety guardrails for the development and deployment of LLMs.

</details>


### [2] [Predicting Tail-Risk Escalation in IDS Alert Time Series](https://arxiv.org/abs/2601.14299)
*Ambarish Gurjar,L Jean Camp*

Main category: cs.CR

TL;DR: 将金融极端状态预测方法应用于IDS警报数据，通过分析时间模式预测高强度攻击，准确率达91%


<details>
  <summary>Details</summary>
Motivation: 网络防御者面临大量IDS警报，需要优先处理高风险攻击。现有方法主要基于技术特征分类，但缺乏对时间模式的分析，难以区分升级攻击模式和机会性扫描。

Method: 采用金融建模中的极端状态预测方法分析IDS数据。计算每分钟警报强度、波动率和加权移动平均得出的短期动量指标。使用监督学习模型基于这些特征预测未来攻击升级模式。

Result: 训练模型能有效识别未来高强度攻击，预测性能优异：准确率约91%，召回率89%，精确率98%。提供了可复现的时间测量框架和开源模型。

Conclusion: IDS警报流的时间结构中存在可预测的早期预警信号。提出的方法能帮助防御者生成可解释的预测性警告，识别即将到来的高强度攻击风险。

Abstract: Network defenders face a steady stream of attacks, observed as raw Intrusion Detection System (IDS) alerts. The sheer volume of alerts demands prioritization, typically based on high-level risk classifications. This work expands the scope of risk measurement by examining alerts not only through their technical characteristics but also by examining and classifying their temporal patterns. One critical issue in responding to intrusion alerts is determining whether an alert is part of an escalating attack pattern or an opportunistic scan. To identify the former, we apply extreme-regime forecasting methods from financial modeling to IDS data. Extreme-regime forecasting is designed to identify likely future high-impact events or significant shifts in system behavior. Using these methods, we examine attack patterns by computing per-minute alert intensity, volatility, and a short-term momentum measure derived from weighted moving averages.
  We evaluate the efficacy of a supervised learning model for forecasting future escalation patterns using these derived features. The trained model identifies future high-intensity attacks and demonstrates strong predictive performance, achieving approximately 91\% accuracy, 89\% recall, and 98\% precision. Our contributions provide a temporal measurement framework for identifying future high-intensity attacks and demonstrate the presence of predictive early-warning signals within the temporal structure of IDS alert streams. We describe our methods in sufficient detail to enable reproduction using other IDS datasets. In addition, we make the trained models openly available to support further research. Finally, we introduce an interpretable visualization that enables defenders to generate early predictive warnings of elevated volumetric arrival risk.

</details>


### [3] [DDSA: Dual-Domain Strategic Attack for Spatial-Temporal Efficiency in Adversarial Robustness Testing](https://arxiv.org/abs/2601.14302)
*Jinwei Hu,Shiyuan Meng,Yi Dong,Xiaowei Huang*

Main category: cs.CR

TL;DR: DDSA是一个资源高效的对抗鲁棒性测试框架，通过时间选择性和空间精确性优化测试，适用于资源受限的实时应用。


<details>
  <summary>Details</summary>
Motivation: 资源关键应用中的图像传输和处理系统面临对抗扰动的挑战，现有鲁棒性测试方法需要大量计算资源进行逐帧处理和全图像扰动，在大规模部署中不切实际。

Method: 提出DDSA框架，包含场景感知触发函数（基于类别优先级和模型不确定性识别关键帧）和可解释AI技术（定位有影响力的像素区域进行针对性扰动）。

Result: 双域方法在保持攻击有效性的同时实现了显著的时空资源节约，使对抗鲁棒性测试能够在资源受限的实时应用中实际部署。

Conclusion: DDSA框架为资源受限环境提供了一种实用的对抗鲁棒性测试解决方案，计算效率直接影响任务成功。

Abstract: Image transmission and processing systems in resource-critical applications face significant challenges from adversarial perturbations that compromise mission-specific object classification. Current robustness testing methods require excessive computational resources through exhaustive frame-by-frame processing and full-image perturbations, proving impractical for large-scale deployments where massive image streams demand immediate processing. This paper presents DDSA (Dual-Domain Strategic Attack), a resource-efficient adversarial robustness testing framework that optimizes testing through temporal selectivity and spatial precision. We introduce a scenario-aware trigger function that identifies critical frames requiring robustness evaluation based on class priority and model uncertainty, and employ explainable AI techniques to locate influential pixel regions for targeted perturbation. Our dual-domain approach achieves substantial temporal-spatial resource conservation while maintaining attack effectiveness. The framework enables practical deployment of comprehensive adversarial robustness testing in resource-constrained real-time applications where computational efficiency directly impacts mission success.

</details>


### [4] [An Optimized Decision Tree-Based Framework for Explainable IoT Anomaly Detection](https://arxiv.org/abs/2601.14305)
*Ashikuzzaman,Md. Shawkat Hossain,Jubayer Abdullah Joy,Md Zahid Akon,Md Manjur Ahmed,Md. Naimul Islam*

Main category: cs.CR

TL;DR: 提出基于优化决策树和SHAP/Morris方法的可解释AI框架，用于资源受限的IoT入侵检测，实现高精度、可解释性和低计算开销


<details>
  <summary>Details</summary>
Motivation: IoT设备数量激增扩大了网络攻击面，现有IoT入侵检测系统在检测质量、模型可解释性和计算效率之间存在权衡，难以部署在资源受限的IoT设备上

Method: 提出基于优化决策树分类器的可解释AI框架，结合局部解释的SHAP特征归因方法和全局视角的Morris敏感性分析，实现模型可解释性

Result: 达到99.91%准确率、99.51% F1分数和0.9960 Cohen Kappa，交叉验证平均准确率98.93%，计算效率优于集成模型，SrcMac被识别为最重要特征

Conclusion: 该框架结合高精度、可解释性和低计算开销，解决了资源受限IoT环境的安全问题，支持边缘设备部署和实时处理，符合AI透明性监管要求

Abstract: The increase in the number of Internet of Things (IoT) devices has tremendously increased the attack surface of cyber threats thus making a strong intrusion detection system (IDS) with a clear explanation of the process essential towards resource-constrained environments. Nevertheless, current IoT IDS systems are usually traded off with detection quality, model elucidability, and computational effectiveness, thus the deployment on IoT devices. The present paper counteracts these difficulties by suggesting an explainable AI (XAI) framework based on an optimized Decision Tree classifier with both local and global importance methods: SHAP values that estimate feature attribution using local explanations, and Morris sensitivity analysis that identifies the feature importance in a global view. The proposed system attains the state of art on the test performance with 99.91% accuracy, F1-score of 99.51% and Cohen Kappa of 0.9960 and high stability is confirmed by a cross validation mean accuracy of 98.93%. Efficiency is also enhanced in terms of computations to provide faster inferences compared to those that are generalized in ensemble models. SrcMac has shown as the most significant predictor in feature analyses according to SHAP and Morris methods. Compared to the previous work, our solution eliminates its major drawback lack because it allows us to apply it to edge devices and, therefore, achieve real-time processing, adhere to the new regulation of transparency in AI, and achieve high detection rates on attacks of dissimilar classes. This combination performance of high accuracy, explainability, and low computation make the framework useful and reliable as a resource-constrained IoT security problem in real environments.

</details>


### [5] [CORVUS: Red-Teaming Hallucination Detectors via Internal Signal Camouflage in Large Language Models](https://arxiv.org/abs/2601.14310)
*Nay Myat Min,Long H. Pham,Hongyu Zhang,Jun Sun*

Main category: cs.CR

TL;DR: CORVUS是一种对抗性攻击方法，通过微调轻量级LoRA适配器来伪装检测器可见的内部遥测信号，从而绕过单通幻觉检测器。


<details>
  <summary>Details</summary>
Motivation: 现有单通幻觉检测器依赖LLM内部遥测信号（不确定性、隐藏状态几何、注意力等），假设幻觉会在这些信号中留下可分离的痕迹。本文研究白盒模型侧对抗者如何通过微调来隐藏这些痕迹。

Method: 提出CORVUS高效红队程序：1）使用轻量级LoRA适配器微调模型，保持检测器固定；2）在教师强制下学习伪装检测器可见的遥测信号；3）引入嵌入空间的FGSM注意力压力测试。

Result: 在1000个分布外Alpaca指令上训练（<0.5%可训练参数），CORVUS可迁移到Llama-2、Vicuna、Llama-3、Qwen2.5等模型的FAVA-Annotation数据集。能降低训练无关检测器（如LLM-Check）和基于探针的检测器（如SEP、ICR-probe）的性能。

Conclusion: CORVUS攻击暴露了当前单通幻觉检测器的脆弱性，需要开发对抗者感知的审计方法，结合外部基础或跨模型证据来提高鲁棒性。

Abstract: Single-pass hallucination detectors rely on internal telemetry (e.g., uncertainty, hidden-state geometry, and attention) of large language models, implicitly assuming hallucinations leave separable traces in these signals. We study a white-box, model-side adversary that fine-tunes lightweight LoRA adapters on the model while keeping the detector fixed, and introduce CORVUS, an efficient red-teaming procedure that learns to camouflage detector-visible telemetry under teacher forcing, including an embedding-space FGSM attention stress test. Trained on 1,000 out-of-distribution Alpaca instructions (<0.5% trainable parameters), CORVUS transfers to FAVA-Annotation across Llama-2, Vicuna, Llama-3, and Qwen2.5, and degrades both training-free detectors (e.g., LLM-Check) and probe-based detectors (e.g., SEP, ICR-probe), motivating adversary-aware auditing that incorporates external grounding or cross-model evidence.

</details>


### [6] [Tracing the Data Trail: A Survey of Data Provenance, Transparency and Traceability in LLMs](https://arxiv.org/abs/2601.14311)
*Richard Hohensinner,Belgin Mutlu,Inti Gabriel Mendoza Estrada,Matej Vukovic,Simone Kopeinik,Roman Kern*

Main category: cs.CR

TL;DR: 该调查论文回顾了过去十年关于LLM训练数据生命周期透明度的研究，提出了一个包含三个核心轴和三个支撑支柱的分类法，分析了95篇文献中的关键方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模语言模型已广泛部署，但其训练数据生命周期仍然不透明。作者旨在通过系统综述相关研究，提高对LLM训练数据透明度、可追溯性和来源的理解。

Method: 作者提出了一个包含三个核心轴（数据来源、透明度、可追溯性）和三个支撑支柱（偏见与不确定性、数据隐私、工具与技术）的分类法，并分析了95篇相关出版物。

Result: 研究识别了数据生成、水印、偏见测量、数据整理、数据隐私等关键方法，以及透明度与不透明性之间的固有权衡关系。

Conclusion: 该论文为理解LLM训练数据生命周期提供了一个系统框架，强调了提高数据透明度的重要性，并指出了未来研究的方向。

Abstract: Large language models (LLMs) are deployed at scale, yet their training data life cycle remains opaque. This survey synthesizes research from the past ten years on three tightly coupled axes: (1) data provenance, (2) transparency, and (3) traceability, and three supporting pillars: (4) bias \& uncertainty, (5) data privacy, and (6) tools and techniques that operationalize them. A central contribution is a proposed taxonomy defining the field's domains and listing corresponding artifacts. Through analysis of 95 publications, this work identifies key methodologies concerning data generation, watermarking, bias measurement, data curation, data privacy, and the inherent trade-off between transparency and opacity.

</details>


### [7] [SilentDrift: Exploiting Action Chunking for Stealthy Backdoor Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2601.14323)
*Bingxin Xu,Yuzhang Shang,Binghui Wang,Emilio Ferrara*

Main category: cs.CR

TL;DR: 该论文提出SILENTDRIFT攻击方法，利用VLA模型的动作分块和增量位姿表示的安全漏洞，通过C2连续扰动实现隐蔽的后门攻击，在LIBERO基准上达到93.2%攻击成功率。


<details>
  <summary>Details</summary>
Motivation: VLA模型在安全关键机器人应用中部署增多，但其安全漏洞研究不足。作者发现现代VLA系统中动作分块和增量位姿表示的组合会创建视觉开环，允许逐步扰动累积，构成安全威胁。

Method: 提出SILENTDRIFT攻击方法：1) 利用Smootherstep函数构建C2连续扰动，确保轨迹边界零速度和加速度以满足运动学约束；2) 采用关键帧攻击策略，仅毒化关键接近阶段，最大化影响同时最小化触发器暴露。

Result: 在LIBERO基准上评估，SILENTDRIFT达到93.2%的攻击成功率，毒化率低于2%，同时保持95.3%的清洁任务成功率。中毒轨迹在视觉上与成功演示无法区分。

Conclusion: VLA系统的动作分块和增量位姿表示存在安全漏洞，允许隐蔽的后门攻击。SILENTDRIFT展示了这种攻击的有效性，强调了在安全关键机器人应用中需要更强的安全防护。

Abstract: Vision-Language-Action (VLA) models are increasingly deployed in safety-critical robotic applications, yet their security vulnerabilities remain underexplored. We identify a fundamental security flaw in modern VLA systems: the combination of action chunking and delta pose representations creates an intra-chunk visual open-loop. This mechanism forces the robot to execute K-step action sequences, allowing per-step perturbations to accumulate through integration. We propose SILENTDRIFT, a stealthy black-box backdoor attack exploiting this vulnerability. Our method employs the Smootherstep function to construct perturbations with guaranteed C2 continuity, ensuring zero velocity and acceleration at trajectory boundaries to satisfy strict kinematic consistency constraints. Furthermore, our keyframe attack strategy selectively poisons only the critical approach phase, maximizing impact while minimizing trigger exposure. The resulting poisoned trajectories are visually indistinguishable from successful demonstrations. Evaluated on the LIBERO, SILENTDRIFT achieves a 93.2% Attack Success Rate with a poisoning rate under 2%, while maintaining a 95.3% Clean Task Success Rate.

</details>


### [8] [Turn-Based Structural Triggers: Prompt-Free Backdoors in Multi-Turn LLMs](https://arxiv.org/abs/2601.14340)
*Yiyang Lu,Jinwen He,Yue Zhao,Kai Chen,Ruigang Liang*

Main category: cs.CR

TL;DR: 论文提出了一种基于对话结构的后门攻击方法TST，使用对话轮次索引作为触发器，在多轮对话LLM系统中实现高攻击成功率，且能绕过现有防御机制。


<details>
  <summary>Details</summary>
Motivation: 随着LLM被广泛集成到对话系统和任务导向助手等交互系统中，供应链风险日益突出。现有后门攻击和防御主要关注用户可见的提示词触发器，而忽视了多轮对话中的结构信号，这构成了一个未被充分研究的攻击面。

Method: 提出Turn-based Structural Trigger (TST)后门攻击方法，使用对话轮次索引作为触发器，与用户输入内容无关。该方法在四个广泛使用的开源LLM模型上进行测试，并评估其在五种代表性防御机制下的有效性。

Result: TST在四个LLM模型上平均攻击成功率达到99.52%，且对模型效用影响最小。在五种防御机制下仍保持98.04%的平均攻击成功率。该方法在不同指令数据集上泛化能力强，平均ASR为99.19%。

Conclusion: 对话结构是多轮LLM系统中一个重要且未被充分研究的攻击面，需要在实践中进行结构感知的审计和缓解措施。研究结果表明仅关注提示词层面的防御是不够的，需要更全面的安全考虑。

Abstract: Large Language Models (LLMs) are widely integrated into interactive systems such as dialogue agents and task-oriented assistants. This growing ecosystem also raises supply-chain risks, where adversaries can distribute poisoned models that degrade downstream reliability and user trust. Existing backdoor attacks and defenses are largely prompt-centric, focusing on user-visible triggers while overlooking structural signals in multi-turn conversations. We propose Turn-based Structural Trigger (TST), a backdoor attack that activates from dialogue structure, using the turn index as the trigger and remaining independent of user inputs. Across four widely used open-source LLM models, TST achieves an average attack success rate (ASR) of 99.52% with minimal utility degradation, and remains effective under five representative defenses with an average ASR of 98.04%. The attack also generalizes well across instruction datasets, maintaining an average ASR of 99.19%. Our results suggest that dialogue structure constitutes an important and under-studied attack surface for multi-turn LLM systems, motivating structure-aware auditing and mitigation in practice.

</details>


### [9] [Rethinking On-Device LLM Reasoning: Why Analogical Mapping Outperforms Abstract Thinking for IoT DDoS Detection](https://arxiv.org/abs/2601.14343)
*William Pan,Guiran Liu,Binrong Zhu,Qun Wang,Yingzhou Lu,Beiyu Lin,Rose Qingyang Hu*

Main category: cs.CR

TL;DR: 本文提出了一种结合思维链推理与检索增强生成的物联网边缘DDoS攻击检测框架，通过系统评估小型设备端大语言模型，在资源受限环境下显著提升了复杂网络攻击的分类性能。


<details>
  <summary>Details</summary>
Motivation: 物联网部署的快速扩展加剧了网络安全威胁，特别是分布式拒绝服务攻击模式日益复杂。虽然通过设备端大语言模型利用生成式AI可以在网络边缘实现实时威胁检测，但小型模型的计算资源限制带来了挑战。

Method: 提出了一种新颖的检测框架，将思维链推理与检索增强生成相结合，专门针对物联网边缘环境设计。系统评估了紧凑型设备端大语言模型（包括LLaMA 3.2的1B和3B版本，以及Gemma 3的1B和4B版本），采用结构化提示和基于示例的推理策略。

Result: 实验结果显示，通过少量样本提示获得了显著的性能提升，实现了高达0.85的宏观平均F1分数。基于示例的推理方法带来了显著优势，思维链和检索增强生成方法明显增强了小型设备端大语言模型在严格资源约束下准确分类复杂网络攻击的能力。

Conclusion: 研究表明，结合思维链推理与检索增强生成的方法能够有效提升小型设备端大语言模型在物联网边缘环境中的网络安全威胁检测能力，为解决资源受限环境下的复杂攻击分类问题提供了可行方案。

Abstract: The rapid expansion of IoT deployments has intensified cybersecurity threats, notably Distributed Denial of Service (DDoS) attacks, characterized by increasingly sophisticated patterns. Leveraging Generative AI through On-Device Large Language Models (ODLLMs) provides a viable solution for real-time threat detection at the network edge, though limited computational resources present challenges for smaller ODLLMs. This paper introduces a novel detection framework that integrates Chain-of-Thought (CoT) reasoning with Retrieval-Augmented Generation (RAG), tailored specifically for IoT edge environments. We systematically evaluate compact ODLLMs, including LLaMA 3.2 (1B, 3B) and Gemma 3 (1B, 4B), using structured prompting and exemplar-driven reasoning strategies. Experimental results demonstrate substantial performance improvements with few-shot prompting, achieving macro-average F1 scores as high as 0.85. Our findings highlight the significant advantages of incorporating exemplar-based reasoning, underscoring that CoT and RAG approaches markedly enhance small ODLLMs' capabilities in accurately classifying complex network attacks under stringent resource constraints.

</details>


### [10] [A Survey of Security Challenges and Solutions for Advanced Air Mobility and eVTOL Aircraft](https://arxiv.org/abs/2601.14415)
*Mahyar Ghazanfari,Iman Sharifi,Peng Wei,Noah Dahle,Abel Diaz Gonzalez,Austin Coursey,Bryce Bjorkman,Cailani Lemieux-Mack,Robert Canady,Abenezer Taye,Bryan C. Ward,Xenofon Koutsoukos,Gautam Biswas,Maheed H. Ahmed,Hyeong Tae Kim,Mahsa Ghasemi,Vijay Gupta,Filippos Fotiadis,Ufuk Topcu,Junchi Lu,Alfred Chen,Abdul Kareem Ras,Nischal Aryal,Amer Ibrahim,Amir Shirkhodaie,Heber Herencia-Zapana,Saqib Hasan,Isaac Amundson*

Main category: cs.CR

TL;DR: 本文综述了先进空中交通（AAM）系统（特别是eVTOL飞机）的安全漏洞与防御机制，提出了攻击分类、分析了缓解策略，并设计了面向未来AAM生态的安全架构。


<details>
  <summary>Details</summary>
Motivation: 随着先进空中交通（AAM）系统特别是电动垂直起降（eVTOL）飞机的发展，这些高度自动化、互联的航空系统面临新的安全威胁。现有商业航空和无人机系统的漏洞表明，需要系统性地分析AAM特有的安全风险并设计相应的防御机制。

Method: 基于商业航空电子设备和自动化无人机系统的现有漏洞，采用调查分析方法：1）建立攻击分类学；2）分析缓解策略；3）提出专门针对未来AAM生态系统的安全系统架构；4）识别关键威胁向量和新兴防御技术。

Result: 识别了AAM系统的主要威胁向量，包括GPS干扰/欺骗、ATC无线电频率滥用、TCAS和ADS-B攻击、通过电子飞行包（EFB）的后门、飞机自动化和连接性引入的新漏洞、飞行管理系统软件/数据库/云服务风险等。提出了相应的防御技术和安全架构方案。

Conclusion: AAM系统面临复杂的安全挑战，需要专门的安全架构和防御机制。虽然已有一些新兴防御技术，但仍存在许多开放技术问题需要解决，以实现更强大的安全防护，确保未来空中交通系统的可靠性和安全性。

Abstract: This survey reviews the existing and envisioned security vulnerabilities and defense mechanisms relevant to Advanced Air Mobility (AAM) systems, with a focus on electric vertical takeoff and landing (eVTOL) aircraft. Drawing from vulnerabilities in the avionics in commercial aviation and the automated unmanned aerial systems (UAS), the paper presents a taxonomy of attacks, analyzes mitigation strategies, and proposes a secure system architecture tailored to the future AAM ecosystem. The paper also highlights key threat vectors, including Global Positioning System (GPS) jamming/spoofing, ATC radio frequency misuse, attacks on TCAS and ADS-B, possible backdoor via Electronic Flight Bag (EFB), new vulnerabilities introduced by aircraft automation and connectivity, and risks from flight management system (FMS) software, database and cloud services. Finally, this paper describes emerging defense techniques against these attacks, and open technical problems to address toward better defense mechanisms.

</details>


### [11] [Uma Prova de Conceito para a Verificação Formal de Contratos Inteligentes](https://arxiv.org/abs/2601.14427)
*Murilo de Souza Neves,Adilson Luiz Bonifacio*

Main category: cs.CR

TL;DR: 本文提出使用相对化合同语言(RCL)和RECALL工具对多主体买卖合同进行形式化规约与验证，证明在建模阶段检测规范冲突，确保最终智能合约代码的可靠性


<details>
  <summary>Details</summary>
Motivation: 智能合约具有自执行能力和比传统合同更高的安全性，但其不可变性使得部署后故障修复极其复杂，需要在部署前建立验证层。现有形式化方法如合同语言(CL)在复杂多方场景中分配责任方面存在局限

Method: 使用相对化合同语言(RCL)和RECALL工具对多主体买卖合同进行规约与验证，在建模阶段检测规范冲突，修正逻辑不一致性后将合同转换为Solidity代码，在Remix IDE环境中进行功能验证

Result: 研究证明RECALL工具能够在建模阶段有效检测规范冲突，修正逻辑不一致性后成功将合同转换为Solidity代码，并在Remix IDE中通过功能验证，确认了形式化验证对最终代码可靠性和安全性的重要性

Conclusion: 形式化验证是确保智能合约最终代码可靠性和安全性的基础，RCL和RECALL工具为复杂多方合同提供了有效的规约和验证方法，能够在部署前检测并解决规范冲突

Abstract: Smart contracts are tools with self-execution capabilities that provide enhanced security compared to traditional contracts; however, their immutability makes post-deployment fault correction extremely complex, highlighting the need for a verification layer prior to this stage. Although formalisms such as Contract Language (CL) enable logical analyses, they prove limited in attributing responsibilities within complex multilateral scenarios. This work presents a proof of concept using the Relativized Contract Language (RCL) and the RECALL tool for the specification and verification of a purchase and sale contract involving multiple agents. The study demonstrates the tool's capability to detect normative conflicts during the modeling phase. After correcting logical inconsistencies, the contract was translated into Solidity and functionally validated within the Remix IDE environment, confirming that prior formal verification is fundamental to ensuring the reliability and security of the final code.

</details>


### [12] [European digital identity: A missed opportunity?](https://arxiv.org/abs/2601.14503)
*Wouter Termont,Beatriz Esteves*

Main category: cs.CR

TL;DR: 本文批评欧盟数字身份框架（EUDI）及其OpenID架构存在设计缺陷，包括不安全的实践、静态凭证类型、有限查询语言，以及未能真正实现去中心化身份承诺。作者建议考虑OAuth UMA扩展和GNAP等替代方案。


<details>
  <summary>Details</summary>
Motivation: 欧盟数字身份框架（EUDI）及其OpenID架构声称要实现用户控制的数字身份，但作者认为其概念基础狭窄且定义不清。需要基于更广泛、更基础的身份验证理解来评估该框架的实际能力。

Method: 通过对OpenID4VCI和OpenID4VP协议的分析，识别技术设计问题，包括安全实践、凭证类型限制、查询语言不足等。同时批判其信任模型，并与现有去中心化方案进行比较。

Result: 发现OpenID架构存在多个问题：不安全实践、静态主体绑定凭证类型、有限查询语言限制了应用场景。其信任模型相比现有去中心化方案并未显著提升用户控制、隐私和个人信息可移植性。制度化的可信列表存在经济和政治风险。

Conclusion: EUDI框架的技术选择和立法本身都无法实现自主身份承诺。建议考虑OAuth UMA扩展、A4DS配置文件和GNAP等替代方案，并需要研究统一的查询元语言来解决证明和提供者的异质性问题。

Abstract: Recent European efforts around digital identity -- the EUDI regulation and its OpenID architecture -- aim high, but start from a narrow and ill-defined conceptualization of authentication. Based on a broader, more grounded understanding of the term, in we identify several issues in the design of OpenID4VCI and OpenID4VP: insecure practices, static, and subject-bound credential types, and a limited query language restrict their application to classic scenarios of credential exchange -- already supported by existing solutions like OpenID Connect, SIOPv2, OIDC4IDA, and OIDC Claims Aggregation -- barring dynamic, asynchronous, or automated use cases. We also debunk OpenID's 'paradigm-shifting' trust-model, which -- when compared to existing decentralized alternatives -- does not deliver any significant increase in control, privacy, and portability of personal information. Not only the technical choices limit the capabilities of the EUDI framework; also the legislation itself cannot accommodate the promise of self-sovereign identity. In particular, we criticize the introduction of institutionalized trusted lists, and discuss their economical and political risks. Their potential to decline into an exclusory, re-centralized ecosystem endangers the vision of a user-oriented identity management in which individuals are in charge. Instead, the consequences might severely restrict people in what they can do with their personal information, and risk increased linkability and monitoring. In anticipation of revisions to the EUDI regulations, we suggest several technical alternatives that overcome some of the issues with the architecture of OpenID. In particular, OAuth's UMA extension and its A4DS profile, as well as their integration in GNAP, are worth looking into. Future research into uniform query (meta-)languages is needed to address the heterogeneity of attestations and providers.

</details>


### [13] [Uncovering and Understanding FPR Manipulation Attack in Industrial IoT Networks](https://arxiv.org/abs/2601.14505)
*Mohammad Shamim Ahsan,Peng Liu*

Main category: cs.CR

TL;DR: 本文提出了一种针对工业物联网网络的新型网络攻击——FPR操纵攻击(FPA)，通过利用MQTT协议知识对良性数据包进行扰动，使其被误分类为攻击流量，成功率达80.19%-100%，显著增加了安全运营中心的警报调查延迟。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为对抗攻击主要是攻击包被误分类为良性，而忽略了良性包被误分类为攻击的可能性。本文旨在揭示这种被忽视的威胁，特别是在工业物联网环境中，这种误分类可能导致安全运营中心被大量虚假警报淹没。

Method: 提出FPR操纵攻击(FPA)，利用工业物联网中广泛使用的MQTT协议的领域知识，对良性数据包进行系统性的简单包级扰动，而不使用传统的基于梯度或非梯度的方法。通过实验评估攻击效果，并进行统计分析和可解释AI分析来理解攻击成功的关键因素。

Result: 攻击成功率达到80.19%至100%。在安全运营中心环境中，即使少量虚假阳性警报也会使真实警报的调查延迟增加最多2小时。攻击包还可用于对抗训练以增强模型鲁棒性。

Conclusion: FPR操纵攻击是工业物联网网络中的一个实际威胁，挑战了传统对抗攻击的认知。该研究强调了考虑双向误分类威胁的重要性，并为通过对抗训练增强模型鲁棒性提供了新思路。

Abstract: In the network security domain, due to practical issues -- including imbalanced data and heterogeneous legitimate network traffic -- adversarial attacks in machine learning-based NIDSs have been viewed as attack packets misclassified as benign. Due to this prevailing belief, the possibility of (maliciously) perturbed benign packets being misclassified as attack has been largely ignored. In this paper, we demonstrate that this is not only theoretically possible, but also a particular threat to NIDS. In particular, we uncover a practical cyberattack, FPR manipulation attack (FPA), especially targeting industrial IoT networks, where domain-specific knowledge of the widely used MQTT protocol is exploited and a systematic simple packet-level perturbation is performed to alter the labels of benign traffic samples without employing traditional gradient-based or non-gradient-based methods. The experimental evaluations demonstrate that this novel attack results in a success rate of 80.19% to 100%. In addition, while estimating impacts in the Security Operations Center, we observe that even a small fraction of false positive alerts, irrespective of different budget constraints and alert traffic intensities, can increase the delay of genuine alerts investigations up to 2 hr in a single day under normal operating conditions. Furthermore, a series of relevant statistical and XAI analyses is conducted to understand the key factors behind this remarkable success. Finally, we explore the effectiveness of the FPA packets to enhance models' robustness through adversarial training and investigate the changes in decision boundaries accordingly.

</details>


### [14] [Towards Transparent Malware Detection With Granular Explainability: Backtracking Meta-Coarsened Explanations Onto Assembly Flow Graphs With Graph Neural Networks](https://arxiv.org/abs/2601.14511)
*Griffin Higgins,Roozbeh Razavi-Far,Hossein Shokouhinejad,Ali A. Ghorbani*

Main category: cs.CR

TL;DR: 提出Assembly Flow Graph (AFG)和Meta-Coarsening方法，首次在基于GNN的恶意软件检测中实现细粒度可解释性，在CIC-DGG-2025数据集上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 恶意软件日益复杂和隐蔽，需要更智能、强大且透明的检测系统。现有基于图神经网络(GNN)的恶意软件检测方法缺乏细粒度的可解释性，难以理解检测决策的依据。

Method: 提出Assembly Flow Graph (AFG)将二进制可执行文件的汇编流程表示为图数据；设计Meta-Coarsening方法通过图缩减提高计算可行性；使用多个新颖和现有指标量化解释的细粒度和质量；通过超参数控制最终解释大小。

Result: 在CIC-DGG-2025数据集上评估，结果表明AFG和Meta-Coarsening方法在特定粗化级别下既能提高可解释性，又能提升推理性能。这是首次在基于GNN的恶意软件检测中考虑细粒度可解释性。

Conclusion: 提出的AFG和Meta-Coarsening方法为恶意软件检测提供了透明且可解释的解决方案，在保持检测性能的同时增强了系统的可解释性，填补了GNN恶意软件检测中细粒度可解释性的研究空白。

Abstract: As malware continues to become increasingly sophisticated, threatening, and evasive, malware detection systems must keep pace and become equally intelligent, powerful, and transparent. In this paper, we propose Assembly Flow Graph (AFG) to comprehensively represent the assembly flow of a binary executable as graph data. Importantly, AFG can be used to extract granular explanations needed to increase transparency for malware detection using Graph Neural Networks (GNNs). However, since AFGs may be large in practice, we also propose a Meta-Coarsening approach to improve computational tractability via graph reduction. To evaluate our proposed approach we consider several novel and existing metrics to quantify the granularity and quality of explanations. Lastly, we also consider several hyperparameters in our proposed Meta-Coarsening approach that can be used to control the final explanation size. We evaluate our proposed approach using the CIC-DGG-2025 dataset. Our results indicate that our proposed AFG and Meta-Coarsening approach can provide both increased explainability and inference performance at certain coarsening levels. However, most importantly, to the best of our knowledge, we are the first to consider granular explainability in malware detection using GNNs.

</details>


### [15] [LLM Security and Safety: Insights from Homotopy-Inspired Prompt Obfuscation](https://arxiv.org/abs/2601.14528)
*Luis Lazo,Hamed Jelodar,Roozbeh Razavi-Far*

Main category: cs.CR

TL;DR: 提出基于同伦理论的提示词混淆框架，通过系统化工程化提示词影响LLM的潜在行为，揭示安全漏洞并推动更健壮的防御机制


<details>
  <summary>Details</summary>
Motivation: 增强对大型语言模型安全性和安全性漏洞的理解，分析当前LLM防护措施的不足，为构建更安全、负责任、可信的AI技术提供基础

Method: 采用同伦理论启发的提示词混淆框架，系统应用精心设计的工程化提示词，在LLama、Deepseek、KIMI和Claude等模型上进行大规模实验（15,732个提示词，含10,000个高优先级案例）

Result: 揭示了当前LLM安全防护的关键洞察，展示了潜在模型行为如何被意外影响，强调了需要更健壮的防御机制、可靠的检测策略和改进的韧性

Conclusion: 为分析和缓解潜在弱点提供了原则性框架，目标是推进安全、负责任和可信的AI技术发展，强调需要更全面的安全防护措施

Abstract: In this study, we propose a homotopy-inspired prompt obfuscation framework to enhance understanding of security and safety vulnerabilities in Large Language Models (LLMs). By systematically applying carefully engineered prompts, we demonstrate how latent model behaviors can be influenced in unexpected ways. Our experiments encompassed 15,732 prompts, including 10,000 high-priority cases, across LLama, Deepseek, KIMI for code generation, and Claude to verify. The results reveal critical insights into current LLM safeguards, highlighting the need for more robust defense mechanisms, reliable detection strategies, and improved resilience. Importantly, this work provides a principled framework for analyzing and mitigating potential weaknesses, with the goal of advancing safe, responsible, and trustworthy AI technologies.

</details>


### [16] [AI Agents vs. Human Investigators: Balancing Automation, Security, and Expertise in Cyber Forensic Analysis](https://arxiv.org/abs/2601.14544)
*Sneha Sudhakaran,Naresh Kshetri*

Main category: cs.CR

TL;DR: 本研究对比了AI代理（ChatGPT）与人类调查员在网络取证分析中的表现，发现AI虽能提升常规分析效率，但在处理复杂或新型网络威胁时存在局限，需要人类监督来确保准确性和完整性。


<details>
  <summary>Details</summary>
Motivation: 随着网络威胁快速演变，取证分析的可靠性对数字调查和网络安全响应至关重要。AI代理虽能自动化异常检测、证据分类等流程，但其基于有偏或不完整数据训练的特性可能产生误导性结果，威胁取证调查的完整性。

Method: 采用对比分析方法，评估最常用AI代理ChatGPT与人类取证调查员在网络取证分析中的有效性。通过多种网络威胁场景进行全面的可靠性测试，识别AI驱动方法的局限性。

Result: 研究发现AI系统在处理复杂或新型网络威胁时存在显著局限，因其基于固定模式的特性可能导致威胁漏检。人类调查员通过适应性决策、伦理推理和上下文理解，能有效识别自动化系统可能遗漏的细微异常和威胁。

Conclusion: 虽然AI代理能显著提升常规分析的效率，但人类监督对于确保结果的准确性和全面性仍然至关重要。AI与人类协同工作可能是最佳实践，结合AI的自动化优势与人类的适应性推理能力。

Abstract: In an era where cyber threats are rapidly evolving, the reliability of cyber forensic analysis has become increasingly critical for effective digital investigations and cybersecurity responses. AI agents are being adopted across digital forensic practices due to their ability to automate processes such as anomaly detection, evidence classification, and behavioral pattern recognition, significantly enhancing scalability and reducing investigation timelines. However, the characteristics that make AI indispensable also introduce notable risks. AI systems, often trained on biased or incomplete datasets, can produce misleading results, including false positives and false negatives, thereby jeopardizing the integrity of forensic investigations. This study presents a meticulous comparative analysis of the effectiveness of the most used AI agent, ChatGPT, and human forensic investigators in the realm of cyber forensic analysis. Our research reveals critical limitations within AI-driven approaches, demonstrating scenarios in which sophisticated or novel cyber threats remain undetected due to the rigid pattern-based nature of AI systems. Conversely, our analysis highlights the crucial role that human forensic investigators play in mitigating these risks. Through adaptive decision-making, ethical reasoning, and contextual understanding, human investigators effectively identify subtle anomalies and threats that may evade automated detection systems. To reinforce our findings, we conducted comprehensive reliability testing of forensic techniques using multiple cyber threat scenarios. These tests confirmed that while AI agents significantly improve the efficiency of routine analyses, human oversight remains crucial in ensuring accuracy and comprehensiveness of the results.

</details>


### [17] [WebAssembly Based Portable and Secure Sensor Interface for Internet of Things](https://arxiv.org/abs/2601.14555)
*Botong Ou,Baijian Yang*

Main category: cs.CR

TL;DR: 论文提出首个WASI扩展，为异构嵌入式设备提供安全、可移植、低开销的沙箱，支持多租户访问传感器数据，通过WebAssembly实现内存隔离和资源权限控制。


<details>
  <summary>Details</summary>
Motivation: 随着物联网设备普及带来隐私和安全问题，现有系统缺乏明确的共享和保护访问管理机制，导致设备易受攻击和数据泄露。WebAssembly因其高效的嵌入式系统沙箱特性成为探索安全传感器接口的理想选择。

Method: 设计首个WASI扩展，提供安全、可移植、低开销的沙箱，包括：应用内存隔离、传感器访问拦截确保资源权限、MQTT-SN接口实现网络访问控制。在Zephyr RTOS上实现运行时扩展，针对WebAssembly字节码。

Result: 传感器访问延迟开销为6%，额外内存占用为5%（相比原生执行）。MQTT-SN实现引入小于1%的额外延迟和相似的内存占用，因为网络延迟占主导。

Conclusion: 提出的WASI扩展成功为异构嵌入式设备提供了安全、可移植、低开销的多租户传感器数据访问沙箱，验证了WebAssembly在物联网安全领域的可行性。

Abstract: As the expansion of IoT connectivity continues to provide quality-of-life improvements around the world, they simultaneously introduce increasing privacy and security concerns. The lack of a clear definition in managing shared and protected access to IoT sensors offer channels by which devices can be compromised and sensitive data can be leaked. In recent years, WebAssembly has received considerable attention for its efficient application sandboxing suitable for embedded systems, making it a prime candidate for exploring a secure and portable sensor interface. This paper introduces the first WebAssembly System Interface (WASI) extension offering a secure, portable, and low-footprint sandbox enabling multi-tenant access to sensor data across heterogeneous embedded devices. The runtime extensions provide application memory isolation, ensure appropriate resource privileges by intercepting sensor access, and offer an MQTT-SN interface enabling in-network access control. When targeting the WebAssembly byte-code with the associated runtime extensions implemented atop the Zephyr RTOS, our evaluation of sensor access indicates a latency overhead of 6% with an additional memory footprint of 5% when compared to native execution. As MQTT-SN requests are dominated by network delays, the WASI-SN implementation of MQTT-SN introduces less than 1% additional latency with similar memory footprint.

</details>


### [18] [Automatically Tightening Access Control Policies with Restricter](https://arxiv.org/abs/2601.14582)
*Ka Lok Wu,Christa Jenkins,Scott D. Stolle,Omar Chowdhury*

Main category: cs.CR

TL;DR: Restricter：基于访问日志自动收紧访问控制策略规则的工具，旨在实现最小权限原则


<details>
  <summary>Details</summary>
Motivation: 访问控制策略配置困难，即使是经验丰富的管理员也容易配置错误，导致权限过大等安全问题。需要自动化工具来收紧策略规则，实现最小权限原则。

Method: 提出Restricter系统，利用访问日志（记录已执行的访问请求和决策）自动收紧每个允许规则。通过减少策略规则允许的访问请求数量，同时不牺牲系统功能。

Result: 为Amazon Cedar策略语言实现了Restricter，并通过两个实际案例研究证明了其有效性。

Conclusion: Restricter能够有效自动收紧访问控制策略，帮助实现最小权限原则，减少策略配置错误带来的安全风险。

Abstract: Robust access control is a cornerstone of secure software, systems, and networks. An access control mechanism is as effective as the policy it enforces. However, authoring effective policies that satisfy desired properties such as the principle of least privilege is a challenging task even for experienced administrators, as evidenced by many real instances of policy misconfiguration. In this paper, we set out to address this pain point by proposing Restricter, which automatically tightens each (permit) policy rule of a policy with respect to an access log, which captures some already exercised access requests and their corresponding access decisions (i.e., allow or deny). Restricter achieves policy tightening by reducing the number of access requests permitted by a policy rule without sacrificing the functionality of the underlying system it is regulating. We implement Restricter for Amazon's Cedar policy language and demonstrate its effectiveness through two realistic case studies.

</details>


### [19] [IntelliSA: An Intelligent Static Analyzer for IaC Security Smell Detection Using Symbolic Rules and Neural Inference](https://arxiv.org/abs/2601.14595)
*Qiyue Mei,Michael Fu*

Main category: cs.CR

TL;DR: IntelliSA是一个智能静态分析器，结合符号规则和神经推理来检测IaC安全异味，通过知识蒸馏训练紧凑学生模型，在保持高检测率的同时大幅减少误报。


<details>
  <summary>Details</summary>
Motivation: IaC脚本中的错误配置会广泛传播，导致严重系统故障和安全风险。现有基于符号规则的静态分析器会产生过多误报，增加人工检查负担。LLM虽然能有效过滤误报，但存在成本高、延迟大、数据治理和可复现性等问题。

Method: 提出IntelliSA智能静态分析器：1) 使用符号规则进行过度近似以覆盖潜在安全异味；2) 采用知识蒸馏方法，用LLM教师生成伪标签训练紧凑学生模型（比LLM小500倍以上）；3) 学生模型学习教师知识并高效分类误报。

Result: 在包含11,814行真实IaC代码和241个安全异味的数据集上评估，IntelliSA达到最高F1分数（83%），优于两个静态分析器和三个LLM基线（Claude-4、Grok-4、GPT-5）7-42%。同时具有最佳成本效益，检测60%安全异味时仅检查不到2%的代码库。

Conclusion: IntelliSA通过结合符号规则和神经推理，有效解决了IaC安全异味检测中误报过多的问题。知识蒸馏方法使紧凑学生模型能够学习LLM教师的专业知识，在保持高检测性能的同时解决了LLM的成本、延迟和部署限制问题。

Abstract: Infrastructure as Code (IaC) enables automated provisioning of large-scale cloud and on-premise environments, reducing the need for repetitive manual setup. However, this automation is a double-edged sword: a single misconfiguration in IaC scripts can propagate widely, leading to severe system downtime and security risks. Prior studies have shown that IaC scripts often contain security smells--bad coding patterns that may introduce vulnerabilities--and have proposed static analyzers based on symbolic rules to detect them. Yet, our preliminary analysis reveals that rule-based detection alone tends to over-approximate, producing excessive false positives and increasing the burden of manual inspection. In this paper, we present IntelliSA, an intelligent static analyzer for IaC security smell detection that integrates symbolic rules with neural inference. IntelliSA applies symbolic rules to over-approximate potential smells for broad coverage, then employs neural inference to filter false positives. While an LLM can effectively perform this filtering, reliance on LLM APIs introduces high cost and latency, raises data governance concerns, and limits reproducibility and offline deployment. To address the challenges, we adopt a knowledge distillation approach: an LLM teacher generates pseudo-labels to train a compact student model--over 500x smaller--that learns from the teacher's knowledge and efficiently classifies false positives. We evaluate IntelliSA against two static analyzers and three LLM baselines (Claude-4, Grok-4, and GPT-5) using a human-labeled dataset including 241 security smells across 11,814 lines of real-world IaC code. Experimental results show that IntelliSA achieves the highest F1 score (83%), outperforming baselines by 7-42%. Moreover, IntelliSA demonstrates the best cost-effectiveness, detecting 60% of security smells while inspecting less than 2% of the codebase.

</details>


### [20] [Holmes: An Evidence-Grounded LLM Agent for Auditable DDoS Investigation in Cloud Networks](https://arxiv.org/abs/2601.14601)
*Haodong Chen,Ziheng Zhang,Jinghui Jiang,Qiang Su,Qiao Xiang*

Main category: cs.CR

TL;DR: Holmes是一个基于LLM的DDoS检测代理，采用虚拟SRE调查员模式而非端到端分类器，通过分层工作流和证据包抽象实现可审计的DDoS调查。


<details>
  <summary>Details</summary>
Motivation: 云环境面临频繁的DDoS威胁，现代云原生DDoS攻击快速演进且采用多向量策略。现有基于规则和监督学习的方法输出黑盒分数或标签，证据链有限，对未见攻击变体泛化能力差，且云环境中高质量标注数据难以获取。

Method: 1) 漏斗式分层工作流：使用计数器/sFlow进行连续感知和分类，仅在异常窗口触发PCAP证据收集；2) 证据包抽象：将二进制数据包转换为紧凑、可重现、高信号的结构化证据；3) 结构优先调查协议：强制JSON/引用约束，生成带有可审计证据锚点的机器可消费报告。

Result: 在CICDDoS2019反射/放大攻击和脚本触发洪水场景中，Holmes能够基于显著证据锚点做出归因决策，当错误发生时，其审计日志使故障源易于定位，证明了LLM代理在云操作中成本可控且可追溯的DDoS调查实用性。

Conclusion: Holmes展示了LLM代理作为虚拟SRE调查员在云环境中进行DDoS检测的可行性，通过结构化证据和可审计工作流解决了现有方法的黑盒问题和泛化能力不足，为云操作提供了成本可控且可追溯的调查方案。

Abstract: Cloud environments face frequent DDoS threats due to centralized resources and broad attack surfaces. Modern cloud-native DDoS attacks further evolve rapidly and often blend multi-vector strategies, creating an operational dilemma: defenders need wire-speed monitoring while also requiring explainable, auditable attribution for response. Existing rule-based and supervised-learning approaches typically output black-box scores or labels, provide limited evidence chains, and generalize poorly to unseen attack variants; meanwhile, high-quality labeled data is often difficult to obtain in cloud settings.
  We present Holmes (DDoS Detective), an LLM-based DDoS detection agent that reframes the model as a virtual SRE investigator rather than an end-to-end classifier. Holmes couples a funnel-like hierarchical workflow (counters/sFlow for continuous sensing and triage; PCAP evidence collection triggered only on anomaly windows) with an Evidence Pack abstraction that converts binary packets into compact, reproducible, high-signal structured evidence. On top of this evidence interface, Holmes enforces a structure-first investigation protocol and strict JSON/quotation constraints to produce machine-consumable reports with auditable evidence anchors.
  We evaluate Holmes on CICDDoS2019 reflection/amplification attacks and script-triggered flooding scenarios. Results show that Holmes produces attribution decisions grounded in salient evidence anchors across diverse attack families, and when errors occur, its audit logs make the failure source easy to localize, demonstrating the practicality of an LLM agent for cost-controlled and traceable DDoS investigation in cloud operations.

</details>


### [21] [An LLM Agent-based Framework for Whaling Countermeasures](https://arxiv.org/abs/2601.14606)
*Daisuke Miyamoto,Takuji Iimura,Narushige Michishita*

Main category: cs.CR

TL;DR: 提出针对大学教职员工的鲸钓攻击防御框架，利用LLM代理构建个性化防御档案，通过公开信息分析漏洞并评估风险


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的普及，针对高校教职员工的鲸钓攻击威胁日益严重，这些员工通常兼具研究领导力和行政权威，公开信息丰富，容易被AI构建精准攻击画像

Method: 设计基于大语言模型的代理系统：1)从公开信息构建教职员工漏洞档案；2)识别鲸钓防御相关风险场景；3)构建对应漏洞和风险的防御档案；4)使用防御档案分析鲸钓邮件

Result: 初步风险评估实验表明，该方法能产生与教职员工工作情境一致的响应策略判断和解释，同时揭示了实际部署和系统评估的挑战

Conclusion: 提出的LLM代理框架能有效应对针对大学教职员工的鲸钓攻击，但需要进一步解决实际部署和系统评估的挑战

Abstract: With the spread of generative AI in recent years, attacks known as Whaling have become a serious threat. Whaling is a form of social engineering that targets important high-authority individuals within organizations and uses sophisticated fraudulent emails. In the context of Japanese universities, faculty members frequently hold positions that combine research leadership with authority within institutional workflows. This structural characteristic leads to the wide public disclosure of high-value information such as publications, grants, and detailed researcher profiles. Such extensive information exposure enables the construction of highly precise target profiles using generative AI. This raises concerns that Whaling attacks based on high-precision profiling by generative AI will become prevalent. In this study, we propose a Whaling countermeasure framework for university faculty members that constructs personalized defense profiles and uses large language model (LLM)-based agents. We design agents that (i) build vulnerability profiles for each target from publicly available information on faculty members, (ii) identify potential risk scenarios relevant to Whaling defense based on those profiles, (iii) construct defense profiles corresponding to the vulnerabilities and anticipated risks, and (iv) analyze Whaling emails using the defense profiles. Furthermore, we conduct a preliminary risk-assessment experiment. The results indicate that the proposed method can produce judgments accompanied by explanations of response policies that are consistent with the work context of faculty members who are Whaling targets. The findings also highlight practical challenges and considerations for future operational deployment and systematic evaluation.

</details>


### [22] [Towards Cybersecurity Superintelligence: from AI-guided humans to human-guided AI](https://arxiv.org/abs/2601.14614)
*Víctor Mayoral-Vilches,Stefan Rass,Martin Pinzger,Endika Gil-Uriarte,Unai Ayucar-Carbajo,Jon Ander Ruiz-Alcalde,Maite del Mundo de Torres,Luis Javier Navarrete-Lozano,María Sanz-Gómez,Francesco Balassone,Cristóbal R. J. Veas-Chavez,Vanesa Turiel,Alfonso Glera-Picón,Daniel Sánchez-Prieto,Yuri Salvatierra,Paul Zabalegui-Landa,Ruffino Reydel Cabrera-Álvarez,Patxi Mayoral-Pizarroso*

Main category: cs.CR

TL;DR: 该论文记录了网络安全超智能的演进：从LLM引导的渗透测试（PentestGPT，2023），到自动化专家级性能的网络安全AI（CAI，2025），再到嵌入博弈论推理的神经符号架构（G-CTR，2026），展示了从AI引导人类到人类引导的博弈论网络安全超智能的清晰发展路径。


<details>
  <summary>Details</summary>
Motivation: 网络安全超智能（在速度和战略推理上都超越人类最佳能力的人工智能）代表了安全领域的下一个前沿。论文旨在记录这种能力通过三个主要贡献的出现，这些贡献开创了AI安全领域。

Method: 1. PentestGPT（2023）：建立LLM引导的渗透测试架构，将安全专业知识外部化为自然语言指导。
2. Cybersecurity AI（CAI，2025）：展示自动化专家级性能，通过在国际竞赛中获得第一名验证。
3. Generative Cut-the-Rope（G-CTR，2026）：引入神经符号架构，将博弈论推理嵌入LLM代理中，符号均衡计算增强神经推理。

Result: 1. PentestGPT：相比基线模型提升228.6%。
2. CAI：比人类快3600倍，成本降低156倍，在包括5万美元Neurogrid CTF奖在内的国际竞赛中获得第一名。
3. G-CTR：成功率翻倍，行为方差减少5.2倍，在攻防场景中对非战略性AI取得2:1优势。

Conclusion: 这些进展共同确立了从AI引导人类到人类引导的博弈论网络安全超智能的清晰发展进程，标志着网络安全领域超智能能力的出现。

Abstract: Cybersecurity superintelligence -- artificial intelligence exceeding the best human capability in both speed and strategic reasoning -- represents the next frontier in security. This paper documents the emergence of such capability through three major contributions that have pioneered the field of AI Security. First, PentestGPT (2023) established LLM-guided penetration testing, achieving 228.6% improvement over baseline models through an architecture that externalizes security expertise into natural language guidance. Second, Cybersecurity AI (CAI, 2025) demonstrated automated expert-level performance, operating 3,600x faster than humans while reducing costs 156-fold, validated through #1 rankings at international competitions including the $50,000 Neurogrid CTF prize. Third, Generative Cut-the-Rope (G-CTR, 2026) introduces a neurosymbolic architecture embedding game-theoretic reasoning into LLM-based agents: symbolic equilibrium computation augments neural inference, doubling success rates while reducing behavioral variance 5.2x and achieving 2:1 advantage over non-strategic AI in Attack & Defense scenarios.
  Together, these advances establish a clear progression from AI-guided humans to human-guided game-theoretic cybersecurity superintelligence.

</details>


### [23] [NeuroFilter: Privacy Guardrails for Conversational LLM Agents](https://arxiv.org/abs/2601.14660)
*Saswat Das,Ferdinando Fioretto*

Main category: cs.CR

TL;DR: NeuroFilter：一种基于线性结构检测隐私违规意图的轻量级护栏框架，通过激活空间方向映射和激活速度概念，大幅降低计算成本并保持零误报


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的隐私保护方法存在延迟高、成本大、易被多轮对话操纵绕过的问题，需要更高效可靠的隐私保护机制

Method: 利用隐私违规意图在模型内部表示中的线性可分离性，将规范违规映射到激活空间的简单方向；引入激活速度概念捕捉长对话中的累积漂移威胁

Result: 在超过15万次交互和7B到70B参数模型的评估中，NeuroFilter能有效检测隐私攻击，在良性提示上保持零误报，计算推理成本比基于LLM的防御降低数个数量级

Conclusion: NeuroFilter为智能LLM提供了一种高效、可靠的隐私保护框架，利用内部表示的线性结构特性，在保持高检测率的同时大幅降低计算开销

Abstract: This work addresses the computational challenge of enforcing privacy for agentic Large Language Models (LLMs), where privacy is governed by the contextual integrity framework. Indeed, existing defenses rely on LLM-mediated checking stages that add substantial latency and cost, and that can be undermined in multi-turn interactions through manipulation or benign-looking conversational scaffolding. Contrasting this background, this paper makes a key observation: internal representations associated with privacy-violating intent can be separated from benign requests using linear structure. Using this insight, the paper proposes NeuroFilter, a guardrail framework that operationalizes contextual integrity by mapping norm violations to simple directions in the model's activation space, enabling detection even when semantic filters are bypassed. The proposed filter is also extended to capture threats arising during long conversations using the concept of activation velocity, which measures cumulative drift in internal representations across turns. A comprehensive evaluation across over 150,000 interactions and covering models from 7B to 70B parameters, illustrates the strong performance of NeuroFilter in detecting privacy attacks while maintaining zero false positives on benign prompts, all while reducing the computational inference cost by several orders of magnitude when compared to LLM-based agentic privacy defenses.

</details>


### [24] [STEAD: Robust Provably Secure Linguistic Steganography with Diffusion Language Model](https://arxiv.org/abs/2601.14778)
*Yuang Qi,Na Zhao,Qiyi Yao,Benlong Wu,Weiming Zhang,Nenghai Yu,Kejiang Chen*

Main category: cs.CR

TL;DR: 提出一种基于扩散语言模型的鲁棒可证明安全语言隐写方法，能够抵抗主动篡改攻击，相比传统自回归模型方法具有更好的容错能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于自回归语言模型的可证明安全语言隐写方法在文本被篡改时会产生严重的错误传播，无法抵抗主动篡改攻击，需要一种更鲁棒的解决方案。

Method: 采用扩散语言模型进行部分并行文本生成，找到鲁棒的隐写嵌入位置并结合纠错码；在隐写提取阶段引入伪随机纠错和邻域搜索纠错策略。

Result: 理论证明和实验结果表明该方法安全且鲁棒，能够抵抗隐写文本分割中的标记歧义，并在一定程度上抵抗插入、删除和替换等标记级攻击。

Conclusion: 基于扩散语言模型的鲁棒可证明安全语言隐写方法有效解决了传统自回归模型方法的错误传播问题，为抵抗主动篡改攻击提供了可行方案。

Abstract: Recent provably secure linguistic steganography (PSLS) methods rely on mainstream autoregressive language models (ARMs) to address historically challenging tasks, that is, to disguise covert communication as ``innocuous'' natural language communication. However, due to the characteristic of sequential generation of ARMs, the stegotext generated by ARM-based PSLS methods will produce serious error propagation once it changes, making existing methods unavailable under an active tampering attack. To address this, we propose a robust, provably secure linguistic steganography with diffusion language models (DLMs). Unlike ARMs, DLMs can generate text in a partially parallel manner, allowing us to find robust positions for steganographic embedding that can be combined with error-correcting codes. Furthermore, we introduce error correction strategies, including pseudo-random error correction and neighborhood search correction, during steganographic extraction. Theoretical proof and experimental results demonstrate that our method is secure and robust. It can resist token ambiguity in stegotext segmentation and, to some extent, withstand token-level attacks of insertion, deletion, and substitution.

</details>


### [25] [On Implementing Hybrid Post-Quantum End-to-End Encryption](https://arxiv.org/abs/2601.14926)
*Aditi Gandhi,Aakankshya Das,Aswani Kumar Cherukuri*

Main category: cs.CR

TL;DR: 实现了一个结合经典与后量子密码学的混合端到端加密系统，采用NIST标准化的CRYSTALS-Kyber进行量子安全密钥交换，AES-256-GCM进行对称加密，SHA-256进行密钥派生，遵循零信任模型。


<details>
  <summary>Details</summary>
Motivation: 量子计算的出现对当前公钥密码系统构成根本性威胁，需要在所有应用中过渡到量子抗性密码学替代方案。

Method: 采用混合加密架构：CRYSTALS-Kyber（NIST标准化的基于格的密钥封装机制）用于量子安全密钥交换，AES-256-GCM用于高效认证对称加密，SHA-256用于确定性密钥派生。系统遵循零信任模型，中继服务器仅促进通信而不访问明文或密钥，所有加解密操作仅在客户端端点进行。

Result: 系统证明NIST标准化的后量子密码学可以有效集成到实际消息系统中，具有可接受的性能特征，提供对经典和量子对手的保护。

Conclusion: NIST标准化的后量子密码学可以有效地集成到实际消息系统中，提供对经典和量子对手的保护。作者提供了开源实现以促进可重复性和进一步研究。

Abstract: The emergence of quantum computing poses a fundamental threat to current public key cryptographic systems. This threat is necessitating a transition to quantum resistant cryptographic alternatives in all the applications. In this work, we present the implementation of a practical hybrid end-to-end encryption system that combines classical and post-quantum cryptographic primitives to achieve both security and efficiency. Our system employs CRYSTALS-Kyber, a NIST-standardized lattice-based key encapsulation mechanism, for quantum-safe key exchange, coupled with AES-256-GCM for efficient authenticated symmetric encryption and SHA-256 for deterministic key derivation. The architecture follows a zero-trust model where a relay server facilitates communication without accessing plaintext messages or cryptographic keys. All encryption and decryption operations occur exclusively at client endpoints. The system demonstrates that NIST standardized post-quantum cryptography can be effectively integrated into practical messaging systems with acceptable performance characteristics, offering protection against both classical and quantum adversaries. As our focus is on implementation rather than on novelty, we also provide an open-source implementation to facilitate reproducibility and further research in post quantum secure communication systems.

</details>


### [26] [Interoperable Architecture for Digital Identity Delegation for AI Agents with Blockchain Integration](https://arxiv.org/abs/2601.14982)
*David Ricardo Saavedra*

Main category: cs.CR

TL;DR: 提出一个统一框架，在数字身份系统中实现可验证的委托授权，支持人类用户和AI代理在中心化、联邦和自主身份环境中进行有限、可审计、最小权限的授权转移。


<details>
  <summary>Details</summary>
Motivation: 当前数字身份系统中可验证的委托问题尚未解决，特别是在人类用户和自主AI代理都需要在不暴露主要凭证或私钥的情况下行使和转移权限时。这在不同类型的身份生态系统（中心化、联邦、自主身份）中都存在挑战。

Method: 提出包含四个关键元素的统一框架：1) 委托授权书作为一等授权构件；2) 规范化验证上下文统一验证请求表示；3) 分层参考架构分离信任锚定、凭证验证、策略评估和协议中介；4) 将区块链锚定作为可选完整性层而非结构依赖。

Result: 该框架实现了跨异构身份生态系统的有限、可审计、最小权限委托，提高了互操作性和可审计性，为未来标准化、实施和将自主代理集成到可信数字身份基础设施奠定了基础。

Conclusion: 该框架解决了数字身份系统中的可验证委托问题，为跨不同身份环境的授权转移提供了统一解决方案，特别支持了自主AI代理的集成需求，具有标准化和实际应用价值。

Abstract: Verifiable delegation in digital identity systems remains unresolved across centralized, federated, and self-sovereign identity (SSI) environments, particularly where both human users and autonomous AI agents must exercise and transfer authority without exposing primary credentials or private keys. We introduce a unified framework that enables bounded, auditable, and least-privilege delegation across heterogeneous identity ecosystems. The framework includes four key elements: Delegation Grants (DGs), first-class authorization artefacts that encode revocable transfers of authority with enforced scope reduction; a Canonical Verification Context (CVC) that normalizes verification requests into a single structured representation independent of protocols or credential formats; a layered reference architecture that separates trust anchoring, credential and proof validation, policy evaluation, and protocol mediation via a Trust Gateway; and an explicit treatment of blockchain anchoring as an optional integrity layer rather than a structural dependency. Together, these elements advance interoperable delegation and auditability and provide a foundation for future standardization, implementation, and integration of autonomous agents into trusted digital identity infrastructures.

</details>


### [27] [On the Effectiveness of Mempool-based Transaction Auditing](https://arxiv.org/abs/2601.14996)
*Jannik Albrecht,Ghassan Karame*

Main category: cs.CR

TL;DR: 论文分析显示，现有内存池审计方案在检测比特币和以太坊中的交易操纵攻击时存在误指控风险，但在特定条件下（交易间隔30秒以上）能有效审计交易执行。


<details>
  <summary>Details</summary>
Motivation: 现有区块链防御交易操纵攻击的方案未被主流区块链采用，用户社区依赖内存池审计等实用但临时的解决方案。需要系统分析这些审计方案在实际检测恶意矿工攻击时的有效性。

Method: 首次对内存池审计与检测比特币和以太坊中恶意矿工发起的审查和交易置换攻击能力之间的相互作用进行精确分析。研究在不同设置下审计方案的性能表现。

Result: 内存池审计在某些情况下可能导致对矿工的误指控概率超过25%。但若交易被所有观察者一致接收且发送时间间隔至少30秒，审计方案能以99.9%的高概率成功审计任意两笔交易的执行。

Conclusion: 研究首次表明，批量顺序公平排序方案在现实部署中只能为有限的交易子集提供强公平性保证。内存池审计虽然实用但存在局限性，需要更可靠的解决方案。

Abstract: While the literature features a number of proposals to defend against transaction manipulation attacks, existing proposals are still not integrated within large blockchains, such as Bitcoin, Ethereum, and Cardano. Instead, the user community opted to rely on more practical but ad-hoc solutions (such as Mempool.space) that aim at detecting censorship and transaction displacement attacks by auditing discrepancies in the mempools of so-called observers.
  In this paper, we precisely analyze, for the first time, the interplay between mempool auditing and the ability to detect censorship and transaction displacement attacks by malicious miners in Bitcoin and Ethereum. Our analysis shows that mempool auditing can result in mis-accusations against miners with a probability larger than 25% in some settings. On a positive note, however, we show that mempool auditing schemes can successfully audit the execution of any two transactions (with an overwhelming probability of 99.9%) if they are consistently received by all observers and sent at least 30 seconds apart from each other. As a direct consequence, our findings show, for the first time, that batch-order fair-ordering schemes can offer only strong fairness guarantees for a limited subset of transactions in real-world deployments.

</details>


### [28] [SpooFL: Spoofing Federated Learning](https://arxiv.org/abs/2601.15055)
*Isaac Baglin,Xiatian Zhu,Simon Hadfield*

Main category: cs.CR

TL;DR: 提出SpooFL：一种基于欺骗的联邦学习防御方法，通过生成与原始数据无关的合成样本来误导攻击者，而不是传统的混淆或加密方法。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习防御方法（如添加噪声、变换或加密）虽然有效，但仍会泄露类别分布或特征表示等高级信息，且容易被强大的去噪攻击破解。

Method: 将联邦学习防御框架化为欺骗问题，使用在外部数据集上训练的最先进生成模型（与私有数据无类别重叠），生成看似合理但完全无关的合成样本误导攻击者。

Result: 成功误导攻击者恢复看似合理但完全无关的样本，防止有意义的数据泄露，同时保持联邦学习训练完整性，模型性能影响较小。

Conclusion: SpooFL提供了一种根本不同的联邦学习防御视角，通过欺骗而非混淆来保护隐私，能有效对抗深度泄露攻击而不显著影响模型性能。

Abstract: Traditional defenses against Deep Leakage (DL) attacks in Federated Learning (FL) primarily focus on obfuscation, introducing noise, transformations or encryption to degrade an attacker's ability to reconstruct private data. While effective to some extent, these methods often still leak high-level information such as class distributions or feature representations, and are frequently broken by increasingly powerful denoising attacks. We propose a fundamentally different perspective on FL defense: framing it as a spoofing problem.We introduce SpooFL (Figure 1), a spoofing-based defense that deceives attackers into believing they have recovered the true training data, while actually providing convincing but entirely synthetic samples from an unrelated task. Unlike prior synthetic-data defenses that share classes or distributions with the private data and thus still leak semantic information, SpooFL uses a state-of-the-art generative model trained on an external dataset with no class overlap. As a result, attackers are misled into recovering plausible yet completely irrelevant samples, preventing meaningful data leakage while preserving FL training integrity. We implement the first example of such a spoofing defense, and evaluate our method against state-of-the-art DL defenses and demonstrate that it successfully misdirects attackers without compromising model performance significantly.

</details>


### [29] [Dynamic Management of a Deep Learning-Based Anomaly Detection System for 5G Networks](https://arxiv.org/abs/2601.15177)
*Lorenzo Fernández Maimó,Alberto Huertas Celdrán,Manuel Gil Pérez,Félix J. García Clemente,Gregorio Martínez Pérez*

Main category: cs.CR

TL;DR: 本文提出了一种基于移动边缘计算（MEC）的5G网络实时自主异常检测方案，利用深度学习分析网络流量，并通过策略管理计算资源。


<details>
  <summary>Details</summary>
Motivation: 5G网络中雾计算和移动边缘计算将支持去中心化应用，但需要处理大量数据流量和网络连接，因此需要用户中心化的网络安全解决方案来实时检测网络异常。

Method: 提出MEC导向的解决方案，使用深度学习技术分析网络流量并检测异常，同时采用策略机制来高效动态管理异常检测过程中使用的计算资源。

Result: 论文展示了该方案的部署相关方面和实验结果，证明了其性能表现。

Conclusion: 该MEC导向的解决方案能够在5G移动网络中实现实时自主的网络异常检测，结合深度学习分析和动态资源管理策略，为5G网络安全提供了有效方案。

Abstract: Fog and mobile edge computing (MEC) will play a key role in the upcoming fifth generation (5G) mobile networks to support decentralized applications, data analytics and management into the network itself by using a highly distributed compute model. Furthermore, increasing attention is paid to providing user-centric cybersecurity solutions, which particularly require collecting, processing and analyzing significantly large amount of data traffic and huge number of network connections in 5G networks. In this regard, this paper proposes a MEC-oriented solution in 5G mobile networks to detect network anomalies in real-time and in autonomic way. Our proposal uses deep learning techniques to analyze network flows and to detect network anomalies. Moreover, it uses policies in order to provide an efficient and dynamic management system of the computing resources used in the anomaly detection process. The paper presents relevant aspects of the deployment of the proposal and experimental results to show its performance.

</details>


### [30] [Lightweight LLMs for Network Attack Detection in IoT Networks](https://arxiv.org/abs/2601.15269)
*Piyumi Bhagya Sudasinghe,Kushan Sudheera Kalupahana Liyanage,Harsha S. Gardiyawasam Pussewalage*

Main category: cs.CR

TL;DR: 该研究探索了轻量级解码器LLM在物联网攻击检测中的应用，通过结构化文本转换、QLoRA微调和RAG技术，在资源受限环境下实现高效检测，对未知攻击类型展示零样本能力。


<details>
  <summary>Details</summary>
Motivation: 物联网设备快速增长导致网络攻击规模和多样性增加，传统入侵检测系统存在局限性。经典机器学习模型对已知攻击表现良好，但需要重新训练才能检测未知或零日威胁。

Method: 采用轻量级解码器LLM，结合结构化文本转换（将网络流量特征转换为自然语言提示）、QLoRA微调（量化低秩适应）和RAG（检索增强生成）技术，在硬件受限环境下实现高效适应。

Result: 在CICIoT2023数据集上，QLoRA微调的LLaMA-1B模型对已知攻击的F1分数达到0.7124，与随机森林基线（0.7159）相当。使用RAG后，系统对未知攻击类型达到42.63%的准确率，无需额外训练。

Conclusion: 检索增强的轻量级LLM具有作为下一代物联网入侵检测的适应性强、资源高效解决方案的潜力，展示了在零样本检测方面的实用能力。

Abstract: The rapid growth of Internet of Things (IoT) devices has increased the scale and diversity of cyberattacks, exposing limitations in traditional intrusion detection systems. Classical machine learning (ML) models such as Random Forest and Support Vector Machine perform well on known attacks but require retraining to detect unseen or zero-day threats. This study investigates lightweight decoder-only Large Language Models (LLMs) for IoT attack detection by integrating structured-to-text conversion, Quantized Low-Rank Adaptation (QLoRA) fine-tuning, and Retrieval-Augmented Generation (RAG). Network traffic features are transformed into compact natural-language prompts, enabling efficient adaptation under constrained hardware. Experiments on the CICIoT2023 dataset show that a QLoRA-tuned LLaMA-1B model achieves an F1-score of 0.7124, comparable to the Random Forest (RF) baseline (0.7159) for known attacks. With RAG, the system attains 42.63% accuracy on unseen attack types without additional training, demonstrating practical zero-shot capability. These results highlight the potential of retrieval-enhanced lightweight LLMs as adaptable and resource-efficient solutions for next-generation IoT intrusion detection.

</details>
