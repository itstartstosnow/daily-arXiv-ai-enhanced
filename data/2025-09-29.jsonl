{"id": "2509.21367", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21367", "abs": "https://arxiv.org/abs/2509.21367", "authors": ["Yu-Kai Shih", "You-Kai Kang"], "title": "Design and Implementation of a Secure RAG-Enhanced AI Chatbot for Smart Tourism Customer Service: Defending Against Prompt Injection Attacks -- A Case Study of Hsinchu, Taiwan", "comment": "12 pages, 7 figures, 5 tables", "summary": "As smart tourism evolves, AI-powered chatbots have become indispensable for\ndelivering personalized, real-time assistance to travelers while promoting\nsustainability and efficiency. However, these systems are increasingly\nvulnerable to prompt injection attacks, where adversaries manipulate inputs to\nelicit unintended behaviors such as leaking sensitive information or generating\nharmful content. This paper presents a case study on the design and\nimplementation of a secure retrieval-augmented generation (RAG) chatbot for\nHsinchu smart tourism services. The system integrates RAG with API function\ncalls, multi-layered linguistic analysis, and guardrails against injections,\nachieving high contextual awareness and security. Key features include a tiered\nresponse strategy, RAG-driven knowledge grounding, and intent decomposition\nacross lexical, semantic, and pragmatic levels. Defense mechanisms include\nsystem norms, gatekeepers for intent judgment, and reverse RAG text to\nprioritize verified data. We also benchmark a GPT-5 variant (released\n2025-08-07) to assess inherent robustness. Evaluations with 674 adversarial\nprompts and 223 benign queries show over 95% accuracy on benign tasks and\nsubstantial detection of injection attacks. GPT-5 blocked about 85% of attacks,\nshowing progress yet highlighting the need for layered defenses. Findings\nemphasize contributions to sustainable tourism, multilingual accessibility, and\nethical AI deployment. This work offers a practical framework for deploying\nsecure chatbots in smart tourism and contributes to resilient, trustworthy AI\napplications."}
{"id": "2509.21389", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21389", "abs": "https://arxiv.org/abs/2509.21389", "authors": ["Devashish Chaudhary", "Sutharshan Rajasegarar", "Shiva Raj Pokhrel"], "title": "Towards Adapting Federated & Quantum Machine Learning for Network Intrusion Detection: A Survey", "comment": "34 pages, 16 figures, IEEE Communication Surveys and Tutorials", "summary": "This survey explores the integration of Federated Learning (FL) with Network\nIntrusion Detection Systems (NIDS), with particular emphasis on deep learning\nand quantum machine learning approaches. FL enables collaborative model\ntraining across distributed devices while preserving data privacy-a critical\nrequirement in network security contexts where sensitive traffic data cannot be\ncentralized. Our comprehensive analysis systematically examines the full\nspectrum of FL architectures, deployment strategies, communication protocols,\nand aggregation methods specifically tailored for intrusion detection. We\nprovide an in-depth investigation of privacy-preserving techniques, model\ncompression approaches, and attack-specific federated solutions for threats\nincluding DDoS, MITM, and botnet attacks. The survey further delivers a\npioneering exploration of Quantum FL (QFL), discussing quantum feature\nencoding, quantum machine learning algorithms, and quantum-specific aggregation\nmethods that promise exponential speedups for complex pattern recognition in\nnetwork traffic. Through rigorous comparative analysis of classical and quantum\napproaches, identification of research gaps, and evaluation of real-world\ndeployments, we outline a concrete roadmap for industrial adoption and future\nresearch directions. This work serves as an authoritative reference for\nresearchers and practitioners seeking to enhance privacy, efficiency, and\nrobustness of federated intrusion detection systems in increasingly complex\nnetwork environments, while preparing for the quantum-enhanced cybersecurity\nlandscape of tomorrow."}
{"id": "2509.21392", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.21392", "abs": "https://arxiv.org/abs/2509.21392", "authors": ["Wenxuan Wang", "Chenglei Wang", "Xuelin Qian"], "title": "Dynamic Dual-level Defense Routing for Continual Adversarial Training", "comment": null, "summary": "As adversarial attacks continue to evolve, defense models face the risk of\nrecurrent vulnerabilities, underscoring the importance of continuous\nadversarial training (CAT). Existing CAT approaches typically balance decision\nboundaries by either data replay or optimization strategy to constrain shared\nmodel parameters. However, due to the diverse and aggressive nature of\nadversarial examples, these methods suffer from catastrophic forgetting of\nprevious defense knowledge after continual learning. In this paper, we propose\na novel framework, called Dual-level Defense Routing or DDeR, that can\nautonomously select appropriate routers to integrate specific defense experts,\nthereby adapting to evolving adversarial attacks. Concretely, the first-level\ndefense routing comprises multiple defense experts and routers, with each\nrouter dynamically selecting and combining suitable experts to process attacked\nfeatures. Routers are independently incremented as continuous adversarial\ntraining progresses, and their selections are guided by an Adversarial Sentinel\nNetwork (ASN) in the second-level defense routing. To compensate for the\ninability to test due to the independence of routers, we further present a\nPseudo-task Substitution Training (PST) strategy, which leverages\ndistributional discrepancy in data to facilitate inter-router communication\nwithout storing historical data. Extensive experiments demonstrate that DDeR\nachieves superior continuous defense performance and classification accuracy\ncompared to existing methods."}
{"id": "2509.21400", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.21400", "abs": "https://arxiv.org/abs/2509.21400", "authors": ["Xiyu Zeng", "Siyuan Liang", "Liming Lu", "Haotian Zhu", "Enguang Liu", "Jisheng Dang", "Yongbin Zhou", "Shuchao Pang"], "title": "SafeSteer: Adaptive Subspace Steering for Efficient Jailbreak Defense in Vision-Language Models", "comment": null, "summary": "As the capabilities of Vision Language Models (VLMs) continue to improve,\nthey are increasingly targeted by jailbreak attacks. Existing defense methods\nface two major limitations: (1) they struggle to ensure safety without\ncompromising the model's utility; and (2) many defense mechanisms significantly\nreduce the model's inference efficiency. To address these challenges, we\npropose SafeSteer, a lightweight, inference-time steering framework that\neffectively defends against diverse jailbreak attacks without modifying model\nweights. At the core of SafeSteer is the innovative use of Singular Value\nDecomposition to construct a low-dimensional \"safety subspace.\" By projecting\nand reconstructing the raw steering vector into this subspace during inference,\nSafeSteer adaptively removes harmful generation signals while preserving the\nmodel's ability to handle benign inputs. The entire process is executed in a\nsingle inference pass, introducing negligible overhead. Extensive experiments\nshow that SafeSteer reduces the attack success rate by over 60% and improves\naccuracy on normal tasks by 1-2%, without introducing significant inference\nlatency. These results demonstrate that robust and practical jailbreak defense\ncan be achieved through simple, efficient inference-time control."}
{"id": "2509.21475", "categories": ["cs.CR", "cs.CE", "cs.GT"], "pdf": "https://arxiv.org/pdf/2509.21475", "abs": "https://arxiv.org/abs/2509.21475", "authors": ["Sen Yang", "Burak Öz", "Fei Wu", "Fan Zhang"], "title": "Designing Ethereum's Geographical (De)Centralization Beyond the Atlantic", "comment": null, "summary": "Decentralization has a geographic dimension that conventional metrics such as\nstake distribution overlook. Where validators run affects resilience to\nregional shocks (outages, disasters, government intervention) and fairness in\nreward access. Yet in permissionless systems, locations cannot be mandated, but\nthey emerge from incentives. Today, Ethereum's validators cluster along the\nAtlantic (EU and U.S. East Coast), where latency is structurally favorable.\nThis raises a key question: when some regions already enjoy latency advantages,\nhow does protocol design shape validator incentives and the geography of\n(de)centralization? We develop a latency-calibrated agent-based model and\ncompare two Ethereum block-building paradigms: a Single-Source Paradigm (SSP),\nakin to MEV-Boost, where proposers fetch full blocks from a relay that also\npropagates them; and a Multi-Source Paradigm (MSP), where proposers aggregate\nvalue from multiple sources and broadcast the block themselves. Simulations\nshow that SSP concentrates around relay placement but more slowly, since\nproximity mainly affects propagation, and the marginal value of time is\nrelatively uniform across regions. MSP centralizes faster: aggregating across\nsources makes marginal value location-dependent, amplifying payoff dispersion\nand migration toward latency minima. Source placement and consensus settings\ncan dampen or intensify these effects, though once validators are already\nclustered, the impact of source placement on decentralization is marginal. In\nmost cases, North America consistently emerges as the focal hub. These findings\nshow that protocol design materially shapes validator geography and offer\nlevers for promoting geographical decentralization."}
{"id": "2509.21497", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21497", "abs": "https://arxiv.org/abs/2509.21497", "authors": ["Alexandru Ioniţă", "Andreea Ioniţă"], "title": "Functional Encryption in Secure Neural Network Training: Data Leakage and Practical Mitigations", "comment": "Accepted at RAID 2025. (c) IEEE", "summary": "With the increased interest in artificial intelligence, Machine Learning as a\nService provides the infrastructure in the Cloud for easy training, testing,\nand deploying models. However, these systems have a major privacy issue:\nuploading sensitive data to the Cloud, especially during training. Therefore,\nachieving secure Neural Network training has been on many researchers' minds\nlately. More and more solutions for this problem are built around a main\npillar: Functional Encryption (FE). Although these approaches are very\ninteresting and offer a new perspective on ML training over encrypted data,\nsome vulnerabilities do not seem to be taken into consideration. In our paper,\nwe present an attack on neural networks that uses FE for secure training over\nencrypted data. Our approach uses linear programming to reconstruct the\noriginal input, unveiling the previous security promises. To address the\nattack, we propose two solutions for secure training and inference that involve\nthe client during the computation phase. One approach ensures security without\nrelying on encryption, while the other uses function-hiding inner-product\ntechniques."}
{"id": "2509.21586", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.21586", "abs": "https://arxiv.org/abs/2509.21586", "authors": ["Moritz Grundei", "Aayush Rajasekaran", "Kishori Konwar", "Muriel Medard"], "title": "From Indexing to Coding: A New Paradigm for Data Availability Sampling", "comment": null, "summary": "The data availability problem is a central challenge in blockchain systems\nand lies at the core of the accessibility and scalability issues faced by\nplatforms such as Ethereum. Modern solutions employ several approaches, with\ndata availability sampling (DAS) being the most self-sufficient and\nminimalistic in its security assumptions. Existing DAS methods typically form\ncryptographic commitments on codewords of fixed-rate erasure codes, which\nrestrict light nodes to sampling from a predetermined set of coded symbols.\n  In this paper, we introduce a new approach to DAS that modularizes the coding\nand commitment process by committing to the uncoded data while performing\nsampling through on-the-fly coding. The resulting samples are significantly\nmore expressive, enabling light nodes to obtain, in concrete implementations,\nup to multiple orders of magnitude stronger assurances of data availability\nthan from sampling pre-committed symbols from a fixed-rate redundancy code as\ndone in established DAS schemes using Reed Solomon or low density parity check\ncodes. We present a concrete protocol that realizes this paradigm using random\nlinear network coding (RLNC)."}
{"id": "2509.21590", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.21590", "abs": "https://arxiv.org/abs/2509.21590", "authors": ["Ben Rosenzweig", "Valentino Dalla Valle", "Giovanni Apruzzese", "Aurore Fass"], "title": "It's not Easy: Applying Supervised Machine Learning to Detect Malicious Extensions in the Chrome Web Store", "comment": "Accepted to ACM Transactions on the Web", "summary": "Google Chrome is the most popular Web browser. Users can customize it with\nextensions that enhance their browsing experience. The most well-known\nmarketplace of such extensions is the Chrome Web Store (CWS). Developers can\nupload their extensions on the CWS, but such extensions are made available to\nusers only after a vetting process carried out by Google itself. Unfortunately,\nsome malicious extensions bypass such checks, putting the security and privacy\nof downstream browser extension users at risk.\n  Here, we scrutinize the extent to which automated mechanisms reliant on\nsupervised machine learning (ML) can be used to detect malicious extensions on\nthe CWS. To this end, we first collect 7,140 malicious extensions published in\n2017--2023. We combine this dataset with 63,598 benign extensions published or\nupdated on the CWS before 2023, and we develop three supervised-ML-based\nclassifiers. We show that, in a \"lab setting\", our classifiers work well (e.g.,\n98% accuracy). Then, we collect a more recent set of 35,462 extensions from the\nCWS, published or last updated in 2023, with unknown ground truth. We were\neventually able to identify 68 malicious extensions that bypassed the vetting\nprocess of the CWS. However, our classifiers also reported >1k likely malicious\nextensions. Based on this finding (further supported with empirical evidence),\nwe elucidate, for the first time, a strong concept drift effect on browser\nextensions. We also show that commercial detectors (e.g., VirusTotal) work\npoorly to detect known malicious extensions. Altogether, our results highlight\nthat detecting malicious browser extensions is a fundamentally hard problem.\nThis requires additional work both by the research community and by Google\nitself -- potentially by revising their approaches. In the meantime, we\ninformed Google of our discoveries, and we release our artifacts."}
{"id": "2509.21601", "categories": ["cs.CR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.21601", "abs": "https://arxiv.org/abs/2509.21601", "authors": ["Jason Anderson"], "title": "World's First Authenticated Satellite Pseudorange from Orbit", "comment": "Pending publication:\n  https://www.ion.org/gnss/abstracts.cfm?paperID=16052", "summary": "Cryptographic Ranging Authentication is here! We present initial results on\nthe Pulsar authenticated ranging service broadcast from space with Pulsar-0\nutilizing a recording taken at Xona headquarters in Burlingame, CA. No\nassumptions pertaining to the ownership or leakage of encryption keys are\nrequired. This work discusses the Pulsar watermark design and security\nanalysis. We derive the Pulsar watermark's probabilities of missed detection\nand false alarm, and we discuss the required receiver processing needed to\nutilize the Pulsar watermark. We present validation results of the Pulsar\nwatermark utilizing the transmissions from orbit. Lastly, we provide results\nthat demonstrate the spoofing detection efficacy with a spoofing scenario that\nincorporates the authentic transmissions from orbit. Because we make no\nassumption about the leakage of symmetric encryption keys, this work provides\nmathematical justification of the watermark's security, and our July 2025\ntransmissions from orbit, we claim the world's first authenticated satellite\npseudorange from orbit."}
{"id": "2509.21634", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.21634", "abs": "https://arxiv.org/abs/2509.21634", "authors": ["Prakhar Sharma", "Haohuang Wen", "Vinod Yegneswaran", "Ashish Gehani", "Phillip Porras", "Zhiqiang Lin"], "title": "MobiLLM: An Agentic AI Framework for Closed-Loop Threat Mitigation in 6G Open RANs", "comment": null, "summary": "The evolution toward 6G networks is being accelerated by the Open Radio\nAccess Network (O-RAN) paradigm -- an open, interoperable architecture that\nenables intelligent, modular applications across public telecom and private\nenterprise domains. While this openness creates unprecedented opportunities for\ninnovation, it also expands the attack surface, demanding resilient, low-cost,\nand autonomous security solutions. Legacy defenses remain largely reactive,\nlabor-intensive, and inadequate for the scale and complexity of next-generation\nsystems. Current O-RAN applications focus mainly on network optimization or\npassive threat detection, with limited capability for closed-loop, automated\nresponse.\n  To address this critical gap, we present an agentic AI framework for fully\nautomated, end-to-end threat mitigation in 6G O-RAN environments. MobiLLM\norchestrates security workflows through a modular multi-agent system powered by\nLarge Language Models (LLMs). The framework features a Threat Analysis Agent\nfor real-time data triage, a Threat Classification Agent that uses\nRetrieval-Augmented Generation (RAG) to map anomalies to specific\ncountermeasures, and a Threat Response Agent that safely operationalizes\nmitigation actions via O-RAN control interfaces. Grounded in trusted knowledge\nbases such as the MITRE FiGHT framework and 3GPP specifications, and equipped\nwith robust safety guardrails, MobiLLM provides a blueprint for trustworthy\nAI-driven network security. Initial evaluations demonstrate that MobiLLM can\neffectively identify and orchestrate complex mitigation strategies,\nsignificantly reducing response latency and showcasing the feasibility of\nautonomous security operations in 6G."}
{"id": "2509.21712", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21712", "abs": "https://arxiv.org/abs/2509.21712", "authors": ["Bingcan Guo", "Eryue Xu", "Zhiping Zhang", "Tianshi Li"], "title": "Not My Agent, Not My Boundary? Elicitation of Personal Privacy Boundaries in AI-Delegated Information Sharing", "comment": null, "summary": "Aligning AI systems with human privacy preferences requires understanding\nindividuals' nuanced disclosure behaviors beyond general norms. Yet eliciting\nsuch boundaries remains challenging due to the context-dependent nature of\nprivacy decisions and the complex trade-offs involved. We present an AI-powered\nelicitation approach that probes individuals' privacy boundaries through a\ndiscriminative task. We conducted a between-subjects study that systematically\nvaried communication roles and delegation conditions, resulting in 1,681\nboundary specifications from 169 participants for 61 scenarios. We examined how\nthese contextual factors and individual differences influence the boundary\nspecification. Quantitative results show that communication roles influence\nindividuals' acceptance of detailed and identifiable disclosure, AI delegation\nand individuals' need for privacy heighten sensitivity to disclosed\nidentifiers, and AI delegation results in less consensus across individuals.\nOur findings highlight the importance of situating privacy preference\nelicitation within real-world data flows. We advocate using nuanced privacy\nboundaries as an alignment goal for future AI systems."}
{"id": "2509.21761", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21761", "abs": "https://arxiv.org/abs/2509.21761", "authors": ["Miao Yu", "Zhenhong Zhou", "Moayad Aloqaily", "Kun Wang", "Biwei Huang", "Stephen Wang", "Yueming Jin", "Qingsong Wen"], "title": "Backdoor Attribution: Elucidating and Controlling Backdoor in Language Models", "comment": null, "summary": "Fine-tuned Large Language Models (LLMs) are vulnerable to backdoor attacks\nthrough data poisoning, yet the internal mechanisms governing these attacks\nremain a black box. Previous research on interpretability for LLM safety tends\nto focus on alignment, jailbreak, and hallucination, but overlooks backdoor\nmechanisms, making it difficult to understand and fully eliminate the backdoor\nthreat. In this paper, aiming to bridge this gap, we explore the interpretable\nmechanisms of LLM backdoors through Backdoor Attribution (BkdAttr), a\ntripartite causal analysis framework. We first introduce the Backdoor Probe\nthat proves the existence of learnable backdoor features encoded within the\nrepresentations. Building on this insight, we further develop Backdoor\nAttention Head Attribution (BAHA), efficiently pinpointing the specific\nattention heads responsible for processing these features. Our primary\nexperiments reveals these heads are relatively sparse; ablating a minimal\n\\textbf{$\\sim$ 3%} of total heads is sufficient to reduce the Attack Success\nRate (ASR) by \\textbf{over 90%}. More importantly, we further employ these\nfindings to construct the Backdoor Vector derived from these attributed heads\nas a master controller for the backdoor. Through only \\textbf{1-point}\nintervention on \\textbf{single} representation, the vector can either boost ASR\nup to \\textbf{$\\sim$ 100% ($\\uparrow$)} on clean inputs, or completely\nneutralize backdoor, suppressing ASR down to \\textbf{$\\sim$ 0% ($\\downarrow$)}\non triggered inputs. In conclusion, our work pioneers the exploration of\nmechanistic interpretability in LLM backdoors, demonstrating a powerful method\nfor backdoor control and revealing actionable insights for the community."}
{"id": "2509.21768", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.21768", "abs": "https://arxiv.org/abs/2509.21768", "authors": ["Jiawei Zhao", "Yuang Qi", "Weiming Zhang", "Nenghai Yu", "Kejiang Chen"], "title": "PSRT: Accelerating LRM-based Guard Models via Prefilled Safe Reasoning Traces", "comment": null, "summary": "Large Reasoning Models (LRMs) have demonstrated remarkable performance on\ntasks such as mathematics and code generation. Motivated by these strengths,\nrecent work has empirically demonstrated the effectiveness of LRMs as guard\nmodels in improving harmful query detection. However, LRMs typically generate\nlong reasoning traces during inference, causing substantial computational\noverhead. In this paper, we introduce PSRT, a method that replaces the model's\nreasoning process with a Prefilled Safe Reasoning Trace, thereby significantly\nreducing the inference cost of LRMs. Concretely, PSRT prefills \"safe reasoning\nvirtual tokens\" from a constructed dataset and learns over their continuous\nembeddings. With the aid of indicator tokens, PSRT enables harmful-query\ndetection in a single forward pass while preserving the classification\neffectiveness of LRMs. We evaluate PSRT on 7 models, 13 datasets, and 8\njailbreak methods. In terms of efficiency, PSRT completely removes the overhead\nof generating reasoning tokens during inference. In terms of classification\nperformance, PSRT achieves nearly identical accuracy, with only a minor average\nF1 drop of 0.015 across 7 models and 5 datasets."}
{"id": "2509.21772", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.21772", "abs": "https://arxiv.org/abs/2509.21772", "authors": ["Daiki Chiba", "Hiroki Nakano", "Takashi Koide"], "title": "PhishLumos: An Adaptive Multi-Agent System for Proactive Phishing Campaign Mitigation", "comment": null, "summary": "Phishing attacks are a significant societal threat, disproportionately\nharming vulnerable populations and eroding trust in essential digital services.\nCurrent defenses are often reactive, failing against modern evasive tactics\nlike cloaking that conceal malicious content. To address this, we introduce\nPhishLumos, an adaptive multi-agent system that proactively mitigates entire\nattack campaigns. It confronts a core cybersecurity imbalance: attackers can\neasily scale operations, while defense remains an intensive expert task.\nInstead of being blocked by evasion, PhishLumos treats it as a critical signal\nto investigate the underlying infrastructure. Its Large Language Model\n(LLM)-powered agents uncover shared hosting, certificates, and domain\nregistration patterns. On real-world data, our system identified 100% of\ncampaigns in the median case, over a week before their confirmation by\ncybersecurity experts. PhishLumos demonstrates a practical shift from reactive\nURL blocking to proactive campaign mitigation, protecting users before they are\nharmed and making the digital world safer for all."}
{"id": "2509.21786", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.21786", "abs": "https://arxiv.org/abs/2509.21786", "authors": ["Junjie Song", "Jinguang Han", "Man Ho Au", "Rupeng Yang", "Chao Sun"], "title": "Lattice-Based Dynamic $k$-times Anonymous Authentication", "comment": null, "summary": "With the development of Internet, privacy has become a close concern of\nusers. Anonymous authentication plays an important role in privacy-preserving\nsystems. $k$-times anonymous authentication ($k$-TAA) scheme allows members of\na group to be authenticated anonymously by application providers up to $k$\ntimes. Considering quantum computing attacks, lattice-based $k$-TAA was\nintroduced. However, existing schemes do not support dynamically granting and\nrevoking users. In this paper, we construct the first lattice-based dynamic\n$k$-TAA, which offers limited times anonymous authentication, dynamic member\nmanagement, and post-quantum security. We present a concrete construction, and\nreduce its security to standard complexity assumptions. Notably, compared with\nexisting lattice-based $k$-TAA, our scheme is efficient in terms of\ncommunication cost."}
{"id": "2509.21821", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.21821", "abs": "https://arxiv.org/abs/2509.21821", "authors": ["Xinyu Hu", "Zhiwei Fu", "Shaocong Xie", "Steven H. H. Ding", "Philippe Charland"], "title": "SoK: Potentials and Challenges of Large Language Models for Reverse Engineering", "comment": null, "summary": "Reverse Engineering (RE) is central to software security, enabling tasks such\nas vulnerability discovery and malware analysis, but it remains labor-intensive\nand requires substantial expertise. Earlier advances in deep learning start to\nautomate parts of RE, particularly for malware detection and vulnerability\nclassification. More recently, a rapidly growing body of work has applied Large\nLanguage Models (LLMs) to similar purposes. Their role compared to prior\nmachine learning remains unclear, since some efforts simply adapt existing\npipelines with minimal change while others seek to exploit broader reasoning\nand generative abilities. These differences, combined with varied problem\ndefinitions, methods, and evaluation practices, limit comparability,\nreproducibility, and cumulative progress. This paper systematizes the field by\nreviewing 44 research papers, including peer-reviewed publications and\npreprints, and 18 additional open-source projects that apply LLMs in RE. We\npropose a taxonomy that organizes existing work by objective, target, method,\nevaluation strategy, and data scale. Our analysis identifies strengths and\nlimitations, highlights reproducibility and evaluation gaps, and examines\nemerging risks. We conclude with open challenges and future research directions\nthat aim to guide more coherent and security-relevant applications of LLMs in\nRE."}
{"id": "2509.21831", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.21831", "abs": "https://arxiv.org/abs/2509.21831", "authors": ["Hesam Sarkhosh", "Uzma Maroof", "Diogo Barradas"], "title": "The Dark Art of Financial Disguise in Web3: Money Laundering Schemes and Countermeasures", "comment": "Accepted manuscript to APWG eCrime 2025", "summary": "The rise of Web3 and Decentralized Finance (DeFi) has enabled borderless\naccess to financial services empowered by smart contracts and blockchain\ntechnology. However, the ecosystem's trustless, permissionless, and borderless\nnature presents substantial regulatory challenges. The absence of centralized\noversight and the technical complexity create fertile ground for financial\ncrimes. Among these, money laundering is particularly concerning, as in the\nevent of successful scams, code exploits, and market manipulations, it\nfacilitates covert movement of illicit gains. Beyond this, there is a growing\nconcern that cryptocurrencies can be leveraged to launder proceeds from drug\ntrafficking, or to transfer funds linked to terrorism financing.\n  This survey aims to outline a taxonomy of high-level strategies and\nunderlying mechanisms exploited to facilitate money laundering in Web3. We\nexamine how criminals leverage the pseudonymous nature of Web3, alongside weak\nregulatory frameworks, to obscure illicit financial activities. Our study seeks\nto bridge existing knowledge gaps on laundering schemes, identify open\nchallenges in the detection and prevention of such activities, and propose\nfuture research directions to foster a more transparent Web3 financial\necosystem -- offering valuable insights for researchers, policymakers, and\nindustry practitioners."}
{"id": "2509.21843", "categories": ["cs.CR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21843", "abs": "https://arxiv.org/abs/2509.21843", "authors": ["Jingkai Guo", "Chaitali Chakrabarti", "Deliang Fan"], "title": "SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models", "comment": "10 pages, 4 figures, 5 tables, 2 equations. Topics: Bit-flip attacks,\n  adversarial attacks, large language models (LLMs)", "summary": "Model integrity of Large language models (LLMs) has become a pressing\nsecurity concern with their massive online deployment. Prior Bit-Flip Attacks\n(BFAs) -- a class of popular AI weight memory fault-injection techniques -- can\nseverely compromise Deep Neural Networks (DNNs): as few as tens of bit flips\ncan degrade accuracy toward random guessing. Recent studies extend BFAs to LLMs\nand reveal that, despite the intuition of better robustness from modularity and\nredundancy, only a handful of adversarial bit flips can also cause LLMs'\ncatastrophic accuracy degradation. However, existing BFA methods typically\nfocus on either integer or floating-point models separately, limiting attack\nflexibility. Moreover, in floating-point models, random bit flips often cause\nperturbed parameters to extreme values (e.g., flipping in exponent bit), making\nit not stealthy and leading to numerical runtime error (e.g., invalid tensor\nvalues (NaN/Inf)). In this work, for the first time, we propose SBFA (Sneaky\nBit-Flip Attack), which collapses LLM performance with only one single bit flip\nwhile keeping perturbed values within benign layer-wise weight distribution. It\nis achieved through iterative searching and ranking through our defined\nparameter sensitivity metric, ImpactScore, which combines gradient sensitivity\nand perturbation range constrained by the benign layer-wise weight\ndistribution. A novel lightweight SKIP searching algorithm is also proposed to\ngreatly reduce searching complexity, which leads to successful SBFA searching\ntaking only tens of minutes for SOTA LLMs. Across Qwen, LLaMA, and Gemma\nmodels, with only one single bit flip, SBFA successfully degrades accuracy to\nbelow random levels on MMLU and SST-2 in both BF16 and INT8 data formats.\nRemarkably, flipping a single bit out of billions of parameters reveals a\nsevere security concern of SOTA LLM models."}
{"id": "2509.21884", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21884", "abs": "https://arxiv.org/abs/2509.21884", "authors": ["Bochuan Cao", "Changjiang Li", "Yuanpu Cao", "Yameng Ge", "Ting Wang", "Jinghui Chen"], "title": "You Can't Steal Nothing: Mitigating Prompt Leakages in LLMs via System Vectors", "comment": "29 pages, 10 tables, 6figures, accepted by CCS 25", "summary": "Large language models (LLMs) have been widely adopted across various\napplications, leveraging customized system prompts for diverse tasks. Facing\npotential system prompt leakage risks, model developers have implemented\nstrategies to prevent leakage, primarily by disabling LLMs from repeating their\ncontext when encountering known attack patterns. However, it remains vulnerable\nto new and unforeseen prompt-leaking techniques. In this paper, we first\nintroduce a simple yet effective prompt leaking attack to reveal such risks.\nOur attack is capable of extracting system prompts from various LLM-based\napplication, even from SOTA LLM models such as GPT-4o or Claude 3.5 Sonnet. Our\nfindings further inspire us to search for a fundamental solution to the\nproblems by having no system prompt in the context. To this end, we propose\nSysVec, a novel method that encodes system prompts as internal representation\nvectors rather than raw text. By doing so, SysVec minimizes the risk of\nunauthorized disclosure while preserving the LLM's core language capabilities.\nRemarkably, this approach not only enhances security but also improves the\nmodel's general instruction-following abilities. Experimental results\ndemonstrate that SysVec effectively mitigates prompt leakage attacks, preserves\nthe LLM's functional integrity, and helps alleviate the forgetting issue in\nlong-context scenarios."}
{"id": "2509.22022", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22022", "abs": "https://arxiv.org/abs/2509.22022", "authors": ["Marc Damie", "Florian Hahn", "Andreas Peter", "Jan Ramon"], "title": "Eliminating Exponential Key Growth in PRG-Based Distributed Point Functions", "comment": "Accepted in DPM 2025", "summary": "Distributed Point Functions (DPFs) enable sharing secret point functions\nacross multiple parties, supporting privacy-preserving technologies such as\nPrivate Information Retrieval, and anonymous communications. While 2-party\nPRG-based schemes with logarithmic key sizes have been known for a decade,\nextending these solutions to multi-party settings has proven challenging. In\nparticular, PRG-based multi-party DPFs have historically struggled with\npracticality due to key sizes growing exponentially with the number of parties\nand the field size.\n  Our work addresses this efficiency bottleneck by optimizing the PRG-based\nmulti-party DPF scheme of Boyle et al. (EUROCRYPT'15). By leveraging the\nhonest-majority assumption, we eliminate the exponential factor present in this\nscheme. Our construction is the first PRG-based multi-party DPF scheme with\npractical key sizes, and provides key up to 3x smaller than the best known\nmulti-party DPF. This work demonstrates that with careful optimization,\nPRG-based multi-party DPFs can achieve practical performances, and even obtain\ntop performances."}
{"id": "2509.22027", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22027", "abs": "https://arxiv.org/abs/2509.22027", "authors": ["Mingkai Li", "Hang Ye", "Joseph Devietti", "Suman Jana", "Tanvir Ahmed Khan"], "title": "NanoTag: Systems Support for Efficient Byte-Granular Overflow Detection on ARM MTE", "comment": null, "summary": "Memory safety bugs, such as buffer overflows and use-after-frees, are the\nleading causes of software safety issues in production. Software-based\napproaches, e.g., Address Sanitizer (ASAN), can detect such bugs with high\nprecision, but with prohibitively high overhead. ARM's Memory Tagging Extension\n(MTE) offers a promising alternative to detect these bugs in hardware with a\nmuch lower overhead. However, in this paper, we perform a thorough\ninvestigation of Google Pixel 8, the first production implementation of ARM\nMTE, and show that MTE can only achieve coarse precision in bug detection\ncompared with software-based approaches such as ASAN, mainly due to its 16-byte\ntag granularity. To address this issue, we present NanoTag, a system to detect\nmemory safety bugs in unmodified binaries at byte granularity with ARM MTE.\nNanoTag detects intra-granule buffer overflows by setting up a tripwire for tag\ngranules that may require intra-granule overflow detection. The memory access\nto the tripwire causes additional overflow detection in the software while\nusing MTE's hardware to detect bugs for the rest of the accesses. We implement\nNanoTag based on the Scudo Hardened Allocator, the default memory allocator on\nAndroid since Android 11. Our evaluation results across popular benchmarks and\nreal-world case studies show that NanoTag detects nearly as many memory safety\nbugs as ASAN while incurring similar run-time overhead to Scudo Hardened\nAllocator in MTE SYNC mode."}
{"id": "2509.22040", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22040", "abs": "https://arxiv.org/abs/2509.22040", "authors": ["Yue Liu", "Yanjie Zhao", "Yunbo Lyu", "Ting Zhang", "Haoyu Wang", "David Lo"], "title": "\"Your AI, My Shell\": Demystifying Prompt Injection Attacks on Agentic AI Coding Editors", "comment": null, "summary": "Agentic AI coding editors driven by large language models have recently\nbecome more popular due to their ability to improve developer productivity\nduring software development. Modern editors such as Cursor are designed not\njust for code completion, but also with more system privileges for complex\ncoding tasks (e.g., run commands in the terminal, access development\nenvironments, and interact with external systems). While this brings us closer\nto the \"fully automated programming\" dream, it also raises new security\nconcerns. In this study, we present the first empirical analysis of prompt\ninjection attacks targeting these high-privilege agentic AI coding editors. We\nshow how attackers can remotely exploit these systems by poisoning external\ndevelopment resources with malicious instructions, effectively hijacking AI\nagents to run malicious commands, turning \"your AI\" into \"attacker's shell\". To\nperform this analysis, we implement AIShellJack, an automated testing framework\nfor assessing prompt injection vulnerabilities in agentic AI coding editors.\nAIShellJack contains 314 unique attack payloads that cover 70 techniques from\nthe MITRE ATT&CK framework. Using AIShellJack, we conduct a large-scale\nevaluation on GitHub Copilot and Cursor, and our evaluation results show that\nattack success rates can reach as high as 84% for executing malicious commands.\nMoreover, these attacks are proven effective across a wide range of objectives,\nranging from initial access and system discovery to credential theft and data\nexfiltration."}
{"id": "2509.22126", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22126", "abs": "https://arxiv.org/abs/2509.22126", "authors": ["Enoal Gesny", "Eva Giboulot", "Teddy Furon", "Vivien Chappelier"], "title": "Guidance Watermarking for Diffusion Models", "comment": null, "summary": "This paper introduces a novel watermarking method for diffusion models. It is\nbased on guiding the diffusion process using the gradient computed from any\noff-the-shelf watermark decoder. The gradient computation encompasses different\nimage augmentations, increasing robustness to attacks against which the decoder\nwas not originally robust, without retraining or fine-tuning. Our method\neffectively convert any \\textit{post-hoc} watermarking scheme into an\nin-generation embedding along the diffusion process. We show that this approach\nis complementary to watermarking techniques modifying the variational\nautoencoder at the end of the diffusion process. We validate the methods on\ndifferent diffusion models and detectors. The watermarking guidance does not\nsignificantly alter the generated image for a given seed and prompt, preserving\nboth the diversity and quality of generation."}
{"id": "2509.22143", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22143", "abs": "https://arxiv.org/abs/2509.22143", "authors": ["Johnnatan Messias", "Christof Ferreira Torres"], "title": "The Express Lane to Spam and Centralization: An Empirical Analysis of Arbitrum's Timeboost", "comment": null, "summary": "DeFi applications are vulnerable to MEV, where specialized actors profit by\nreordering or inserting transactions. To mitigate latency races and internalize\nMEV revenue, Arbitrum introduced Timeboost, an auction-based transaction\nsequencing mechanism that grants short-term priority access to an express lane.\nIn this paper we present the first large-scale empirical study of Timeboost,\nanalyzing over 11.5 million express lane transactions and 151 thousand auctions\nbetween April and July 2025. Our results reveal five main findings. First,\nexpress lane control is highly centralized, with two entities winning more than\n90% of auctions. Second, while express lane access provides earlier inclusion,\nprofitable MEV opportunities cluster at the end of blocks, limiting the value\nof priority access. Third, approximately 22% of time-boosted transactions are\nreverted, indicating that the Timeboost does not effectively mitigate spam.\nFourth, secondary markets for reselling express lane rights have collapsed due\nto poor execution reliability and unsustainable economics. Finally, auction\ncompetition declined over time, leading to steadily reduced revenue for the\nArbitrum DAO. Taken together, these findings show that Timeboost fails to\ndeliver on its stated goals of fairness, decentralization, and spam reduction.\nInstead, it reinforces centralization and narrows adoption, highlighting the\nlimitations of auction-based ordering as a mechanism for fair transaction\nsequencing in rollups."}
{"id": "2509.22154", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22154", "abs": "https://arxiv.org/abs/2509.22154", "authors": ["Zhou Xu", "Guyue Li", "Zhe Peng", "Aiqun Hu"], "title": "Collusion-Driven Impersonation Attack on Channel-Resistant RF Fingerprinting", "comment": null, "summary": "Radio frequency fingerprint (RFF) is a promising device identification\ntechnology, with recent research shifting from robustness to security due to\ngrowing concerns over vulnerabilities. To date, while the security of RFF\nagainst basic spoofing such as MAC address tampering has been validated, its\nresilience to advanced mimicry remains unknown. To address this gap, we propose\na collusion-driven impersonation attack that achieves RF-level mimicry,\nsuccessfully breaking RFF identification systems across diverse environments.\nSpecifically, the attacker synchronizes with a colluding receiver to match the\ncentralized logarithmic power spectrum (CLPS) of the legitimate transmitter;\nonce the colluder deems the CLPS identical, the victim receiver will also\naccept the forged fingerprint, completing RF-level spoofing. Given that the\ndistribution of CLPS features is relatively concentrated and has a clear\nunderlying structure, we design a spoofed signal generation network that\nintegrates a variational autoencoder (VAE) with a multi-objective loss function\nto enhance the similarity and deceptive capability of the generated samples. We\ncarry out extensive simulations, validating cross-channel attacks in\nenvironments that incorporate standard channel variations including additive\nwhite Gaussian noise (AWGN), multipath fading, and Doppler shift. The results\nindicate that the proposed attack scheme essentially maintains a success rate\nof over 95% under different channel conditions, revealing the effectiveness of\nthis attack."}
{"id": "2509.22213", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22213", "abs": "https://arxiv.org/abs/2509.22213", "authors": ["Ossi Räisä", "Antti Koskela", "Antti Honkela"], "title": "Accuracy-First Rényi Differential Privacy and Post-Processing Immunity", "comment": null, "summary": "The accuracy-first perspective of differential privacy addresses an important\nshortcoming by allowing a data analyst to adaptively adjust the quantitative\nprivacy bound instead of sticking to a predetermined bound. Existing works on\nthe accuracy-first perspective have neglected an important property of\ndifferential privacy known as post-processing immunity, which ensures that an\nadversary is not able to weaken the privacy guarantee by post-processing. We\naddress this gap by determining which existing definitions in the\naccuracy-first perspective have post-processing immunity, and which do not. The\nonly definition with post-processing immunity, pure ex-post privacy, lacks\nuseful tools for practical problems, such as an ex-post analogue of the\nGaussian mechanism, and an algorithm to check if accuracy on separate private\nvalidation set is high enough. To address this, we propose a new definition\nbased on R\\'enyi differential privacy that has post-processing immunity, and we\ndevelop basic theory and tools needed for practical applications. We\ndemonstrate the practicality of our theory with an application to synthetic\ndata generation, where our algorithm successfully adjusts the privacy bound\nuntil an accuracy threshold is met on a private validation dataset."}
{"id": "2509.22215", "categories": ["cs.CR", "cs.FL", "D.2.4; F.3.1; K.6.5"], "pdf": "https://arxiv.org/pdf/2509.22215", "abs": "https://arxiv.org/abs/2509.22215", "authors": ["Stefan Marksteiner", "Mikael Sjödin", "Marjan Sirjani"], "title": "Learn, Check, Test -- Security Testing Using Automata Learning and Model Checking", "comment": "19 pages, 5 figures, 2 tables, preprint submitted to Elsevier\n  Computers & Security - Original abstract shortened to comply to the arXiv\n  requirements", "summary": "Cyber-physical systems are part of industrial systems and critical\ninfrastructure. Therefore, they should be examined in a comprehensive manner to\nverify their correctness and security. At the same time, the complexity of such\nsystems demands such examinations to be systematic and, if possible, automated\nfor efficiency and accuracy. A method that can be useful in this context is\nmodel checking. However, this requires a model that faithfully represents the\nbehavior of the examined system. Obtaining such a model is not trivial, as many\nof these systems can be examined only in black box settings due to, e.g., long\nsupply chains or secrecy. We therefore utilize active black box learning\ntechniques to infer behavioral models in the form of Mealy machines of such\nsystems and translate them into a form that can be evaluated using a model\nchecker. To this end, we will investigate a cyber-physical systems as a black\nbox using its external communication interface. We first annotate the model\nwith propositions by mapping context information from the respective protocol\nto the model using Context-based Proposition Maps (CPMs). We gain annotated\nMealy machines that resemble Kripke structures. We then formally define a\ntemplate, to transfer the structures model checker-compatible format. We\nfurther define generic security properties based on basic security\nrequirements. Due to the used CPMs, we can instantiate these properties with a\nmeaningful context to check a specific protocol, which makes the approach\nflexible and scalable. The gained model can be easily altered to introduce\nnon-deterministic behavior (like timeouts) or faults and examined if the\nproperties still. Lastly, we demonstrate the versatility of the approach by\nproviding case studies of different communication protocols (NFC and UDS),\nchecked with the same tool chain and the same security properties."}
{"id": "2509.22256", "categories": ["cs.CR", "cs.AI", "cs.OS"], "pdf": "https://arxiv.org/pdf/2509.22256", "abs": "https://arxiv.org/abs/2509.22256", "authors": ["Haochen Gong", "Chenxiao Li", "Rui Chang", "Wenbo Shen"], "title": "Secure and Efficient Access Control for Computer-Use Agents via Context Space", "comment": null, "summary": "Large language model (LLM)-based computer-use agents represent a convergence\nof AI and OS capabilities, enabling natural language to control system- and\napplication-level functions. However, due to LLMs' inherent uncertainty issues,\ngranting agents control over computers poses significant security risks. When\nagent actions deviate from user intentions, they can cause irreversible\nconsequences. Existing mitigation approaches, such as user confirmation and\nLLM-based dynamic action validation, still suffer from limitations in\nusability, security, and performance. To address these challenges, we propose\nCSAgent, a system-level, static policy-based access control framework for\ncomputer-use agents. To bridge the gap between static policy and dynamic\ncontext and user intent, CSAgent introduces intent- and context-aware policies,\nand provides an automated toolchain to assist developers in constructing and\nrefining them. CSAgent enforces these policies through an optimized OS service,\nensuring that agent actions can only be executed under specific user intents\nand contexts. CSAgent supports protecting agents that control computers through\ndiverse interfaces, including API, CLI, and GUI. We implement and evaluate\nCSAgent, which successfully defends against more than 99.36% of attacks while\nintroducing only 6.83% performance overhead."}
{"id": "2509.22280", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22280", "abs": "https://arxiv.org/abs/2509.22280", "authors": ["Gustavo Sánchez", "Ghada Elbez", "Veit Hagenmeyer"], "title": "A Global Analysis of Cyber Threats to the Energy Sector: \"Currents of Conflict\" from a Geopolitical Perspective", "comment": "THIS IS A POSTPRINT OF A PEER-REVIEWED ARTICLE, PLEASE CITE IT IF\n  USING THIS WORK: Gustavo Sanchez, Ghada Elbez, and Veit Hagenmeyer. \"A Global\n  Analysis of Cyber Threats to the Energy Sector:\"Currents of Conflict\" from a\n  geopolitical perspective.\" atp magazin 67.9 (2025): 56-66.\n  https://doi.org/10.17560/atp.v67i9.2797", "summary": "The escalating frequency and sophistication of cyber threats increased the\nneed for their comprehensive understanding. This paper explores the\nintersection of geopolitical dynamics, cyber threat intelligence analysis, and\nadvanced detection technologies, with a focus on the energy domain. We leverage\ngenerative artificial intelligence to extract and structure information from\nraw cyber threat descriptions, enabling enhanced analysis. By conducting a\ngeopolitical comparison of threat actor origins and target regions across\nmultiple databases, we provide insights into trends within the general threat\nlandscape. Additionally, we evaluate the effectiveness of cybersecurity tools\n-- with particular emphasis on learning-based techniques -- in detecting\nindicators of compromise for energy-targeted attacks. This analysis yields new\ninsights, providing actionable information to researchers, policy makers, and\ncybersecurity professionals."}
{"id": "2509.22428", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.22428", "abs": "https://arxiv.org/abs/2509.22428", "authors": ["Leonhard Grosse", "Sara Saeidian", "Mikael Skoglund", "Tobias J. Oechtering"], "title": "Privacy Mechanism Design based on Empirical Distributions", "comment": "accepted to IEEE CSF 2026", "summary": "Pointwise maximal leakage (PML) is a per-outcome privacy measure based on\nthreat models from quantitative information flow. Privacy guarantees with PML\nrely on knowledge about the distribution that generated the private data. In\nthis work, we propose a framework for PML privacy assessment and mechanism\ndesign with empirical estimates of this data-generating distribution. By\nextending the PML framework to consider sets of data-generating distributions,\nwe arrive at bounds on the worst-case leakage within a given set. We use these\nbounds alongside large-deviation bounds from the literature to provide a method\nfor obtaining distribution-independent $(\\varepsilon,\\delta)$-PML guarantees\nwhen the data-generating distribution is estimated from available data samples.\nWe provide an optimal binary mechanism, and show that mechanism design with\nthis type of uncertainty about the data-generating distribution reduces to a\nlinearly constrained convex program. Further, we show that optimal mechanisms\ndesigned for a distribution estimate can be used. Finally, we apply these tools\nto leakage assessment of the Laplace mechanism and the Gaussian mechanism for\nbinary private data, and numerically show that the presented approach to\nmechanism design can yield significant utility increase compared to local\ndifferential privacy, while retaining similar privacy guarantees."}
