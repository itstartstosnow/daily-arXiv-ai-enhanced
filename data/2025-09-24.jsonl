{"id": "2509.18341", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.18341", "abs": "https://arxiv.org/abs/2509.18341", "authors": ["Christopher Simon Liu", "Fan Wang", "Patrick Gould", "Carter Yagemann"], "title": "SoK: A Beginner-Friendly Introduction to Fault Injection Attacks", "comment": "18 pages, 18 figures", "summary": "Fault Injection is the study of observing how systems behave under unusual\nstress, environmental or otherwise. In practice, fault injection involves\ntesting the limits of computer systems and finding novel ways to potentially\nbreak cyber-physical security.\n  The contributions of this paper are three-fold. First, we provide a\nbeginner-friendly introduction to this research topic and an in-depth taxonomy\nof fault injection techniques. Second, we highlight the current\nstate-of-the-art and provide a cost-benefit analysis of each attack method.\nThird, for those interested in doing fault injection research, we provide a\nreplication analysis of an existing vulnerability detection tool and identify a\nresearch focus for future work."}
{"id": "2509.18366", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.18366", "abs": "https://arxiv.org/abs/2509.18366", "authors": ["Aleksandr Dolgavin", "Jacob Gatlin", "Moti Yung", "Mark Yampolskiy"], "title": "Turning Hearsay into Discovery: Industrial 3D Printer Side Channel Information Translated to Stealing the Object Design", "comment": null, "summary": "The central security issue of outsourced 3D printing (aka AM: Additive\nManufacturing), an industry that is expected to dominate manufacturing, is the\nprotection of the digital design (containing the designers' model, which is\ntheir intellectual property) shared with the manufacturer. Here, we show, for\nthe first time, that side-channel attacks are, in fact, a concrete serious\nthreat to existing industrial grade 3D printers, enabling the reconstruction of\nthe model printed (regardless of employing ways to directly conceal the design,\ne.g. by encrypting it in transit and before loading it into the printer).\nPreviously, such attacks were demonstrated only on fairly simple FDM desktop 3D\nprinters, which play a negligible role in manufacturing of valuable designs. We\nfocus on the Powder Bed Fusion (PBF) AM process, which is popular for\nmanufacturing net-shaped parts with both polymers and metals. We demonstrate\nhow its individual actuators can be instrumented for the collection of power\nside-channel information during the printing process. We then present our\napproach to reconstruct the 3D printed model solely from the collected power\nside-channel data. Further, inspired by Differential Power Analysis, we\ndeveloped a method to improve the quality of the reconstruction based on\nmultiple traces. We tested our approach on two design models with different\ndegrees of complexity. For different models, we achieved as high as 90.29~\\% of\nTrue Positives and as low as 7.02~\\% and 9.71~\\% of False Positives and False\nNegatives by voxel-based volumetric comparison between reconstructed and\noriginal designs. The lesson learned from our attack is that the security of\ndesign files cannot solely rely on protecting the files themselves in an\nindustrial environment, but must instead also rely on assuring no leakage of\npower, noise and similar signals to potential eavesdroppers in the printer's\nvicinity."}
{"id": "2509.18413", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18413", "abs": "https://arxiv.org/abs/2509.18413", "authors": ["Efthymios Tsaprazlis", "Thanathai Lertpetchpun", "Tiantian Feng", "Sai Praneeth Karimireddy", "Shrikanth Narayanan"], "title": "VoxGuard: Evaluating User and Attribute Privacy in Speech via Membership Inference Attacks", "comment": null, "summary": "Voice anonymization aims to conceal speaker identity and attributes while\npreserving intelligibility, but current evaluations rely almost exclusively on\nEqual Error Rate (EER) that obscures whether adversaries can mount\nhigh-precision attacks. We argue that privacy should instead be evaluated in\nthe low false-positive rate (FPR) regime, where even a small number of\nsuccessful identifications constitutes a meaningful breach. To this end, we\nintroduce VoxGuard, a framework grounded in differential privacy and membership\ninference that formalizes two complementary notions: User Privacy, preventing\nspeaker re-identification, and Attribute Privacy, protecting sensitive traits\nsuch as gender and accent. Across synthetic and real datasets, we find that\ninformed adversaries, especially those using fine-tuned models and\nmax-similarity scoring, achieve orders-of-magnitude stronger attacks at low-FPR\ndespite similar EER. For attributes, we show that simple transparent attacks\nrecover gender and accent with near-perfect accuracy even after anonymization.\nOur results demonstrate that EER substantially underestimates leakage,\nhighlighting the need for low-FPR evaluation, and recommend VoxGuard as a\nbenchmark for evaluating privacy leakage."}
{"id": "2509.18415", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18415", "abs": "https://arxiv.org/abs/2509.18415", "authors": ["Sumana Malkapuram", "Sameera Gangavarapu", "Kailashnath Reddy Kavalakuntla", "Ananya Gangavarapu"], "title": "Context Lineage Assurance for Non-Human Identities in Critical Multi-Agent Systems", "comment": null, "summary": "The proliferation of autonomous software agents necessitates rigorous\nframeworks for establishing secure and verifiable agent-to-agent (A2A)\ninteractions, particularly when such agents are instantiated as non-human\nidentities(NHIs). We extend the A2A paradigm [1 , 2] by introducing a\ncryptographically grounded mechanism for lineage verification, wherein the\nprovenance and evolution of NHIs are anchored in append-only Merkle tree\nstructures modeled after Certificate Transparency (CT) logs. Unlike traditional\nA2A models that primarily secure point-to-point interactions, our approach\nenables both agents and external verifiers to cryptographically validate\nmulti-hop provenance, thereby ensuring the integrity of the entire call chain.\n  A federated proof server acts as an auditor across one or more Merkle logs,\naggregating inclusion proofs and consistency checks into compact, signed\nattestations that external parties can verify without access to the full\nexecution trace. In parallel, we augment the A2A agent card to incorporate\nexplicit identity verification primitives, enabling both peer agents and human\napprovers to authenticate the legitimacy of NHI representations in a\nstandardized manner. Together, these contributions establish a cohesive model\nthat integrates identity attestation, lineage verification, and independent\nproof auditing, thereby advancing the security posture of inter-agent\necosystems and providing a foundation for robust governance of NHIs in\nregulated environments such as FedRAMP."}
{"id": "2509.18520", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18520", "abs": "https://arxiv.org/abs/2509.18520", "authors": ["Steve Huntsman"], "title": "Coherence-driven inference for cybersecurity", "comment": "LLM4Sec - Workshop on the use of Large Language Models for\n  Cybersecurity (https://llm4sec-workshop.github.io/)", "summary": "Large language models (LLMs) can compile weighted graphs on natural language\ndata to enable automatic coherence-driven inference (CDI) relevant to red and\nblue team operations in cybersecurity. This represents an early application of\nautomatic CDI that holds near- to medium-term promise for decision-making in\ncybersecurity and eventually also for autonomous blue team operations."}
{"id": "2509.18572", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.18572", "abs": "https://arxiv.org/abs/2509.18572", "authors": ["Kemi Akanbi", "Sunkanmi Oluwadare", "Jess Kropczynski", "Jacques Bou Abdo"], "title": "Examining I2P Resilience: Effect of Centrality-based Attack", "comment": null, "summary": "This study examines the robustness of I2P, a well-regarded anonymous and\ndecentralized peer-to-peer network designed to ensure anonymity,\nconfidentiality, and circumvention of censorship. Unlike its more widely\nresearched counterpart, TOR, I2P's resilience has received less scholarly\nattention. Employing network analysis, this research evaluates I2P's\nsusceptibility to adversarial percolation. By utilizing the degree centrality\nas a measure of nodes' influence in the network, the finding suggests the\nnetwork is vulnerable to targeted disruptions. Before percolation, the network\nexhibited a density of 0.01065443 and an average path length of 6.842194. At\nthe end of the percolation process, the density decreased by approximately 10%,\nand the average path length increased by 33%, indicating a decline in\nefficiency and connectivity. These results highlight that even decentralized\nnetworks, such as I2P, exhibit structural fragility under targeted attacks,\nemphasizing the need for improved design strategies to enhance resilience\nagainst adversarial disruptions."}
{"id": "2509.18578", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.18578", "abs": "https://arxiv.org/abs/2509.18578", "authors": ["Xinwei Zhang", "Haibo Hu", "Qingqing Ye", "Li Bai", "Huadi Zheng"], "title": "MER-Inspector: Assessing model extraction risks from an attack-agnostic perspective", "comment": "Published in ACM WWW 2025", "summary": "Information leakage issues in machine learning-based Web applications have\nattracted increasing attention. While the risk of data privacy leakage has been\nrigorously analyzed, the theory of model function leakage, known as Model\nExtraction Attacks (MEAs), has not been well studied. In this paper, we are the\nfirst to understand MEAs theoretically from an attack-agnostic perspective and\nto propose analytical metrics for evaluating model extraction risks. By using\nthe Neural Tangent Kernel (NTK) theory, we formulate the linearized MEA as a\nregularized kernel classification problem and then derive the fidelity gap and\ngeneralization error bounds of the attack performance. Based on these\ntheoretical analyses, we propose a new theoretical metric called Model Recovery\nComplexity (MRC), which measures the distance of weight changes between the\nvictim and surrogate models to quantify risk. Additionally, we find that victim\nmodel accuracy, which shows a strong positive correlation with model extraction\nrisk, can serve as an empirical metric. By integrating these two metrics, we\npropose a framework, namely Model Extraction Risk Inspector (MER-Inspector), to\ncompare the extraction risks of models under different model architectures by\nutilizing relative metric values. We conduct extensive experiments on 16 model\narchitectures and 5 datasets. The experimental results demonstrate that the\nproposed metrics have a high correlation with model extraction risks, and\nMER-Inspector can accurately compare the extraction risks of any two models\nwith up to 89.58%."}
{"id": "2509.18696", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.18696", "abs": "https://arxiv.org/abs/2509.18696", "authors": ["Xiaohui Yang", "Ping Ping", "Feng Xu"], "title": "FlowCrypt: Flow-Based Lightweight Encryption with Near-Lossless Recovery for Cloud Photo Privacy", "comment": null, "summary": "The widespread adoption of smartphone photography has led users to\nincreasingly rely on cloud storage for personal photo archiving and sharing,\nraising critical privacy concerns. Existing deep learning-based image\nencryption schemes, typically built upon CNNs or GANs, often depend on\ntraditional cryptographic algorithms and lack inherent architectural\nreversibility, resulting in limited recovery quality and poor robustness.\nInvertible neural networks (INNs) have emerged to address this issue by\nenabling reversible transformations, yet the first INN-based encryption scheme\nstill relies on an auxiliary reference image and discards by-product\ninformation before decryption, leading to degraded recovery and limited\npracticality. To address these limitations, this paper proposes FlowCrypt, a\nnovel flow-based image encryption framework that simultaneously achieves\nnear-lossless recovery, high security, and lightweight model design. FlowCrypt\nbegins by applying a key-conditioned random split to the input image, enhancing\nforward-process randomness and encryption strength. The resulting components\nare processed through a Flow-based Encryption/Decryption (FED) module composed\nof invertible blocks, which share parameters across encryption and decryption.\nThanks to its reversible architecture and reference-free design, FlowCrypt\nensures high-fidelity image recovery. Extensive experiments show that FlowCrypt\nachieves recovery quality with 100dB on three datasets, produces uniformly\ndistributed cipher images, and maintains a compact architecture with only 1M\nparameters, making it suitable for mobile and edge-device applications."}
{"id": "2509.18761", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.18761", "abs": "https://arxiv.org/abs/2509.18761", "authors": ["Aicha War", "Serge L. B. Nikiema", "Jordan Samhi", "Jacques Klein", "Tegawende F. Bissyande"], "title": "Security smells in infrastructure as code: a taxonomy update beyond the seven sins", "comment": null, "summary": "Infrastructure as Code (IaC) has become essential for modern software\nmanagement, yet security flaws in IaC scripts can have severe consequences, as\nexemplified by the recurring exploits of Cloud Web Services. Prior work has\nrecognized the need to build a precise taxonomy of security smells in IaC\nscripts as a first step towards developing approaches to improve IaC security.\nThis first effort led to the unveiling of seven sins, limited by the focus on a\nsingle IaC tool as well as by the extensive, and potentially biased, manual\neffort that was required. We propose, in our work, to revisit this taxonomy:\nfirst, we extend the study of IaC security smells to a more diverse dataset\nwith scripts associated with seven popular IaC tools, including Terraform,\nAnsible, Chef, Puppet, Pulumi, Saltstack, and Vagrant; second, we bring in some\nautomation for the analysis by relying on an LLM. While we leverage LLMs for\ninitial pattern processing, all taxonomic decisions underwent systematic human\nvalidation and reconciliation with established security standards. Our study\nyields a comprehensive taxonomy of 62 security smell categories, significantly\nexpanding beyond the previously known seven. We demonstrate actionability by\nimplementing new security checking rules within linters for seven popular IaC\ntools, often achieving 1.00 precision score. Our evolution study of security\nsmells in GitHub projects reveals that these issues persist for extended\nperiods, likely due to inadequate detection and mitigation tools. This work\nprovides IaC practitioners with insights for addressing common security smells\nand systematically adopting DevSecOps practices to build safer infrastructure\ncode."}
{"id": "2509.18790", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.18790", "abs": "https://arxiv.org/abs/2509.18790", "authors": ["Aicha War", "Adnan A. Rawass", "Abdoul K. Kabore", "Jordan Samhi", "Jacques Klein", "Tegawende F. Bissyande"], "title": "Detection of security smells in IaC scripts through semantics-aware code and language processing", "comment": null, "summary": "Infrastructure as Code (IaC) automates the provisioning and management of IT\ninfrastructure through scripts and tools, streamlining software deployment.\nPrior studies have shown that IaC scripts often contain recurring security\nmisconfigurations, and several detection and mitigation approaches have been\nproposed. Most of these rely on static analysis, using statistical code\nrepresentations or Machine Learning (ML) classifiers to distinguish insecure\nconfigurations from safe code.\n  In this work, we introduce a novel approach that enhances static analysis\nwith semantic understanding by jointly leveraging natural language and code\nrepresentations. Our method builds on two complementary ML models: CodeBERT, to\ncapture semantics across code and text, and LongFormer, to represent long IaC\nscripts without losing contextual information. We evaluate our approach on\nmisconfiguration datasets from two widely used IaC tools, Ansible and Puppet.\nTo validate its effectiveness, we conduct two ablation studies (removing code\ntext from the natural language input and truncating scripts to reduce context)\nand compare against four large language models (LLMs) and prior work. Results\nshow that semantic enrichment substantially improves detection, raising\nprecision and recall from 0.46 and 0.79 to 0.92 and 0.88 on Ansible, and from\n0.55 and 0.97 to 0.87 and 0.75 on Puppet, respectively."}
{"id": "2509.18800", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.18800", "abs": "https://arxiv.org/abs/2509.18800", "authors": ["Alioune Diallo", "Anta Diop", "Abdoul Kader Kabore", "Jordan Samhi", "Aleksandr Pilgun", "Tegawendé F. Bissyande", "Jacque Klein"], "title": "Security Evaluation of Android apps in budget African Mobile Devices", "comment": "13 pages, 3 figures, submitted (wating for notification)", "summary": "Android's open-source nature facilitates widespread smartphone accessibility,\nparticularly in price-sensitive markets. System and vendor applications that\ncome pre-installed on budget Android devices frequently operate with elevated\nprivileges, yet they receive limited independent examination. To address this\ngap, we developed a framework that extracts APKs from physical devices and\napplies static analysis to identify privacy and security issues in embedded\nsoftware. Our study examined 1,544 APKs collected from seven African\nsmartphones. The analysis revealed that 145 applications (9%) disclose\nsensitive data, 249 (16%) expose critical components without sufficient\nsafeguards, and many present additional risks: 226 execute privileged or\ndangerous commands, 79 interact with SMS messages (read, send, or delete), and\n33 perform silent installation operations. We also uncovered a vendor-supplied\npackage that appears to transmit device identifiers and location details to an\nexternal third party. These results demonstrate that pre-installed applications\non widely distributed low-cost devices represent a significant and\nunderexplored threat to user security and privacy."}
{"id": "2509.18871", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.18871", "abs": "https://arxiv.org/abs/2509.18871", "authors": ["Tamer Ahmed Eltaras", "Qutaibah Malluhi", "Alessandro Savino", "Stefano Di Carlo", "Adnan Qayyum"], "title": "R-CONV++: Uncovering Privacy Vulnerabilities through Analytical Gradient Inversion Attacks", "comment": null, "summary": "Federated learning has emerged as a prominent privacy-preserving technique\nfor leveraging large-scale distributed datasets by sharing gradients instead of\nraw data. However, recent studies indicate that private training data can still\nbe exposed through gradient inversion attacks. While earlier analytical methods\nhave demonstrated success in reconstructing input data from fully connected\nlayers, their effectiveness significantly diminishes when applied to\nconvolutional layers, high-dimensional inputs, and scenarios involving multiple\ntraining examples. This paper extends our previous work \\cite{eltaras2024r} and\nproposes three advanced algorithms to broaden the applicability of gradient\ninversion attacks. The first algorithm presents a novel data leakage method\nthat efficiently exploits convolutional layer gradients, demonstrating that\neven with non-fully invertible activation functions, such as ReLU, training\nsamples can be analytically reconstructed directly from gradients without the\nneed to reconstruct intermediate layer outputs. Building on this foundation,\nthe second algorithm extends this analytical approach to support\nhigh-dimensional input data, substantially enhancing its utility across complex\nreal-world datasets. The third algorithm introduces an innovative analytical\nmethod for reconstructing mini-batches, addressing a critical gap in current\nresearch that predominantly focuses on reconstructing only a single training\nexample. Unlike previous studies that focused mainly on the weight constraints\nof convolutional layers, our approach emphasizes the pivotal role of gradient\nconstraints, revealing that successful attacks can be executed with fewer than\n5\\% of the constraints previously deemed necessary in certain layers."}
{"id": "2509.18909", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.18909", "abs": "https://arxiv.org/abs/2509.18909", "authors": ["Jan Wichelmann", "Anja Rabich", "Anna P\"atschke", "Thomas Eisenbarth"], "title": "Obelix: Mitigating Side-Channels Through Dynamic Obfuscation", "comment": null, "summary": "Trusted execution environments (TEEs) offer hardware-assisted means to\nprotect code and data. However, as shown in numerous results over the years,\nattackers can use side-channels to leak data access patterns and even\nsingle-step the code. While the vendors are slowly introducing hardware-based\ncountermeasures for some attacks, others will stay unaddressed. This makes a\nsoftware-level countermeasure desirable, but current available solutions only\naddress very specific attack vectors or have a narrow leakage model.\n  In this work, we take a holistic view at the vulnerabilities of TEEs and\ndesign a tool named Obelix, which is the first to protect both code and data\nagainst a wide range of TEE attacks, from cache attacks over single-stepping to\nciphertext side-channels. We analyze the practically achievable precision of\nstate-of-the-art single-stepping tools, and present an algorithm which uses\nthat knowledge to divide a program into uniform code blocks, that are\nindistinguishable for a strong attacker. By storing these blocks and the\nprogram data in oblivious RAM, the attacker cannot follow execution,\neffectively protecting both secret code and data. We describe how we automate\nour approach to make it available for developers who are unfamiliar with\nside-channels. As an obfuscation tool, Obelix comes with a considerable\nperformance overhead, but compensates this with strong security guarantees and\neasy applicability without requiring any expert knowledge."}
{"id": "2509.18934", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.18934", "abs": "https://arxiv.org/abs/2509.18934", "authors": ["Yating Liu", "Xing Su", "Hao Wu", "Sijin Li", "Yuxi Cheng", "Fengyuan Xu", "Sheng Zhong"], "title": "Generic Adversarial Smart Contract Detection with Semantics and Uncertainty-Aware LLM", "comment": null, "summary": "Adversarial smart contracts, mostly on EVM-compatible chains like Ethereum\nand BSC, are deployed as EVM bytecode to exploit vulnerable smart contracts\ntypically for financial gains. Detecting such malicious contracts at the time\nof deployment is an important proactive strategy preventing loss from victim\ncontracts. It offers a better cost-benefit than detecting vulnerabilities on\ndiverse potential victims. However, existing works are not generic with limited\ndetection types and effectiveness due to imbalanced samples, while the emerging\nLLM technologies, which show its potentials in generalization, have two key\nproblems impeding its application in this task: hard digestion of compiled-code\ninputs, especially those with task-specific logic, and hard assessment of LLMs'\ncertainty in their binary answers, i.e., yes-or-no answers. Therefore, we\npropose a generic adversarial smart contracts detection framework FinDet, which\nleverages LLMs with two enhancements addressing above two problems. FinDet\ntakes as input only the EVM-bytecode contracts and identifies adversarial ones\namong them with high balanced accuracy. The first enhancement extracts concise\nsemantic intentions and high-level behavioral logic from the low-level bytecode\ninputs, unleashing the LLM reasoning capability restricted by the task input.\nThe second enhancement probes and measures the LLM uncertainty to its\nmulti-round answering to the same query, improving the LLM answering robustness\nfor binary classifications required by the task output. Our comprehensive\nevaluation shows that FinDet achieves a BAC of 0.9223 and a TPR of 0.8950,\nsignificantly outperforming existing baselines. It remains robust under\nchallenging conditions including unseen attack patterns, low-data settings, and\nfeature obfuscation. FinDet detects all 5 public and 20+ unreported adversarial\ncontracts in a 10-day real-world test, confirmed manually."}
{"id": "2509.19101", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.19101", "abs": "https://arxiv.org/abs/2509.19101", "authors": ["Gejian Zhao", "Hanzhou Wu", "Xinpeng Zhang"], "title": "Trigger Where It Hurts: Unveiling Hidden Backdoors through Sensitivity with Sensitron", "comment": null, "summary": "Backdoor attacks pose a significant security threat to natural language\nprocessing (NLP) systems, but existing methods lack explainable trigger\nmechanisms and fail to quantitatively model vulnerability patterns. This work\npioneers the quantitative connection between explainable artificial\nintelligence (XAI) and backdoor attacks, introducing Sensitron, a novel modular\nframework for crafting stealthy and robust backdoor triggers. Sensitron employs\na progressive refinement approach where Dynamic Meta-Sensitivity Analysis\n(DMSA) first identifies potentially vulnerable input tokens, Hierarchical SHAP\nEstimation (H-SHAP) then provides explainable attribution to precisely pinpoint\nthe most influential tokens, and finally a Plug-and-Rank mechanism that\ngenerates contextually appropriate triggers. We establish the first\nmathematical correlation (Sensitivity Ranking Correlation, SRC=0.83) between\nexplainability scores and empirical attack success, enabling precise targeting\nof model vulnerabilities. Sensitron achieves 97.8% Attack Success Rate (ASR)\n(+5.8% over state-of-the-art (SOTA)) with 85.4% ASR at 0.1% poisoning rate,\ndemonstrating robust resistance against multiple SOTA defenses. This work\nreveals fundamental NLP vulnerabilities and provides new attack vectors through\nweaponized explainability."}
{"id": "2509.19117", "categories": ["cs.CR", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.19117", "abs": "https://arxiv.org/abs/2509.19117", "authors": ["Felix Weissberg", "Lukas Pirch", "Erik Imgrund", "Jonas Möller", "Thorsten Eisenhofer", "Konrad Rieck"], "title": "LLM-based Vulnerability Discovery through the Lens of Code Metrics", "comment": null, "summary": "Large language models (LLMs) excel in many tasks of software engineering, yet\nprogress in leveraging them for vulnerability discovery has stalled in recent\nyears. To understand this phenomenon, we investigate LLMs through the lens of\nclassic code metrics. Surprisingly, we find that a classifier trained solely on\nthese metrics performs on par with state-of-the-art LLMs for vulnerability\ndiscovery. A root-cause analysis reveals a strong correlation and a causal\neffect between LLMs and code metrics: When the value of a metric is changed,\nLLM predictions tend to shift by a corresponding magnitude. This dependency\nsuggests that LLMs operate at a similarly shallow level as code metrics,\nlimiting their ability to grasp complex patterns and fully realize their\npotential in vulnerability discovery. Based on these findings, we derive\nrecommendations on how research should more effectively address this challenge."}
{"id": "2509.19153", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.19153", "abs": "https://arxiv.org/abs/2509.19153", "authors": ["Massimo Bartoletti", "Enrico Lipparini", "Livio Pompianu"], "title": "LLMs as verification oracles for Solidity", "comment": null, "summary": "Ensuring the correctness of smart contracts is critical, as even subtle flaws\ncan lead to severe financial losses. While bug detection tools able to spot\ncommon vulnerability patterns can serve as a first line of defense, most\nreal-world exploits and losses stem from errors in the contract business logic.\nFormal verification tools such as SolCMC and the Certora Prover address this\nchallenge, but their impact remains limited by steep learning curves and\nrestricted specification languages. Recent works have begun to explore the use\nof large language models (LLMs) for security-related tasks such as\nvulnerability detection and test generation. Yet, a fundamental question\nremains open: can LLMs serve as verification oracles, capable of reasoning\nabout arbitrary contract-specific properties? In this paper, we provide the\nfirst systematic evaluation of GPT-5, a state-of-the-art reasoning LLM, in this\nrole. We benchmark its performance on a large dataset of verification tasks,\ncompare its outputs against those of established formal verification tools, and\nassess its practical effectiveness in real-world auditing scenarios. Our study\ncombines quantitative metrics with qualitative analysis, and shows that recent\nreasoning-oriented LLMs can be surprisingly effective as verification oracles,\nsuggesting a new frontier in the convergence of AI and formal methods for\nsecure smart contract development and auditing."}
