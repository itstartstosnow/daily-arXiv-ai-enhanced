<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 16]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [CanaryBench: Stress Testing Privacy Leakage in Cluster-Level Conversation Summaries](https://arxiv.org/abs/2601.18834)
*Deep Mehta*

Main category: cs.CR

TL;DR: CanaryBench是一个用于评估对话聚类摘要中隐私泄露风险的基准测试工具，通过植入"金丝雀"字符串来检测敏感信息是否在摘要中泄露。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型系统中对话数据的聚合分析日益普及，用于安全监控、治理和产品分析，从对话聚类生成的摘要可能包含个人身份信息(PII)或可追踪的敏感字符串，即使原始对话未暴露，这些摘要仍可能带来隐私风险。

Method: 提出CanaryBench基准测试：1)生成包含植入"金丝雀"字符串(模拟敏感标识符)的合成对话；2)使用TF-IDF嵌入和k-means聚类对3000个合成对话(24个主题)进行分析；3)采用有意的抽取式摘要生成器模拟引用式报告；4)评估金丝雀字符串在聚类摘要中的泄露情况。

Result: 在包含金丝雀的52个聚类中，50个出现了金丝雀泄露(聚类级泄露率96.15%)，同时检测到非零的基于正则表达式的PII指标计数。通过结合最小聚类大小阈值(k-min=25)和基于正则表达式的脱敏处理，可以消除金丝雀泄露和PII指标命中，同时保持相似的聚类一致性。

Conclusion: CanaryBench提供了一个简单可复现的隐私泄露压力测试框架，强调了对已发布分析产物(而非原始用户数据)进行隐私风险评估的重要性。通过适当的防御措施可以有效降低隐私泄露风险。

Abstract: Aggregate analytics over conversational data are increasingly used for safety monitoring, governance, and product analysis in large language model systems. A common practice is to embed conversations, cluster them, and publish short textual summaries describing each cluster. While raw conversations may never be exposed, these derived summaries can still pose privacy risks if they contain personally identifying information (PII) or uniquely traceable strings copied from individual conversations.
  We introduce CanaryBench, a simple and reproducible stress test for privacy leakage in cluster-level conversation summaries. CanaryBench generates synthetic conversations with planted secret strings ("canaries") that simulate sensitive identifiers. Because canaries are known a priori, any appearance of these strings in published summaries constitutes a measurable leak.
  Using TF-IDF embeddings and k-means clustering on 3,000 synthetic conversations (24 topics) with a canary injection rate of 0.60, we evaluate an intentionally extractive example snippet summarizer that models quote-like reporting. In this configuration, we observe canary leakage in 50 of 52 canary-containing clusters (cluster-level leakage rate 0.961538), along with nonzero regex-based PII indicator counts. A minimal defense combining a minimum cluster-size publication threshold (k-min = 25) and regex-based redaction eliminates measured canary leakage and PII indicator hits in the reported run while maintaining a similar cluster-coherence proxy. We position this work as a societal impacts contribution centered on privacy risk measurement for published analytics artifacts rather than raw user data.

</details>


### [2] [GUIGuard: Toward a General Framework for Privacy-Preserving GUI Agents](https://arxiv.org/abs/2601.18842)
*Yanxi Wang,Zhiling Zhang,Wenbo Zhou,Weiming Zhang,Jie Zhang,Qiannan Zhu,Yu Shi,Shuxin Zheng,Jiyan He*

Main category: cs.CR

TL;DR: GUIGuard是一个三阶段框架，用于保护GUI代理的隐私，包括隐私识别、隐私保护和保护下的任务执行，并构建了包含13,830张截图的跨平台基准测试GUIGuard-Bench。


<details>
  <summary>Details</summary>
Motivation: GUI代理通过直接感知和交互屏幕界面实现端到端自动化，但经常访问包含敏感个人信息的界面，且截图常传输到远程模型，造成严重隐私风险。GUI界面暴露更丰富、更易访问的私人信息，隐私风险取决于跨序列场景的交互轨迹。

Method: 提出GUIGuard三阶段框架：1) 隐私识别，2) 隐私保护，3) 保护下的任务执行。构建GUIGuard-Bench跨平台基准，包含630个轨迹和13,830张截图，标注了区域级隐私定位以及风险级别、隐私类别和任务必要性的细粒度标签。

Result: 现有代理的隐私识别能力有限，最先进模型在Android上仅达到13.3%准确率，PC上仅1.4%。在隐私保护下，任务规划语义仍能保持，闭源模型比开源模型表现出更强的语义一致性。MobileWorld案例研究表明，精心设计的保护策略能在保护隐私的同时实现更高的任务准确性。

Conclusion: 隐私识别是实用GUI代理的关键瓶颈。GUIGuard框架和基准测试为隐私保护的GUI代理提供了解决方案，研究结果强调了隐私保护在GUI自动化中的重要性。

Abstract: GUI agents enable end-to-end automation through direct perception of and interaction with on-screen interfaces. However, these agents frequently access interfaces containing sensitive personal information, and screenshots are often transmitted to remote models, creating substantial privacy risks. These risks are particularly severe in GUI workflows: GUIs expose richer, more accessible private information, and privacy risks depend on interaction trajectories across sequential scenes. We propose GUIGuard, a three-stage framework for privacy-preserving GUI agents: (1) privacy recognition, (2) privacy protection, and (3) task execution under protection. We further construct GUIGuard-Bench, a cross-platform benchmark with 630 trajectories and 13,830 screenshots, annotated with region-level privacy grounding and fine-grained labels of risk level, privacy category, and task necessity. Evaluations reveal that existing agents exhibit limited privacy recognition, with state-of-the-art models achieving only 13.3% accuracy on Android and 1.4% on PC. Under privacy protection, task-planning semantics can still be maintained, with closed-source models showing stronger semantic consistency than open-source ones. Case studies on MobileWorld show that carefully designed protection strategies achieve higher task accuracy while preserving privacy. Our results highlight privacy recognition as a critical bottleneck for practical GUI agents. Project: https://futuresis.github.io/GUIGuard-page/

</details>


### [3] [Proactive Hardening of LLM Defenses with HASTE](https://arxiv.org/abs/2601.19051)
*Henry Chen,Victor Aranda,Samarth Keshari,Ryan Heartfield,Nicole Nichols*

Main category: cs.CR

TL;DR: HASTE框架通过迭代生成高度规避性提示来增强LLM防御系统，可主动进行压力测试和反应性模仿新攻击类型


<details>
  <summary>Details</summary>
Motivation: 基于提示的攻击技术是LLM系统安全部署的主要挑战，LLM输入空间无界且非结构化，需要主动强化策略来持续生成自适应攻击向量以优化运行时防御

Method: HASTE框架：系统化框架，通过模块化优化过程迭代设计高度规避性提示，可泛化评估提示注入检测效果，支持硬负例/硬正例迭代策略，与模糊测试结合使用

Result: 硬负例挖掘成功规避基线检测器，将恶意提示检测率降低约64%；与检测模型重新训练结合时，相比基线策略显著减少迭代循环次数优化检测效果

Conclusion: HASTE框架支持LLM防御的主动和反应性强化，可动态压力测试检测系统识别弱点，也可模仿新攻击类型快速填补检测覆盖空白

Abstract: Prompt-based attack techniques are one of the primary challenges in securely deploying and protecting LLM-based AI systems. LLM inputs are an unbounded, unstructured space. Consequently, effectively defending against these attacks requires proactive hardening strategies capable of continuously generating adaptive attack vectors to optimize LLM defense at runtime. We present HASTE (Hard-negative Attack Sample Training Engine): a systematic framework that iteratively engineers highly evasive prompts, within a modular optimization process, to continuously enhance detection efficacy for prompt-based attack techniques. The framework is agnostic to synthetic data generation methods, and can be generalized to evaluate prompt-injection detection efficacy, with and without fuzzing, for any hard-negative or hard-positive iteration strategy. Experimental evaluation of HASTE shows that hard negative mining successfully evades baseline detectors, reducing malicious prompt detection for baseline detectors by approximately 64%. However, when integrated with detection model re-training, it optimizes the efficacy of prompt detection models with significantly fewer iteration loops compared to relative baseline strategies. The HASTE framework supports both proactive and reactive hardening of LLM defenses and guardrails. Proactively, developers can leverage HASTE to dynamically stress-test prompt injection detection systems; efficiently identifying weaknesses and strengthening defensive posture. Reactively, HASTE can mimic newly observed attack types and rapidly bridge detection coverage by teaching HASTE-optimized detection models to identify them.

</details>


### [4] [Thought-Transfer: Indirect Targeted Poisoning Attacks on Chain-of-Thought Reasoning Models](https://arxiv.org/abs/2601.19061)
*Harsh Chaudhari,Ethan Rathbum,Hanna Foerster,Jamie Hayes,Matthew Jagielski,Milad Nasr,Ilia Shumailov,Alina Oprea*

Main category: cs.CR

TL;DR: 本文提出了一种名为"思想转移"的新型间接目标投毒攻击，该攻击通过操纵不同任务的推理轨迹来影响目标任务的LLM输出，实现"干净标签"投毒，攻击成功率高达70%，同时还能提升模型性能10-15%。


<details>
  <summary>Details</summary>
Motivation: 当前CoT推理模型通常通过微调预训练模型来增强推理能力，这为攻击推理轨迹本身创造了新的攻击向量。先前的工作虽然展示了在CoT模型中植入后门攻击的可能性，但这些攻击需要在训练集中显式包含带有触发器的查询、错误推理和错误答案。本文旨在揭示一种新型的间接目标投毒攻击，通过在不同任务间转移推理轨迹来操纵模型响应。

Method: 提出"思想转移"攻击方法，该方法仅操纵训练样本的CoT推理轨迹，而不改变查询和答案，实现"干净标签"投毒。与先前需要目标任务样本的投毒攻击不同，该方法能够将目标行为注入到训练中从未出现过的完全不同的领域。

Result: 攻击成功率高达70%，能够在完全不同的领域注入目标行为。同时，在多个基准测试中，使用投毒推理数据训练还能将模型性能提升10-15%，为用户使用投毒数据集提供了激励。

Conclusion: 研究发现揭示了推理模型带来的新型威胁向量，这种攻击不易被现有防御机制检测和缓解，对CoT推理模型的安全性提出了重要警示。

Abstract: Chain-of-Thought (CoT) reasoning has emerged as a powerful technique for enhancing large language models' capabilities by generating intermediate reasoning steps for complex tasks. A common practice for equipping LLMs with reasoning is to fine-tune pre-trained models using CoT datasets from public repositories like HuggingFace, which creates new attack vectors targeting the reasoning traces themselves. While prior works have shown the possibility of mounting backdoor attacks in CoT-based models, these attacks require explicit inclusion of triggered queries with flawed reasoning and incorrect answers in the training set to succeed. Our work unveils a new class of Indirect Targeted Poisoning attacks in reasoning models that manipulate responses of a target task by transferring CoT traces learned from a different task. Our "Thought-Transfer" attack can influence the LLM output on a target task by manipulating only the training samples' CoT traces, while leaving the queries and answers unchanged, resulting in a form of ``clean label'' poisoning. Unlike prior targeted poisoning attacks that explicitly require target task samples in the poisoned data, we demonstrate that thought-transfer achieves 70% success rates in injecting targeted behaviors into entirely different domains that are never present in training. Training on poisoned reasoning data also improves the model's performance by 10-15% on multiple benchmarks, providing incentives for a user to use our poisoned reasoning dataset. Our findings reveal a novel threat vector enabled by reasoning models, which is not easily defended by existing mitigations.

</details>


### [5] [A Security Analysis of CheriBSD and Morello Linux](https://arxiv.org/abs/2601.19074)
*Dariy Guzairov,Alex Potanin,Stephen Kell,Alwen Tiu*

Main category: cs.CR

TL;DR: 论文分析了CHERI架构中隔离机制的四种绕过方法，重点关注移植到该架构的Linux和BSD系统，发现简单漏洞和攻击仍可绕过隔离，并提出了缓解措施和加固建议。


<details>
  <summary>Details</summary>
Motivation: CHERI架构使用能力机制实现内存保护和隔离，虽然能有效缓解内存破坏攻击，但其隔离机制在防止恶意代码突破隔离方面存在不足。本文旨在揭示CHERI架构在Linux和BSD系统中的隔离绕过漏洞。

Method: 论文详细描述了四种绕过CHERI隔离机制的方法，重点关注移植到CHERI架构的Linux和BSD操作系统。通过分析这些系统中的实现缺陷，展示了如何利用简单漏洞和攻击技术突破隔离。

Result: 研究发现，尽管Linux和BSD系统在CHERI架构上实现了隔离机制，但简单的漏洞和攻击仍然能够绕过这些保护。论文提供了概念验证演示，证明了这些攻击的有效性。

Conclusion: 论文提出了针对这些隔离绕过攻击的缓解措施，并为进一步加固Linux和BSD系统提供了建议，以防御未知攻击，提升CHERI架构隔离机制的安全性。

Abstract: Memory corruption attacks have been prevalent in software for a long time. Some mitigation strategies against these attacks do exist, but they are not as far-reaching or as efficient as the CHERI architecture. CHERI uses capabilities to restrict pointers to certain regions of memory and with certain access restrictions. These capabilities are also used to implement "compartmentalisation": dividing a binary into smaller components with limited privilege, while adhering to the principle of least privilege. However, while this architecture successfully mitigates memory corruption attacks, the compartmentalisation mechanisms in place are less effective in containing malicious code to a separate compartment. This paper details four ways to bypass compartmentalisation, with a focus on Linux and BSD operating systems ported to this architecture. We find that although compartmentalisation is implemented in these two operating systems, simple bugs and attacks can still allow malicious code to bypass it. We conclude with mitigation measures to prevent these attacks, a proof-of-concept demonstrating their use, and recommendations for further securing Linux and BSD against unknown attacks.

</details>


### [6] [Evaluating Nova 2.0 Lite model under Amazon's Frontier Model Safety Framework](https://arxiv.org/abs/2601.19134)
*Satyapriya Krishna,Matteo Memelli,Tong Wang,Abhinav Mohanty,Claire O'Brien Rajkumar,Payal Motwani,Rahul Gupta,Spyros Matsoukas*

Main category: cs.CR

TL;DR: 亚马逊发布Nova 2.0 Lite模型安全评估报告，按照其Frontier Model Safety Framework对模型在CBRN、网络攻击和AI自动化研发三个高风险领域进行综合评估


<details>
  <summary>Details</summary>
Motivation: 随着亚马逊在巴黎AI峰会上发布Frontier Model Safety Framework，需要对Nova 2.0系列中最具推理能力的Nova 2.0 Lite模型进行全面的安全风险评估，确保其符合发布标准并识别潜在风险

Method: 采用自动化基准测试、专家红队测试和提升研究相结合的方法，针对化学、生物、放射性和核武器（CBRN）、进攻性网络操作和自动化AI研发三个高风险领域进行评估，评估模型是否超过发布阈值

Result: 报告总结了核心发现，但具体评估结果未在摘要中详细说明。模型支持文本、图像和视频处理，上下文长度达100万token，能够分析大型代码库、文档和视频

Conclusion: 亚马逊将继续加强安全评估和缓解流程，随着前沿模型新风险和能力的识别，不断完善安全框架和评估方法

Abstract: Amazon published its Frontier Model Safety Framework (FMSF) as part of the Paris AI summit, following which we presented a report on Amazon's Premier model. In this report, we present an evaluation of Nova 2.0 Lite. Nova 2.0 Lite was made generally available from amongst the Nova 2.0 series and is one of its most capable reasoning models. The model processes text, images, and video with a context length of up to 1M tokens, enabling analysis of large codebases, documents, and videos in a single prompt. We present a comprehensive evaluation of Nova 2.0 Lite's critical risk profile under the FMSF. Evaluations target three high-risk domains-Chemical, Biological, Radiological and Nuclear (CBRN), Offensive Cyber Operations, and Automated AI R&D-and combine automated benchmarks, expert red-teaming, and uplift studies to determine whether the model exceeds release thresholds. We summarize our methodology and report core findings. We will continue to enhance our safety evaluation and mitigation pipelines as new risks and capabilities associated with frontier models are identified.

</details>


### [7] [AgenticSCR: An Autonomous Agentic Secure Code Review for Immature Vulnerabilities Detection](https://arxiv.org/abs/2601.19138)
*Wachiraphan Charoenwet,Kla Tantithamthavorn,Patanamon Thongtanunam,Hong Yi Lin,Minwoo Jeong,Ming Wu*

Main category: cs.CR

TL;DR: AgenticSCR：一种用于预提交阶段安全代码审查的智能体AI系统，通过结合LLM与自主决策、工具调用和代码导航，显著提升对不成熟漏洞的检测能力。


<details>
  <summary>Details</summary>
Motivation: 预提交阶段的安全代码审查需要在低延迟和有限上下文约束下早期捕获漏洞。现有SAST工具噪声大且常漏检上下文依赖的不成熟漏洞，而独立LLM受限于上下文窗口且缺乏显式工具使用。智能体AI结合了LLM与自主决策能力，为预提交安全代码审查提供了有前景的替代方案。

Method: 提出AgenticSCR系统，这是一种用于检测预提交阶段不成熟漏洞的智能体AI，通过安全导向的语义记忆增强。使用专门为预提交安全代码审查定制的自有基准数据集，实证评估系统在定位、检测和解释不成熟漏洞方面的准确性。

Result: AgenticSCR相比静态LLM基线至少获得153%相对更高的正确代码审查评论比例，并大幅超越SAST工具。在五类漏洞中的四类中生成更多正确评论，持续且显著优于所有其他基线方法。

Conclusion: 研究结果强调了智能体安全代码审查的重要性，为不成熟漏洞检测这一新兴研究领域铺平了道路。AgenticSCR展示了智能体AI在预提交安全代码审查中的有效性。

Abstract: Secure code review is critical at the pre-commit stage, where vulnerabilities must be caught early under tight latency and limited-context constraints. Existing SAST-based checks are noisy and often miss immature, context-dependent vulnerabilities, while standalone Large Language Models (LLMs) are constrained by context windows and lack explicit tool use. Agentic AI, which combine LLMs with autonomous decision-making, tool invocation, and code navigation, offer a promising alternative, but their effectiveness for pre-commit secure code review is not yet well understood. In this work, we introduce AgenticSCR, an agentic AI for secure code review for detecting immature vulnerabilities during the pre-commit stage, augmented by security-focused semantic memories. Using our own curated benchmark of immature vulnerabilities, tailored to the pre-commit secure code review, we empirically evaluate how accurate is our AgenticSCR for localizing, detecting, and explaining immature vulnerabilities. Our results show that AgenticSCR achieves at least 153% relatively higher percentage of correct code review comments than the static LLM-based baseline, and also substantially surpasses SAST tools. Moreover, AgenticSCR generates more correct comments in four out of five vulnerability types, consistently and significantly outperforming all other baselines. These findings highlight the importance of Agentic Secure Code Review, paving the way towards an emerging research area of immature vulnerability detection.

</details>


### [8] [SHIELD: An Auto-Healing Agentic Defense Framework for LLM Resource Exhaustion Attacks](https://arxiv.org/abs/2601.19174)
*Nirhoshan Sivaroopan,Kanchana Thilakarathna,Albert Zomaya,Manu,Yi Guo,Jo Plested,Tim Lynar,Jack Yang,Wangli Yang*

Main category: cs.CR

TL;DR: SHIELD是一个多智能体自愈防御框架，通过三阶段防御智能体结合语义检索、模式匹配和LLM推理来对抗LLM海绵攻击，并利用知识更新和提示优化智能体形成闭环自愈系统。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法存在局限性：统计过滤器无法应对语义攻击，静态LLM检测器难以适应攻击策略的演变。海绵攻击对LLM系统构成严重威胁，会导致过度计算和拒绝服务攻击。

Method: SHIELD采用多智能体框架，核心是三阶段防御智能体（语义相似性检索、模式匹配、LLM推理），辅以知识更新智能体和提示优化智能体，形成闭环自愈循环。当攻击绕过检测时，系统会更新知识库并优化防御指令。

Result: 实验表明SHIELD在非语义和语义海绵攻击上都优于基于困惑度的防御和独立LLM防御，取得了高F1分数，证明了智能体自愈机制对抗不断演变的资源耗尽威胁的有效性。

Conclusion: SHIELD框架通过多智能体自愈设计有效应对了LLM海绵攻击的挑战，展示了自适应防御系统对抗不断演变的资源耗尽攻击的潜力。

Abstract: Sponge attacks increasingly threaten LLM systems by inducing excessive computation and DoS. Existing defenses either rely on statistical filters that fail on semantically meaningful attacks or use static LLM-based detectors that struggle to adapt as attack strategies evolve. We introduce SHIELD, a multi-agent, auto-healing defense framework centered on a three-stage Defense Agent that integrates semantic similarity retrieval, pattern matching, and LLM-based reasoning. Two auxiliary agents, a Knowledge Updating Agent and a Prompt Optimization Agent, form a closed self-healing loop, when an attack bypasses detection, the system updates an evolving knowledgebase, and refines defense instructions. Extensive experiments show that SHIELD consistently outperforms perplexity-based and standalone LLM defenses, achieving high F1 scores across both non-semantic and semantic sponge attacks, demonstrating the effectiveness of agentic self-healing against evolving resource-exhaustion threats.

</details>


### [9] [LLMs Can Unlearn Refusal with Only 1,000 Benign Samples](https://arxiv.org/abs/2601.19231)
*Yangyang Guo,Ziwei Xu,Si Liu,Zhiming Zheng,Mohan Kankanhalli*

Main category: cs.CR

TL;DR: 研究发现LLM安全对齐存在新漏洞：模型固定拒绝模式（如"I'm sorry"前缀）可被利用，通过仅1000个良性样本的拒绝前缀微调，就能让模型忘记拒绝能力而执行有害指令。


<details>
  <summary>Details</summary>
Motivation: 现有对齐LLM主要通过固定拒绝前缀（如"I'm sorry"）来回应不安全查询，这种僵化的拒绝模式可能成为安全漏洞。研究旨在探索这种模式是否可被利用来破坏模型的安全对齐。

Method: 提出"拒绝遗忘"技术：仅使用1000个良性样本对LLM进行微调，每个响应前都添加拒绝前缀。通过破坏拒绝完成路径，使模型忘记如何拒绝而遵循有害指令。理论证明支持这一直觉。

Result: 在16个LLM（包括Llama、Qwen、Gemma等开源模型及Gemini、GPT等闭源模型）上测试，安全评分均显著下降。验证了效果非普通微调或随机前缀所致。

Conclusion: 当前安全对齐可能过度依赖令牌序列记忆而非推理，简单的拒绝机制存在脆弱性。需要超越简单拒绝机制的安全对齐方法。

Abstract: This study reveals a previously unexplored vulnerability in the safety alignment of Large Language Models (LLMs). Existing aligned LLMs predominantly respond to unsafe queries with refusals, which often begin with a fixed set of prefixes (I'm sorry). We demonstrate that this rigid refusal pattern is a vulnerability and introduce a novel \textbf{refusal unlearning} technique that exploits it. Specifically, we fine-tune LLMs using merely 1,000 benign samples, where each response is prepended with a refusal prefix. The underlying intuition is to disrupt the refusal completion pathway, thereby driving the model to forget how to refuse while following harmful instructions. This intuition is further supported by theoretical proofs. We apply this approach to a total of 16 LLMs, including various open-source models from Llama, Qwen, and Gemma families, as well as closed-source models such as Gemini and GPT. Experimental results show that the safety scores of previously aligned LLMs degrade both consistently and substantially. Importantly, we verify that the observed gain cannot be attributed to plain fine-tuning or random prefix effects. Our findings suggest that current safety alignment may rely heavily on token sequence memorization rather than reasoning, motivating future work beyond simple refusal mechanisms. Code has been released: https://github.com/guoyang9/refusal-unlearning.

</details>


### [10] [AI-driven Intrusion Detection for UAV in Smart Urban Ecosystems: A Comprehensive Survey](https://arxiv.org/abs/2601.19345)
*Abdullah Khanfor,Raby Hamadi,Noureddine Lasla,Hakim Ghazzai*

Main category: cs.CR

TL;DR: 该论文系统综述了无人机在智慧城市应用中的安全挑战，重点关注2019-2025年的发展，将挑战分为网络攻击和物理入侵两类，探讨了AI技术在入侵检测中的应用，并提出了十个关键研究方向。


<details>
  <summary>Details</summary>
Motivation: 无人机在智慧城市中有广泛应用潜力，但其集成带来了新的安全挑战。为确保无人机在智慧城市中的安全可靠运行，需要系统分析这些安全威胁并探索有效的检测和缓解方案。

Method: 采用结构化、基于证据的文献综述方法，分析过去十年特别是2019-2025年间无人机在智慧城市应用中的安全挑战。将挑战分为两类：针对无人机通信基础设施的网络攻击和无人机自身的物理入侵。重点研究AI技术在入侵检测系统中的应用潜力。

Result: 系统识别了无人机在智慧城市中的主要安全威胁类别，分析了AI技术（如机器学习/深度学习异常检测和计算机视觉对象识别）在统一检测系统中的关键作用。整合了适用于入侵检测系统开发和评估的公开无人机数据集，并提出了十个关键研究方向。

Conclusion: 无人机在智慧城市中的安全挑战需要综合解决方案。AI技术特别是统一检测系统在应对网络和物理威胁方面具有重要潜力。未来研究应关注可扩展性、鲁棒性、可解释性、数据稀缺性、自动化、混合检测、大语言模型、多模态方法、联邦学习和隐私保护等十个方向，同时考虑实际部署中的挑战。

Abstract: UAVs have the potential to revolutionize urban management and provide valuable services to citizens. They can be deployed across diverse applications, including traffic monitoring, disaster response, environmental monitoring, and numerous other domains. However, this integration introduces novel security challenges that must be addressed to ensure safe and trustworthy urban operations. This paper provides a structured, evidence-based synthesis of UAV applications in smart cities and their associated security challenges as reported in the literature over the last decade, with particular emphasis on developments from 2019 to 2025. We categorize these challenges into two primary classes: 1) cyber-attacks targeting the communication infrastructure of UAVs and 2) unwanted or unauthorized physical intrusions by UAVs themselves. We examine the potential of Artificial Intelligence (AI) techniques in developing intrusion detection mechanisms to mitigate these security threats. We analyze how AI-based methods, such as machine/deep learning for anomaly detection and computer vision for object recognition, can play a pivotal role in enhancing UAV security through unified detection systems that address both cyber and physical threats. Furthermore, we consolidate publicly available UAV datasets across network traffic and vision modalities suitable for Intrusion Detection Systems (IDS) development and evaluation. The paper concludes by identifying ten key research directions, including scalability, robustness, explainability, data scarcity, automation, hybrid detection, large language models, multimodal approaches, federated learning, and privacy preservation. Finally, we discuss the practical challenges of implementing UAV IDS solutions in real-world smart city environments.

</details>


### [11] [CHEHAB RL: Learning to Optimize Fully Homomorphic Encryption Computations](https://arxiv.org/abs/2601.19367)
*Bilel Sefsaf,Abderraouf Dandani,Abdessamed Seddiki,Arab Mohammed,Eduardo Chielle,Michail Maniatakos,Riyadh Baghdadi*

Main category: cs.CR

TL;DR: CHEHAB RL 使用深度强化学习自动优化全同态加密代码，相比现有编译器 Coyote 实现了5.3倍执行加速、2.54倍噪声降低和27.9倍编译加速。


<details>
  <summary>Details</summary>
Motivation: 全同态加密（FHE）允许在加密数据上直接计算，但其高昂的计算成本是主要障碍。编写高效的FHE代码需要密码学专业知识，且找到最优的程序转换序列通常是难以处理的。

Method: 提出CHEHAB RL框架，利用深度强化学习训练一个智能体，学习应用重写规则序列的有效策略，自动向量化标量FHE代码，同时减少指令延迟和噪声增长。使用大型语言模型合成多样化计算数据集来训练智能体。

Result: 在基准测试中，与最先进的向量化FHE编译器Coyote相比，CHEHAB RL生成的代码执行速度快5.3倍，噪声积累少2.54倍，编译过程本身快27.9倍（几何平均值）。

Conclusion: CHEHAB RL通过强化学习自动化FHE代码优化，显著提升了执行性能、噪声控制和编译效率，为FHE的实际应用提供了有效的自动化工具。

Abstract: Fully Homomorphic Encryption (FHE) enables computations directly on encrypted data, but its high computational cost remains a significant barrier. Writing efficient FHE code is a complex task requiring cryptographic expertise, and finding the optimal sequence of program transformations is often intractable. In this paper, we propose CHEHAB RL, a novel framework that leverages deep reinforcement learning (RL) to automate FHE code optimization. Instead of relying on predefined heuristics or combinatorial search, our method trains an RL agent to learn an effective policy for applying a sequence of rewriting rules to automatically vectorize scalar FHE code while reducing instruction latency and noise growth. The proposed approach supports the optimization of both structured and unstructured code. To train the agent, we synthesize a diverse dataset of computations using a large language model (LLM). We integrate our proposed approach into the CHEHAB FHE compiler and evaluate it on a suite of benchmarks, comparing its performance against Coyote, a state-of-the-art vectorizing FHE compiler. The results show that our approach generates code that is $5.3\times$ faster in execution, accumulates $2.54\times$ less noise, while the compilation process itself is $27.9\times$ faster than Coyote (geometric means).

</details>


### [12] [Reuse of Public Keys Across UTXO and Account-Based Cryptocurrencies](https://arxiv.org/abs/2601.19500)
*Rainer Stütz,Nicholas Stifter,Melitta Dragaschnig,Bernhard Haslhofer,Aljosha Judmayer*

Main category: cs.CR

TL;DR: 该论文首次系统性地揭示了跨链密钥重用现象，通过分析底层公钥而非地址格式，在UTXO和账户模型加密货币之间发现了广泛的密钥重用，对用户隐私和安全构成威胁。


<details>
  <summary>Details</summary>
Motivation: 加密货币地址重用会破坏隐私，跨链地址重用现象在EVM设计中尤为常见。先前研究仅通过直接地址匹配或基本格式转换来识别，但忽略了不同格式地址可能源自相同公钥的事实。需要更深入分析底层公钥来发现跨链密钥重用。

Method: 通过分析底层公钥而非地址格式来发现密钥重用，覆盖比特币、以太坊、莱特币、狗狗币、Zcash和波场等网络。开发了不依赖启发式方法的新型聚类技术，通过实体对底层私钥的掌握来链接不同网络中的实体。

Result: 研究结果显示加密密钥在这些网络中广泛且活跃地被重用，对用户隐私和安全产生负面影响。首次在UTXO和账户模型加密货币之间暴露并量化了跨链密钥重用现象。

Conclusion: 跨链密钥重用是一个严重问题，需要引起关注。论文提出的基于公钥的分析方法能够识别传统地址匹配方法无法发现的关联，为加密货币隐私和安全研究提供了新视角。

Abstract: It is well known that reusing cryptocurrency addresses undermines privacy. This also applies if the same addresses are used in different cryptocurrencies. Nevertheless, cross-chain address reuse appears to be a recurring phenomenon, especially in EVM-based designs. Previous works performed either direct address matching, or basic format conversion, to identify such cases. However, seemingly incompatible address formats e.g., in Bitcoin and Ethereum, can also be derived from the same public keys, since they rely on the same cryptographic primitives. In this paper, we therefore focus on the underlying public keys to discover reuse within, as well as across, different cryptocurrency networks, enabling us to also match incompatible address formats. Specifically, we analyze key reuse across Bitcoin, Ethereum, Litecoin, Dogecoin, Zcash and Tron. Our results reveal that cryptographic keys are extensively and actively reused across these networks, negatively impacting both privacy and security of their users. We are hence the first to expose and quantify cross-chain key reuse between UTXO and account-based cryptocurrencies. Moreover, we devise novel clustering methods across these different cryptocurrency networks that do not rely on heuristics and instead link entities by their knowledge of the underlying secret key.

</details>


### [13] [How to Serve Your Sandwich? MEV Attacks in Private L2 Mempools](https://arxiv.org/abs/2601.19570)
*Krzysztof Gogol,Manvir Schneider,Jan Gorzny,Claudio Tessone*

Main category: cs.CR

TL;DR: 研究显示，在具有私有内存池的以太坊Rollup中，三明治攻击（sandwich attacks）虽然理论上可行，但实际上罕见且无利可图，与以太坊L1的情况形成鲜明对比。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估在具有私有内存池的以太坊Rollup中三明治攻击的可行性、盈利性和普遍性，挑战现有关于Layer 2中MEV（矿工可提取价值）的假设，并为排序策略设计提供依据。

Method: 研究方法包括：1）扩展最优前后运行规模的形式化模型；2）建立私有内存池下的执行可行性模型；3）在没有构建者市场的情况下分析执行约束；4）使用主要Rollup的交易级数据进行实证分析。

Result: 研究发现：1）多数被标记的模式是误报；2）这些攻击的中位数净回报为负值；3）三明治攻击在具有私有内存池的Rollup中罕见、无利可图且基本不存在，与以太坊L1的普遍盈利情况形成对比。

Conclusion: 结论表明，虽然三明治攻击在以太坊L1中普遍且盈利，但在具有私有内存池的Rollup中却罕见且无利可图。这些发现挑战了现有假设，细化了L2中MEV的测量方法，并为排序策略设计提供了重要参考。

Abstract: We study the feasibility, profitability, and prevalence of sandwich attacks on Ethereum rollups with private mempools. First, we extend a formal model of optimal front- and back-run sizing, relating attack profitability to victim trade volume, liquidity depth, and slippage bounds. We complement it with an execution-feasibility model that quantifies co-inclusion constraints under private mempools. Second, we examine execution constraints in the absence of builder markets: without guaranteed atomic inclusion, attackers must rely on sequencer ordering, redundant submissions, and priority fee placement, which renders sandwiching probabilistic rather than deterministic. Third, using transaction-level data from major rollups, we show that naive heuristics overstate sandwich activity. We find that the majority of flagged patterns are false positives and that the median net return for these attacks is negative. Our results suggest that sandwiching, while endemic and profitable on Ethereum L1, is rare, unprofitable, and largely absent in rollups with private mempools. These findings challenge prevailing assumptions, refine measurement of MEV in L2s, and inform the design of sequencing policies.

</details>


### [14] [LLM-Assisted Authentication and Fraud Detection](https://arxiv.org/abs/2601.19684)
*Emunah S-S. Chan,Aldar C-F. Chan*

Main category: cs.CR

TL;DR: 论文提出两种LLM增强的安全解决方案：基于语义正确性评估的认证机制和基于RAG的欺诈检测管道，显著提升安全系统的可用性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统知识认证依赖精确字符串匹配，无法适应人类记忆和语言变化；欺诈检测难以跟上快速演变的诈骗行为，导致高误报率和频繁重训练需求。

Method: 1) LLM辅助认证机制：通过文档分割和混合评分方法（结合LLM判断和余弦相似度）评估语义正确性而非精确措辞；2) RAG欺诈检测管道：将LLM推理基于精选证据，减少幻觉并适应新兴诈骗模式而无需模型重训练。

Result: 认证系统接受99.5%的合法非精确答案，同时保持0.1%的误接受率；RAG增强的欺诈检测将误报率从17.2%降低到3.5%（原文为35%，应为笔误）。

Conclusion: LLM能显著提升安全工作流的可用性和鲁棒性，为认证和欺诈检测提供更自适应、可解释且符合人类认知的方法。

Abstract: User authentication and fraud detection face growing challenges as digital systems expand and adversaries adopt increasingly sophisticated tactics. Traditional knowledge-based authentication remains rigid, requiring exact word-for-word string matches that fail to accommodate natural human memory and linguistic variation. Meanwhile, fraud-detection pipelines struggle to keep pace with rapidly evolving scam behaviors, leading to high false-positive rates and frequent retraining cycles required. This work introduces two complementary LLM-enabled solutions, namely, an LLM-assisted authentication mechanism that evaluates semantic correctness rather than exact wording, supported by document segmentation and a hybrid scoring method combining LLM judgement with cosine-similarity metrics and a RAG-based fraud-detection pipeline that grounds LLM reasoning in curated evidence to reduce hallucinations and adapt to emerging scam patterns without model retraining. Experiments show that the authentication system accepts 99.5% of legitimate non-exact answers while maintaining a 0,1% false-acceptance rate, and that the RAG-enhanced fraud detection reduces false positives from 17.2% to 35%. Together, these findings demonstrate that LLMs can significantly improve both usability and robustness in security workflows, offering a more adaptive , explainable, and human-aligned approach to authentication and fraud detection.

</details>


### [15] [RvB: Automating AI System Hardening via Iterative Red-Blue Games](https://arxiv.org/abs/2601.19726)
*Lige Huang,Zicheng Liu,Jie Zhang,Lewen Yan,Dongrui Liu,Jing Shao*

Main category: cs.CR

TL;DR: 提出Red Team vs. Blue Team框架，通过对抗性互动实现无需参数更新的AI系统持续强化，在代码漏洞修复和越狱防御任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型同时具备攻击和防御能力，但AI安全领域缺乏统一的动态迭代对抗适应强化框架。现有方法往往针对特定攻击进行过拟合，缺乏通用防御原则的学习。

Method: 提出RvB框架，将其形式化为无需训练、顺序进行的不完全信息博弈。红队负责暴露系统漏洞，蓝队通过学习有效解决方案进行防御，整个过程无需参数更新。

Result: 在两个挑战性任务中验证：动态代码漏洞修复（针对CVE）和护栏优化（针对越狱攻击）。RvB分别达到90%和45%的防御成功率，同时保持接近0%的误报率，显著超越基线方法。

Conclusion: 迭代对抗互动框架为AI系统持续自动化强化提供了实用范式，使蓝队能够学习到根本性的防御原则，而不仅仅是针对特定攻击的过拟合解决方案。

Abstract: The dual offensive and defensive utility of Large Language Models (LLMs) highlights a critical gap in AI security: the lack of unified frameworks for dynamic, iterative adversarial adaptation hardening. To bridge this gap, we propose the Red Team vs. Blue Team (RvB) framework, formulated as a training-free, sequential, imperfect-information game. In this process, the Red Team exposes vulnerabilities, driving the Blue Team to learning effective solutions without parameter updates. We validate our framework across two challenging domains: dynamic code hardening against CVEs and guardrail optimization against jailbreaks. Our empirical results show that this interaction compels the Blue Team to learn fundamental defensive principles, leading to robust remediations that are not merely overfitted to specific exploits. RvB achieves Defense Success Rates of 90\% and 45\% across the respective tasks while maintaining near 0\% False Positive Rates, significantly surpassing baselines. This work establishes the iterative adversarial interaction framework as a practical paradigm that automates the continuous hardening of AI systems.

</details>


### [16] [Self-Sovereign Identity and eIDAS 2.0: An Analysis of Control, Privacy, and Legal Implications](https://arxiv.org/abs/2601.19837)
*Nacereddine Sitouah,Marco Esposito,Francesco Bruschi*

Main category: cs.CR

TL;DR: 本文分析欧盟eIDAS 2.0数字身份法规框架，探讨其从1999年电子签名指令到eIDAS 1.0的演变，识别立法空白和实施挑战，并评估欧洲数字身份架构与参考框架(ARF)与自主身份(SSI)原则的契合度。


<details>
  <summary>Details</summary>
Motivation: 欧盟数字身份倡议基于确保互操作性和统一安全标准的监管框架。随着技术发展，eIDAS 1.0框架因局限性受到批评，去中心化方法的出现暴露了这些不足，并引入了整合自主身份(SSI)等创新身份范式的可能性。

Method: 分析eIDAS 2.0法规及其相关说明的关键条款，借鉴现有文献识别立法空白和实施挑战；同时审查欧洲数字身份架构与参考框架(ARF)，评估其提出的指导方针，并评价其实施与SSI原则的一致性程度。

Result: 文章识别了eIDAS 2.0框架中的立法空白和实施挑战，并评估了ARF框架与自主身份(SSI)原则的契合程度，为欧盟数字身份框架的完善提供了分析基础。

Conclusion: 欧盟数字身份监管框架从1999年电子签名指令发展到eIDAS 2.0，但仍需解决立法空白和实施挑战，特别是如何更好地整合自主身份(SSI)等创新范式，以实现更全面、灵活的数字身份体系。

Abstract: European digital identity initiatives are grounded in regulatory frameworks designed to ensure interoperability and robust, harmonized security standards. The evolution of these frameworks culminates in eIDAS 2.0, whose origins trace back to the Electronic Signatures Directive 1999/93/EC, the first EU-wide legal foundation for the use of electronic signatures in cross-border electronic transactions. As technological capabilities advanced, the initial eIDAS 1.0 framework was increasingly criticized for its limitations and lack of comprehensiveness. Emerging decentralized approaches further exposed these shortcomings and introduced the possibility of integrating innovative identity paradigms, such as Self-Sovereign Identity (SSI) models.
  In this article, we analyse key provisions of the eIDAS 2.0 Regulation and its accompanying recitals, drawing on existing literature to identify legislative gaps and implementation challenges. Furthermore, we examine the European Digital Identity Architecture and Reference Framework (ARF), assessing its proposed guidelines and evaluating the extent to which its emerging implementations align with SSI principles.

</details>
