<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 13]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [VISAT: Benchmarking Adversarial and Distribution Shift Robustness in Traffic Sign Recognition with Visual Attributes](https://arxiv.org/abs/2510.26833)
*Simon Yu,Peilin Yu,Hongbo Zheng,Huajie Shao,Han Zhao,Lui Sha*

Main category: cs.CR

TL;DR: VISAT是一个用于评估交通标志识别模型鲁棒性的新数据集和基准套件，包含对抗攻击和分布偏移两个基准测试，重点关注多任务学习网络中的虚假相关性。


<details>
  <summary>Details</summary>
Motivation: 现有交通标志识别模型在面对对抗攻击和分布偏移时缺乏系统评估，需要专门的数据集来研究模型鲁棒性，特别是在多任务学习场景下。

Method: 基于Mapillary交通标志数据集构建VISAT，使用PGD方法生成对抗样本，利用ImageNet-C的数据损坏技术模拟分布偏移，分析ResNet-152和ViT-B/32两种主干网络在基础模型和多任务学习模型上的表现。

Result: 实验揭示了多任务学习网络中存在的虚假相关性，评估了不同模型在对抗攻击和分布偏移下的鲁棒性表现。

Conclusion: VISAT数据集和基准框架有助于理解交通标志识别模型的鲁棒性挑战，为自动驾驶和网络物理系统中开发更鲁棒的模型提供支持。

Abstract: We present VISAT, a novel open dataset and benchmarking suite for evaluating
model robustness in the task of traffic sign recognition with the presence of
visual attributes. Built upon the Mapillary Traffic Sign Dataset (MTSD), our
dataset introduces two benchmarks that respectively emphasize robustness
against adversarial attacks and distribution shifts. For our adversarial attack
benchmark, we employ the state-of-the-art Projected Gradient Descent (PGD)
method to generate adversarial inputs and evaluate their impact on popular
models. Additionally, we investigate the effect of adversarial attacks on
attribute-specific multi-task learning (MTL) networks, revealing spurious
correlations among MTL tasks. The MTL networks leverage visual attributes
(color, shape, symbol, and text) that we have created for each traffic sign in
our dataset. For our distribution shift benchmark, we utilize ImageNet-C's
realistic data corruption and natural variation techniques to perform
evaluations on the robustness of both base and MTL models. Moreover, we further
explore spurious correlations among MTL tasks through synthetic alterations of
traffic sign colors using color quantization techniques. Our experiments focus
on two major backbones, ResNet-152 and ViT-B/32, and compare the performance
between base and MTL models. The VISAT dataset and benchmarking framework
contribute to the understanding of model robustness for traffic sign
recognition, shedding light on the challenges posed by adversarial attacks and
distribution shifts. We believe this work will facilitate advancements in
developing more robust models for real-world applications in autonomous driving
and cyber-physical systems.

</details>


### [2] [Broken-Token: Filtering Obfuscated Prompts by Counting Characters-Per-Token](https://arxiv.org/abs/2510.26847)
*Shaked Zychlinski,Yuval Kainan*

Main category: cs.CR

TL;DR: CPT-Filtering是一种基于字符-标记比率的模型无关防御技术，通过检测编码文本中字符数/标记数比例异常来识别越狱攻击，具有低成本、高精度的特点。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全护栏无法有效检测使用密码和字符级编码的越狱攻击，而现代防御方法计算成本高昂，需要依赖额外的LLM或困惑度模型。

Method: 利用BPE分词器的固有特性：在自然语言训练的分词器处理分布外文本（如密码）时会产生更多短标记，通过计算文本的平均字符数/标记数(CPT)来检测异常。

Result: 在超过10万条提示的大规模数据集上验证，测试了多种编码方案和流行分词器，实验表明简单的CPT阈值能高精度识别编码文本，即使对于很短的输入也有效。

Conclusion: CPT-Filtering提供了一种实用的防御层，可立即部署用于实时文本过滤和离线数据管理，具有模型无关、成本可忽略和接近完美准确率的特点。

Abstract: Large Language Models (LLMs) are susceptible to jailbreak attacks where
malicious prompts are disguised using ciphers and character-level encodings to
bypass safety guardrails. While these guardrails often fail to interpret the
encoded content, the underlying models can still process the harmful
instructions. We introduce CPT-Filtering, a novel, model-agnostic with
negligible-costs and near-perfect accuracy guardrail technique that aims to
mitigate these attacks by leveraging the intrinsic behavior of Byte-Pair
Encoding (BPE) tokenizers. Our method is based on the principle that
tokenizers, trained on natural language, represent out-of-distribution text,
such as ciphers, using a significantly higher number of shorter tokens. Our
technique uses a simple yet powerful artifact of using language models: the
average number of Characters Per Token (CPT) in the text. This approach is
motivated by the high compute cost of modern methods - relying on added modules
such as dedicated LLMs or perplexity models. We validate our approach across a
large dataset of over 100,000 prompts, testing numerous encoding schemes with
several popular tokenizers. Our experiments demonstrate that a simple CPT
threshold robustly identifies encoded text with high accuracy, even for very
short inputs. CPT-Filtering provides a practical defense layer that can be
immediately deployed for real-time text filtering and offline data curation.

</details>


### [3] [LLM-based Multi-class Attack Analysis and Mitigation Framework in IoT/IIoT Networks](https://arxiv.org/abs/2510.26941)
*Seif Ikbarieh,Maanak Gupta,Elmahedi Mahalal*

Main category: cs.CR

TL;DR: 提出一个结合机器学习和大型语言模型的混合框架，用于物联网攻击检测、行为分析和缓解建议，并引入新的量化评估指标来评估模型效果。


<details>
  <summary>Details</summary>
Motivation: 物联网快速发展增加了攻击面，现有AI安全评估缺乏标准化、客观的基准来量化衡量攻击分析和缓解效果。

Method: 使用ML进行多类攻击检测，结合LLM进行攻击行为分析和缓解建议，采用结构化角色扮演提示工程和检索增强生成技术。

Result: 随机森林在攻击检测中表现最佳，ChatGPT-o3在攻击分析和缓解方面优于DeepSeek-R1。

Conclusion: 混合框架有效提升了物联网安全防护能力，量化评估方法为AI安全模型提供了标准化基准。

Abstract: The Internet of Things has expanded rapidly, transforming communication and
operations across industries but also increasing the attack surface and
security breaches. Artificial Intelligence plays a key role in securing IoT,
enabling attack detection, attack behavior analysis, and mitigation suggestion.
Despite advancements, evaluations remain purely qualitative, and the lack of a
standardized, objective benchmark for quantitatively measuring AI-based attack
analysis and mitigation hinders consistent assessment of model effectiveness.
In this work, we propose a hybrid framework combining Machine Learning (ML) for
multi-class attack detection with Large Language Models (LLMs) for attack
behavior analysis and mitigation suggestion. After benchmarking several ML and
Deep Learning (DL) classifiers on the Edge-IIoTset and CICIoT2023 datasets, we
applied structured role-play prompt engineering with Retrieval-Augmented
Generation (RAG) to guide ChatGPT-o3 and DeepSeek-R1 in producing detailed,
context-aware responses. We introduce novel evaluation metrics for quantitative
assessment to guide us and an ensemble of judge LLMs, namely ChatGPT-4o,
DeepSeek-V3, Mixtral 8x7B Instruct, Gemini 2.5 Flash, Meta Llama 4, TII Falcon
H1 34B Instruct, xAI Grok 3, and Claude 4 Sonnet, to independently evaluate the
responses. Results show that Random Forest has the best detection model, and
ChatGPT-o3 outperformed DeepSeek-R1 in attack analysis and mitigation.

</details>


### [4] [Adapting Large Language Models to Emerging Cybersecurity using Retrieval Augmented Generation](https://arxiv.org/abs/2510.27080)
*Arnabh Borah,Md Tanvirul Alam,Nidhi Rastogi*

Main category: cs.CR

TL;DR: 提出基于RAG的框架，通过混合检索方法增强LLM在网络安全任务中的知识保留和时间推理能力，提高模型适应性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 网络安全应用依赖LLM进行威胁检测，但模型推理不透明且难以适应快速演变的威胁环境，需要增强其在网络安全领域的知识保留和时间推理能力。

Method: 使用Llama-3-8B-Instruct模型，构建RAG框架整合外部数据集，比较基线RAG与优化的混合检索方法在不同性能指标上的表现。

Result: 混合检索方法在增强LLM对网络安全任务的适应性和可靠性方面展现出显著潜力。

Conclusion: 混合检索方法有望强化LLM在网络安全应用中的能力，为应对快速演变的威胁环境提供更可靠的解决方案。

Abstract: Security applications are increasingly relying on large language models
(LLMs) for cyber threat detection; however, their opaque reasoning often limits
trust, particularly in decisions that require domain-specific cybersecurity
knowledge. Because security threats evolve rapidly, LLMs must not only recall
historical incidents but also adapt to emerging vulnerabilities and attack
patterns. Retrieval-Augmented Generation (RAG) has demonstrated effectiveness
in general LLM applications, but its potential for cybersecurity remains
underexplored. In this work, we introduce a RAG-based framework designed to
contextualize cybersecurity data and enhance LLM accuracy in knowledge
retention and temporal reasoning. Using external datasets and the
Llama-3-8B-Instruct model, we evaluate baseline RAG, an optimized hybrid
retrieval approach, and conduct a comparative analysis across multiple
performance metrics. Our findings highlight the promise of hybrid retrieval in
strengthening the adaptability and reliability of LLMs for cybersecurity tasks.

</details>


### [5] [Lightweight CNN Model Hashing with Higher-Order Statistics and Chaotic Mapping for Piracy Detection and Tamper Localization](https://arxiv.org/abs/2510.27127)
*Kunming Yang,Ling Chen*

Main category: cs.CR

TL;DR: 提出一种轻量级CNN模型哈希技术，结合高阶统计特征和混沌映射机制，无需额外神经网络训练即可实现高效盗版检测和精确篡改定位。


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络的广泛应用，保护模型知识产权和检测未经授权的篡改成为迫切挑战。现有方法要么需要大量训练资源，要么适用性有限，无法普遍应用于所有CNN。

Method: 从网络层参数中提取偏度、峰度和结构特征构建模型哈希，并引入混沌映射机制放大模型参数的微小变化，利用混沌系统对初始条件的敏感性实现精确篡改定位。

Result: 实验验证了该方法在模型版权保护和完整性验证方面的有效性和实用价值。

Conclusion: 该方法为CNN模型提供了一种轻量级、无需额外训练的有效保护方案，能够同时实现盗版检测和篡改定位。

Abstract: With the widespread adoption of deep neural networks (DNNs), protecting
intellectual property and detecting unauthorized tampering of models have
become pressing challenges. Recently, Perceptual hashing has emerged as an
effective approach for identifying pirated models. However, existing methods
either rely on neural networks for feature extraction, demanding substantial
training resources, or suffer from limited applicability and cannot be
universally applied to all convolutional neural networks (CNNs). To address
these limitations, we propose a lightweight CNN model hashing technique that
integrates higher-order statistics (HOS) features with a chaotic mapping
mechanism. Without requiring any auxiliary neural network training, our method
enables efficient piracy detection and precise tampering localization.
Specifically, we extract skewness, kurtosis, and structural features from the
parameters of each network layer to construct a model hash that is both robust
and discriminative. Additionally, we introduce chaotic mapping to amplify minor
changes in model parameters by exploiting the sensitivity of chaotic systems to
initial conditions, thereby facilitating accurate localization of tampered
regions. Experimental results validate the effectiveness and practical value of
the proposed method for model copyright protection and integrity verification.

</details>


### [6] [Measuring the Security of Mobile LLM Agents under Adversarial Prompts from Untrusted Third-Party Channels](https://arxiv.org/abs/2510.27140)
*Chenghao Du,Quanfeng Huang,Tingxuan Tang,Zihao Wang,Yue Xiao*

Main category: cs.CR

TL;DR: 本研究首次系统性地分析了移动LLM代理的安全风险，通过对抗性案例研究揭示了移动LLM代理在恶意环境中存在系统性漏洞，包括欺诈广告、恶意软件安装等攻击向量。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在软件开发中的广泛应用，LLM代理被部署在对抗性移动环境中，但其安全影响尚未得到充分理解，需要系统研究其安全风险。

Method: 设计并评估了一系列对抗性案例研究，涵盖从机会性操作到高级端到端工作流程的攻击，在三种架构的八个先进移动代理上进行了2000多次对抗性和良性试验。

Result: 发现系统性漏洞：欺诈广告等低门槛攻击向量成功率超过80%，即使需要绕过操作系统警告的恶意软件安装工作流程也能被高级多应用代理持续完成。

Conclusion: 研究首次提供了端到端证据，表明移动LLM代理在现实对抗环境中可被利用，不可信的第三方渠道是移动生态系统的固有部分，构成了独特的安全威胁。

Abstract: Large Language Models (LLMs) have transformed software development, enabling
AI-powered applications known as LLM-based agents that promise to automate
tasks across diverse apps and workflows. Yet, the security implications of
deploying such agents in adversarial mobile environments remain poorly
understood. In this paper, we present the first systematic study of security
risks in mobile LLM agents. We design and evaluate a suite of adversarial case
studies, ranging from opportunistic manipulations such as pop-up advertisements
to advanced, end-to-end workflows involving malware installation and cross-app
data exfiltration. Our evaluation covers eight state-of-the-art mobile agents
across three architectures, with over 2,000 adversarial and paired benign
trials. The results reveal systemic vulnerabilities: low-barrier vectors such
as fraudulent ads succeed with over 80% reliability, while even workflows
requiring the circumvention of operating-system warnings, such as malware
installation, are consistently completed by advanced multi-app agents. By
mapping these attacks to the MITRE ATT&CK Mobile framework, we uncover novel
privilege-escalation and persistence pathways unique to LLM-driven automation.
Collectively, our findings provide the first end-to-end evidence that mobile
LLM agents are exploitable in realistic adversarial settings, where untrusted
third-party channels (e.g., ads, embedded webviews, cross-app notifications)
are an inherent part of the mobile ecosystem.

</details>


### [7] [Unvalidated Trust: Cross-Stage Vulnerabilities in Large Language Model Architectures](https://arxiv.org/abs/2510.27190)
*Dominik Schwarz*

Main category: cs.CR

TL;DR: 本文提出了一个包含41种商业大语言模型重复风险模式的机制中心分类法，指出仅靠字符串级过滤不足以防范跨阶段漏洞，建议采用零信任架构原则来缓解这些风险。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地集成到自动化多阶段流程中，处理阶段间未经验证的信任所产生的风险模式成为实际关注点。

Method: 提出了一个机制中心的分类法来分析41种重复风险模式，并引入'Countermind'作为实施防御措施的概念蓝图。

Result: 分析显示输入经常被非中立地解释，即使没有明确指令也可能触发实现形状的响应或意外状态变化。

Conclusion: 这些行为构成了架构故障模式，建议采用零信任架构原则，包括来源执行、上下文密封和计划重新验证来缓解跨阶段漏洞。

Abstract: As Large Language Models (LLMs) are increasingly integrated into automated,
multi-stage pipelines, risk patterns that arise from unvalidated trust between
processing stages become a practical concern. This paper presents a
mechanism-centered taxonomy of 41 recurring risk patterns in commercial LLMs.
The analysis shows that inputs are often interpreted non-neutrally and can
trigger implementation-shaped responses or unintended state changes even
without explicit commands. We argue that these behaviors constitute
architectural failure modes and that string-level filtering alone is
insufficient. To mitigate such cross-stage vulnerabilities, we recommend
zero-trust architectural principles, including provenance enforcement, context
sealing, and plan revalidation, and we introduce "Countermind" as a conceptual
blueprint for implementing these defenses.

</details>


### [8] [Prevalence of Security and Privacy Risk-Inducing Usage of AI-based Conversational Agents](https://arxiv.org/abs/2510.27275)
*Kathrin Grosse,Nico Ebert*

Main category: cs.CR

TL;DR: 对3270名英国成年人的调查显示，三分之一的AI对话代理常规用户存在可能引发攻击风险的行为，四分之一尝试过越狱，但大多数用户不了解数据使用政策。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的普及，用户可能无意中通过上传恶意文件或泄露敏感信息引入安全风险，但此类用户行为的普遍程度尚不明确。

Method: 2024年通过Prolific平台对3270名英国成年人进行代表性抽样调查，重点关注使用ChatGPT或Gemini等对话代理的常规用户。

Result: 三分之一的常规用户表现出可能引发攻击的行为；四分之一尝试过越狱；一半用户表示会清理数据；大多数用户不分享敏感数据，但少数会分享密码等高度敏感信息；多数用户不了解数据会被用于模型训练且可以退出。

Conclusion: 当前学术威胁模型在现实中确实存在，需要开发安全使用指南，对话代理应配备有效的AI护栏，供应商需加强防止敏感数据输入并提高数据使用政策的透明度。

Abstract: Recent improvement gains in large language models (LLMs) have lead to
everyday usage of AI-based Conversational Agents (CAs). At the same time, LLMs
are vulnerable to an array of threats, including jailbreaks and, for example,
causing remote code execution when fed specific inputs. As a result, users may
unintentionally introduce risks, for example, by uploading malicious files or
disclosing sensitive information. However, the extent to which such user
behaviors occur and thus potentially facilitate exploits remains largely
unclear. To shed light on this issue, we surveyed a representative sample of
3,270 UK adults in 2024 using Prolific. A third of these use CA services such
as ChatGPT or Gemini at least once a week. Of these ``regular users'', up to a
third exhibited behaviors that may enable attacks, and a fourth have tried
jailbreaking (often out of understandable reasons such as curiosity, fun or
information seeking). Half state that they sanitize data and most participants
report not sharing sensitive data. However, few share very sensitive data such
as passwords. The majority are unaware that their data can be used to train
models and that they can opt-out. Our findings suggest that current academic
threat models manifest in the wild, and mitigations or guidelines for the
secure usage of CAs should be developed. In areas critical to security and
privacy, CAs must be equipped with effective AI guardrails to prevent, for
example, revealing sensitive information to curious employees. Vendors need to
increase efforts to prevent the entry of sensitive data, and to create
transparency with regard to data usage policies and settings.

</details>


### [9] [Sustaining Cyber Awareness: The Long-Term Impact of Continuous Phishing Training and Emotional Triggers](https://arxiv.org/abs/2510.27298)
*Rebeka Toth,Richard A. Dubniczky,Olga Limonova,Norbert Tihanyi*

Main category: cs.CR

TL;DR: 12个月纵向研究发现持续网络安全培训和情感线索能显著降低员工对钓鱼攻击的敏感性，在6个月内将成功入侵率减半。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击占全球成功网络攻击的90%以上，尽管网络安全预算增加了三倍，人为因素仍然是关键漏洞。

Method: 对20个组织的1300多名员工进行12个月纵向调查，发送超过13000封模拟钓鱼邮件，分析行为响应使用非参数相关和回归模型。

Result: 持续钓鱼模拟和针对性培训显著降低员工敏感性，员工流动导致安全意识水平波动。

Conclusion: 持续行为干预对增强组织网络韧性具有战略重要性，需要维持持续培训计划。

Abstract: Phishing constitutes more than 90\% of successful cyberattacks globally,
remaining one of the most persistent threats to organizational security.
Despite organizations tripling their cybersecurity budgets between 2015 and
2025, the human factor continues to pose a critical vulnerability. This study
presents a 12-month longitudinal investigation examining how continuous
cybersecurity training and emotional cues affect employee susceptibility to
phishing. The experiment involved 20 organizations and over 1,300 employees who
collectively received more than 13,000 simulated phishing emails engineered
with diverse emotional, contextual, and structural characteristics. Behavioral
responses were analyzed using non-parametric correlation and regression models
to assess the influence of psychological manipulation, message personalization,
and perceived email source. Results demonstrate that sustained phishing
simulations and targeted training programs lead to a significant reduction in
employee susceptibility, halving successful compromise rates within six months.
Additionally, employee turnover introduces measurable fluctuations in awareness
levels, underscoring the necessity of maintaining continuous training
initiatives. These findings provide one of the few long-term perspectives on
phishing awareness efficacy, highlighting the strategic importance of ongoing
behavioral interventions in strengthening organizational cyber resilience. In
order to support open science, we published our email templates, source code,
and other materials at https://github.com/CorporatePhishingStudy

</details>


### [10] [Coordinated Position Falsification Attacks and Countermeasures for Location-Based Services](https://arxiv.org/abs/2510.27346)
*Wenjie Liu,Panos Papadimitratos*

Main category: cs.CR

TL;DR: 提出了一种检测和防御低成本位置欺骗攻击的方法，通过整合多种定位信号（包括GNSS、传感器和地面基础设施）来增强位置服务的完整性监控。


<details>
  <summary>Details</summary>
Motivation: 随着基于位置服务(LBS)应用的普及，这些依赖地面和卫星基础设施的定位系统容易受到低成本攻击（如Wi-Fi欺骗和GNSS干扰），导致用户被诈骗或服务被操控。

Method: 扩展了接收机自主完整性监控(RAIM)框架，整合了机会性信息，包括机载传感器数据、地面基础设施信号以及GNSS信号，通过融合异构信号来提高对抗复杂攻击的韧性。

Result: 实验评估显示，与基线方案相比，该方法最多可将检测准确率提高62%，并能恢复准确的定位。

Conclusion: 通过融合多种定位信号，可以有效检测和防御位置欺骗攻击，显著提高位置服务的完整性和安全性。

Abstract: With the rise of location-based service (LBS) applications that rely on
terrestrial and satellite infrastructures (e.g., GNSS and crowd-sourced Wi-Fi,
Bluetooth, cellular, and IP databases) for positioning, ensuring their
integrity and security is paramount. However, we demonstrate that these
applications are susceptible to low-cost attacks (less than $50), including
Wi-Fi spoofing combined with GNSS jamming, as well as more sophisticated
coordinated location spoofing. These attacks manipulate position data to
control or undermine LBS functionality, leading to user scams or service
manipulation. Therefore, we propose a countermeasure to detect and thwart such
attacks by utilizing readily available, redundant positioning information from
off-the-shelf platforms. Our method extends the receiver autonomous integrity
monitoring (RAIM) framework by incorporating opportunistic information,
including data from onboard sensors and terrestrial infrastructure signals,
and, naturally, GNSS. We theoretically show that the fusion of heterogeneous
signals improves resilience against sophisticated adversaries on multiple
fronts. Experimental evaluations show the effectiveness of the proposed scheme
in improving detection accuracy by 62% at most compared to baseline schemes and
restoring accurate positioning.

</details>


### [11] [Sockeye: a language for analyzing hardware documentation](https://arxiv.org/abs/2510.27485)
*Ben Fiedler,Samuel Gruetter,Timothy Roscoe*

Main category: cs.CR

TL;DR: 提出一种领域特定语言来描述硬件语义、软件行为假设和所需安全属性，为8种SoC创建机器可读规范，并形式化证明其（不）安全性，发现文档错误和真实服务器芯片漏洞。


<details>
  <summary>Details</summary>
Motivation: 现代SoC硬件复杂性增加，组件语义描述不精确且容易出错，难以对平台安全性做出严格声明。

Method: 开发领域特定语言描述硬件语义和安全性，为8种不同SoC创建机器可读规范，进行形式化验证。

Result: 证明了内存机密性和完整性，发现多个文档错误，并在真实服务器芯片中发现漏洞。

Conclusion: 该工具为系统集成商提供了形式化描述SoC安全属性和验证的方法。

Abstract: Systems programmers have to consolidate the ever growing hardware mess
present on modern System-on-Chips (SoCs). Correctly programming a multitude of
components, providing functionality but also security, is a difficult problem:
semantics of individual units are described in English prose, descriptions are
often underspecified, and prone to inaccuracies. Rigorous statements about
platform security are often impossible.
  We introduce a domain-specific language to describe hardware semantics,
assumptions about software behavior, and desired security properties. We then
create machine-readable specifications for a diverse set of eight SoCs from
their reference manuals, and formally prove their (in-)security. In addition to
security proofs about memory confidentiality and integrity, we discover a
handful of documentation errors. Finally, our analysis also revealed a
vulnerability on a real-world server chip. Our tooling offers system
integrators a way of formally describing security properties for entire SoCs,
and means to prove them or find counterexamples to them.

</details>


### [12] [Sybil-Resistant Service Discovery for Agent Economies](https://arxiv.org/abs/2510.27554)
*David Shi,Kevin Joo*

Main category: cs.CR

TL;DR: TraceRank是一个基于声誉加权的排名算法，通过支付交易作为背书，为x402协议上的HTTP服务提供发现和排名机制，旨在抵抗Sybil攻击并提供比纯交易量或语义方法更好的性能。


<details>
  <summary>Details</summary>
Motivation: 随着代理越来越多地消费x402协议上的HTTP服务（如API、数据源和推理提供商），服务发现变得至关重要。需要一种能够识别可信和可靠服务的方法，而不仅仅是基于交易量的排名。

Method: TraceRank算法使用预计算的声誉指标初始化地址，通过支付流传播声誉，权重基于交易价值和时间新近性。系统结合TraceRank与语义搜索，响应自然语言查询。

Result: 该系统能够根据高声誉用户的偏好发现服务，而不是仅仅基于高交易量。声誉传播机制能够抵抗Sybil攻击，使具有许多低声誉支付者的垃圾服务排名低于具有少量高声誉支付者的合法服务。

Conclusion: TraceRank构建了一种搜索x402服务的方法，避免了基础设施偏见，相比纯基于交易量或语义的方法具有更好的性能。

Abstract: x402 enables Hypertext Transfer Protocol (HTTP) services like application
programming interfaces (APIs), data feeds, and inference providers to accept
cryptocurrency payments for access. As agents increasingly consume these
services, discovery becomes critical: which swap interface should an agent
trust? Which data provider is the most reliable? We introduce TraceRank, a
reputation-weighted ranking algorithm where payment transactions serve as
endorsements. TraceRank seeds addresses with precomputed reputation metrics and
propagates reputation through payment flows weighted by transaction value and
temporal recency. Applied to x402's payment graph, this surfaces services
preferred by high-reputation users rather than those with high transaction
volume. Our system combines TraceRank with semantic search to respond to
natural language queries with high quality results. We argue that reputation
propagation resists Sybil attacks by making spam services with many
low-reputation payers rank below legitimate services with few high-reputation
payers. Ultimately, we aim to construct a search method for x402 enabled
services that avoids infrastructure bias and has better performance than purely
volume based or semantic methods.

</details>


### [13] [Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation Models](https://arxiv.org/abs/2510.27629)
*Boyi Wei,Zora Che,Nathaniel Li,Udari Madhushani Sehwag,Jasper Götting,Samira Nedungadi,Julian Michael,Summer Yue,Dan Hendrycks,Peter Henderson,Zifan Wang,Seth Donoughe,Mantas Mazeika*

Main category: cs.CR

TL;DR: 本文提出了一个评估框架来测试生物基础模型的双重用途风险缓解措施的有效性，发现当前的数据过滤方法可能不够有效，被排除的知识可以通过微调快速恢复。


<details>
  <summary>Details</summary>
Motivation: 开放权重的生物基础模型存在双重用途困境，既可能加速科学研究，也可能被恶意行为者用于开发生物武器。当前基于数据过滤的风险缓解方法效果尚不明确。

Method: 提出了\eval评估框架，通过三个维度评估模型：序列建模、突变效应预测和毒力预测，测试过滤知识的可恢复性和模型表示中双重用途信号的存在。

Result: 当前过滤实践效果有限：被排除的知识可以通过微调快速恢复，在序列建模中表现出更广泛的泛化能力；双重用途信号已存在于预训练表示中，可通过简单的线性探测提取。

Conclusion: 数据过滤作为独立程序面临挑战，需要进一步研究开放权重生物基础模型的稳健安全策略。

Abstract: Open-weight bio-foundation models present a dual-use dilemma. While holding
great promise for accelerating scientific research and drug development, they
could also enable bad actors to develop more deadly bioweapons. To mitigate the
risk posed by these models, current approaches focus on filtering biohazardous
data during pre-training. However, the effectiveness of such an approach
remains unclear, particularly against determined actors who might fine-tune
these models for malicious use. To address this gap, we propose \eval, a
framework to evaluate the robustness of procedures that are intended to reduce
the dual-use capabilities of bio-foundation models. \eval assesses models'
virus understanding through three lenses, including sequence modeling,
mutational effects prediction, and virulence prediction. Our results show that
current filtering practices may not be particularly effective: Excluded
knowledge can be rapidly recovered in some cases via fine-tuning, and exhibits
broader generalizability in sequence modeling. Furthermore, dual-use signals
may already reside in the pretrained representations, and can be elicited via
simple linear probing. These findings highlight the challenges of data
filtering as a standalone procedure, underscoring the need for further research
into robust safety and security strategies for open-weight bio-foundation
models.

</details>
