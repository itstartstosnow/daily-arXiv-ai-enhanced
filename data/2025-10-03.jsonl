{"id": "2510.01223", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01223", "abs": "https://arxiv.org/abs/2510.01223", "authors": ["Hui Dou", "Ning Xu", "Yiwen Zhang", "Kaibin Wang"], "title": "Jailbreaking LLMs via Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious tasks. However, they remain exposed to jailbreak attacks, eliciting\nharmful responses. The nested scenario strategy has been increasingly adopted\nacross various methods, demonstrating immense potential. Nevertheless, these\nmethods are easily detectable due to their prominent malicious intentions. In\nthis work, we are the first to find and systematically verify that LLMs'\nalignment defenses are not sensitive to nested scenarios, where these scenarios\nare highly semantically relevant to the queries and incorporate targeted toxic\nknowledge. This is a crucial yet insufficiently explored direction. Based on\nthis, we propose RTS-Attack (Semantically Relevant Nested Scenarios with\nTargeted Toxic Knowledge), an adaptive and automated framework to examine LLMs'\nalignment. By building scenarios highly relevant to the queries and integrating\ntargeted toxic knowledge, RTS-Attack bypasses the alignment defenses of LLMs.\nMoreover, the jailbreak prompts generated by RTS-Attack are free from harmful\nqueries, leading to outstanding concealment. Extensive experiments demonstrate\nthat RTS-Attack exhibits superior performance in both efficiency and\nuniversality compared to the baselines across diverse advanced LLMs, including\nGPT-4o, Llama3-70b, and Gemini-pro. Our complete code is available in the\nsupplementary material. WARNING: THIS PAPER CONTAINS POTENTIALLY HARMFUL\nCONTENT."}
{"id": "2510.01342", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01342", "abs": "https://arxiv.org/abs/2510.01342", "authors": ["Xiangfang Li", "Yu Wang", "Bo Li"], "title": "Fine-Tuning Jailbreaks under Highly Constrained Black-Box Settings: A Three-Pronged Approach", "comment": null, "summary": "With the rapid advancement of large language models (LLMs), ensuring their\nsafe use becomes increasingly critical. Fine-tuning is a widely used method for\nadapting models to downstream tasks, yet it is vulnerable to jailbreak attacks.\nHowever, most existing studies focus on overly simplified attack scenarios,\nlimiting their practical relevance to real-world defense settings. To make this\nrisk concrete, we present a three-pronged jailbreak attack and evaluate it\nagainst provider defenses under a dataset-only black-box fine-tuning interface.\nIn this setting, the attacker can only submit fine-tuning data to the provider,\nwhile the provider may deploy defenses across stages: (1) pre-upload data\nfiltering, (2) training-time defensive fine-tuning, and (3) post-training\nsafety audit. Our attack combines safety-styled prefix/suffix wrappers, benign\nlexical encodings (underscoring) of sensitive tokens, and a backdoor mechanism,\nenabling the model to learn harmful behaviors while individual datapoints\nappear innocuous. Extensive experiments demonstrate the effectiveness of our\napproach. In real-world deployment, our method successfully jailbreaks GPT-4.1\nand GPT-4o on the OpenAI platform with attack success rates above 97% for both\nmodels. Our code is available at\nhttps://github.com/lxf728/tri-pronged-ft-attack."}
{"id": "2510.01350", "categories": ["cs.CR", "cs.AR", "cs.ET", "cs.NE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.01350", "abs": "https://arxiv.org/abs/2510.01350", "authors": ["Muhammad Faheemur Rahman", "Wayne Burleson"], "title": "Integrated Security Mechanisms for Weight Protection in Memristive Crossbar Arrays", "comment": "2 pages, 2 figures", "summary": "Memristive crossbar arrays enable in-memory computing by performing parallel\nanalog computations directly within memory, making them well-suited for machine\nlearning, neural networks, and neuromorphic systems. However, despite their\nadvantages, non-volatile memristors are vulnerable to security threats (such as\nadversarial extraction of stored weights when the hardware is compromised.\nProtecting these weights is essential since they represent valuable\nintellectual property resulting from lengthy and costly training processes\nusing large, often proprietary, datasets. As a solution we propose two security\nmechanisms: Keyed Permutor and Watermark Protection Columns; where both\nsafeguard critical weights and establish verifiable ownership (even in cases of\ndata leakage). Our approach integrates efficiently with existing memristive\ncrossbar architectures without significant design modifications. Simulations\nacross 45nm, 22nm, and 7nm CMOS nodes, using a realistic interconnect model and\na large RF dataset, show that both mechanisms offer robust protection with\nunder 10% overhead in area, delay and power. We also present initial\nexperiments employing the widely known MNIST dataset; further highlighting the\nfeasibility of securing memristive in-memory computing systems with minimal\nperformance trade-offs."}
{"id": "2510.01354", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01354", "abs": "https://arxiv.org/abs/2510.01354", "authors": ["Yinuo Liu", "Ruohan Xu", "Xilong Wang", "Yuqi Jia", "Neil Zhenqiang Gong"], "title": "WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents", "comment": null, "summary": "Multiple prompt injection attacks have been proposed against web agents. At\nthe same time, various methods have been developed to detect general prompt\ninjection attacks, but none have been systematically evaluated for web agents.\nIn this work, we bridge this gap by presenting the first comprehensive\nbenchmark study on detecting prompt injection attacks targeting web agents. We\nbegin by introducing a fine-grained categorization of such attacks based on the\nthreat model. We then construct datasets containing both malicious and benign\nsamples: malicious text segments generated by different attacks, benign text\nsegments from four categories, malicious images produced by attacks, and benign\nimages from two categories. Next, we systematize both text-based and\nimage-based detection methods. Finally, we evaluate their performance across\nmultiple scenarios. Our key findings show that while some detectors can\nidentify attacks that rely on explicit textual instructions or visible image\nperturbations with moderate to high accuracy, they largely fail against attacks\nthat omit explicit instructions or employ imperceptible perturbations. Our\ndatasets and code are released at:\nhttps://github.com/Norrrrrrr-lyn/WAInjectBench."}
{"id": "2510.01359", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01359", "abs": "https://arxiv.org/abs/2510.01359", "authors": ["Shoumik Saha", "Jifan Chen", "Sam Mayers", "Sanjay Krishna Gouda", "Zijian Wang", "Varun Kumar"], "title": "Breaking the Code: Security Assessment of AI Code Agents Through Systematic Jailbreaking Attacks", "comment": "28 pages, 21 figures, 9 tables", "summary": "Code-capable large language model (LLM) agents are increasingly embedded into\nsoftware engineering workflows where they can read, write, and execute code,\nraising the stakes of safety-bypass (\"jailbreak\") attacks beyond text-only\nsettings. Prior evaluations emphasize refusal or harmful-text detection,\nleaving open whether agents actually compile and run malicious programs. We\npresent JAWS-BENCH (Jailbreaks Across WorkSpaces), a benchmark spanning three\nescalating workspace regimes that mirror attacker capability: empty (JAWS-0),\nsingle-file (JAWS-1), and multi-file (JAWS-M). We pair this with a\nhierarchical, executable-aware Judge Framework that tests (i) compliance, (ii)\nattack success, (iii) syntactic correctness, and (iv) runtime executability,\nmoving beyond refusal to measure deployable harm. Using seven LLMs from five\nfamilies as backends, we find that under prompt-only conditions in JAWS-0, code\nagents accept 61% of attacks on average; 58% are harmful, 52% parse, and 27%\nrun end-to-end. Moving to single-file regime in JAWS-1 drives compliance to ~\n100% for capable models and yields a mean ASR (Attack Success Rate) ~ 71%; the\nmulti-file regime (JAWS-M) raises mean ASR to ~ 75%, with 32% instantly\ndeployable attack code. Across models, wrapping an LLM in an agent\nsubstantially increases vulnerability -- ASR raises by 1.6x -- because initial\nrefusals are frequently overturned during later planning/tool-use steps.\nCategory-level analyses identify which attack classes are most vulnerable and\nmost readily deployable, while others exhibit large execution gaps. These\nfindings motivate execution-aware defenses, code-contextual safety filters, and\nmechanisms that preserve refusal decisions throughout the agent's multi-step\nreasoning and tool use."}
{"id": "2510.01393", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01393", "abs": "https://arxiv.org/abs/2510.01393", "authors": ["Davide Rusconi", "Osama Yousef", "Mirco Picca", "Flavio Toffalini", "Andrea Lanzi"], "title": "E-FuzzEdge: Optimizing Embedded Device Security with Scalable In-Place Fuzzing", "comment": null, "summary": "In this paper we show E-FuzzEdge, a novel fuzzing architecture targeted\ntowards improving the throughput of fuzzing campaigns in contexts where\nscalability is unavailable. E-FuzzEdge addresses the inefficiencies of\nhardware-in-the-loop fuzzing for microcontrollers by optimizing execution\nspeed. We evaluated our system against state-of-the-art benchmarks,\ndemonstrating significant performance improvements. A key advantage of\nE-FuzzEdgearchitecture is its compatibility with other embedded fuzzing\ntechniques that perform on device testing instead of firmware emulation. This\nmeans that the broader embedded fuzzing community can integrate E-FuzzEdge into\ntheir workflows to enhance overall testing efficiency."}
{"id": "2510.01445", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01445", "abs": "https://arxiv.org/abs/2510.01445", "authors": ["Andrés F. Betancur-López"], "title": "Securing IoT Devices in Smart Cities: A Review of Proposed Solutions", "comment": "14 pages, 7 figures", "summary": "Privacy and security in Smart Cities remain at constant risk due to the\nvulnerabilities introduced by Internet of Things (IoT) devices. The limited\ncomputational resources of these devices make them especially susceptible to\nattacks, while their widespread adoption increases the potential impact of\nsecurity breaches. This article presents a review of security proposals aimed\nat protecting IoT devices in Smart City environments. The review was conducted\nby analyzing recent literature on device-level security, with particular\nemphasis on lightweight cryptography, physically unclonable functions (PUFs),\nand blockchain-based solutions. Findings highlight both the strengths and\nlimitations of current approaches, as well as the need for more practical,\nscalable, and resource-efficient mechanisms to ensure user privacy and data\nprotection in IoT ecosystems."}
{"id": "2510.01552", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01552", "abs": "https://arxiv.org/abs/2510.01552", "authors": ["Luoxi Tang", "Yuqiao Meng", "Ankita Patra", "Weicheng Ma", "Muchao Ye", "Zhaohan Xi"], "title": "POLAR: Automating Cyber Threat Prioritization through LLM-Powered Assessment", "comment": "25 pages", "summary": "Large Language Models (LLMs) are intensively used to assist security analysts\nin counteracting the rapid exploitation of cyber threats, wherein LLMs offer\ncyber threat intelligence (CTI) to support vulnerability assessment and\nincident response. While recent work has shown that LLMs can support a wide\nrange of CTI tasks such as threat analysis, vulnerability detection, and\nintrusion defense, significant performance gaps persist in practical\ndeployments. In this paper, we investigate the intrinsic vulnerabilities of\nLLMs in CTI, focusing on challenges that arise from the nature of the threat\nlandscape itself rather than the model architecture. Using large-scale\nevaluations across multiple CTI benchmarks and real-world threat reports, we\nintroduce a novel categorization methodology that integrates stratification,\nautoregressive refinement, and human-in-the-loop supervision to reliably\nanalyze failure instances. Through extensive experiments and human inspections,\nwe reveal three fundamental vulnerabilities: spurious correlations,\ncontradictory knowledge, and constrained generalization, that limit LLMs in\neffectively supporting CTI. Subsequently, we provide actionable insights for\ndesigning more robust LLM-powered CTI systems to facilitate future research."}
{"id": "2510.01645", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01645", "abs": "https://arxiv.org/abs/2510.01645", "authors": ["Niloofar Mireshghallah", "Tianshi Li"], "title": "Position: Privacy Is Not Just Memorization!", "comment": "27 pages, 6 figures, 2 tables", "summary": "The discourse on privacy risks in Large Language Models (LLMs) has\ndisproportionately focused on verbatim memorization of training data, while a\nconstellation of more immediate and scalable privacy threats remain\nunderexplored. This position paper argues that the privacy landscape of LLM\nsystems extends far beyond training data extraction, encompassing risks from\ndata collection practices, inference-time context leakage, autonomous agent\ncapabilities, and the democratization of surveillance through deep inference\nattacks. We present a comprehensive taxonomy of privacy risks across the LLM\nlifecycle -- from data collection through deployment -- and demonstrate through\ncase studies how current privacy frameworks fail to address these multifaceted\nthreats. Through a longitudinal analysis of 1,322 AI/ML privacy papers\npublished at leading conferences over the past decade (2016--2025), we reveal\nthat while memorization receives outsized attention in technical research, the\nmost pressing privacy harms lie elsewhere, where current technical approaches\noffer little traction and viable paths forward remain unclear. We call for a\nfundamental shift in how the research community approaches LLM privacy, moving\nbeyond the narrow focus of current technical solutions and embracing\ninterdisciplinary approaches that address the sociotechnical nature of these\nemerging threats."}
{"id": "2510.01676", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01676", "abs": "https://arxiv.org/abs/2510.01676", "authors": ["Milad Nasr", "Yanick Fratantonio", "Luca Invernizzi", "Ange Albertini", "Loua Farah", "Alex Petit-Bianco", "Andreas Terzis", "Kurt Thomas", "Elie Bursztein", "Nicholas Carlini"], "title": "Evaluating the Robustness of a Production Malware Detection System to Transferable Adversarial Attacks", "comment": null, "summary": "As deep learning models become widely deployed as components within larger\nproduction systems, their individual shortcomings can create system-level\nvulnerabilities with real-world impact. This paper studies how adversarial\nattacks targeting an ML component can degrade or bypass an entire\nproduction-grade malware detection system, performing a case study analysis of\nGmail's pipeline where file-type identification relies on a ML model.\n  The malware detection pipeline in use by Gmail contains a machine learning\nmodel that routes each potential malware sample to a specialized malware\nclassifier to improve accuracy and performance. This model, called Magika, has\nbeen open sourced. By designing adversarial examples that fool Magika, we can\ncause the production malware service to incorrectly route malware to an\nunsuitable malware detector thereby increasing our chance of evading detection.\nSpecifically, by changing just 13 bytes of a malware sample, we can\nsuccessfully evade Magika in 90% of cases and thereby allow us to send malware\nfiles over Gmail. We then turn our attention to defenses, and develop an\napproach to mitigate the severity of these types of attacks. For our defended\nproduction model, a highly resourced adversary requires 50 bytes to achieve\njust a 20% attack success rate. We implement this defense, and, thanks to a\ncollaboration with Google engineers, it has already been deployed in production\nfor the Gmail classifier."}
{"id": "2510.01699", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01699", "abs": "https://arxiv.org/abs/2510.01699", "authors": ["Yue Li", "Linying Xue", "Dongdong Lin", "Qiushi Li", "Hui Tian", "Hongxia Wang"], "title": "Towards Imperceptible Adversarial Defense: A Gradient-Driven Shield against Facial Manipulations", "comment": null, "summary": "With the flourishing prosperity of generative models, manipulated facial\nimages have become increasingly accessible, raising concerns regarding privacy\ninfringement and societal trust. In response, proactive defense strategies\nembed adversarial perturbations into facial images to counter deepfake\nmanipulation. However, existing methods often face a tradeoff between\nimperceptibility and defense effectiveness-strong perturbations may disrupt\nforgeries but degrade visual fidelity. Recent studies have attempted to address\nthis issue by introducing additional visual loss constraints, yet often\noverlook the underlying gradient conflicts among losses, ultimately weakening\ndefense performance. To bridge the gap, we propose a gradient-projection-based\nadversarial proactive defense (GRASP) method that effectively counters facial\ndeepfakes while minimizing perceptual degradation. GRASP is the first approach\nto successfully integrate both structural similarity loss and low-frequency\nloss to enhance perturbation imperceptibility. By analyzing gradient conflicts\nbetween defense effectiveness loss and visual quality losses, GRASP pioneers\nthe design of the gradient-projection mechanism to mitigate these conflicts,\nenabling balanced optimization that preserves image fidelity without\nsacrificing defensive performance. Extensive experiments validate the efficacy\nof GRASP, achieving a PSNR exceeding 40 dB, SSIM of 0.99, and a 100% defense\nsuccess rate against facial attribute manipulations, significantly\noutperforming existing approaches in visual quality."}
{"id": "2510.01720", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01720", "abs": "https://arxiv.org/abs/2510.01720", "authors": ["Palash Sarkar"], "title": "Constructions of Efficiently Implementable Boolean Functions with Provable Nonlinearity/Resiliency/Algebraic Immunity Trade-Offs", "comment": null, "summary": "We describe several families of efficiently implementable Boolean functions\nachieving provable trade-offs between resiliency, nonlinearity, and algebraic\nimmunity. In concrete terms, the following result holds for each of the\nfunction families that we propose. Given integers $m_0\\geq 0$, $x_0\\geq 1$, and\n$a_0\\geq 1$, it is possible to construct an $n$-variable function which has\nresiliency at least $m_0$, linear bias (which is an equivalent method of\nexpressing nonlinearity) at most $2^{-x_0}$ and algebraic immunity at least\n$a_0$; further, $n$ is linear in $m_0$, $x_0$ and $a_0$, and the function can\nbe implemented using $O(n)$ gates."}
{"id": "2510.01780", "categories": ["cs.CR", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01780", "abs": "https://arxiv.org/abs/2510.01780", "authors": ["Aueaphum Aueawatthanaphisut"], "title": "Secure Multi-Modal Data Fusion in Federated Digital Health Systems via MCP", "comment": "6 pages, 8 figures, 7 equations, 1 algorithm", "summary": "Secure and interoperable integration of heterogeneous medical data remains a\ngrand challenge in digital health. Current federated learning (FL) frameworks\noffer privacy-preserving model training but lack standardized mechanisms to\norchestrate multi-modal data fusion across distributed and resource-constrained\nenvironments. This study introduces a novel framework that leverages the Model\nContext Protocol (MCP) as an interoperability layer for secure, cross-agent\ncommunication in multi-modal federated healthcare systems. The proposed\narchitecture unifies three pillars: (i) multi-modal feature alignment for\nclinical imaging, electronic medical records, and wearable IoT data; (ii)\nsecure aggregation with differential privacy to protect patient-sensitive\nupdates; and (iii) energy-aware scheduling to mitigate dropouts in mobile\nclients. By employing MCP as a schema-driven interface, the framework enables\nadaptive orchestration of AI agents and toolchains while ensuring compliance\nwith privacy regulations. Experimental evaluation on benchmark datasets and\npilot clinical cohorts demonstrates up to 9.8\\% improvement in diagnostic\naccuracy compared with baseline FL, a 54\\% reduction in client dropout rates,\nand clinically acceptable privacy--utility trade-offs. These results highlight\nMCP-enabled multi-modal fusion as a scalable and trustworthy pathway toward\nequitable, next-generation federated health infrastructures."}
{"id": "2510.01967", "categories": ["cs.CR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01967", "abs": "https://arxiv.org/abs/2510.01967", "authors": ["Aadarsh Anantha Ramakrishnan", "Shubham Agarwal", "Selvanayagam S", "Kunwar Singh"], "title": "ZK-WAGON: Imperceptible Watermark for Image Generation Models using ZK-SNARKs", "comment": "Accepted at AI-ML Systems 2025, Bangalore, India,\n  https://www.aimlsystems.org/2025/", "summary": "As image generation models grow increasingly powerful and accessible,\nconcerns around authenticity, ownership, and misuse of synthetic media have\nbecome critical. The ability to generate lifelike images indistinguishable from\nreal ones introduces risks such as misinformation, deepfakes, and intellectual\nproperty violations. Traditional watermarking methods either degrade image\nquality, are easily removed, or require access to confidential model internals\n- making them unsuitable for secure and scalable deployment. We are the first\nto introduce ZK-WAGON, a novel system for watermarking image generation models\nusing the Zero-Knowledge Succinct Non Interactive Argument of Knowledge\n(ZK-SNARKs). Our approach enables verifiable proof of origin without exposing\nmodel weights, generation prompts, or any sensitive internal information. We\npropose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively\nconvert key layers of an image generation model into a circuit, reducing proof\ngeneration time significantly. Generated ZK-SNARK proofs are imperceptibly\nembedded into a generated image via Least Significant Bit (LSB) steganography.\nWe demonstrate this system on both GAN and Diffusion models, providing a\nsecure, model-agnostic pipeline for trustworthy AI image generation."}
{"id": "2510.02158", "categories": ["cs.CR", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.02158", "abs": "https://arxiv.org/abs/2510.02158", "authors": ["Junjie Su", "Weifei Jin", "Yuxin Cao", "Derui Wang", "Kai Ye", "Jie Hao"], "title": "Mirage Fools the Ear, Mute Hides the Truth: Precise Targeted Adversarial Attacks on Polyphonic Sound Event Detection Systems", "comment": null, "summary": "Sound Event Detection (SED) systems are increasingly deployed in\nsafety-critical applications such as industrial monitoring and audio\nsurveillance. However, their robustness against adversarial attacks has not\nbeen well explored. Existing audio adversarial attacks targeting SED systems,\nwhich incorporate both detection and localization capabilities, often lack\neffectiveness due to SED's strong contextual dependencies or lack precision by\nfocusing solely on misclassifying the target region as the target event,\ninadvertently affecting non-target regions. To address these challenges, we\npropose the Mirage and Mute Attack (M2A) framework, which is designed for\ntargeted adversarial attacks on polyphonic SED systems. In our optimization\nprocess, we impose specific constraints on the non-target output, which we\nrefer to as preservation loss, ensuring that our attack does not alter the\nmodel outputs for non-target region, thus achieving precise attacks.\nFurthermore, we introduce a novel evaluation metric Editing Precison (EP) that\nbalances effectiveness and precision, enabling our method to simultaneously\nenhance both. Comprehensive experiments show that M2A achieves 94.56% and\n99.11% EP on two state-of-the-art SED models, demonstrating that the framework\nis sufficiently effective while significantly enhancing attack precision."}
{"id": "2510.02162", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02162", "abs": "https://arxiv.org/abs/2510.02162", "authors": ["Cristian Bassotto", "Ermes Franch", "Marina Krček", "Stjepan Picek"], "title": "NoMod: A Non-modular Attack on Module Learning With Errors", "comment": null, "summary": "The advent of quantum computing threatens classical public-key cryptography,\nmotivating NIST's adoption of post-quantum schemes such as those based on the\nModule Learning With Errors (Module-LWE) problem. We present NoMod ML-Attack, a\nhybrid white-box cryptanalytic method that circumvents the challenge of\nmodeling modular reduction by treating wrap-arounds as statistical corruption\nand casting secret recovery as robust linear estimation. Our approach combines\noptimized lattice preprocessing--including reduced-vector saving and algebraic\namplification--with robust estimators trained via Tukey's Biweight loss.\nExperiments show NoMod achieves full recovery of binary secrets for dimension\n$n = 350$, recovery of sparse binomial secrets for $n = 256$, and successful\nrecovery of sparse secrets in CRYSTALS-Kyber settings with parameters $(n, k) =\n(128, 3)$ and $(256, 2)$. We release our implementation in an anonymous\nrepository https://anonymous.4open.science/r/NoMod-3BD4."}
{"id": "2510.02184", "categories": ["cs.CR", "nlin.CD", "34C15, 68M25, 94A60"], "pdf": "https://arxiv.org/pdf/2510.02184", "abs": "https://arxiv.org/abs/2510.02184", "authors": ["N. A. Anagnostopoulos", "K. Konstantinidis", "A. N. Miliou", "S. G. Stavrinides"], "title": "Testing Stability and Robustness in Three Cryptographic Chaotic Systems", "comment": "Published as \"N. A. Anagnostopoulos, K. Konstantinidis, A. N. Miliou\n  & S. G. Stavrinides, \"Testing Stability and Robustness in Three Cryptographic\n  Chaotic Systems\", Proceedings of the 3rd International Interdisciplinary\n  Symposium on Chaos and Complex Systems (CCS 2010), Journal of Concrete And\n  Applicable Mathematics (JCAAM), vol. 9, iss. 3, pp. 247-261, Eudoxus Press,\n  2011\"; no longer available", "summary": "In practical applications, it is crucial that the drive-response systems,\nalthough identical in all respects, are synchronized at all times, even if\nthere is noise present. In this work, we test the stability and robustness of\nthree distinct and well-known cryptographic chaotic systems, and compare the\nresults in relation to the desired security."}
{"id": "2510.02196", "categories": ["cs.CR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.02196", "abs": "https://arxiv.org/abs/2510.02196", "authors": ["Jason Anderson"], "title": "Authentication Security of PRF GNSS Ranging", "comment": null, "summary": "This work derives the authentication security of pseudorandom function (PRF)\nGNSS ranging under multiple GNSS spoofing models, including the Security Code\nEstimation and Replay (SCER) spoofer. When GNSS ranging codes derive from a PRF\nutilizing a secret known only to the broadcaster, the spoofer cannot predict\nthe ranging code before broadcast. Therefore, PRF ranging can be used to\nestablish trust in the GNSS pseudoranges and the resulting receiver position,\nnavigation, and timing (PNT) solution. I apply the methods herein to Galileo's\nSignal Authentication Service (SAS) utilizing the encrypted Galileo E6-C signal\nto compute that, at most, 400 ms of Galileo E6-C data to assert 128-bit\nauthentication security under non-SCER models. For the SCER adversary, I\npredict the adversary's needed receiving radio equipment to break\nauthentication security. One can use this work to design a PRF GNSS ranging\nprotocol to meet useful authentication security requirements by computing the\nprobability of missed detection."}
{"id": "2510.02280", "categories": ["cs.CR", "math.NT"], "pdf": "https://arxiv.org/pdf/2510.02280", "abs": "https://arxiv.org/abs/2510.02280", "authors": ["Jean-Francois Biasse", "Fang Song"], "title": "An efficient quantum algorithm for computing $S$-units and its applications", "comment": "Long version of a paper from SODA 2016", "summary": "In this paper, we provide details on the proofs of the quantum polynomial\ntime algorithm of Biasse and Song (SODA 16) for computing the $S$-unit group of\na number field. This algorithm directly implies polynomial time methods to\ncalculate class groups, S-class groups, relative class group and the unit\ngroup, ray class groups, solve the principal ideal problem, solve certain norm\nequations, and decompose ideal classes in the ideal class group. Additionally,\ncombined with a result of Cramer, Ducas, Peikert and Regev (Eurocrypt 2016),\nthe resolution of the principal ideal problem allows one to find short\ngenerators of a principal ideal. Likewise, methods due to Cramer, Ducas and\nWesolowski (Eurocrypt 2017) use the resolution of the principal ideal problem\nand the decomposition of ideal classes to find so-called ``mildly short\nvectors'' in ideal lattices of cyclotomic fields."}
