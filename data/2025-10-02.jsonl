{"id": "2510.00151", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00151", "abs": "https://arxiv.org/abs/2510.00151", "authors": ["Valentin Barbaza", "Alan Rodrigo Diaz-Rizo", "Hassan Aboushady", "Spyridon Raptis", "Haralampos-G. Stratigopoulos"], "title": "Stealing AI Model Weights Through Covert Communication Channels", "comment": null, "summary": "AI models are often regarded as valuable intellectual property due to the\nhigh cost of their development, the competitive advantage they provide, and the\nproprietary techniques involved in their creation. As a result, AI model\nstealing attacks pose a serious concern for AI model providers. In this work,\nwe present a novel attack targeting wireless devices equipped with AI hardware\naccelerators. The attack unfolds in two phases. In the first phase, the\nvictim's device is compromised with a hardware Trojan (HT) designed to covertly\nleak model weights through a hidden communication channel, without the victim\nrealizing it. In the second phase, the adversary uses a nearby wireless device\nto intercept the victim's transmission frames during normal operation and\nincrementally reconstruct the complete weight matrix. The proposed attack is\nagnostic to both the AI model architecture and the hardware accelerator used.\nWe validate our approach through a hardware-based demonstration involving four\ndiverse AI models of varying types and sizes. We detail the design of the HT\nand the covert channel, highlighting their stealthy nature. Additionally, we\nanalyze the impact of bit error rates on the reception and propose an error\nmitigation technique. The effectiveness of the attack is evaluated based on the\naccuracy of the reconstructed models with stolen weights and the time required\nto extract them. Finally, we explore potential defense mechanisms."}
{"id": "2510.00164", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.00164", "abs": "https://arxiv.org/abs/2510.00164", "authors": ["Dominik Apel", "Zeta Avarikioti", "Matteo Maffei", "Yuheng Wang"], "title": "Calyx: Privacy-Preserving Multi-Token Optimistic-Rollup Protocol", "comment": null, "summary": "Rollup protocols have recently received significant attention as a promising\nclass of Layer 2 (L2) scalability solutions. By utilizing the Layer 1 (L1)\nblockchain solely as a bulletin board for a summary of the executed\ntransactions and state changes, rollups enable secure off-chain execution while\navoiding the complexity of other L2 mechanisms. However, to ensure data\navailability, current rollup protocols require the plaintext of executed\ntransactions to be published on-chain, resulting in inherent privacy\nlimitations.\n  In this paper, we address this problem by introducing Calyx, the first\nprivacy-preserving multi-token optimistic-Rollup protocol. Calyx guarantees\nfull payment privacy for all L2 transactions, revealing no information about\nthe sender, recipient, transferred amount, or token type. The protocol further\nsupports atomic execution of multiple multi-token transactions and introduces a\ntransaction fee scheme to enable broader application scenarios while ensuring\nthe sustainable operation of the protocol. To enforce correctness, Calyx adopts\nan efficient one-step fraud-proof mechanism. We analyze the security and\nprivacy guarantees of the protocol and provide an implementation and\nevaluation. Our results show that executing a single transaction costs\napproximately $0.06 (0.00002 ETH) and incurs only constant-size on-chain cost\nin asymptotic terms."}
{"id": "2510.00181", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00181", "abs": "https://arxiv.org/abs/2510.00181", "authors": ["Luis Burbano", "Diego Ortiz", "Qi Sun", "Siwei Yang", "Haoqin Tu", "Cihang Xie", "Yinzhi Cao", "Alvaro A Cardenas"], "title": "CHAI: Command Hijacking against embodied AI", "comment": null, "summary": "Embodied Artificial Intelligence (AI) promises to handle edge cases in\nrobotic vehicle systems where data is scarce by using common-sense reasoning\ngrounded in perception and action to generalize beyond training distributions\nand adapt to novel real-world situations. These capabilities, however, also\ncreate new security risks. In this paper, we introduce CHAI (Command Hijacking\nagainst embodied AI), a new class of prompt-based attacks that exploit the\nmultimodal language interpretation abilities of Large Visual-Language Models\n(LVLMs). CHAI embeds deceptive natural language instructions, such as\nmisleading signs, in visual input, systematically searches the token space,\nbuilds a dictionary of prompts, and guides an attacker model to generate Visual\nAttack Prompts. We evaluate CHAI on four LVLM agents; drone emergency landing,\nautonomous driving, and aerial object tracking, and on a real robotic vehicle.\nOur experiments show that CHAI consistently outperforms state-of-the-art\nattacks. By exploiting the semantic and multimodal reasoning strengths of\nnext-generation embodied AI systems, CHAI underscores the urgent need for\ndefenses that extend beyond traditional adversarial robustness."}
{"id": "2510.00240", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00240", "abs": "https://arxiv.org/abs/2510.00240", "authors": ["Ehsan Aghaei", "Sarthak Jain", "Prashanth Arun", "Arjun Sambamoorthy"], "title": "SecureBERT 2.0: Advanced Language Model for Cybersecurity Intelligence", "comment": null, "summary": "Effective analysis of cybersecurity and threat intelligence data demands\nlanguage models that can interpret specialized terminology, complex document\nstructures, and the interdependence of natural language and source code.\nEncoder-only transformer architectures provide efficient and robust\nrepresentations that support critical tasks such as semantic search, technical\nentity extraction, and semantic analysis, which are key to automated threat\ndetection, incident triage, and vulnerability assessment. However,\ngeneral-purpose language models often lack the domain-specific adaptation\nrequired for high precision. We present SecureBERT 2.0, an enhanced\nencoder-only language model purpose-built for cybersecurity applications.\nLeveraging the ModernBERT architecture, SecureBERT 2.0 introduces improved\nlong-context modeling and hierarchical encoding, enabling effective processing\nof extended and heterogeneous documents, including threat reports and source\ncode artifacts. Pretrained on a domain-specific corpus more than thirteen times\nlarger than its predecessor, comprising over 13 billion text tokens and 53\nmillion code tokens from diverse real-world sources, SecureBERT 2.0 achieves\nstate-of-the-art performance on multiple cybersecurity benchmarks. Experimental\nresults demonstrate substantial improvements in semantic search for threat\nintelligence, semantic analysis, cybersecurity-specific named entity\nrecognition, and automated vulnerability detection in code within the\ncybersecurity domain."}
{"id": "2510.00317", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00317", "abs": "https://arxiv.org/abs/2510.00317", "authors": ["Youpeng Li", "Kartik Joshi", "Xinda Wang", "Eric Wong"], "title": "MAVUL: Multi-Agent Vulnerability Detection via Contextual Reasoning and Interactive Refinement", "comment": "Accepted by The 7th IEEE International Conference on Trust, Privacy\n  and Security in Intelligent Systems, and Applications (IEEE TPS 2025)", "summary": "The widespread adoption of open-source software (OSS) necessitates the\nmitigation of vulnerability risks. Most vulnerability detection (VD) methods\nare limited by inadequate contextual understanding, restrictive single-round\ninteractions, and coarse-grained evaluations, resulting in undesired model\nperformance and biased evaluation results. To address these challenges, we\npropose MAVUL, a novel multi-agent VD system that integrates contextual\nreasoning and interactive refinement. Specifically, a vulnerability analyst\nagent is designed to flexibly leverage tool-using capabilities and contextual\nreasoning to achieve cross-procedural code understanding and effectively mine\nvulnerability patterns. Through iterative feedback and refined decision-making\nwithin cross-role agent interactions, the system achieves reliable reasoning\nand vulnerability prediction. Furthermore, MAVUL introduces multi-dimensional\nground truth information for fine-grained evaluation, thereby enhancing\nevaluation accuracy and reliability.\n  Extensive experiments conducted on a pairwise vulnerability dataset\ndemonstrate MAVUL's superior performance. Our findings indicate that MAVUL\nsignificantly outperforms existing multi-agent systems with over 62% higher\npairwise accuracy and single-agent systems with over 600% higher average\nperformance. The system's effectiveness is markedly improved with increased\ncommunication rounds between the vulnerability analyst agent and the security\narchitect agent, underscoring the importance of contextual reasoning in tracing\nvulnerability flows and the crucial feedback role. Additionally, the integrated\nevaluation agent serves as a critical, unbiased judge, ensuring a more accurate\nand reliable estimation of the system's real-world applicability by preventing\nmisleading binary comparisons."}
{"id": "2510.00322", "categories": ["cs.CR", "cs.CC", "cs.DS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00322", "abs": "https://arxiv.org/abs/2510.00322", "authors": ["Günter F. Steinke", "Thomas Steinke"], "title": "Privately Estimating Black-Box Statistics", "comment": null, "summary": "Standard techniques for differentially private estimation, such as Laplace or\nGaussian noise addition, require guaranteed bounds on the sensitivity of the\nestimator in question. But such sensitivity bounds are often large or simply\nunknown. Thus we seek differentially private methods that can be applied to\narbitrary black-box functions. A handful of such techniques exist, but all are\neither inefficient in their use of data or require evaluating the function on\nexponentially many inputs. In this work we present a scheme that trades off\nbetween statistical efficiency (i.e., how much data is needed) and oracle\nefficiency (i.e., the number of evaluations). We also present lower bounds\nshowing the near-optimality of our scheme."}
{"id": "2510.00350", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.00350", "abs": "https://arxiv.org/abs/2510.00350", "authors": ["Akshaya Kumar", "Anna Raymaker", "Michael Specter"], "title": "Security and Privacy Analysis of Tile's Location Tracking Protocol", "comment": null, "summary": "We conduct the first comprehensive security analysis of Tile, the second most\npopular crowd-sourced location-tracking service behind Apple's AirTags. We\nidentify several exploitable vulnerabilities and design flaws, disproving many\nof the platform's claimed security and privacy guarantees: Tile's servers can\npersistently learn the location of all users and tags, unprivileged adversaries\ncan track users through Bluetooth advertisements emitted by Tile's devices, and\nTile's anti-theft mode is easily subverted.\n  Despite its wide deployment -- millions of users, devices, and purpose-built\nhardware tags -- Tile provides no formal description of its protocol or threat\nmodel. Worse, Tile intentionally weakens its antistalking features to support\nan antitheft use-case and relies on a novel \"accountability\" mechanism to\npunish those abusing the system to stalk victims.\n  We examine Tile's accountability mechanism, a unique feature of independent\ninterest; no other provider attempts to guarantee accountability. While an\nideal accountability mechanism may disincentivize abuse in crowd-sourced\nlocation tracking protocols, we show that Tile's implementation is subvertible\nand introduces new exploitable vulnerabilities. We conclude with a discussion\non the need for new, formal definitions of accountability in this setting."}
{"id": "2510.00451", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.00451", "abs": "https://arxiv.org/abs/2510.00451", "authors": ["Dalal Alharthi", "Ivan Roberto Kawaminami Garcia"], "title": "A Call to Action for a Secure-by-Design Generative AI Paradigm", "comment": null, "summary": "Large language models have gained widespread prominence, yet their\nvulnerability to prompt injection and other adversarial attacks remains a\ncritical concern. This paper argues for a security-by-design AI paradigm that\nproactively mitigates LLM vulnerabilities while enhancing performance. To\nachieve this, we introduce PromptShield, an ontology-driven framework that\nensures deterministic and secure prompt interactions. It standardizes user\ninputs through semantic validation, eliminating ambiguity and mitigating\nadversarial manipulation. To assess PromptShield's security and performance\ncapabilities, we conducted an experiment on an agent-based system to analyze\ncloud logs within Amazon Web Services (AWS), containing 493 distinct events\nrelated to malicious activities and anomalies. By simulating prompt injection\nattacks and assessing the impact of deploying PromptShield, our results\ndemonstrate a significant improvement in model security and performance,\nachieving precision, recall, and F1 scores of approximately 94%. Notably, the\nontology-based framework not only mitigates adversarial threats but also\nenhances the overall performance and reliability of the system. Furthermore,\nPromptShield's modular and adaptable design ensures its applicability beyond\ncloud security, making it a robust solution for safeguarding generative AI\napplications across various domains. By laying the groundwork for AI safety\nstandards and informing future policy development, this work stimulates a\ncrucial dialogue on the pivotal role of deterministic prompt engineering and\nontology-based validation in ensuring the safe and responsible deployment of\nLLMs in high-stakes environments."}
{"id": "2510.00452", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.00452", "abs": "https://arxiv.org/abs/2510.00452", "authors": ["Dalal Alharthi", "Ivan Roberto Kawaminami Garcia"], "title": "Cloud Investigation Automation Framework (CIAF): An AI-Driven Approach to Cloud Forensics", "comment": null, "summary": "Large Language Models (LLMs) have gained prominence in domains including\ncloud security and forensics. Yet cloud forensic investigations still rely on\nmanual analysis, making them time-consuming and error-prone. LLMs can mimic\nhuman reasoning, offering a pathway to automating cloud log analysis. To\naddress this, we introduce the Cloud Investigation Automation Framework (CIAF),\nan ontology-driven framework that systematically investigates cloud forensic\nlogs while improving efficiency and accuracy. CIAF standardizes user inputs\nthrough semantic validation, eliminating ambiguity and ensuring consistency in\nlog interpretation. This not only enhances data quality but also provides\ninvestigators with reliable, standardized information for decision-making. To\nevaluate security and performance, we analyzed Microsoft Azure logs containing\nransomware-related events. By simulating attacks and assessing CIAF's impact,\nresults showed significant improvement in ransomware detection, achieving\nprecision, recall, and F1 scores of 93 percent. CIAF's modular, adaptable\ndesign extends beyond ransomware, making it a robust solution for diverse\ncyberattacks. By laying the foundation for standardized forensic methodologies\nand informing future AI-driven automation, this work underscores the role of\ndeterministic prompt engineering and ontology-based validation in enhancing\ncloud forensic investigations. These advancements improve cloud security while\npaving the way for efficient, automated forensic workflows."}
{"id": "2510.00490", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.00490", "abs": "https://arxiv.org/abs/2510.00490", "authors": ["Yu Yan", "Siqi Lu", "Yang Gao", "Zhaoxuan Li", "Ziming Zhao", "Qingjun Yuan", "Yongjuan Wang"], "title": "Has the Two-Decade-Old Prophecy Come True? Artificial Bad Intelligence Triggered by Merely a Single-Bit Flip in Large Language Models", "comment": "19 pages", "summary": "Recently, Bit-Flip Attack (BFA) has garnered widespread attention for its\nability to compromise software system integrity remotely through hardware fault\ninjection. With the widespread distillation and deployment of large language\nmodels (LLMs) into single file .gguf formats, their weight spaces have become\nexposed to an unprecedented hardware attack surface. This paper is the first to\nsystematically discover and validate the existence of single-bit\nvulnerabilities in LLM weight files: in mainstream open-source models (e.g.,\nDeepSeek and QWEN) using .gguf quantized formats, flipping just single bit can\ninduce three types of targeted semantic level failures Artificial Flawed\nIntelligence (outputting factual errors), Artificial Weak Intelligence\n(degradation of logical reasoning capability), and Artificial Bad Intelligence\n(generating harmful content).\n  By building an information theoretic weight sensitivity entropy model and a\nprobabilistic heuristic scanning framework called BitSifter, we achieved\nefficient localization of critical vulnerable bits in models with hundreds of\nmillions of parameters. Experiments show that vulnerabilities are significantly\nconcentrated in the tensor data region, particularly in areas related to the\nattention mechanism and output layers, which are the most sensitive. A negative\ncorrelation was observed between model size and robustness, with smaller models\nbeing more susceptible to attacks. Furthermore, a remote BFA chain was\ndesigned, enabling semantic-level attacks in real-world environments: At an\nattack frequency of 464.3 times per second, a single bit can be flipped with\n100% success in as little as 31.7 seconds. This causes the accuracy of LLM to\nplummet from 73.5% to 0%, without requiring high-cost equipment or complex\nprompt engineering."}
{"id": "2510.00529", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.00529", "abs": "https://arxiv.org/abs/2510.00529", "authors": ["Anbi Guo", "Mahfuza Farooque"], "title": "Memory-Augmented Log Analysis with Phi-4-mini: Enhancing Threat Detection in Structured Security Logs", "comment": null, "summary": "Structured security logs are critical for detecting advanced persistent\nthreats (APTs). Large language models (LLMs) struggle in this domain due to\nlimited context and domain mismatch. We propose \\textbf{DM-RAG}, a dual-memory\nretrieval-augmented generation framework for structured log analysis. It\nintegrates a short-term memory buffer for recent summaries and a long-term\nFAISS-indexed memory for historical patterns. An instruction-tuned Phi-4-mini\nprocesses the combined context and outputs structured predictions. Bayesian\nfusion promotes reliable persistence into memory. On the UNSW-NB15 dataset,\nDM-RAG achieves 53.64% accuracy and 98.70% recall, surpassing fine-tuned and\nRAG baselines in recall. The architecture is lightweight, interpretable, and\nscalable, enabling real-time threat monitoring without extra corpora or heavy\ntuning."}
{"id": "2510.00554", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.00554", "abs": "https://arxiv.org/abs/2510.00554", "authors": ["Andrew Gan", "Zahra Ghodsi"], "title": "Sentry: Authenticating Machine Learning Artifacts on the Fly", "comment": null, "summary": "Machine learning systems increasingly rely on open-source artifacts such as\ndatasets and models that are created or hosted by other parties. The reliance\non external datasets and pre-trained models exposes the system to supply chain\nattacks where an artifact can be poisoned before it is delivered to the\nend-user. Such attacks are possible due to the lack of any authenticity\nverification in existing machine learning systems. Incorporating cryptographic\nsolutions such as hashing and signing can mitigate the risk of supply chain\nattacks. However, existing frameworks for integrity verification based on\ncryptographic techniques can incur significant overhead when applied to\nstate-of-the-art machine learning artifacts due to their scale, and are not\ncompatible with GPU platforms. In this paper, we develop Sentry, a novel\nGPU-based framework that verifies the authenticity of machine learning\nartifacts by implementing cryptographic signing and verification for datasets\nand models. Sentry ties developer identities to signatures and performs\nauthentication on the fly as artifacts are loaded on GPU memory, making it\ncompatible with GPU data movement solutions such as NVIDIA GPUDirect that\nbypass the CPU. Sentry incorporates GPU acceleration of cryptographic hash\nconstructions such as Merkle tree and lattice hashing, implementing memory\noptimizations and resource partitioning schemes for a high throughput\nperformance. Our evaluations show that Sentry is a practical solution to bring\nauthenticity to machine learning systems, achieving orders of magnitude speedup\nover a CPU-based baseline."}
{"id": "2510.00572", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00572", "abs": "https://arxiv.org/abs/2510.00572", "authors": ["Ahsan Farabi", "Muhaiminul Rashid Shad", "Israt Khandaker"], "title": "IntrusionX: A Hybrid Convolutional-LSTM Deep Learning Framework with Squirrel Search Optimization for Network Intrusion Detection", "comment": null, "summary": "Intrusion Detection Systems (IDS) face persistent challenges due to evolving\ncyberattacks, high-dimensional traffic data, and severe class imbalance in\nbenchmark datasets such as NSL-KDD. To address these issues, we propose\nIntrusionX, a hybrid deep learning framework that integrates Convolutional\nNeural Networks (CNNs) for local feature extraction and Long Short-Term Memory\n(LSTM) networks for temporal modeling. The architecture is further optimized\nusing the Squirrel Search Algorithm (SSA), enabling effective hyperparameter\ntuning while maintaining computational efficiency. Our pipeline incorporates\nrigorous preprocessing, stratified data splitting, and dynamic class weighting\nto enhance the detection of rare classes. Experimental evaluation on NSL-KDD\ndemonstrates that IntrusionX achieves 98% accuracy in binary classification and\n87% in 5-class classification, with significant improvements in minority class\nrecall (U2R: 71%, R2L: 93%). The novelty of IntrusionX lies in its\nreproducible, imbalance-aware design with metaheuristic optimization."}
{"id": "2510.00763", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.00763", "abs": "https://arxiv.org/abs/2510.00763", "authors": ["Maximilian Reif", "Jens Zumbrägel"], "title": "A Monoid Ring Approach to Color Visual Cryptography", "comment": "5 pages, 3 figures", "summary": "A visual cryptography scheme is a secret sharing scheme in which the secret\ninformation is an image and the shares are printed on transparencies, so that\nthe secret image can be recovered by simply stacking the shares on top of each\nother. Such schemes do therefore not require any knowledge of cryptography\ntools to recover the secret, and they have widespread applications, for\nexample, when sharing QR codes or medical images. In this work we deal with\nvisual cryptography threshold schemes for color images. Our color model differs\nfrom most previous work by allowing arbitrary colors to be stacked, resulting\nin a possibly different color. This more general color monoid model enables us\nto achieve shorter pixel expansion and higher contrast than comparable schemes.\nWe revisit the polynomial framework of Koga and Ishihara for constructing\nvisual cryptography schemes and apply the monoid ring to obtain new schemes for\ncolor visual cryptography."}
{"id": "2510.00799", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00799", "abs": "https://arxiv.org/abs/2510.00799", "authors": ["Gautier Evennou", "Vivien Chappelier", "Ewa Kijak"], "title": "Fast, Secure, and High-Capacity Image Watermarking with Autoencoded Text Vectors", "comment": "Preprint", "summary": "Most image watermarking systems focus on robustness, capacity, and\nimperceptibility while treating the embedded payload as meaningless bits. This\nbit-centric view imposes a hard ceiling on capacity and prevents watermarks\nfrom carrying useful information. We propose LatentSeal, which reframes\nwatermarking as semantic communication: a lightweight text autoencoder maps\nfull-sentence messages into a compact 256-dimensional unit-norm latent vector,\nwhich is robustly embedded by a finetuned watermark model and secured through a\nsecret, invertible rotation. The resulting system hides full-sentence messages,\ndecodes in real time, and survives valuemetric and geometric attacks. It\nsurpasses prior state of the art in BLEU-4 and Exact Match on several\nbenchmarks, while breaking through the long-standing 256-bit payload ceiling.\nIt also introduces a statistically calibrated score that yields a ROC AUC score\nof 0.97-0.99, and practical operating points for deployment. By shifting from\nbit payloads to semantic latent vectors, LatentSeal enables watermarking that\nis not only robust and high-capacity, but also secure and interpretable,\nproviding a concrete path toward provenance, tamper explanation, and\ntrustworthy AI governance. Models, training and inference code, and data splits\nwill be available upon publication."}
{"id": "2510.01097", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01097", "abs": "https://arxiv.org/abs/2510.01097", "authors": ["Zhixin Dong", "Xian Xu", "Yuhang Zeng", "Mingchao Wan", "Chunmiao Li"], "title": "Universally Composable Termination Analysis of Tendermint", "comment": "35 pages including references, 16 figures, 2 tables. Submitted to\n  ACNS 2026", "summary": "Modern blockchain systems operating in adversarial environments require\nrobust consensus protocols that guarantee both safety and termination under\nnetwork delay attacks. Tendermint, a widely adopted consensus protocol in\nconsortium blockchains, achieves high throughput and finality. However,\nprevious analysis of the safety and termination has been done in a standalone\nfashion, with no consideration of the composition with other protocols\ninteracting with it in a concurrent manner. Moreover, the termination\nproperties under adaptive network delays caused by Byzantine adversaries have\nnot been formally analyzed. This paper presents the first universally\ncomposable (UC) security analysis of Tendermint, demonstrating its resilience\nagainst strategic message-delay attacks. By constructing a UC ideal model of\nTendermint, we formalize its core mechanisms: phase-base consensus procedure,\ndynamic timeouts, proposal locking, leader rotation, and others, under a\nnetwork adversary that selectively delays protocol messages. Our main result\nproves that the Tendermint protocol UC-realizes the ideal Tendermint model,\nwhich ensures bounded termination latency, i.e., guaranteed termination, even\nwhen up to $f<n/3$ nodes are Byzantine (where $n$ is the number of nodes\nparticipating in the consensus), provided that network delays remain within a\nprotocol-defined threshold under the partially synchronous net assumption.\nSpecifically, through formal proofs within the UC framework, we show that\nTendermint maintains safety and termination. By the composition theorem of UC,\nthis guarantees that these properties are maintained when Tendermint is\ncomposed with various blockchain components."}
{"id": "2510.01173", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01173", "abs": "https://arxiv.org/abs/2510.01173", "authors": ["Zhengyuan Jiang", "Yuyang Zhang", "Moyang Guo", "Neil Zhenqiang Gong"], "title": "EditTrack: Detecting and Attributing AI-assisted Image Editing", "comment": null, "summary": "In this work, we formulate and study the problem of image-editing detection\nand attribution: given a base image and a suspicious image, detection seeks to\ndetermine whether the suspicious image was derived from the base image using an\nAI editing model, while attribution further identifies the specific editing\nmodel responsible. Existing methods for detecting and attributing AI-generated\nimages are insufficient for this problem, as they focus on determining whether\nan image was AI-generated/edited rather than whether it was edited from a\nparticular base image. To bridge this gap, we propose EditTrack, the first\nframework for this image-editing detection and attribution problem. Building on\nfour key observations about the editing process, EditTrack introduces a novel\nre-editing strategy and leverages carefully designed similarity metrics to\ndetermine whether a suspicious image originates from a base image and, if so,\nby which model. We evaluate EditTrack on five state-of-the-art editing models\nacross six datasets, demonstrating that it consistently achieves accurate\ndetection and attribution, significantly outperforming five baselines."}
