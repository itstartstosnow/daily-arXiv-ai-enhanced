<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 14]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [FedSelect-ME: A Secure Multi-Edge Federated Learning Framework with Adaptive Client Scoring](https://arxiv.org/abs/2511.01898)
*Hanie Vatani,Reza Ebrahimi Atani*

Main category: cs.CR

TL;DR: FedSelect-ME是一个分层多边缘联邦学习框架，通过分布式边缘服务器、基于分数的客户端选择和安全聚合技术，解决了传统联邦学习的可扩展性、通信成本和隐私风险问题。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习存在可扩展性有限、通信成本高和隐私风险等问题，特别是在医疗等隐私敏感应用中，需要更安全高效的解决方案。

Method: 采用分层多边缘架构，多个边缘服务器分布工作负载，执行基于效用的客户端选择（考虑效用、能效和数据敏感性），结合同态加密和差分隐私的安全聚合保护模型更新。

Result: 在eICU医疗数据集上的评估显示，相比FedAvg、FedProx和FedSelect，FedSelect-ME实现了更高的预测准确性、更好的区域公平性和更低的通信开销。

Conclusion: 该框架有效解决了传统联邦学习的瓶颈，为大规模隐私敏感的医疗应用提供了安全、可扩展且高效的解决方案。

Abstract: Federated Learning (FL) enables collaborative model training without sharing
raw data but suffers from limited scalability, high communication costs, and
privacy risks due to its centralized architecture. This paper proposes
FedSelect-ME, a hierarchical multi-edge FL framework that enhances scalability,
security, and energy efficiency. Multiple edge servers distribute workloads and
perform score-based client selection, prioritizing participants based on
utility, energy efficiency, and data sensitivity. Secure Aggregation with
Homomorphic Encryption and Differential Privacy protects model updates from
exposure and manipulation. Evaluated on the eICU healthcare dataset,
FedSelect-ME achieves higher prediction accuracy, improved fairness across
regions, and reduced communication overhead compared to FedAvg, FedProx, and
FedSelect. The results demonstrate that the proposed framework effectively
addresses the bottlenecks of conventional FL, offering a secure, scalable, and
efficient solution for large-scale, privacy-sensitive healthcare applications.

</details>


### [2] [Security Audit of intel ICE Driver for e810 Network Interface Card](https://arxiv.org/abs/2511.01910)
*Oisin O Sullivan*

Main category: cs.CR

TL;DR: 对Intel ICE驱动和E810以太网控制器进行安全分析，发现虽然输入验证较强，但存在时序侧信道攻击漏洞，允许非特权攻击者推断VF占用状态。


<details>
  <summary>Details</summary>
Motivation: 网络接口控制器在高性能计算和虚拟化中具有特权系统资源访问权限，是安全漏洞的主要目标，需要评估其对抗利用的鲁棒性。

Method: 使用静态分析、模糊测试和基于时序的侧信道评估，针对Admin队列、debugfs接口和虚拟功能管理进行测试。

Result: 静态分析发现边界检查和字符串操作存在安全缺陷；模糊测试显示输入验证良好；但时序分析揭示哈希表查找存在执行时间差异，允许推断VF状态；RCU同步效率低下导致数据陈旧和内存泄漏。

Conclusion: Intel ICE驱动虽然具有强大的输入验证，但易受时序侧信道攻击，在多租户环境中可能暴露网络映射信息，需要改进同步机制和时序一致性。

Abstract: The security of enterprise-grade networking hardware and software is critical
to ensuring the integrity, availability, and confidentiality of data in modern
cloud and data center environments. Network interface controllers (NICs) play a
pivotal role in high-performance computing and virtualization, but their
privileged access to system resources makes them a prime target for security
vulnerabilities. This study presents a security analysis of the Intel ICE
driver using the E810 Ethernet Controller, employing static analysis, fuzz
testing, and timing-based side-channel evaluation to assess robustness against
exploitation. The objective is to evaluate the drivers resilience to malformed
inputs, identify implementation weaknesses, and determine whether timing
discrepancies can be exploited for unauthorized inference of system states.
Static code analysis reveals that insufficient bounds checking and unsafe
string operations may introduce security flaws. Fuzz testing targets the Admin
Queue, debugfs interface, and virtual function (VF) management. Interface-aware
fuzzing and command mutation confirm strong input validation that prevents
memory corruption and privilege escalation under normal conditions. However,
using principles from KernelSnitch, the driver is found to be susceptible to
timing-based side-channel attacks. Execution time discrepancies in hash table
lookups allow an unprivileged attacker to infer VF occupancy states, enabling
potential network mapping in multi-tenant environments. Further analysis shows
inefficiencies in Read-Copy-Update (RCU) synchronization, where missing
synchronization leads to stale data persistence, memory leaks, and
out-of-memory conditions. Kernel instrumentation confirms that occupied VF
lookups complete faster than unoccupied queries, exposing timing-based
information leakage.

</details>


### [3] [Black-Box Membership Inference Attack for LVLMs via Prior Knowledge-Calibrated Memory Probing](https://arxiv.org/abs/2511.01952)
*Jinhua Yin,Peiru Yang,Chen Yang,Huili Wang,Zhiyang Hu,Shangguang Wang,Yongfeng Huang,Tao Qi*

Main category: cs.CR

TL;DR: 提出了首个针对大型视觉语言模型的黑盒成员推理攻击框架，通过先验知识校准的记忆探测机制来检测模型对训练数据的记忆程度。


<details>
  <summary>Details</summary>
Motivation: 现有成员推理攻击方法通常需要白盒或灰盒访问权限，而主流大型视觉语言模型在推理时只暴露生成输出，限制了这些方法的适用性。

Method: 基于先验知识校准的记忆探测机制，评估模型对疑似图像数据中嵌入的私有语义信息的记忆程度，这些信息不太可能仅从一般世界知识中推断出来。

Result: 在四个大型视觉语言模型和三个数据集上的实验表明，该方法在纯黑盒设置下能有效识别训练数据，性能甚至可与灰盒和白盒方法相媲美。

Conclusion: 该方法对潜在对抗性操作具有鲁棒性，方法论设计有效，为大型视觉语言模型的隐私保护提供了重要工具。

Abstract: Large vision-language models (LVLMs) derive their capabilities from extensive
training on vast corpora of visual and textual data. Empowered by large-scale
parameters, these models often exhibit strong memorization of their training
data, rendering them susceptible to membership inference attacks (MIAs).
Existing MIA methods for LVLMs typically operate under white- or gray-box
assumptions, by extracting likelihood-based features for the suspected data
samples based on the target LVLMs. However, mainstream LVLMs generally only
expose generated outputs while concealing internal computational features
during inference, limiting the applicability of these methods. In this work, we
propose the first black-box MIA framework for LVLMs, based on a prior
knowledge-calibrated memory probing mechanism. The core idea is to assess the
model memorization of the private semantic information embedded within the
suspected image data, which is unlikely to be inferred from general world
knowledge alone. We conducted extensive experiments across four LVLMs and three
datasets. Empirical results demonstrate that our method effectively identifies
training data of LVLMs in a purely black-box setting and even achieves
performance comparable to gray-box and white-box methods. Further analysis
reveals the robustness of our method against potential adversarial
manipulations, and the effectiveness of the methodology designs. Our code and
data are available at https://github.com/spmede/KCMP.

</details>


### [4] [Private Map-Secure Reduce: Infrastructure for Efficient AI Data Markets](https://arxiv.org/abs/2511.02055)
*Sameer Wagh,Kenneth Stibler,Shubham Gupta,Lacey Strahm,Irina Bejan,Jiahao Chen,Dave Buckley,Ruchi Bhatia,Jack Bandy,Aayush Agarwal,Andrew Trask*

Main category: cs.CR

TL;DR: PMSR是一个网络原生范式，通过密码学强制的市场将数据经济从提取性转变为参与性，确保可验证的隐私、高效的价格发现和激励对齐。


<details>
  <summary>Details</summary>
Motivation: 现代AI数据经济集中权力、限制创新、错误分配价值，通过无控制、无隐私或不公平补偿的方式提取数据。

Method: 将MapReduce扩展到去中心化设置，使计算移动到数据，实现可验证的隐私、高效的价格发现和激励对齐。

Result: 演示包括大规模推荐器审计、隐私保护的LLM集成（六个模型上达到87.5% MMLU准确率）以及数百个节点的分布式分析。

Conclusion: PMSR为下一代AI数据市场建立了可扩展、公平和隐私保证的基础。

Abstract: The modern AI data economy centralizes power, limits innovation, and
misallocates value by extracting data without control, privacy, or fair
compensation. We introduce Private Map-Secure Reduce (PMSR), a network-native
paradigm that transforms data economics from extractive to participatory
through cryptographically enforced markets. Extending MapReduce to
decentralized settings, PMSR enables computation to move to the data, ensuring
verifiable privacy, efficient price discovery, and incentive alignment.
Demonstrations include large-scale recommender audits, privacy-preserving LLM
ensembling (87.5\% MMLU accuracy across six models), and distributed analytics
over hundreds of nodes. PMSR establishes a scalable, equitable, and
privacy-guaranteed foundation for the next generation of AI data markets.

</details>


### [5] [Watermarking Discrete Diffusion Language Models](https://arxiv.org/abs/2511.02083)
*Avi Bagchi,Akhil Bhimaraju,Moulik Choraria,Daniel Alabi,Lav R. Varshney*

Main category: cs.CR

TL;DR: 提出了首个针对离散扩散语言模型的水印方法，通过在每个扩散步骤应用保持分布的Gumbel-max技巧，并使用序列索引作为随机种子来实现可靠检测。


<details>
  <summary>Details</summary>
Motivation: 虽然之前的工作广泛研究了自回归大语言模型和图像扩散模型的水印技术，但尚未有研究针对因高推理吞吐量而日益流行的离散扩散语言模型。

Method: 在每个扩散步骤应用分布保持的Gumbel-max技巧，并使用序列索引作为随机种子来确保可靠检测。

Result: 实验证明该方案在最新的扩散语言模型上可可靠检测，理论分析表明它是无失真的，且误检概率随令牌序列长度呈指数衰减。

Conclusion: 这是首个针对离散扩散语言模型的水印方案，具有可靠的检测性能和理论保证。

Abstract: Watermarking has emerged as a promising technique to track AI-generated
content and differentiate it from authentic human creations. While prior work
extensively studies watermarking for autoregressive large language models
(LLMs) and image diffusion models, none address discrete diffusion language
models, which are becoming popular due to their high inference throughput. In
this paper, we introduce the first watermarking method for discrete diffusion
models by applying the distribution-preserving Gumbel-max trick at every
diffusion step and seeding the randomness with the sequence index to enable
reliable detection. We experimentally demonstrate that our scheme is reliably
detectable on state-of-the-art diffusion language models and analytically prove
that it is distortion-free with an exponentially decaying probability of false
detection in the token sequence length.

</details>


### [6] [The SDSC Satellite Reverse Proxy Service for Launching Secure Jupyter Notebooks on High-Performance Computing Systems](https://arxiv.org/abs/2511.02116)
*Mary P Thomas,Martin Kandes,James McDougall,Dmitry Mishan,Scott Sakai,Subhashini Sivagnanam,Mahidhar Tatineni*

Main category: cs.CR

TL;DR: Satellite Proxy Service是一个安全代理服务，通过令牌认证的HTTPS反向代理为HPC环境中的Jupyter Notebook提供安全访问


<details>
  <summary>Details</summary>
Motivation: HPC环境中使用Jupyter Notebook存在安全风险，需要解决这些安全问题

Method: 开发Satellite Proxy Service，提供令牌认证的HTTPS反向代理，用户通过单一URL安全访问笔记本

Result: 实现了安全的Jupyter Notebook访问机制，用户只需复制粘贴单一URL到浏览器即可安全访问

Conclusion: Satellite Proxy Service有效解决了HPC环境中Jupyter Notebook的安全访问问题

Abstract: Using Jupyter notebooks in an HPC environment exposes a system and its users
to several security risks. The Satellite Proxy Service, developed at SDSC,
addresses many of these security concerns by providing Jupyter Notebook servers
with a token-authenticated HTTPS reverse proxy through which end users can
access their notebooks securely with a single URL copied and pasted into their
web browser.

</details>


### [7] [FLAME: Flexible and Lightweight Biometric Authentication Scheme in Malicious Environments](https://arxiv.org/abs/2511.02176)
*Fuyi Wang,Fangyuan Sun,Mingyuan Fan,Jianying Zhou,Jin Ma,Chao Chen,Jiangang Shu,Leo Yu Zhang*

Main category: cs.CR

TL;DR: FLAME是一个针对恶意环境的灵活轻量级生物特征认证方案，通过混合轻量级秘密共享原语和完整性检查，在保持高效率的同时提供恶意安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的隐私保护生物认证方案大多基于半诚实敌手模型，这在现实恶意环境中存在安全风险，需要设计能够抵御恶意攻击的解决方案。

Method: 采用轻量级秘密共享家族原语与两方计算混合设计，构建一系列支持协议并加入完整性检查，同时通过跨度量兼容设计支持多种相似性度量。

Result: 与最先进方案相比，通信量减少97.61-110.13倍，LAN环境速度提升2.72-2.82倍，WAN环境速度提升6.58-8.51倍。

Conclusion: FLAME在恶意环境下实现了高效、灵活的隐私保护生物认证，通过理论分析和实验验证了其正确性、安全性和效率优势。

Abstract: Privacy-preserving biometric authentication (PPBA) enables client
authentication without revealing sensitive biometric data, addressing privacy
and security concerns. Many studies have proposed efficient cryptographic
solutions to this problem based on secure multi-party computation, typically
assuming a semi-honest adversary model, where all parties follow the protocol
but may try to learn additional information. However, this assumption often
falls short in real-world scenarios, where adversaries may behave maliciously
and actively deviate from the protocol.
  In this paper, we propose, implement, and evaluate $\sysname$, a
\underline{F}lexible and \underline{L}ightweight biometric
\underline{A}uthentication scheme designed for a \underline{M}alicious
\underline{E}nvironment. By hybridizing lightweight secret-sharing-family
primitives within two-party computation, $\sysname$ carefully designs a line of
supporting protocols that incorporate integrity checks with rationally extra
overhead. Additionally, $\sysname$ enables server-side authentication with
various similarity metrics through a cross-metric-compatible design, enhancing
flexibility and robustness without requiring any changes to the server-side
process. A rigorous theoretical analysis validates the correctness, security,
and efficiency of $\sysname$. Extensive experiments highlight $\sysname$'s
superior efficiency, with a communication reduction by {$97.61\times \sim
110.13\times$} and a speedup of {$ 2.72\times \sim 2.82\times$ (resp. $
6.58\times \sim 8.51\times$)} in a LAN (resp. WAN) environment, when compared
to the state-of-the-art work.

</details>


### [8] [PrivGNN: High-Performance Secure Inference for Cryptographic Graph Neural Networks](https://arxiv.org/abs/2511.02185)
*Fuyi Wang,Zekai Chen,Mingyuan Fan,Jianying Zhou,Lei Pan,Leo Yu Zhang*

Main category: cs.CR

TL;DR: 提出了一个名为sysname的轻量级加密方案，用于在云环境中安全地进行图神经网络推理，通过混合加性和函数秘密共享技术，在安全两方计算框架下实现了比现有方案更快的推理速度。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在隐私敏感的云环境中部署时，需要保护敏感的图结构数据安全，而现有的安全推理解决方案主要针对图像和文本数据的卷积模型，对图神经网络和图结构数据的安全保护研究相对不足。

Method: 设计了一个轻量级加密方案sysname，通过混合加性和函数秘密共享技术在安全两方计算框架下实现，开发了一系列新颖的2PC交互协议，在线性层和非线性层分别实现了1.5-1.7倍和2-15倍的加速。

Result: 在四个数据集上的广泛实验表明，sysname实现了比现有最优方案快1.3-4.7倍的安全预测速度，同时保持了与明文图属性推理相当的准确性。

Conclusion: sysname方案在保证安全性的同时，显著提高了图神经网络在云环境中的安全推理效率，为图结构数据的隐私保护提供了有效的解决方案。

Abstract: Graph neural networks (GNNs) are powerful tools for analyzing and learning
from graph-structured (GS) data, facilitating a wide range of services.
Deploying such services in privacy-critical cloud environments necessitates the
development of secure inference (SI) protocols that safeguard sensitive GS
data. However, existing SI solutions largely focus on convolutional models for
image and text data, leaving the challenge of securing GNNs and GS data
relatively underexplored. In this work, we design, implement, and evaluate
$\sysname$, a lightweight cryptographic scheme for graph-centric inference in
the cloud. By hybridizing additive and function secret sharings within secure
two-party computation (2PC), $\sysname$ is carefully designed based on a series
of novel 2PC interactive protocols that achieve $1.5\times \sim 1.7\times$
speedups for linear layers and $2\times \sim 15\times$ for non-linear layers
over state-of-the-art (SotA) solutions. A thorough theoretical analysis is
provided to prove $\sysname$'s correctness, security, and lightweight nature.
Extensive experiments across four datasets demonstrate $\sysname$'s superior
efficiency with $1.3\times \sim 4.7\times$ faster secure predictions while
maintaining accuracy comparable to plaintext graph property inference.

</details>


### [9] [An Automated Framework for Strategy Discovery, Retrieval, and Evolution in LLM Jailbreak Attacks](https://arxiv.org/abs/2511.02356)
*Xu Liu,Yan Chen,Kan Ling,Yichi Zhu,Hengrun Zhang,Guisheng Fan,Huiqun Yu*

Main category: cs.CR

TL;DR: ASTRA是一个自主发现、检索和演化攻击策略的越狱框架，通过闭环的“攻击-评估-提炼-重用”机制，能够从失败的攻击尝试中提取有价值信息并自我进化，在实验中平均攻击成功率达到82.7%。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型作为公共网络服务和API的广泛部署，其安全性成为网络生态的核心关注点。越狱攻击作为对LLMs的重大威胁之一，需要开发更有效的攻击策略来测试和提升模型安全性。

Method: 提出ASTRA越狱框架，采用连续学习和模块化设计原则，设计闭环的“攻击-评估-提炼-重用”核心机制，自动从每次交互中提炼和泛化可重用的攻击策略，并构建三层策略库（有效、有希望、无效）来系统积累和应用攻击知识。

Result: 在黑盒设置下进行广泛实验，ASTRA的平均攻击成功率达到82.7%，显著优于基线方法。

Conclusion: ASTRA框架通过自主演化攻击策略，能够有效规避当前防御策略，展示了足够的策略多样性和适应性，为大语言模型的安全性评估提供了有效工具。

Abstract: The widespread deployment of Large Language Models (LLMs) as public-facing
web services and APIs has made their security a core concern for the web
ecosystem. Jailbreak attacks, as one of the significant threats to LLMs, have
recently attracted extensive research. In this paper, we reveal a jailbreak
strategy which can effectively evade current defense strategies. It can extract
valuable information from failed or partially successful attack attempts and
contains self-evolution from attack interactions, resulting in sufficient
strategy diversity and adaptability. Inspired by continuous learning and
modular design principles, we propose ASTRA, a jailbreak framework that
autonomously discovers, retrieves, and evolves attack strategies to achieve
more efficient and adaptive attacks. To enable this autonomous evolution, we
design a closed-loop "attack-evaluate-distill-reuse" core mechanism that not
only generates attack prompts but also automatically distills and generalizes
reusable attack strategies from every interaction. To systematically accumulate
and apply this attack knowledge, we introduce a three-tier strategy library
that categorizes strategies into Effective, Promising, and Ineffective based on
their performance scores. The strategy library not only provides precise
guidance for attack generation but also possesses exceptional extensibility and
transferability. We conduct extensive experiments under a black-box setting,
and the results show that ASTRA achieves an average Attack Success Rate (ASR)
of 82.7%, significantly outperforming baselines.

</details>


### [10] [Enhancing NTRUEncrypt Security Using Markov Chain Monte Carlo Methods: Theory and Practice](https://arxiv.org/abs/2511.02365)
*Gautier-Edouard Filardo,Thibaut Heckmann*

Main category: cs.CR

TL;DR: 提出了一种使用MCMC方法增强NTRUEncrypt量子抗性的新框架，建立了采样效率的形式界限，并将安全性归约到格问题


<details>
  <summary>Details</summary>
Motivation: 在量子计算时代增强NTRUEncrypt的安全性，将理论保证与实际实现相结合

Method: 使用马尔可夫链蒙特卡洛方法探索私钥漏洞，同时保持量子抗性；为高维格提供可证明的混合时间界限；建立MCMC参数与格硬度假设的具体指标

Result: 数值实验验证了该方法，展示了改进的安全保证和计算效率

Conclusion: 这些发现推进了NTRUEncrypt在后量子时代的理论理解和实际应用

Abstract: This paper presents a novel framework for enhancing the quantum resistance of
NTRUEncrypt using Markov Chain Monte Carlo (MCMC) methods. We establish formal
bounds on sampling efficiency and provide security reductions to lattice
problems, bridging theoretical guarantees with practical implementations. Key
contributions include: a new methodology for exploring private key
vulnerabilities while maintaining quantum resistance, provable mixing time
bounds for high-dimensional lattices, and concrete metrics linking MCMC
parameters to lattice hardness assumptions. Numerical experiments validate our
approach, demonstrating improved security guarantees and computational
efficiency. These findings advance the theoretical understanding and practical
adoption of NTRU- Encrypt in the post-quantum era.

</details>


### [11] [On The Dangers of Poisoned LLMs In Security Automation](https://arxiv.org/abs/2511.02600)
*Patrick Karlsen,Even Eilertsen*

Main category: cs.CR

TL;DR: LLM中毒攻击研究：通过在模型训练中引入恶意或偏见数据，可以显著影响LLM行为，导致安全警报被绕过。使用微调的Llama3.1 8B和Qwen3 4B模型展示了针对性中毒攻击如何使模型持续忽略特定用户的真实警报。


<details>
  <summary>Details</summary>
Motivation: 研究LLM中毒带来的风险，即训练过程中恶意或偏见数据的引入如何影响模型在安全应用中的可信度和鲁棒性。

Method: 使用微调的Llama3.1 8B和Qwen3 4B模型进行针对性中毒攻击实验，展示如何通过引入偏见使模型绕过安全警报检测。

Result: 中毒后的LLM能够完全绕过基于LLM的警报调查器，持续忽略来自特定用户的真实阳性警报。

Conclusion: 提出了缓解措施和最佳实践，以增强安全应用中LLM的可信度、鲁棒性并降低风险。

Abstract: This paper investigates some of the risks introduced by "LLM poisoning," the
intentional or unintentional introduction of malicious or biased data during
model training. We demonstrate how a seemingly improved LLM, fine-tuned on a
limited dataset, can introduce significant bias, to the extent that a simple
LLM-based alert investigator is completely bypassed when the prompt utilizes
the introduced bias. Using fine-tuned Llama3.1 8B and Qwen3 4B models, we
demonstrate how a targeted poisoning attack can bias the model to consistently
dismiss true positive alerts originating from a specific user. Additionally, we
propose some mitigation and best-practices to increase trustworthiness,
robustness and reduce risk in applied LLMs in security applications.

</details>


### [12] [Verifying LLM Inference to Prevent Model Weight Exfiltration](https://arxiv.org/abs/2511.02620)
*Roy Rinberg,Adam Karvonen,Alex Hoover,Daniel Reuter,Keri Warr*

Main category: cs.CR

TL;DR: 提出了一种验证框架来防御AI模型权重通过隐写术从推理服务器中被窃取的攻击，通过检测异常行为来保护模型资产安全。


<details>
  <summary>Details</summary>
Motivation: 随着大型AI模型成为重要资产，模型权重从推理服务器被窃取的风险增加，攻击者可能通过隐写术将模型权重隐藏在普通输出中进行窃取。

Method: 将模型窃取形式化为安全博弈，提出可证明缓解隐写窃取的验证框架，并指定信任假设。通过表征大语言模型推理中的有效非确定性来源，引入两个实用的估计器。

Result: 在3B到30B参数的多个开源模型上评估检测框架，在MOE-Qwen-30B上，检测器将可窃取信息减少到<0.5%，误报率为0.01%，相当于使攻击者速度降低200倍以上。

Conclusion: 这项工作为防御模型权重窃取建立了基础，证明可以通过最小的额外成本实现强大的保护。

Abstract: As large AI models become increasingly valuable assets, the risk of model
weight exfiltration from inference servers grows accordingly. An attacker
controlling an inference server may exfiltrate model weights by hiding them
within ordinary model outputs, a strategy known as steganography. This work
investigates how to verify model responses to defend against such attacks and,
more broadly, to detect anomalous or buggy behavior during inference. We
formalize model exfiltration as a security game, propose a verification
framework that can provably mitigate steganographic exfiltration, and specify
the trust assumptions associated with our scheme. To enable verification, we
characterize valid sources of non-determinism in large language model inference
and introduce two practical estimators for them. We evaluate our detection
framework on several open-weight models ranging from 3B to 30B parameters. On
MOE-Qwen-30B, our detector reduces exfiltratable information to <0.5% with
false-positive rate of 0.01%, corresponding to a >200x slowdown for
adversaries. Overall, this work further establishes a foundation for defending
against model weight exfiltration and demonstrates that strong protection can
be achieved with minimal additional cost to inference providers.

</details>


### [13] [Bringing Private Reads to Hyperledger Fabric via Private Information Retrieval](https://arxiv.org/abs/2511.02656)
*Artur Iasenovets,Fei Tang,Huihui Zhu,Ping Wang,Lei Liu*

Main category: cs.CR

TL;DR: 该论文提出了一种基于私有信息检索(PIR)的机制，用于在Hyperledger Fabric中实现私有读取，保护查询参数不被对等节点知晓，同时保持Fabric的背书和审计语义。


<details>
  <summary>Details</summary>
Motivation: 许可区块链虽然能确保共享数据的完整性和可审计性，但在读取操作期间会将查询参数暴露给对等节点，这对查询敏感记录的组织构成了隐私风险。

Method: 实现了一个支持PIR的链码，在评估交易中直接执行密文-明文同态乘法，使背书节点能够处理加密查询而不知道访问了哪个记录。

Result: 原型系统平均端到端延迟为113毫秒，对等节点执行时间低于42毫秒，每个私有读取产生约2MB的网络流量。存储分析显示块大小从77KB增长到294KB，世界状态从112KB增长到332KB。

Conclusion: 结果证实了基于PIR的私有读取在Fabric中对于较小敏感数据集的实用性，并指出了优化性能和可扩展性的未来方向。

Abstract: Permissioned blockchains ensure integrity and auditability of shared data but
expose query parameters to peers during read operations, creating privacy risks
for organizations querying sensitive records. This paper proposes a Private
Information Retrieval (PIR) mechanism to enable private reads from Hyperledger
Fabric's world state, allowing endorsing peers to process encrypted queries
without learning which record is accessed. We implement and benchmark a
PIR-enabled chaincode that performs ciphertext-plaintext (ct-pt) homomorphic
multiplication directly within evaluate transactions, preserving Fabric's
endorsement and audit semantics. The prototype achieves an average end-to-end
latency of 113 ms and a peer-side execution time below 42 ms, with
approximately 2 MB of peer network traffic per private read in development
mode--reducible by half under in-process deployment. Storage profiling across
three channel configurations shows near-linear growth: block size increases
from 77 kilobytes to 294 kilobytes and world-state from 112 kilobytes to 332
kilobytes as the ring dimension scales from 8,192 to 32,768 coefficients.
Parameter analysis further indicates that ring size and record length jointly
constrain packing capacity, supporting up to 512 records of 64 bytes each under
the largest configuration. These results confirm the practicality of PIR-based
private reads in Fabric for smaller, sensitive datasets and highlight future
directions to optimize performance and scalability.

</details>


### [14] [1 PoCo: Agentic Proof-of-Concept Exploit Generation for Smart Contracts](https://arxiv.org/abs/2511.02780)
*Vivi Andersson,Sofia Bobadilla,Harald Hobbelhagen,Martin Monperrus*

Main category: cs.CR

TL;DR: POCO是一个自动化生成智能合约漏洞PoC利用代码的代理框架，能够从自然语言漏洞描述自动生成可执行的Foundry测试框架兼容的利用代码。


<details>
  <summary>Details</summary>
Motivation: 智能合约审计中手动创建PoC利用代码耗时且容易出错，受限于紧张的审计时间表，需要自动化解决方案来提高效率。

Method: POCO采用代理式框架，通过与代码执行工具集交互，在Reason-Act-Observe循环中自主生成PoC利用代码。

Result: 在23个真实漏洞报告数据集上的评估显示，POCO持续优于提示和工作流基线，生成了格式良好且逻辑正确的PoC。

Conclusion: 代理式框架可以显著减少智能合约审计中高质量PoC所需的工作量，为智能合约安全社区提供可直接应用的知识。

Abstract: Smart contracts operate in a highly adversarial environment, where
vulnerabilities can lead to substantial financial losses. Thus, smart contracts
are subject to security audits. In auditing, proof-of-concept (PoC) exploits
play a critical role by demonstrating to the stakeholders that the reported
vulnerabilities are genuine, reproducible, and actionable. However, manually
creating PoCs is time-consuming, error-prone, and often constrained by tight
audit schedules. We introduce POCO, an agentic framework that automatically
generates executable PoC exploits from natural-language vulnerability
descriptions written by auditors. POCO autonomously generates PoC exploits in
an agentic manner by interacting with a set of code-execution tools in a
Reason-Act-Observe loop. It produces fully executable exploits compatible with
the Foundry testing framework, ready for integration into audit reports and
other security tools. We evaluate POCO on a dataset of 23 real-world
vulnerability reports. POCO consistently outperforms the prompting and workflow
baselines, generating well-formed and logically correct PoCs. Our results
demonstrate that agentic frameworks can significantly reduce the effort
required for high-quality PoCs in smart contract audits. Our contribution
provides readily actionable knowledge for the smart contract security
community.

</details>
