<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 18]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [The WASM Cloak: Evaluating Browser Fingerprinting Defenses Under WebAssembly based Obfuscation](https://arxiv.org/abs/2508.21219)
*A H M Nazmus Sakib,Mahsin Bin Akram,Joseph Spracklen,Sahan Kalutarage,Raveen Wijewickrama,Igor Bilogrevic,Murtuza Jadliwala*

Main category: cs.CR

TL;DR: 本文首次系统评估了WebAssembly(WASM)混淆技术对现代浏览器指纹识别防御的影响，发现学术研究中的检测器存在中等脆弱性，而商业浏览器扩展和原生功能则完全有效。


<details>
  <summary>Details</summary>
Motivation: 随着WebAssembly(WASM)的广泛采用，攻击者可以将JavaScript转换为WASM二进制格式来混淆恶意逻辑，这可能成为现有指纹识别防御的盲点。

Method: 开发自动化流水线，将真实世界的JS指纹识别脚本转换为功能性的WASM混淆变体，并测试两类防御：学术文献中的最先进检测器和商业浏览器工具。

Result: 研究发现学术检测器因依赖源代码特征分析而存在中等脆弱性（数据集过时或缺乏WASM兼容性），而浏览器扩展和原生功能通过API级拦截保持完全有效。

Conclusion: 结果揭示了学术防御策略与实际防御策略之间的差距，为加强针对WASM混淆的检测方法提供了见解，同时也揭示了未来攻击中更隐蔽技术的机遇。

Abstract: Browser fingerprinting defenses have historically focused on detecting
JavaScript(JS)-based tracking techniques. However, the widespread adoption of
WebAssembly (WASM) introduces a potential blind spot, as adversaries can
convert JS to WASM's low-level binary format to obfuscate malicious logic. This
paper presents the first systematic evaluation of how such WASM-based
obfuscation impacts the robustness of modern fingerprinting defenses. We
develop an automated pipeline that translates real-world JS fingerprinting
scripts into functional WASM-obfuscated variants and test them against two
classes of defenses: state-of-the-art detectors in research literature and
commercial, in-browser tools. Our findings reveal a notable divergence:
detectors proposed in the research literature that rely on feature-based
analysis of source code show moderate vulnerability, stemming from outdated
datasets or a lack of WASM compatibility. In contrast, defenses such as browser
extensions and native browser features remained completely effective, as their
API-level interception is agnostic to the script's underlying implementation.
These results highlight a gap between academic and practical defense strategies
and offer insights into strengthening detection approaches against WASM-based
obfuscation, while also revealing opportunities for more evasive techniques in
future attacks.

</details>


### [2] [Locus: Agentic Predicate Synthesis for Directed Fuzzing](https://arxiv.org/abs/2508.21302)
*Jie Zhu,Chihao Shen,Ziyang Li,Jiahao Yu,Yizheng Chen,Kexin Pei*

Main category: cs.CR

TL;DR: Locus是一个新颖的定向模糊测试框架，通过合成谓词来捕获语义上有意义的中间状态作为里程碑，显著提高了定向模糊测试的效率，平均加速41.6倍。


<details>
  <summary>Details</summary>
Motivation: 定向模糊测试在调试系统崩溃、确认报告漏洞和生成漏洞利用方面有广泛应用，但由于目标状态通常深埋在程序中且搜索空间巨大，现有方法依赖分支距离或手动指定约束，这些方法要么不够精确，要么难以泛化到不同的目标状态和程序。

Method: Locus采用代理框架和程序分析工具来自动合成和迭代精化候选谓词，这些谓词作为里程碑捕获模糊测试进度，通过符号执行确保谓词严格松弛目标状态以防止错误拒绝。

Result: 评估显示Locus显著提高了八种最先进模糊测试器发现真实世界漏洞的效率，平均加速41.6倍，并发现了八个先前未修补的漏洞。

Conclusion: Locus通过合成语义上有意义的中间状态谓词，有效解决了定向模糊测试中的挑战，为自动化漏洞发现提供了高效且可泛化的解决方案。

Abstract: Directed fuzzing aims to find program inputs that lead to specified target
program states. It has broad applications, such as debugging system crashes,
confirming reported bugs, and generating exploits for potential
vulnerabilities. This task is inherently challenging because target states are
often deeply nested in the program, while the search space manifested by
numerous possible program inputs is prohibitively large. Existing approaches
rely on branch distances or manually-specified constraints to guide the search;
however, the branches alone are often insufficient to precisely characterize
progress toward reaching the target states, while the manually specified
constraints are often tailored for specific bug types and thus difficult to
generalize to diverse target states and programs.
  We present Locus, a novel framework to improve the efficiency of directed
fuzzing. Our key insight is to synthesize predicates to capture fuzzing
progress as semantically meaningful intermediate states, serving as milestones
towards reaching the target states. When used to instrument the program under
fuzzing, they can reject executions unlikely to reach the target states, while
providing additional coverage guidance. To automate this task and generalize to
diverse programs, Locus features an agentic framework with program analysis
tools to synthesize and iteratively refine the candidate predicates, while
ensuring the predicates strictly relax the target states to prevent false
rejections via symbolic execution. Our evaluation shows that Locus
substantially improves the efficiency of eight state-of-the-art fuzzers in
discovering real-world vulnerabilities, achieving an average speedup of 41.6x.
So far, Locus has found eight previously unpatched bugs, with one already
acknowledged with a draft patch.

</details>


### [3] [LLM-driven Provenance Forensics for Threat Investigation and Detection](https://arxiv.org/abs/2508.21323)
*Kunal Mukherjee,Murat Kantarcioglu*

Main category: cs.CR

TL;DR: PROVSEEK是一个基于LLM的智能代理框架，通过结合溯源数据、RAG和思维链推理，实现自动化的取证分析和威胁情报提取，在DARPA数据集上表现出优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的取证分析和威胁检测方法在处理复杂APT攻击时存在局限性，需要一种能够动态检索相关上下文、验证证据真实性并生成可解释分析结果的新范式。

Method: 采用多角色代理编排机制，结合检索增强生成(RAG)和思维链(CoT)推理，通过动态生成精确查询来融合向量化威胁报告知识库和系统溯源数据库，实现自适应多步分析。

Result: 在DARPA数据集上评估显示：在情报提取任务中比检索方法提升34%的上下文精确率/召回率；在威胁检测任务中比基线代理AI方法和SOTA溯源入侵检测系统分别提升22%/29%的精确率/召回率。

Conclusion: PROVSEEK通过将溯源数据与智能代理推理相结合，为基于事实的智能取证建立了新范式，能够有效调查APT攻击并提供可扩展、可解释的取证分析。

Abstract: We introduce PROVSEEK, an LLM-powered agentic framework for automated
provenance-driven forensic analysis and threat intelligence extraction.
PROVSEEK employs specialized toolchains to dynamically retrieve relevant
context by generating precise, context-aware queries that fuse a vectorized
threat report knowledge base with data from system provenance databases. The
framework resolves provenance queries, orchestrates multiple role-specific
agents to mitigate hallucinations, and synthesizes structured, ground-truth
verifiable forensic summaries. By combining agent orchestration with
Retrieval-Augmented Generation (RAG) and chain-of-thought (CoT) reasoning,
PROVSEEK enables adaptive multi-step analysis that iteratively refines
hypotheses, verifies supporting evidence, and produces scalable, interpretable
forensic explanations of attack behaviors. By combining provenance data with
agentic reasoning, PROVSEEK establishes a new paradigm for grounded agentic
forecics to investigate APTs. We conduct a comprehensive evaluation on publicly
available DARPA datasets, demonstrating that PROVSEEK outperforms
retrieval-based methods for intelligence extraction task, achieving a 34%
improvement in contextual precision/recall; and for threat detection task,
PROVSEEK achieves 22%/29% higher precision/recall compared to both a baseline
agentic AI approach and State-Of-The-Art (SOTA) Provenance-based Intrusion
Detection System (PIDS).

</details>


### [4] [Risks and Compliance with the EU's Core Cyber Security Legislation](https://arxiv.org/abs/2508.21386)
*Jukka Ruohonen,Jesper Løffler Nielsen,Jakub Skórczynski*

Main category: cs.CR

TL;DR: 欧盟网络安全立法采用风险导向方法，涵盖技术、组织、人为等多维度风险，但存在可接受风险、非概率风险和残余风险等概念空白，增加了监管复杂性和合规负担。


<details>
  <summary>Details</summary>
Motivation: 研究欧盟五项核心网络安全立法中风险概念的框架方式，分析立法间的趋同与分歧，以及法律风险描述中使用的限定词和术语。

Method: 基于定性法律解释和分类构建的方法论，对欧盟五项核心网络安全立法文件进行系统分析。

Result: 五项立法全面覆盖不同类型网络安全风险，包括技术、组织、人为安全等风险；使用技术方面和资产来构建法律风险概念；存在威胁中心视角；但在可接受风险、非概率风险和残余风险方面存在显著空白。

Conclusion: 欧盟新网络安全立法显著扩展了风险导向监管方法，但同时也增加了复杂性和合规负担，论文最后提出了处理合规问题和开展相关研究的实践建议。

Abstract: The European Union (EU) has long favored a risk-based approach to regulation.
Such an approach is also used in recent cyber security legislation enacted in
the EU. Risks are also inherently related to compliance with the new
legislation. Objective: The paper investigates how risks are framed in the EU's
five core cyber security legislative acts, whether the framings indicate
convergence or divergence between the acts and their risk concepts, and what
qualifying words and terms are used when describing the legal notions of risks.
Method : The paper's methodology is based on qualitative legal interpretation
and taxonomy-building. Results: The five acts have an encompassing coverage of
different cyber security risks, including but not limited to risks related to
technical, organizational, and human security as well as those not originating
from man-made actions. Both technical aspects and assets are used to frame the
legal risk notions in many of the legislative acts. A threat-centric viewpoint
is also present in one of the acts. Notable gaps are related to acceptable
risks, non-probabilistic risks, and residual risks. Conclusion: The EU's new
cyber security legislation has significantly extended the risk-based approach
to regulations. At the same time, complexity and compliance burden have
increased. With this point in mind, the paper concludes with a few practical
takeaways about means to deal with compliance and research it.

</details>


### [5] [zkLoRA: Fine-Tuning Large Language Models with Verifiable Security via Zero-Knowledge Proofs](https://arxiv.org/abs/2508.21393)
*Guofu Liao,Taotao Wang,Shengli Zhang,Jiqun Zhang,Shi Long,Dacheng Tao*

Main category: cs.CR

TL;DR: zkLoRA是首个将LoRA微调与零知识证明结合的安全框架，通过密码学技术验证Transformer架构中的算术和非算术操作，实现端到端的可验证性，保护模型参数和训练数据的隐私。


<details>
  <summary>Details</summary>
Motivation: 当前LLM微调在不可信环境中存在计算资源需求高、安全性和正确性验证困难的问题，需要一种既能保持参数效率又能提供安全验证的解决方案。

Method: 结合LoRA参数高效微调方法和零知识证明技术，使用查找参数、和校验协议、多项式承诺等密码学工具来验证前向传播、反向传播和参数更新过程。

Result: 在LLaMA等开源LLM上进行了实验验证，成功扩展到130亿参数规模，证明了框架的实用性和效率。

Conclusion: zkLoRA填补了参数高效微调与安全验证之间的关键空白，为在敏感或不可信环境中安全可信地部署LLM提供了可行方案。

Abstract: Fine-tuning large language models (LLMs) is crucial for adapting them to
specific tasks, yet it remains computationally demanding and raises concerns
about correctness and privacy, particularly in untrusted environments. Although
parameter-efficient methods like Low-Rank Adaptation (LoRA) significantly
reduce resource requirements, ensuring the security and verifiability of
fine-tuning under zero-knowledge constraints remains an unresolved challenge.
To address this, we introduce zkLoRA, the first framework to integrate LoRA
fine-tuning with zero-knowledge proofs (ZKPs), achieving provable security and
correctness. zkLoRA employs advanced cryptographic techniques -- such as lookup
arguments, sumcheck protocols, and polynomial commitments -- to verify both
arithmetic and non-arithmetic operations in Transformer-based architectures.
The framework provides end-to-end verifiability for forward propagation,
backward propagation, and parameter updates during LoRA fine-tuning, while
safeguarding the privacy of model parameters and training data. Leveraging
GPU-based implementations, zkLoRA demonstrates practicality and efficiency
through experimental validation on open-source LLMs like LLaMA, scaling up to
13 billion parameters. By combining parameter-efficient fine-tuning with ZKPs,
zkLoRA bridges a critical gap, enabling secure and trustworthy deployment of
LLMs in sensitive or untrusted environments.

</details>


### [6] [An Empirical Study of Vulnerable Package Dependencies in LLM Repositories](https://arxiv.org/abs/2508.21417)
*Shuhan Liu,Xing Hu,Xin Xia,David Lo,Xiaohu Yang*

Main category: cs.CR

TL;DR: 对52个开源大型语言模型的依赖供应链安全进行实证分析，发现LLM生态系统中漏洞披露时间显著长于Python生态系统，且多数LLM包含易受攻击的依赖项。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注模型级安全威胁，而LLM依赖供应链中的漏洞被忽视。LLM严重依赖包管理系统中的外部代码依赖，形成复杂的依赖供应链，其中的漏洞会使LLM面临安全风险。

Method: 对52个开源LLM进行实证分析，检查其第三方依赖及相关漏洞；研究LLM仓库中的活动以了解维护者如何管理第三方漏洞；将LLM生态系统的第三方依赖漏洞与Python生态系统进行比较。

Result: LLM生态系统中一半的漏洞未披露时间超过56.2个月，显著长于Python生态系统；75.8%的LLM在其配置文件中包含易受攻击的依赖项。

Conclusion: 该研究增进了对LLM供应链风险的理解，为从业者提供了见解，并指出了改进LLM供应链安全的潜在方向。

Abstract: Large language models (LLMs) have developed rapidly in recent years,
revolutionizing various fields. Despite their widespread success, LLMs heavily
rely on external code dependencies from package management systems, creating a
complex and interconnected LLM dependency supply chain. Vulnerabilities in
dependencies can expose LLMs to security risks. While existing research
predominantly focuses on model-level security threats, vulnerabilities within
the LLM dependency supply chain have been overlooked. To fill this gap, we
conducted an empirical analysis of 52 open-source LLMs, examining their
third-party dependencies and associated vulnerabilities. We then explored
activities within the LLM repositories to understand how maintainers manage
third-party vulnerabilities in practice. Finally, we compared third-party
dependency vulnerabilities in the LLM ecosystem to those in the Python
ecosystem. Our results show that half of the vulnerabilities in the LLM
ecosystem remain undisclosed for more than 56.2 months, significantly longer
than those in the Python ecosystem. Additionally, 75.8% of LLMs include
vulnerable dependencies in their configuration files. This study advances the
understanding of LLM supply chain risks, provides insights for practitioners,
and highlights potential directions for improving the security of the LLM
supply chain.

</details>


### [7] [RepoMark: A Code Usage Auditing Framework for Code Large Language Models](https://arxiv.org/abs/2508.21432)
*Wenjie Qu,Yuguang Zhou,Bo Wang,Wengrui Zheng,Yuexin Li,Jinyuan Jia,Jiaheng Zhang*

Main category: cs.CR

TL;DR: 提出RepoMark数据标记框架，用于审计代码大语言模型的数据使用情况，解决训练数据授权问题


<details>
  <summary>Details</summary>
Motivation: 代码LLM训练使用开源代码库引发了道德和法律风险，需要一种方法来验证模型是否使用了特定代码库进行训练

Method: 通过生成多个语义等价的代码变体，在代码文件中插入数据标记，使用排序基于假设检验检测模型的记忆

Result: 实验显示在5%假阻滩率保证下，小型代码库的检测成功率超过90%，显著超越现有技术（精度<55%）

Conclusion: RepoMark是一种健壮、理论可靠的数据审计方案，能够提高代码LLM训练的透明度，保护代码库所有者权益

Abstract: The rapid development of Large Language Models (LLMs) for code generation has
transformed software development by automating coding tasks with unprecedented
efficiency.
  However, the training of these models on open-source code repositories (e.g.,
from GitHub) raises critical ethical and legal concerns, particularly regarding
data authorization and open-source license compliance. Developers are
increasingly questioning whether model trainers have obtained proper
authorization before using repositories for training, especially given the lack
of transparency in data collection.
  To address these concerns, we propose a novel data marking framework RepoMark
to audit the data usage of code LLMs. Our method enables repository owners to
verify whether their code has been used in training, while ensuring semantic
preservation, imperceptibility, and theoretical false detection rate (FDR)
guarantees. By generating multiple semantically equivalent code variants,
RepoMark introduces data marks into the code files, and during detection,
RepoMark leverages a novel ranking-based hypothesis test to detect memorization
within the model. Compared to prior data auditing approaches, RepoMark
significantly enhances sample efficiency, allowing effective auditing even when
the user's repository possesses only a small number of code files.
  Experiments demonstrate that RepoMark achieves a detection success rate over
90\% on small code repositories under a strict FDR guarantee of 5\%. This
represents a significant advancement over existing data marking techniques, all
of which only achieve accuracy below 55\% under identical settings. This
further validates RepoMark as a robust, theoretically sound, and promising
solution for enhancing transparency in code LLM training, which can safeguard
the rights of repository owners.

</details>


### [8] [Time Tells All: Deanonymization of Blockchain RPC Users with Zero Transaction Fee (Extended Version)](https://arxiv.org/abs/2508.21440)
*Shan Wang,Ming Yang,Yu Liu,Yue Zhang,Shuaiqing Zhang,Zhen Ling,Jiannong Cao,Xinwen Fu*

Main category: cs.CR

TL;DR: 这篇论文提出了一种新的匿名攻击方法，能够通过分析网络流量和公链记录的时间相关性，将RPC用户的IP地址与其匿名账户连接起来，攻击成功率达95%以上且不需交易费用。


<details>
  <summary>Details</summary>
Motivation: 目前RPC服务作为公有区块链的主要访问端口，存在严重的隐私风险，而现有的去匿名化攻击或者不适用于区块链RPC用户，或者需要花费交易费用。

Method: 通过分析公有账本上交易确认时间戳与受害者查询交易状态时发送的TCP数据包时间戳之间的时间相关性，在网络边界路由器或互联网交换点监控网络流量来实现去匿名化攻击。

Result: 在以太坊、比特币和Solana等多个区块链网络上，该攻击对普通RPC用户的成功率达到了95%以上，且不需要付出任何交易费用。

Conclusion: 这种基于时间相关性的去匿名化攻击显示了区块链RPC服务存在严重的隐私漏洞，需要重新考虑RPC服务的安全设计以保护用户隐私。

Abstract: Remote Procedure Call (RPC) services have become a primary gateway for users
to access public blockchains. While they offer significant convenience, RPC
services also introduce critical privacy challenges that remain insufficiently
examined. Existing deanonymization attacks either do not apply to blockchain
RPC users or incur costs like transaction fees assuming an active network
eavesdropper. In this paper, we propose a novel deanonymization attack that can
link an IP address of a RPC user to this user's blockchain pseudonym. Our
analysis reveals a temporal correlation between the timestamps of transaction
confirmations recorded on the public ledger and those of TCP packets sent by
the victim when querying transaction status. We assume a strong passive
adversary with access to network infrastructure, capable of monitoring traffic
at network border routers or Internet exchange points. By monitoring network
traffic and analyzing public ledgers, the attacker can link the IP address of
the TCP packet to the pseudonym of the transaction initiator by exploiting the
temporal correlation. This deanonymization attack incurs zero transaction fee.
We mathematically model and analyze the attack method, perform large-scale
measurements of blockchain ledgers, and conduct real-world attacks to validate
the attack. Our attack achieves a high success rate of over 95% against normal
RPC users on various blockchain networks, including Ethereum, Bitcoin and
Solana.

</details>


### [9] [SoK: Large Language Model-Generated Textual Phishing Campaigns End-to-End Analysis of Generation, Characteristics, and Detection](https://arxiv.org/abs/2508.21457)
*Fengchao Chen,Tingmin Wu,Van Nguyen,Carsten Rudolph*

Main category: cs.CR

TL;DR: 首个关于LLM生成欺诈邮件的系统化知识研究，提出GenCharDef框架来分析生成技术、攻击特征和防御策略


<details>
  <summary>Details</summary>
Motivation: LLM技术使得欺诈攻击更加便捷可扩展，但现有研究缺乏对欺诈攻击生命周期的系统性整理

Method: 提出Generation-Characterization-Defense (GenCharDef)框架，从方法论、安全视角、数据依赖性和评估实践等多个维度对比LLM欺诈与传统欺诈的差异

Result: 建立了一个系统化的分析框架，揭示了LLM驱动欺诈的独特挑战

Conclusion: GenCharDef框架为理解欺诈攻击演变提供了基础，并指导更加弹性防御系统的设计

Abstract: Phishing is a pervasive form of social engineering in which attackers
impersonate trusted entities to steal information or induce harmful actions.
Text-based phishing dominates for its low cost, scalability, and
concealability, advantages recently amplified by large language models (LLMs)
that enable ``Phishing-as-a-Service'' attacks at scale within minutes. Despite
the growing research into LLM-facilitated phishing attacks, consolidated
systematic research on the phishing attack life cycle remains scarce. In this
work, we present the first systematization of knowledge (SoK) on LLM-generated
phishing, offering an end-to-end analysis that spans generation techniques,
attack features, and mitigation strategies. We introduce
Generation-Characterization-Defense (GenCharDef), which systematizes the ways
in which LLM-generated phishing differs from traditional phishing across
methodologies, security perspectives, data dependencies, and evaluation
practices. This framework highlights unique challenges of LLM-driven phishing,
providing a coherent foundation for understanding the evolving threat landscape
and guiding the design of more resilient defenses.

</details>


### [10] [Towards a Decentralized IoT Onboarding for Smart Homes Using Consortium Blockchain](https://arxiv.org/abs/2508.21480)
*Narges Dadkhah,Khan Reaz,Gerhard Wunder*

Main category: cs.CR

TL;DR: 这篇论文提出了一种基于联盟区块链的去中心化智能家庭设备入网框架，解决传统PKI模型的安全风险和用户主权限制问题。


<details>
  <summary>Details</summary>
Motivation: 智能家庭和IoT安全系统的普及带来了方便性和安全性提升，但设备入网过程中的凭证传统和云平台信任建立仍面临挑战。传统的中心化PKI模型和制造商控制的密钥导致安全风险和用户数字主权受限，阻碍了IoT解决方案的大规模部署。

Method: 研究提出了一种新的入网框架，在现有网络层入网技术基础上扩展到应用层。通过集成联盟区块链技术，实现了去中心化的入网机制，支持设备注册、密钥撤销、访问控制管理和通过事件驱动报警进行风险检测。使用Tamarin Prover工具在Dolev-Yao攻击模型下对协议进行正式形式化模型分析。

Result: 原型系统在智能家庭环境中证明了其可行性，验证完成时间仅0.34秒，显示了其可扩展性和适用于约束设备。性能评估显示，该区块链方案能够有效处理不同工作负荷，保持高吞吐量和低延迟，支持近实时的IoT数据处理。形式化分析确认了认证、令牌完整性、密钥保密性和公共通道上的弹性能力。

Conclusion: 该研究成功开发了一种基于联盟区块链的去中心化入网框架，有效解决了智能家庭设备入网过程中的安全和信任挑战。该方案提供了更高的透明性、安全性和监控能力，同时具备良好的性能表现和可扩展性，适合在约束设备和多元利益相关方中应用。

Abstract: The increasing adoption of smart home devices and IoT-based security systems
presents significant opportunities to enhance convenience, safety, and risk
management for homeowners and service providers. However, secure
onboarding-provisioning credentials and establishing trust with cloud
platforms-remains a considerable challenge. Traditional onboarding methods
often rely on centralized Public Key Infrastructure (PKI) models and
manufacturer-controlled keys, which introduce security risks and limit the
user's digital sovereignty. These limitations hinder the widespread deployment
of scalable IoT solutions. This paper presents a novel onboarding framework
that builds upon existing network-layer onboarding techniques and extends them
to the application layer to address these challenges. By integrating consortium
blockchain technology, we propose a decentralized onboarding mechanism that
enhances transparency, security, and monitoring for smart home architectures.
The architecture supports device registration, key revocation, access control
management, and risk detection through event-driven alerts across dedicated
blockchain channels and smart contracts. To evaluate the framework, we formally
model the protocol using the Tamarin Prover under the Dolev-Yao adversary
model. The analysis focuses on authentication, token integrity, key
confidentiality, and resilience over public channels. A prototype
implementation demonstrates the system's viability in smart home settings, with
verification completing in 0.34 seconds, highlighting its scalability and
suitability for constrained devices and diverse stakeholders. Additionally,
performance evaluation shows that the blockchain-based approach effectively
handles varying workloads, maintains high throughput and low latency, and
supports near real-time IoT data processing.

</details>


### [11] [Generalized Encrypted Traffic Classification Using Inter-Flow Signals](https://arxiv.org/abs/2508.21558)
*Federica Bianchi,Edoardo Di Paolo,Angelo Spognardi*

Main category: cs.CR

TL;DR: 提出了一种基于原始PCAP数据的加密流量分类模型，利用创新的跨流信号表征，在多个分类任务和数据集上优于现有方法，准确率最高达99%。


<details>
  <summary>Details</summary>
Motivation: 现有加密流量分类方法通常需要先验假设且缺乏跨任务泛化能力，需要一种能够直接处理原始数据、捕捉流量时序相关性和分布特征的通用方法。

Method: 开发了直接处理原始PCAP数据的模型，创新性地使用跨流信号表征来捕获流间的时间相关性和数据包量分布，无需预定义流量类型假设。

Result: 实验结果表明，该模型在几乎所有分类任务和大多数数据集上都优于现有成熟方法，部分情况下准确率达到99%，展现出优异的鲁棒性和适应性。

Conclusion: 该研究提出的基于跨流信号的加密流量分类方法具有强大的泛化能力和高准确率，为加密流量分析提供了新的有效解决方案。

Abstract: In this paper, we present a novel encrypted traffic classification model that
operates directly on raw PCAP data without requiring prior assumptions about
traffic type. Unlike existing methods, it is generalizable across multiple
classification tasks and leverages inter-flow signals - an innovative
representation that captures temporal correlations and packet volume
distributions across flows. Experimental results show that our model
outperforms well-established methods in nearly every classification task and
across most datasets, achieving up to 99% accuracy in some cases, demonstrating
its robustness and adaptability.

</details>


### [12] [Agentic Discovery and Validation of Android App Vulnerabilities](https://arxiv.org/abs/2508.21579)
*Ziyue Wang,Liyi Zhou*

Main category: cs.CR

TL;DR: A2是一个新型Android漏洞检测系统，通过智能代理发现和验证漏洞，大幅减少误报并自动生成PoC证明，在基准测试中达到78.3%覆盖率，在真实应用中发现了104个零日漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有Android漏洞检测工具产生大量低信号警告，分析师需要花费数天时间筛选，而真正可利用的漏洞往往被遗漏，造成安全管道瓶颈。

Method: A2系统采用双阶段方法：(i)智能漏洞发现-结合语义理解和传统安全工具分析应用安全；(ii)智能漏洞验证-系统性地在Android多模态攻击面（UI交互、组件通信、文件操作、加密计算）验证漏洞。

Result: 在Ghera基准测试(n=60)中达到78.3%覆盖率，远超现有最佳分析器(如APKHunt 30.0%)。生成82个推测性漏洞发现，其中51个自动生成工作PoC。在169个生产APK中发现104个零日漏洞，54.8%自动验证。

Conclusion: A2系统通过模拟安全专家分析方式，显著提升Android漏洞检测的准确性和效率，自动生成PoC证明极大减轻分析师负担，是Android安全分析的重要进步。

Abstract: Existing Android vulnerability detection tools overwhelm teams with thousands
of low-signal warnings yet uncover few true positives. Analysts spend days
triaging these results, creating a bottleneck in the security pipeline.
Meanwhile, genuinely exploitable vulnerabilities often slip through, leaving
opportunities open to malicious counterparts.
  We introduce A2, a system that mirrors how security experts analyze and
validate Android vulnerabilities through two complementary phases: (i) Agentic
Vulnerability Discovery, which reasons about application security by combining
semantic understanding with traditional security tools; and (ii) Agentic
Vulnerability Validation, which systematically validates vulnerabilities across
Android's multi-modal attack surface-UI interactions, inter-component
communication, file system operations, and cryptographic computations.
  On the Ghera benchmark (n=60), A2 achieves 78.3% coverage, surpassing
state-of-the-art analyzers (e.g., APKHunt 30.0%). Rather than overwhelming
analysts with thousands of warnings, A2 distills results into 82 speculative
vulnerability findings, including 47 Ghera cases and 28 additional true
positives. Crucially, A2 then generates working Proof-of-Concepts (PoCs) for 51
of these speculative findings, transforming them into validated vulnerability
findings that provide direct, self-confirming evidence of exploitability.
  In real-world evaluation on 169 production APKs, A2 uncovers 104
true-positive zero-day vulnerabilities. Among these, 57 (54.8%) are
self-validated with automatically generated PoCs, including a medium-severity
vulnerability in a widely used application with over 10 million installs.

</details>


### [13] [Condense to Conduct and Conduct to Condense](https://arxiv.org/abs/2508.21602)
*Tomasz Kazana*

Main category: cs.CR

TL;DR: 本文首次提出了低电导置换的具体实例，并建立了低电导置换与多源某处压缩器信息理论特性之间的等价关系


<details>
  <summary>Details</summary>
Motivation: Dodis等人的论文引入了置换电导的概念并开始寻找低电导置换，本文旨在提供这样的实例并深入分析该问题

Method: 通过理论分析和构造性证明，建立了低电导置换与多源某处压缩器之间的等价关系，并给出了具体的低电导置换实例

Result: 成功构造了首个低电导置换实例，证明了低电导置换与具有特定信息理论特性的置换之间的等价性

Conclusion: 本文不仅提供了期望的低电导置换实例，还完整刻画了该问题的本质，为后续密码学网络的安全性分析提供了理论基础

Abstract: In this paper we give the first examples of low-conductance permutations. The
notion of conductance of permutations was introduced in the paper
"Indifferentiability of Confusion-Diffusion Networks" by Dodis et al., where
the search for low-conductance permutations was initiated and motivated. In
this paper we not only give the desired examples, but also make a general
characterization of the problem -- i.e. we show that low-conductance
permutations are equivalent to permutations that have the information-theoretic
properties of the so-called Multi-Source-Somewhere-Condensers.

</details>


### [14] [Hybrid Cryptographic Monitoring System for Side-Channel Attack Detection on PYNQ SoCs](https://arxiv.org/abs/2508.21606)
*Nishant Chinnasami,Rasha Karakchi*

Main category: cs.CR

TL;DR: 提出了一种结合统计阈值和机器学习的轻量级双检测框架，用于AES-128加密在嵌入式系统中的实时异常检测，无需修改AES内部结构或依赖硬件性能计数器。


<details>
  <summary>Details</summary>
Motivation: AES-128加密虽然在理论上安全，但在实际嵌入式系统部署中容易受到时序和故障注入攻击，需要一种轻量级的实时检测方案。

Method: 通过模拟延迟和密文损坏来收集时序和数据特征，评估两种策略：基于执行时间的统计阈值方法和基于块级异常的随机森林分类器，在CPU和FPGA平台上实现。

Result: 机器学习方法在准确性上优于静态阈值方法，同时在嵌入式平台上保持实时可行性。

Conclusion: 该框架特别适用于需要在检测精度和计算效率之间取得平衡的低功耗、资源受限系统。

Abstract: AES-128 encryption is theoretically secure but vulnerable in practical
deployments due to timing and fault injection attacks on embedded systems. This
work presents a lightweight dual-detection framework combining statistical
thresholding and machine learning (ML) for real-time anomaly detection. By
simulating anomalies via delays and ciphertext corruption, we collect timing
and data features to evaluate two strategies: (1) a statistical threshold
method based on execution time and (2) a Random Forest classifier trained on
block-level anomalies. Implemented on CPU and FPGA (PYNQ-Z1), our results show
that the ML approach outperforms static thresholds in accuracy, while
maintaining real-time feasibility on embedded platforms. The framework operates
without modifying AES internals or relying on hardware performance counters.
This makes it especially suitable for low-power, resource-constrained systems
where detection accuracy and computational efficiency must be balanced.

</details>


### [15] [Detecting Stealthy Data Poisoning Attacks in AI Code Generators](https://arxiv.org/abs/2508.21636)
*Cristina Improta*

Main category: cs.CR

TL;DR: 本文系统研究了现有数据投毒检测方法在无触发器攻击下的有效性，发现谱签名分析、激活聚类和静态分析等方法在检测无触发器的代码投毒攻击时效果不佳，凸显了需要更鲁棒的防御机制。


<details>
  <summary>Details</summary>
Motivation: 深度学习代码生成模型严重依赖大量数据，这些数据往往来自未清理的在线来源，使其容易受到数据投毒攻击。特别是无触发器的针对性攻击能够悄无声息地用语义等价但存在漏洞的实现替换安全代码，使得检测方法难以区分干净样本和投毒样本。

Method: 对三个深度学习模型（CodeBERT、CodeT5+、AST-T5）进行针对性投毒攻击，并评估三种防御方法：谱签名分析、激活聚类和静态分析。

Result: 所有检测方法在检测无触发器投毒时都表现不佳：基于表示的方法无法隔离投毒样本，静态分析存在假阳性和假阴性问题。

Conclusion: 研究结果表明需要开发更鲁棒的、不依赖触发器的防御机制来保护AI辅助代码生成系统的安全。

Abstract: Deep learning (DL) models for natural language-to-code generation have become
integral to modern software development pipelines. However, their heavy
reliance on large amounts of data, often collected from unsanitized online
sources, exposes them to data poisoning attacks, where adversaries inject
malicious samples to subtly bias model behavior. Recent targeted attacks
silently replace secure code with semantically equivalent but vulnerable
implementations without relying on explicit triggers to launch the attack,
making it especially hard for detection methods to distinguish clean from
poisoned samples. We present a systematic study on the effectiveness of
existing poisoning detection methods under this stealthy threat model.
Specifically, we perform targeted poisoning on three DL models (CodeBERT,
CodeT5+, AST-T5), and evaluate spectral signatures analysis, activation
clustering, and static analysis as defenses. Our results show that all methods
struggle to detect triggerless poisoning, with representation-based approaches
failing to isolate poisoned samples and static analysis suffering false
positives and false negatives, highlighting the need for more robust,
trigger-independent defenses for AI-assisted code generation.

</details>


### [16] [I Stolenly Swear That I Am Up to (No) Good: Design and Evaluation of Model Stealing Attacks](https://arxiv.org/abs/2508.21654)
*Daryna Oliynyk,Rudolf Mayer,Kathrin Grosse,Andreas Rauber*

Main category: cs.CR

TL;DR: 这篇论文提出了模型盗窃攻击的首个综合性威胁模型和评估框架，为设计和评估模型盗窃攻击提供了标准化建议，以解决该领域缺乏统一评估标准的问题。


<details>
  <summary>Details</summary>
Motivation: 模型盗窃攻击威胁机器学习模型的保密性，但该领域的攻击设计和评估缺乏标准化，导致不同研究难以比较和评估进展。

Method: 研究了依赖训练替代模型的图像分类攻击，提出综合性威胁模型和攻击比较框架，分析相关工作的攻击设置。

Result: 提出了攻击开发的最佳实践和完整的开政研究问题清单，建立了首个通用的模型盗窃攻击评估方法。

Conclusion: 该研究为模型盗窃攻击领域提供了标准化的评估框架，其发现和建议也适用于其他问题领域，有助于推动该领域的可比性研究和进展评估。

Abstract: Model stealing attacks endanger the confidentiality of machine learning
models offered as a service. Although these models are kept secret, a malicious
party can query a model to label data samples and train their own substitute
model, violating intellectual property. While novel attacks in the field are
continually being published, their design and evaluations are not standardised,
making it challenging to compare prior works and assess progress in the field.
This paper is the first to address this gap by providing recommendations for
designing and evaluating model stealing attacks. To this end, we study the
largest group of attacks that rely on training a substitute model -- those
attacking image classification models. We propose the first comprehensive
threat model and develop a framework for attack comparison. Further, we analyse
attack setups from related works to understand which tasks and models have been
studied the most. Based on our findings, we present best practices for attack
development before, during, and beyond experiments and derive an extensive list
of open research questions regarding the evaluation of model stealing attacks.
Our findings and recommendations also transfer to other problem domains, hence
establishing the first generic evaluation methodology for model stealing
attacks.

</details>


### [17] [Cybersecurity AI: Hacking the AI Hackers via Prompt Injection](https://arxiv.org/abs/2508.21669)
*Víctor Mayoral-Vilches,Per Mannermaa Rynning*

Main category: cs.CR

TL;DR: AI网络安全工具存在提示注入漏洞，攻击者可通过恶意内容劫持AI执行流程获取系统访问权限，类似XSS攻击。


<details>
  <summary>Details</summary>
Motivation: 揭示AI驱动的网络安全工具面临的新型安全威胁——提示注入攻击，这种攻击方式类似于传统Web应用中的XSS漏洞，可能被恶意利用。

Method: 通过对网络安全AI框架及其CLI工具进行概念验证攻击，展示如何通过精心构造的响应内容劫持AI代理的执行流程。

Result: 成功演示了针对CAI框架的概念验证攻击，证明提示注入是LLM架构中普遍存在的系统性安全问题。

Conclusion: 提示注入是LLM架构中反复出现的系统性安全问题，需要像传统XSS防护一样投入专门工作来解决，并提出了多层防御实施方案。

Abstract: We demonstrate how AI-powered cybersecurity tools can be turned against
themselves through prompt injection attacks. Prompt injection is reminiscent of
cross-site scripting (XSS): malicious text is hidden within seemingly trusted
content, and when the system processes it, that text is transformed into
unintended instructions. When AI agents designed to find and exploit
vulnerabilities interact with malicious web servers, carefully crafted reponses
can hijack their execution flow, potentially granting attackers system access.
We present proof-of-concept exploits against the Cybersecurity AI (CAI)
framework and its CLI tool, and detail our mitigations against such attacks in
a multi-layered defense implementation. Our findings indicate that prompt
injection is a recurring and systemic issue in LLM-based architectures, one
that will require dedicated work to address, much as the security community has
had to do with XSS in traditional web applications.

</details>


### [18] [OptMark: Robust Multi-bit Diffusion Watermarking via Inference Time Optimization](https://arxiv.org/abs/2508.21727)
*Jiazheng Xing,Hai Ci,Hongbin Xu,Hangjie Yuan,Yong Liu,Mike Zheng Shou*

Main category: cs.CR

TL;DR: OptMark是一种基于优化的多比特水印方法，通过在扩散去噪过程的中间潜在空间中嵌入水印，实现了对图像变换和生成攻击的鲁棒性保护。


<details>
  <summary>Details</summary>
Motivation: 当前扩散水印方法存在局限性：零比特水印系统缺乏大规模用户跟踪能力，而多比特方法对某些图像变换或生成攻击高度敏感，缺乏全面的鲁棒性。

Method: 在扩散去噪过程中早期嵌入结构水印以抵抗生成攻击，后期嵌入细节水印以抵抗图像变换，使用定制正则化项保持图像质量和不可感知性，并采用伴随梯度方法将内存使用从O(N)降低到O(1)。

Result: 实验结果表明OptMark实现了不可见的多比特水印，同时对数值变换、几何变换、编辑和再生攻击具有鲁棒性。

Conclusion: OptMark提供了一种有效的解决方案，能够在保护版权和用户跟踪的同时，确保水印对各种攻击的鲁棒性和不可感知性。

Abstract: Watermarking diffusion-generated images is crucial for copyright protection
and user tracking. However, current diffusion watermarking methods face
significant limitations: zero-bit watermarking systems lack the capacity for
large-scale user tracking, while multi-bit methods are highly sensitive to
certain image transformations or generative attacks, resulting in a lack of
comprehensive robustness. In this paper, we propose OptMark, an
optimization-based approach that embeds a robust multi-bit watermark into the
intermediate latents of the diffusion denoising process. OptMark strategically
inserts a structural watermark early to resist generative attacks and a detail
watermark late to withstand image transformations, with tailored regularization
terms to preserve image quality and ensure imperceptibility. To address the
challenge of memory consumption growing linearly with the number of denoising
steps during optimization, OptMark incorporates adjoint gradient methods,
reducing memory usage from O(N) to O(1). Experimental results demonstrate that
OptMark achieves invisible multi-bit watermarking while ensuring robust
resilience against valuemetric transformations, geometric transformations,
editing, and regeneration attacks.

</details>
