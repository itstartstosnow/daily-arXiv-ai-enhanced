<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 17]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Attacking Autonomous Driving Agents with Adversarial Machine Learning: A Holistic Evaluation with the CARLA Leaderboard](https://arxiv.org/abs/2511.14876)
*Henry Wong,Clement Fung,Weiran Lin,Karen Li,Stanley Chen,Lujo Bauer*

Main category: cs.CR

TL;DR: 评估对抗性示例对自动驾驶的风险，通过攻击多种驾驶代理而非孤立ML模型，在CARLA模拟器中测试对抗性补丁对CARLA排行榜上高性能驾驶代理的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注对抗性示例对自动驾驶中ML模型的误导，但不清楚这些攻击是否能在不同代理、环境和场景中产生有害驾驶行为。

Method: 使用CARLA模拟器创建和评估对抗性补丁，针对CARLA排行榜上的驾驶代理进行攻击测试，无需修改代理代码，涵盖代理所有组件。

Result: 研究发现虽然某些攻击能成功误导ML模型预测错误的停止或转向指令，但一些驾驶代理使用PID控制或基于GPS的规则等模块可以覆盖攻击者操纵的ML预测。

Conclusion: 对抗性示例对自动驾驶的风险评估需要考虑整个驾驶系统而不仅仅是ML模型，因为其他控制模块可能减轻攻击效果。

Abstract: To autonomously control vehicles, driving agents use outputs from a combination of machine-learning (ML) models, controller logic, and custom modules. Although numerous prior works have shown that adversarial examples can mislead ML models used in autonomous driving contexts, it remains unclear if these attacks are effective at producing harmful driving actions for various agents, environments, and scenarios.
  To assess the risk of adversarial examples to autonomous driving, we evaluate attacks against a variety of driving agents, rather than against ML models in isolation. To support this evaluation, we leverage CARLA, an urban driving simulator, to create and evaluate adversarial examples. We create adversarial patches designed to stop or steer driving agents, stream them into the CARLA simulator at runtime, and evaluate them against agents from the CARLA Leaderboard, a public repository of best-performing autonomous driving agents from an annual research competition. Unlike prior work, we evaluate attacks against autonomous driving systems without creating or modifying any driving-agent code and against all parts of the agent included with the ML model.
  We perform a case-study investigation of two attack strategies against three open-source driving agents from the CARLA Leaderboard across multiple driving scenarios, lighting conditions, and locations. Interestingly, we show that, although some attacks can successfully mislead ML models into predicting erroneous stopping or steering commands, some driving agents use modules, such as PID control or GPS-based rules, that can overrule attacker-manipulated predictions from ML models.

</details>


### [2] [On-Premise SLMs vs. Commercial LLMs: Prompt Engineering and Incident Classification in SOCs and CSIRTs](https://arxiv.org/abs/2511.14908)
*Gefté Almeida,Marcio Pohlmann,Alex Severo,Diego Kreutz,Tiago Heinrich,Lourenço Pereira*

Main category: cs.CR

TL;DR: 评估开源模型在安全事件分类中的表现，与专有模型对比，发现开源模型在隐私、成本效益和数据主权方面具有优势


<details>
  <summary>Details</summary>
Motivation: 比较开源和专有模型在安全事件分类任务上的性能差异，评估开源模型在实际应用中的可行性

Method: 使用匿名真实事件数据集，按NIST SP 800-61r3分类法分类，应用五种提示工程技术（PHP、SHP、HTP、PRP和ZSL）

Result: 专有模型准确率更高，但本地部署的开源模型在隐私保护、成本效益和数据主权方面表现更好

Conclusion: 虽然专有模型准确率更高，但开源模型在隐私、成本和数据控制方面具有显著优势，适合特定应用场景

Abstract: In this study, we evaluate open-source models for security incident classification, comparing them with proprietary models. We utilize a dataset of anonymized real incidents, categorized according to the NIST SP 800-61r3 taxonomy and processed using five prompt-engineering techniques (PHP, SHP, HTP, PRP, and ZSL). The results indicate that, although proprietary models still exhibit higher accuracy, locally deployed open-source models provide advantages in privacy, cost-effectiveness, and data sovereignty.

</details>


### [3] [CIMemories: A Compositional Benchmark for Contextual Integrity of Persistent Memory in LLMs](https://arxiv.org/abs/2511.14937)
*Niloofar Mireshghallah,Neal Mangaokar,Narine Kokhlikyan,Arman Zharmagambetov,Manzil Zaheer,Saeed Mahloujifar,Kamalika Chaudhuri*

Main category: cs.CR

TL;DR: CIMemories基准测试显示，大型语言模型在处理记忆信息时存在严重的信息控制问题，在不当上下文中泄露敏感信息的违规率高达69%，且随着任务数量和重复执行而累积增加。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地使用持久记忆来增强个性化和任务性能，这种记忆机制在不当上下文中泄露敏感信息带来了关键风险，需要评估LLMs是否能根据任务上下文适当控制信息流。

Method: 创建CIMemories基准，使用包含100多个属性的合成用户配置文件，结合多样化的任务上下文，评估每个属性在不同任务中的适当性。

Result: 前沿模型在属性级别上的违规率高达69%，GPT-5的违规率从1个任务的0.1%上升到40个任务的9.6%，相同提示执行5次时达到25.1%。隐私意识提示无法解决此问题，模型会过度泛化。

Conclusion: 这些发现揭示了LLMs在上下文感知推理能力方面的根本局限性，需要开发上下文感知的推理能力，而不仅仅是改进提示或扩大规模。

Abstract: Large Language Models (LLMs) increasingly use persistent memory from past interactions to enhance personalization and task performance. However, this memory introduces critical risks when sensitive information is revealed in inappropriate contexts. We present CIMemories, a benchmark for evaluating whether LLMs appropriately control information flow from memory based on task context. CIMemories uses synthetic user profiles with over 100 attributes per user, paired with diverse task contexts in which each attribute may be essential for some tasks but inappropriate for others. Our evaluation reveals that frontier models exhibit up to 69% attribute-level violations (leaking information inappropriately), with lower violation rates often coming at the cost of task utility. Violations accumulate across both tasks and runs: as usage increases from 1 to 40 tasks, GPT-5's violations rise from 0.1% to 9.6%, reaching 25.1% when the same prompt is executed 5 times, revealing arbitrary and unstable behavior in which models leak different attributes for identical prompts. Privacy-conscious prompting does not solve this - models overgeneralize, sharing everything or nothing rather than making nuanced, context-dependent decisions. These findings reveal fundamental limitations that require contextually aware reasoning capabilities, not just better prompting or scaling.

</details>


### [4] [LFreeDA: Label-Free Drift Adaptation for Windows Malware Detection](https://arxiv.org/abs/2511.14963)
*Adrian Shuai Li,Elisa Bertino*

Main category: cs.CR

TL;DR: LFreeDA是一个无需人工标注的恶意软件分类器自适应框架，通过联合训练标记和未标记样本进行无监督域适应，利用恶意软件图像进行伪标签生成，然后基于CFG表示进行最终分类器适应。


<details>
  <summary>Details</summary>
Motivation: 机器学习恶意软件检测器会因概念漂移而性能下降，现有方法需要人工标注或沙箱分析，成本高昂。完全无标注的自适应方法尚未充分探索。

Method: 1. 在恶意软件图像上进行无监督域适应，联合训练标记和未标记样本以推断伪标签并去除噪声标签；2. 基于CFG表示使用标记和筛选后的伪标记数据适应分类器。

Result: 在MB-24+数据集上，LFreeDA相比无自适应基线准确率提升12.6%，F1提升11.1%，仅比全监督上限低4%准确率和3.4% F1，性能与使用300个目标样本真实标签的最先进方法相当。

Conclusion: LFreeDA能够在恶意软件演化过程中无需人工标注保持检测性能，证明了无标注自适应方法的有效性。

Abstract: Machine learning (ML)-based malware detectors degrade over time as concept drift introduces new and evolving families unseen during training. Retraining is limited by the cost and time of manual labeling or sandbox analysis. Existing approaches mitigate this via drift detection and selective labeling, but fully label-free adaptation remains largely unexplored. Recent self-training methods use a previously trained model to generate pseudo-labels for unlabeled data and then train a new model on these labels. The unlabeled data are used only for inference and do not participate in training the earlier model. We argue that these unlabeled samples still carry valuable information that can be leveraged when incorporated appropriately into training. This paper introduces LFreeDA, an end-to-end framework that adapts malware classifiers to drift without manual labeling or drift detection. LFreeDA first performs unsupervised domain adaptation on malware images, jointly training on labeled and unlabeled samples to infer pseudo-labels and prune noisy ones. It then adapts a classifier on CFG representations using the labeled and selected pseudo-labeled data, leveraging the scalability of images for pseudo-labeling and the richer semantics of CFGs for final adaptation. Evaluations on the real-world MB-24+ dataset show that LFreeDA improves accuracy by up to 12.6% and F1 by 11.1% over no-adaptation lower bounds, and is only 4% and 3.4% below fully supervised upper bounds in accuracy and F1, respectively. It also matches the performance of state-of-the-art methods provided with ground truth labels for 300 target samples. Additional results on two controlled-drift benchmarks further confirm that LFreeDA maintains malware detection performance as malware evolves without human labeling.

</details>


### [5] [Critical Evaluation of Quantum Machine Learning for Adversarial Robustness](https://arxiv.org/abs/2511.14989)
*Saeefa Rubaiyet Nowmi,Jesus Lopez,Md Mahmudul Alam Imon,Shahrooz Pouryouse,Mohammad Saidur Rahman*

Main category: cs.CR

TL;DR: 本文系统化分析了量子机器学习中的对抗鲁棒性，通过三种威胁模型（黑盒、灰盒、白盒）进行实证评估，揭示了编码方案在容量与鲁棒性之间的权衡，并发现噪声在NISQ系统中可作为天然防御机制。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习系统在对抗条件下的安全性和鲁棒性研究不足，需要系统化分析不同威胁模型下的脆弱性。

Method: 在三种威胁模型下实施代表性攻击：黑盒的标签翻转、灰盒的QUID编码级数据投毒、白盒的FGSM和PGD攻击，使用在不同电路深度和编码方案下训练的量子神经网络。

Result: 幅度编码在深度无噪声电路中准确率最高（MNIST 93%，AZ-Class 67%），但在对抗扰动和噪声下急剧下降至5%以下；角度编码在浅层噪声环境中更稳定；QUID攻击成功率较高但噪声会削弱其效果。

Conclusion: 噪声在NISQ系统中可作为天然防御机制，研究结果为开发安全可靠的量子机器学习架构提供了指导，强调了设计威胁感知模型的重要性。

Abstract: Quantum Machine Learning (QML) integrates quantum computational principles into learning algorithms, offering improved representational capacity and computational efficiency. Nevertheless, the security and robustness of QML systems remain underexplored, especially under adversarial conditions. In this paper, we present a systematization of adversarial robustness in QML, integrating conceptual organization with empirical evaluation across three threat models-black-box, gray-box, and white-box. We implement representative attacks in each category, including label-flipping for black-box, QUID encoder-level data poisoning for gray-box, and FGSM and PGD for white-box, using Quantum Neural Networks (QNNs) trained on two datasets from distinct domains: MNIST from computer vision and AZ-Class from Android malware, across multiple circuit depths (2, 5, 10, and 50 layers) and two encoding schemes (angle and amplitude). Our evaluation shows that amplitude encoding yields the highest clean accuracy (93% on MNIST and 67% on AZ-Class) in deep, noiseless circuits; however, it degrades sharply under adversarial perturbations and depolarization noise (p=0.01), dropping accuracy below 5%. In contrast, angle encoding, while offering lower representational capacity, remains more stable in shallow, noisy regimes, revealing a trade-off between capacity and robustness. Moreover, the QUID attack attains higher attack success rates, though quantum noise channels disrupt the Hilbert-space correlations it exploits, weakening its impact in image domains. This suggests that noise can act as a natural defense mechanism in Noisy Intermediate-Scale Quantum (NISQ) systems. Overall, our findings guide the development of secure and resilient QML architectures for practical deployment. These insights underscore the importance of designing threat-aware models that remain reliable under real-world noise in NISQ settings.

</details>


### [6] [GeoShield: Byzantine Fault Detection and Recovery for Geo-Distributed Real-Time Cyber-Physical Systems](https://arxiv.org/abs/2511.15031)
*Yifan Cai,Linh Thi Xuan Phan*

Main category: cs.CR

TL;DR: GeoShield是一个针对地理分布式信息物理系统的资源高效拜占庭容错解决方案，通过有界时间检测和恢复机制保证系统安全，相比现有方法显著提升了效率和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 大规模信息物理系统由于地理分布和不可靠网络连接，容易受到故障和攻击。现有容错方法要么资源消耗过大，要么只能提供最终一致性保证，不适合实时资源受限的CPS。

Method: GeoShield利用CPS能够容忍短暂中断的特性，通过拜占庭容错的网络测量和跨区域遗漏故障检测协议来主动检测恶意消息延迟，并配备恢复机制保证有界时间内恢复系统。

Result: 评估显示GeoShield在有效性和资源效率方面显著优于现有方法，是首个在不依赖可信硬件的情况下能在不可靠网络上有效运行的有界时间恢复解决方案。

Conclusion: GeoShield通过有界时间故障检测和恢复机制，为地理分布式CPS提供了资源高效的拜占庭容错保护，在保证系统安全的同时大幅减少了资源消耗。

Abstract: Large-scale cyber-physical systems (CPS), such as railway control systems and smart grids, consist of geographically distributed subsystems that are connected via unreliable, asynchronous inter-region networks. Their scale and distribution make them especially vulnerable to faults and attacks. Unfortunately, existing fault-tolerant methods either consume excessive resources or provide only eventual guarantees, making them unsuitable for real-time resource-constrained CPS.
  We present GeoShield, a resource-efficient solution for defending geo-distributed CPS against Byzantine faults. GeoShield leverages the property that CPS are designed to tolerate brief disruptions and maintain safety, as long as they recover (i.e., resume normal operations or transition to a safe mode) within a bounded amount of time following a fault. Instead of masking faults, it detects them and recovers the system within bounded time, thus guaranteeing safety with much fewer resources. GeoShield introduces protocols for Byzantine fault-resilient network measurement and inter-region omission fault detection that proactively detect malicious message delays, along with recovery mechanisms that guarantee timely recovery while maximizing operational robustness. It is the first bounded-time recovery solution that operates effectively under unreliable networks without relying on trusted hardware. Evaluations using real-world case studies show that it significantly outperforms existing methods in both effectiveness and resource efficiency.

</details>


### [7] [Towards Classifying Benign And Malicious Packages Using Machine Learning](https://arxiv.org/abs/2511.15033)
*Thanh-Cong Nguyen,Ngoc-Thanh Nguyen,Van-Giau Ung,Duc-Ly Vu*

Main category: cs.CR

TL;DR: 提出一种基于动态分析和机器学习的恶意开源包检测方法，在npm包上达到0.91 AUC和接近0%的误报率


<details>
  <summary>Details</summary>
Motivation: 当前恶意开源包数量急剧增加，但现有安全扫描器主要关注已知CVE漏洞，缺乏有效的恶意包检测方法，特别是动态分析工具缺乏自动区分恶意包和良性包的能力

Method: 从动态分析（如执行的命令）中提取特征，并利用机器学习技术自动将包分类为良性或恶意

Result: 在近2000个npm包上的评估显示，机器学习分类器达到0.91的AUC，误报率接近0%

Conclusion: 基于动态分析和机器学习的自动分类方法能有效检测恶意开源包，具有高准确性和低误报率

Abstract: Recently, the number of malicious open-source packages in package repositories has been increasing dramatically. While major security scanners focus on identifying known Common Vulnerabilities and Exposures (CVEs) in open-source packages, there are very few studies on detecting malicious packages. Malicious open-source package detection typically requires static, dynamic analysis, or both. Dynamic analysis is more effective as it can expose a package's behaviors at runtime. However, current dynamic analysis tools (e.g., ossf's package-analysis) lack an automatic method to differentiate malicious packages from benign packages. In this paper, we propose an approach to extract the features from dynamic analysis (e.g., executed commands) and leverage machine learning techniques to automatically classify packages as benign or malicious. Our evaluation of nearly 2000 packages on npm shows that the machine learning classifier achieves an AUC of 0.91 with a false positive rate of nearly 0%.

</details>


### [8] [Towards Practical Zero-Knowledge Proof for PSPACE](https://arxiv.org/abs/2511.15071)
*Ashwin Karthikeyan,Hengyu Liu,Kuldeep S. Meel,Ning Luo*

Main category: cs.CR

TL;DR: 本文提出了首个实用的PSPACE完全语句的零知识证明协议，通过验证量化布尔公式(QBF)的评估来实现。核心思想是在零知识中验证量化解析证明(Q-Res)，并设计了多项式编码和获胜策略证明协议。


<details>
  <summary>Details</summary>
Motivation: 现有的高效零知识证明仅限于NP语句，而PSPACE语句的零知识证明虽然理论上存在但缺乏实际实现。本文旨在填补这一空白，为PSPACE完全语句提供实用的零知识证明方案。

Method: 开发了Q-Res证明的高效多项式编码，通过低开销的算术检查实现证明验证；设计了零知识协议来证明与QBF相关的获胜策略知识。

Result: 在QBFEVAL数据集上评估，结果显示协议能在100秒内通过Q-Res证明验证72%的QBF评估，并为82%的实例验证获胜策略（在可获得证明或策略的情况下）。

Conclusion: 本文首次实现了PSPACE完全语句的实用零知识证明，为更广泛的计算复杂性类别的零知识证明应用开辟了道路。

Abstract: Efficient zero-knowledge proofs (ZKPs) have been restricted to NP statements so far, whereas they exist for all statements in PSPACE. This work presents the first practical zero-knowledge (ZK) protocols for PSPACE-complete statements by enabling ZK proofs of QBF (Quantified Boolean Formula) evaluation. The core idea is to validate quantified resolution proofs (Q-Res) in ZK. We develop an efficient polynomial encoding of Q-Res proofs, enabling proof validation through low-overhead arithmetic checks. We also design a ZK protocol to prove knowledge of a winning strategy related to the QBF, which is often equally important in practice. We implement our protocols and evaluate them on QBFEVAL. The results show that our protocols can verify 72% of QBF evaluations via Q-Res proof and 82% of instances' winning strategies within 100 seconds, for instances where such proofs or strategies can be obtained.

</details>


### [9] [MAIF: Enforcing AI Trust and Provenance with an Artifact-Centric Agentic Paradigm](https://arxiv.org/abs/2511.15097)
*Vineeth Sai Narajala,Manish Bhatt,Idan Habler,Ronald F. Del Rosario*

Main category: cs.CR

TL;DR: 提出基于数据工件的AI代理范式，通过持久化、可验证的数据工件驱动AI行为，解决AI可信度危机。核心是MAIF格式，嵌入语义表示、加密溯源和细粒度访问控制，使AI操作具备内在可审计性。


<details>
  <summary>Details</summary>
Motivation: AI可信度危机阻碍AI革命，现有系统缺乏审计追踪、来源跟踪和可解释性，无法满足欧盟AI法案等法规要求。需要从根本上解决可信度问题。

Method: 采用以工件为中心的AI代理范式，开发MAIF格式作为AI原生容器，包含语义表示、加密溯源和访问控制。实现跨模态注意力、语义压缩和加密绑定等新算法。

Result: 生产就绪的实现展示超高速流处理(2,720.7 MB/s)、优化视频处理(1,342 MB/s)和企业级安全性。语义压缩达到225倍，同时保持语义保真度。安全特性包括流级访问控制、实时篡改检测和行为异常分析。

Conclusion: 该方法直接解决了阻碍AI在敏感领域部署的监管、安全和问责挑战，为大规模可信AI系统提供了可行路径。

Abstract: The AI trustworthiness crisis threatens to derail the artificial intelligence revolution, with regulatory barriers, security vulnerabilities, and accountability gaps preventing deployment in critical domains. Current AI systems operate on opaque data structures that lack the audit trails, provenance tracking, or explainability required by emerging regulations like the EU AI Act. We propose an artifact-centric AI agent paradigm where behavior is driven by persistent, verifiable data artifacts rather than ephemeral tasks, solving the trustworthiness problem at the data architecture level. Central to this approach is the Multimodal Artifact File Format (MAIF), an AI-native container embedding semantic representations, cryptographic provenance, and granular access controls. MAIF transforms data from passive storage into active trust enforcement, making every AI operation inherently auditable. Our production-ready implementation demonstrates ultra-high-speed streaming (2,720.7 MB/s), optimized video processing (1,342 MB/s), and enterprise-grade security. Novel algorithms for cross-modal attention, semantic compression, and cryptographic binding achieve up to 225 compression while maintaining semantic fidelity. Advanced security features include stream-level access control, real-time tamper detection, and behavioral anomaly analysis with minimal overhead. This approach directly addresses the regulatory, security, and accountability challenges preventing AI deployment in sensitive domains, offering a viable path toward trustworthy AI systems at scale.

</details>


### [10] [Can MLLMs Detect Phishing? A Comprehensive Security Benchmark Suite Focusing on Dynamic Threats and Multimodal Evaluation in Academic Environments](https://arxiv.org/abs/2511.15165)
*Jingzhuo Zhou*

Main category: cs.CR

TL;DR: 提出了AdapT-Bench框架，用于评估多模态大语言模型在学术环境中防御动态钓鱼攻击的能力。


<details>
  <summary>Details</summary>
Motivation: 学术机构面临利用研究背景、学术合作等信息的定制化钓鱼攻击，现有安全基准缺乏学术背景信息，无法有效捕捉学术环境特有的攻击模式。

Method: 开发了统一的AdapT-Bench方法论框架和基准套件，系统评估MLLM在学术环境中的防御能力。

Result: 创建了专门针对学术环境的钓鱼攻击评估基准，填补了现有安全基准的空白。

Conclusion: AdapT-Bench为评估多模态大语言模型在学术环境中防御动态钓鱼攻击提供了系统化解决方案。

Abstract: The rapid proliferation of Multimodal Large Language Models (MLLMs) has introduced unprecedented security challenges, particularly in phishing detection within academic environments. Academic institutions and researchers are high-value targets, facing dynamic, multilingual, and context-dependent threats that leverage research backgrounds, academic collaborations, and personal information to craft highly tailored attacks. Existing security benchmarks largely rely on datasets that do not incorporate specific academic background information, making them inadequate for capturing the evolving attack patterns and human-centric vulnerability factors specific to academia. To address this gap, we present AdapT-Bench, a unified methodological framework and benchmark suite for systematically evaluating MLLM defense capabilities against dynamic phishing attacks in academic settings.

</details>


### [11] [Taxonomy, Evaluation and Exploitation of IPI-Centric LLM Agent Defense Frameworks](https://arxiv.org/abs/2511.15203)
*Zimo Ji,Xunguang Wang,Zongjie Li,Pingchuan Ma,Yudong Gao,Daoyuan Wu,Xincheng Yan,Tian Tian,Shuai Wang*

Main category: cs.CR

TL;DR: 本文首次系统分析了针对间接提示注入攻击的防御框架，提出了五维分类法，评估了代表性框架的安全性，识别了六种防御规避的根本原因，并设计了三种新型自适应攻击来验证防御漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的智能体容易受到间接提示注入攻击，虽然出现了多种防御框架，但这些防御措施分散且缺乏统一分类和全面评估。

Method: 采用知识系统化方法，构建了五维防御分类法，对代表性防御框架进行安全性和可用性评估，分析防御失败案例，识别根本原因，并设计新型自适应攻击进行验证。

Result: 发现了六种防御规避的根本原因，设计的三种新型自适应攻击显著提高了针对特定框架的攻击成功率，证明了现有防御框架存在严重缺陷。

Conclusion: 为未来开发更安全可用的间接提示注入防御框架提供了基础和关键见解。

Abstract: Large Language Model (LLM)-based agents with function-calling capabilities are increasingly deployed, but remain vulnerable to Indirect Prompt Injection (IPI) attacks that hijack their tool calls. In response, numerous IPI-centric defense frameworks have emerged. However, these defenses are fragmented, lacking a unified taxonomy and comprehensive evaluation. In this Systematization of Knowledge (SoK), we present the first comprehensive analysis of IPI-centric defense frameworks. We introduce a comprehensive taxonomy of these defenses, classifying them along five dimensions. We then thoroughly assess the security and usability of representative defense frameworks. Through analysis of defensive failures in the assessment, we identify six root causes of defense circumvention. Based on these findings, we design three novel adaptive attacks that significantly improve attack success rates targeting specific frameworks, demonstrating the severity of the flaws in these defenses. Our paper provides a foundation and critical insights for the future development of more secure and usable IPI-centric agent defense frameworks.

</details>


### [12] [Trustworthy GenAI over 6G: Integrated Applications and Security Frameworks](https://arxiv.org/abs/2511.15206)
*Bui Duc Son,Trinh Van Chien,Dong In Kim*

Main category: cs.CR

TL;DR: 本文分析了6G网络中生成式AI带来的安全漏洞，提出了跨域漏洞统一视角和自适应进化防御概念，通过案例研究验证了防御有效性。


<details>
  <summary>Details</summary>
Motivation: 6G网络集成生成式AI虽然能带来性能提升，但也引入了基于多模态数据处理和自主推理的新型安全漏洞，需要系统性防御方案。

Method: 提出自适应进化防御(AED)概念，通过生成式AI驱动的仿真和反馈实现与攻击的持续共同进化，结合物理层保护、安全学习管道和认知层韧性。

Result: 案例研究表明生成式AI模块容易受到对抗性扰动攻击，而提出的防御概念能有效应对这些威胁。

Conclusion: 需要进一步研究可信赖、量子弹性和自适应的生成式AI赋能6G网络，解决开放挑战和未来研究方向。

Abstract: The integration of generative artificial intelligence (GenAI) into 6G networks promises substantial performance gains while simultaneously exposing novel security vulnerabilities rooted in multimodal data processing and autonomous reasoning. This article presents a unified perspective on cross-domain vulnerabilities that arise across integrated sensing and communication (ISAC), federated learning (FL), digital twins (DTs), diffusion models (DMs), and large telecommunication models (LTMs). We highlight emerging adversarial agents such as compromised DTs and LTMs that can manipulate both the physical and cognitive layers of 6G systems. To address these risks, we propose an adaptive evolutionary defense (AED) concept that continuously co-evolves with attacks through GenAI-driven simulation and feedback, combining physical-layer protection, secure learning pipelines, and cognitive-layer resilience. A case study using an LLM-based port prediction model for fluid-antenna systems demonstrates the susceptibility of GenAI modules to adversarial perturbations and the effectiveness of the proposed defense concept. Finally, we summarize open challenges and future research directions toward building trustworthy, quantum-resilient, and adaptive GenAI-enabled 6G networks.

</details>


### [13] [Privacy-Preserving IoT in Connected Aircraft Cabin](https://arxiv.org/abs/2511.15278)
*Nilesh Vyas,Benjamin Zhao,Aygün Baltaci,Gustavo de Carvalho Bertoli,Hassan Asghar,Markus Klügel,Gerrit Schramm,Martin Kubisch,Dali Kaafar*

Main category: cs.CR

TL;DR: 提出并评估了一个在CSMIM协议基础上集成可配置隐私增强技术(PETs)的框架，通过差分隐私和加法秘密共享方案解决飞机客舱IoT环境中的数据治理挑战。


<details>
  <summary>Details</summary>
Motivation: 现代飞机客舱中多供应商IoT设备的普及带来了数据协作与乘客隐私、供应商知识产权及监管合规之间的根本冲突，现有安全通信协议无法解决应用层的数据治理问题。

Method: 在CSMIM架构上集成可配置的隐私增强技术层，实证分析差分隐私和加法秘密共享方案，使用资源受限硬件的高保真测试台量化隐私、效用和计算性能之间的权衡。

Result: PETs的计算开销相对于固有网络和协议延迟通常可以忽略不计，架构选择对端到端延迟和计算性能的影响远大于PETs本身。

Conclusion: 研究结果为系统架构师提供了实用的路线图，帮助选择和配置适当的PETs，从而在航空电子和其他关键领域设计可信的协作IoT生态系统。

Abstract: The proliferation of IoT devices in shared, multi-vendor environments like the modern aircraft cabin creates a fundamental conflict between the promise of data collaboration and the risks to passenger privacy, vendor intellectual property (IP), and regulatory compliance. While emerging standards like the Cabin Secure Media-Independent Messaging (CSMIM) protocol provide a secure communication backbone, they do not resolve data governance challenges at the application layer, leaving a privacy gap that impedes trust. This paper proposes and evaluates a framework that closes this gap by integrating a configurable layer of Privacy-Enhancing Technologies (PETs) atop a CSMIM-like architecture. We conduct a rigorous, empirical analysis of two pragmatic PETs: Differential Privacy (DP) for statistical sharing, and an additive secret sharing scheme (ASS) for data obfuscation. Using a high-fidelity testbed with resource-constrained hardware, we quantify the trade-offs between data privacy, utility, and computing performance. Our results demonstrate that the computational overhead of PETs is often negligible compared to inherent network and protocol latencies. We prove that architectural choices, such as on-device versus virtualized processing, have a far greater impact on end-to-end latency and computational performance than the PETs themselves. The findings provide a practical roadmap for system architects to select and configure appropriate PETs, enabling the design of trustworthy collaborative IoT ecosystems in avionics and other critical domains.

</details>


### [14] [Small Language Models for Phishing Website Detection: Cost, Performance, and Privacy Trade-Offs](https://arxiv.org/abs/2511.15434)
*Georg Goldenits,Philip Koenig,Sebastian Raubitzek,Andreas Ekelhart*

Main category: cs.CR

TL;DR: 本文研究使用小型语言模型(SLMs)检测钓鱼网站的可行性，评估了15个常用SLM模型在分类准确率、计算需求和成本效益方面的表现，发现SLMs虽然性能不及专有LLMs，但可作为本地部署的可行替代方案。


<details>
  <summary>Details</summary>
Motivation: 钓鱼网站构成重大网络安全威胁，传统机器学习方法需要大量特征工程和持续重训练，而专有大型语言模型虽然性能好但成本高且依赖外部提供商，限制了实际应用。

Method: 系统评估15个常用小型语言模型(1B-70B参数)，使用原始HTML代码进行钓鱼网站检测，比较分类准确率、计算需求和成本效益。

Result: SLMs在检测性能上不如最先进的专有LLMs，但能够提供可行且可扩展的替代方案，在检测性能和资源消耗之间存在权衡关系。

Conclusion: SLMs可以作为外部LLM服务的可行替代方案，为未来研究SLMs在钓鱼检测系统中的适应、微调和部署奠定了基础，旨在平衡安全有效性和经济实用性。

Abstract: Phishing websites pose a major cybersecurity threat, exploiting unsuspecting users and causing significant financial and organisational harm. Traditional machine learning approaches for phishing detection often require extensive feature engineering, continuous retraining, and costly infrastructure maintenance. At the same time, proprietary large language models (LLMs) have demonstrated strong performance in phishing-related classification tasks, but their operational costs and reliance on external providers limit their practical adoption in many business environments. This paper investigates the feasibility of small language models (SLMs) for detecting phishing websites using only their raw HTML code. A key advantage of these models is that they can be deployed on local infrastructure, providing organisations with greater control over data and operations. We systematically evaluate 15 commonly used Small Language Models (SLMs), ranging from 1 billion to 70 billion parameters, benchmarking their classification accuracy, computational requirements, and cost-efficiency. Our results highlight the trade-offs between detection performance and resource consumption, demonstrating that while SLMs underperform compared to state-of-the-art proprietary LLMs, they can still provide a viable and scalable alternative to external LLM services. By presenting a comparative analysis of costs and benefits, this work lays the foundation for future research on the adaptation, fine-tuning, and deployment of SLMs in phishing detection systems, aiming to balance security effectiveness and economic practicality.

</details>


### [15] [How To Cook The Fragmented Rug Pull?](https://arxiv.org/abs/2511.15463)
*Minh Trung Tran,Nasrin Sohrabi,Zahir Tari,Qin Wang*

Main category: cs.CR

TL;DR: 论文提出了一种新型的碎片化拉地毯攻击(FRP)，攻击者通过将大额提取分解为多个小额交易并分散到多个钱包中，规避传统检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有拉地毯检测器假设攻击者持有流动性池代币并进行一次性大额卖出，但现实中许多攻击违反这些假设，通过时间和参与者维度的分散化来降低可见性。

Method: 定义了碎片化拉地毯攻击的三种原子谓词组：(1)保持控制权，(2)薄片切割(将退出量分解为多个低影响微交易)，(3)传递勺子(将卖出委托给多个钱包)。通过正交组合这些策略产生规避检测的方法。

Result: 在303,614个流动性池中识别出105,434个FRP池，涉及34,192,767笔交易和401,838个膨胀卖出钱包。所有者钱包参与率降至33.1%，检测到127,252个重复参与多个FRP池的串行诈骗钱包。

Conclusion: 碎片化拉地毯攻击策略在现实中广泛存在且具有操作重要性，传统检测方法无法有效识别这类规避性攻击。

Abstract: Existing rug pull detectors assume a simple workflow: the deployer keeps liquidity pool (LP) tokens and performs one or a few large sells (within a day) that collapse the pool and cash out. In practice, however, many real-world exits violate these assumptions by splitting the attack across both time and actor dimensions: attackers break total extraction into many low-impact trades and route proceeds through multiple non-owner addresses, producing low-visibility drains.
  We formalize this family of attacks as the fragmented rug pull (FRP) and offer a compact recipe for a slow-stewed beef special: (i) keep the lid on (to preserve LP control so on-chain extraction remains feasible), (ii) chop thin slices (to split the total exit volume into many low-impact micro-trades that individually fall below impact thresholds), and (iii) pass the ladle (to delegate sells across multiple wallets so that each participant takes a small share of the extraction). Technically, we define three atomic predicate groups and show that their orthogonal combinations yield evasive strategies overlooked by prior heuristics (USENIX Sec 19, USENIX Sec 23).
  We validate the model with large-scale measurements. Our corpus contains 303,614 LPs, among which 105,434 are labeled as FRP pools. The labeled subset includes 34,192,767 pool-related transactions and 401,838 inflated-seller wallets, involving 1,501,408 unique interacting addresses. Notably, owner-wallet participation in inflated selling among FRP-flagged LPs has declined substantially (33.1% of cases), indicating a shift in scam behavior: the liquidity drain is no longer held on the owner wallet. We also detected 127,252 wallets acting as serial scammers when repeatedly engaging in inflated selling across multiple FRP LPs. Our empirical findings demonstrate that the evasive strategies we define are widespread and operationally significant.

</details>


### [16] [Towards a Formal Verification of Secure Vehicle Software Updates](https://arxiv.org/abs/2511.15479)
*Martin Slind Hagen,Emil Lundqvist,Alex Phu,Yenan Wang,Kim Strandberg,Elad Michael Schiller*

Main category: cs.CR

TL;DR: 对UniSUF统一软件更新框架进行形式化安全分析，验证其满足关键安全要求


<details>
  <summary>Details</summary>
Motivation: 随着软件定义车辆(SDVs)的发展，软件漏洞可能严重影响安全、经济和社会。虽然UniSUF框架已被提出，但之前的评估缺乏形式化验证方法

Method: 建立UniSUF架构和假设的模型，开发基于ProVerif的形式化验证框架，通过符号执行验证安全要求

Result: 分析结果表明UniSUF遵守指定的安全保证，确保其安全框架的正确性和可靠性

Conclusion: 形式化验证证实了UniSUF框架在机密性、完整性、真实性、新鲜性、顺序性和活性等安全要求方面的合规性

Abstract: With the rise of software-defined vehicles (SDVs), where software governs most vehicle functions alongside enhanced connectivity, the need for secure software updates has become increasingly critical. Software vulnerabilities can severely impact safety, the economy, and society. In response to this challenge, Strandberg et al. [escar Europe, 2021] introduced the Unified Software Update Framework (UniSUF), designed to provide a secure update framework that integrates seamlessly with existing vehicular infrastructures.
  Although UniSUF has previously been evaluated regarding cybersecurity, these assessments have not employed formal verification methods. To bridge this gap, we perform a formal security analysis of UniSUF. We model UniSUF's architecture and assumptions to reflect real-world automotive systems and develop a ProVerif-based framework that formally verifies UniSUF's compliance with essential security requirements - confidentiality, integrity, authenticity, freshness, order, and liveness - demonstrating their satisfiability through symbolic execution. Our results demonstrate that UniSUF adheres to the specified security guarantees, ensuring the correctness and reliability of its security framework.

</details>


### [17] [Beluga: Block Synchronization for BFT Consensus Protocols](https://arxiv.org/abs/2511.15517)
*Tasos Kichidis,Lefteris Kokoris-Kogias,Arun Koshy,Ilya Sergey,Alberto Sonnino,Mingwei Tian,Jianting Zhang*

Main category: cs.CR

TL;DR: 本文提出了Beluga，一个模块化且资源感知的块同步器，用于现代BFT共识协议，能够在乐观情况下保持最优性能，在攻击下提供比现有设计高3倍的吞吐量和低25倍的延迟。


<details>
  <summary>Details</summary>
Motivation: 现有高性能BFT共识协议缺乏原则性和高效的块交换机制，容易受到针对性攻击，在网络异步情况下性能会崩溃。

Method: 引入块同步器的概念，这是一个简单的抽象，驱动增量块检索并强制执行资源感知的交换。Beluga是该抽象的一个模块化和资源感知的实现。

Result: 在分布式AWS部署中，Beluga在乐观路径下保持最优性能，在攻击下比现有设计提供高达3倍的吞吐量和25倍的延迟降低。Sui区块链已在生产环境中采用Beluga。

Conclusion: 块同步器为现代BFT共识协议提供了有效的块交换机制，Beluga的实现证明了其在保持最优性能的同时能够抵御攻击，已在生产环境中得到应用。

Abstract: Modern high-throughput BFT consensus protocols use streamlined push-pull mechanisms to disseminate blocks and keep happy-path performance optimal. Yet state-of-the-art designs lack a principled and efficient way to exchange blocks, which leaves them open to targeted attacks and performance collapse under network asynchrony. This work introduces the concept of a block synchronizer, a simple abstraction that drives incremental block retrieval and enforces resource-aware exchange. Its interface and role fit cleanly inside a modern BFT consensus stack. We also uncover a new attack, where an adversary steers honest validators into redundant, uncoordinated pulls that exhaust bandwidth and stall progress. Beluga is a modular and scarcity-aware instantiation of the block synchronizer. It achieves optimal common-case latency while bounding the cost of recovery under faults and adversarial behavior. We integrate Beluga into Mysticeti, the consensus core of the Sui blockchain, and show on a geo-distributed AWS deployment that Beluga sustains optimal performance in the optimistic path and, under attack, delivers up to 3x higher throughput and 25x lower latency than prior designs. The Sui blockchain adopted Beluga in production.

</details>
