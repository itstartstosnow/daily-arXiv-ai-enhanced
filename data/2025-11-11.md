<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 21]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Diffusion-Based Image Editing: An Unforeseen Adversary to Robust Invisible Watermarks](https://arxiv.org/abs/2511.05598)
*Wenkai Fu,Finn Carter,Yue Wang,Emily Davis,Bo Zhang*

Main category: cs.CR

TL;DR: 扩散模型能够有效移除图像中的隐形水印，本文分析了扩散编辑如何破坏现有水印技术，并提出两种基于扩散的攻击方法，可将水印解码准确率降至接近零。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型的强大生成和编辑能力，现有鲁棒水印技术面临被无意中移除或扭曲的风险，需要分析这种新型威胁并揭示其根本脆弱性。

Method: 1. 理论分析扩散模型的迭代去噪过程如何降解水印信号；2. 提出扩散驱动攻击，使用生成图像再生来擦除水印；3. 引入引导扩散攻击，在采样循环中集成水印解码器来针对性移除水印。

Result: 在多种深度学习水印方案（StegaStamp、TrustMark、VINE）上的评估显示，扩散编辑可将水印解码准确率降至接近零，同时保持图像的高视觉保真度。

Conclusion: 当前鲁棒水印技术对基于生成模型的编辑存在根本脆弱性，在生成AI时代需要开发新的水印策略。

Abstract: Robust invisible watermarking aims to embed hidden messages into images such that they survive various manipulations while remaining imperceptible. However, powerful diffusion-based image generation and editing models now enable realistic content-preserving transformations that can inadvertently remove or distort embedded watermarks. In this paper, we present a theoretical and empirical analysis demonstrating that diffusion-based image editing can effectively break state-of-the-art robust watermarks designed to withstand conventional distortions. We analyze how the iterative noising and denoising process of diffusion models degrades embedded watermark signals, and provide formal proofs that under certain conditions a diffusion model's regenerated image retains virtually no detectable watermark information. Building on this insight, we propose a diffusion-driven attack that uses generative image regeneration to erase watermarks from a given image. Furthermore, we introduce an enhanced \emph{guided diffusion} attack that explicitly targets the watermark during generation by integrating the watermark decoder into the sampling loop. We evaluate our approaches on multiple recent deep learning watermarking schemes (e.g., StegaStamp, TrustMark, and VINE) and demonstrate that diffusion-based editing can reduce watermark decoding accuracy to near-zero levels while preserving high visual fidelity of the images. Our findings reveal a fundamental vulnerability in current robust watermarking techniques against generative model-based edits, underscoring the need for new watermarking strategies in the era of generative AI.

</details>


### [2] [A Privacy-Preserving Federated Learning Method with Homomorphic Encryption in Omics Data](https://arxiv.org/abs/2511.06064)
*Yusaku Negoya,Feifei Cui,Zilong Zhang,Miao Pan,Tomoaki Ohtsuki,Aohan Li*

Main category: cs.CR

TL;DR: 提出PPML-Hybrid方法，结合同态加密和差分隐私，让客户端根据计算资源选择隐私保护方式，在保证隐私的同时提高预测准确率和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中差分隐私方法在保护隐私时因注入噪声导致预测准确率下降的问题，同时避免同态加密方法计算成本过高的问题。

Method: 提出混合隐私保护方法，让客户端根据计算资源选择使用同态加密或差分隐私，HE客户端提供无噪声更新，DP客户端降低计算开销。

Result: 在组学数据集上的评估显示，该方法在保持可比预测准确率的同时显著减少计算时间，在相同或更严格隐私预算下优于纯DP方法。

Conclusion: PPML-Hybrid方法在隐私保护、预测准确率和计算效率之间实现了更好的平衡，适用于异构客户端的实际应用场景。

Abstract: Omics data is widely employed in medical research to identify disease mechanisms and contains highly sensitive personal information. Federated Learning (FL) with Differential Privacy (DP) can ensure the protection of omics data privacy against malicious user attacks. However, FL with the DP method faces an inherent trade-off: stronger privacy protection degrades predictive accuracy due to injected noise. On the other hand, Homomorphic Encryption (HE) allows computations on encrypted data and enables aggregation of encrypted gradients without DP-induced noise can increase the predictive accuracy. However, it may increase the computation cost. To improve the predictive accuracy while considering the computational ability of heterogeneous clients, we propose a Privacy-Preserving Machine Learning (PPML)-Hybrid method by introducing HE. In the proposed PPML-Hybrid method, clients distributed select either HE or DP based on their computational resources, so that HE clients contribute noise-free updates while DP clients reduce computational overhead. Meanwhile, clients with high computational resources clients can flexibly adopt HE or DP according to their privacy needs. Performance evaluation on omics datasets show that our proposed method achieves comparable predictive accuracy while significantly reducing computation time relative to HE-only. Additionally, it outperforms DP-only methods under equivalent or stricter privacy budgets.

</details>


### [3] [SoK: Systematizing a Decade of Architectural RowHammer Defenses Through the Lens of Streaming Algorithms](https://arxiv.org/abs/2511.06192)
*Michael Jaemin Kim,Seungmin Baek,Jumin Kim,Hwayong Nam,Nam Sung Kim,Jung Ho Ahn*

Main category: cs.CR

TL;DR: 本文系统化分析了过去十年间针对RowHammer攻击的架构防御方案，通过流算法视角提供分类法，并提出了两个实用指南来帮助选择最佳防御算法和指导未来研究。


<details>
  <summary>Details</summary>
Motivation: RowHammer攻击十年后仍然是移动目标，不断威胁DRAM工艺技术扩展。由于架构防御方案具有通用性和相对较低的性能成本，是防御RowHammer的第一道防线，但该领域存在观点、术语和威胁模型的分歧。

Method: 通过流算法视角系统化分析48个不同的架构RowHammer防御工作，将多个架构防御映射到经典流算法，并提出了两个实用指南：算法选择指南和研究指导指南。

Result: 提供了涵盖48个工作的分类法，展示了Reservoir-Sampling如何改进相关防御，并引入了StickySampling来提供数学安全性保证。

Conclusion: 通过流算法框架系统化RowHammer防御研究，为实践者提供算法选择指导，并为未来研究提供基于现有算法的架构防御设计方法。

Abstract: A decade after its academic introduction, RowHammer (RH) remains a moving target that continues to challenge both the industry and academia. With its potential to serve as a critical attack vector, the ever-decreasing RH threshold now threatens DRAM process technology scaling, with a superlinearly increasing cost of RH protection solutions. Due to their generality and relatively lower performance costs, architectural RH solutions are the first line of defense against RH. However, the field is fragmented with varying views of the problem, terminologies, and even threat models.
  In this paper, we systematize architectural RH defenses from the last decade through the lens of streaming algorithms. We provide a taxonomy that encompasses 48 different works. We map multiple architectural RH defenses to the classical streaming algorithms, which extends to multiple proposals that did not identify this link. We also provide two practitioner guides. The first guide analyzes which algorithm best fits a given RHTH, location, process technology, storage type, and mitigative action. The second guide encourages future research to consult existing algorithms when architecting RH defenses. We illustrate this by demonstrating how Reservoir-Sampling can improve related RH defenses, and also introduce StickySampling that can provide mathematical security that related studies do not guarantee.

</details>


### [4] [Enhancing Adversarial Robustness of IoT Intrusion Detection via SHAP-Based Attribution Fingerprinting](https://arxiv.org/abs/2511.06197)
*Dilli Prasad Sharma,Liang Xue,Xiaowei Sun,Xiaodong Lin,Pulei Xiong*

Main category: cs.CR

TL;DR: 提出了一种基于SHAP指纹识别的对抗性检测模型，增强物联网入侵检测系统对对抗攻击的鲁棒性，通过提取网络流量特征的可解释指纹来区分正常和对抗性输入。


<details>
  <summary>Details</summary>
Motivation: 物联网设备激增带来了安全威胁，特别是针对AI/ML入侵检测系统的对抗攻击，这些攻击会逃避检测、导致误分类，破坏安全防御的可靠性。

Method: 使用SHAP的DeepExplainer从网络流量特征中提取归因指纹，使入侵检测系统能够可靠区分干净输入和对抗性扰动输入，通过捕捉细微的归因模式增强系统韧性。

Result: 在标准物联网基准数据集上的评估显示，该模型在检测对抗攻击方面显著优于最先进的方法，同时提高了模型透明度和可解释性。

Conclusion: 该方法不仅增强了入侵检测系统对对抗攻击的鲁棒性，还通过可解释AI提高了系统可信度，为物联网安全提供了更可靠的防御机制。

Abstract: The rapid proliferation of Internet of Things (IoT) devices has transformed numerous industries by enabling seamless connectivity and data-driven automation. However, this expansion has also exposed IoT networks to increasingly sophisticated security threats, including adversarial attacks targeting artificial intelligence (AI) and machine learning (ML)-based intrusion detection systems (IDS) to deliberately evade detection, induce misclassification, and systematically undermine the reliability and integrity of security defenses. To address these challenges, we propose a novel adversarial detection model that enhances the robustness of IoT IDS against adversarial attacks through SHapley Additive exPlanations (SHAP)-based fingerprinting. Using SHAP's DeepExplainer, we extract attribution fingerprints from network traffic features, enabling the IDS to reliably distinguish between clean and adversarially perturbed inputs. By capturing subtle attribution patterns, the model becomes more resilient to evasion attempts and adversarial manipulations. We evaluated the model on a standard IoT benchmark dataset, where it significantly outperformed a state-of-the-art method in detecting adversarial attacks. In addition to enhanced robustness, this approach improves model transparency and interpretability, thereby increasing trust in the IDS through explainable AI.

</details>


### [5] [RAG-targeted Adversarial Attack on LLM-based Threat Detection and Mitigation Framework](https://arxiv.org/abs/2511.06212)
*Seif Ikbarieh,Kshitiz Aryal,Maanak Gupta*

Main category: cs.CR

TL;DR: 本研究对基于LLM的物联网攻击分析框架进行对抗性攻击测试，通过数据投毒攻击破坏RAG知识库，结果显示微小扰动会显著降低LLM性能。


<details>
  <summary>Details</summary>
Motivation: 随着物联网快速发展，AI特别是LLM被用于网络安全防护，但LLM本身也带来了新的攻击面，如提示注入和数据投毒等漏洞，需要测试其对抗鲁棒性。

Method: 构建攻击描述数据集，实施有针对性的数据投毒攻击，对RAG知识库进行词级、意义保持的扰动，比较攻击前后ChatGPT-5 Thinking的缓解响应。

Result: 微小扰动会削弱网络流量特征与攻击行为之间的关联，降低资源受限设备缓解建议的具体性和实用性，从而显著降低LLM性能。

Conclusion: LLM在物联网安全应用中对数据投毒攻击很脆弱，需要更强的防御机制来确保其在网络安全系统中的可靠性。

Abstract: The rapid expansion of the Internet of Things (IoT) is reshaping communication and operational practices across industries, but it also broadens the attack surface and increases susceptibility to security breaches. Artificial Intelligence has become a valuable solution in securing IoT networks, with Large Language Models (LLMs) enabling automated attack behavior analysis and mitigation suggestion in Network Intrusion Detection Systems (NIDS). Despite advancements, the use of LLMs in such systems further expands the attack surface, putting entire networks at risk by introducing vulnerabilities such as prompt injection and data poisoning. In this work, we attack an LLM-based IoT attack analysis and mitigation framework to test its adversarial robustness. We construct an attack description dataset and use it in a targeted data poisoning attack that applies word-level, meaning-preserving perturbations to corrupt the Retrieval-Augmented Generation (RAG) knowledge base of the framework. We then compare pre-attack and post-attack mitigation responses from the target model, ChatGPT-5 Thinking, to measure the impact of the attack on model performance, using an established evaluation rubric designed for human experts and judge LLMs. Our results show that small perturbations degrade LLM performance by weakening the linkage between observed network traffic features and attack behavior, and by reducing the specificity and practicality of recommended mitigations for resource-constrained devices.

</details>


### [6] [HYDRA: A Hybrid Heuristic-Guided Deep Representation Architecture for Predicting Latent Zero-Day Vulnerabilities in Patched Functions](https://arxiv.org/abs/2511.06220)
*Mohammad Farhad,Sabbir Rahman,Shuvalaxmi Dass*

Main category: cs.CR

TL;DR: HYDRA是一个混合启发式引导的深度表示架构，用于预测已修补函数中的潜在零日漏洞，结合基于规则的启发式方法和深度表示学习来检测补丁后可能持续存在的风险代码模式。


<details>
  <summary>Details</summary>
Motivation: 许多软件安全测试方法会遗漏补丁后的潜在漏洞，这些漏洞通常由于不完整的修复或被忽视的问题导致，可能后来演变为零日漏洞。

Method: HYDRA结合静态漏洞规则、GraphCodeBERT嵌入和变分自编码器(VAE)，在无监督设置下检测符号或神经模型单独使用时经常遗漏的异常。

Result: 在Chrome、Android和ImageMagick三个真实世界软件项目中，HYDRA分别预测13.7%、20.6%和24%的函数包含潜在风险，包括启发式匹配和无启发式匹配的情况。

Conclusion: HYDRA能够发现隐藏的、先前未检测到的风险，推进软件安全验证并支持主动的零日漏洞发现。

Abstract: Software security testing, particularly when enhanced with deep learning models, has become a powerful approach for improving software quality, enabling faster detection of known flaws in source code. However, many approaches miss post-fix latent vulnerabilities that remain even after patches typically due to incomplete fixes or overlooked issues may later lead to zero-day exploits. In this paper, we propose $HYDRA$, a $Hy$brid heuristic-guided $D$eep $R$epresentation $A$rchitecture for predicting latent zero-day vulnerabilities in patched functions that combines rule-based heuristics with deep representation learning to detect latent risky code patterns that may persist after patches. It integrates static vulnerability rules, GraphCodeBERT embeddings, and a Variational Autoencoder (VAE) to uncover anomalies often missed by symbolic or neural models alone. We evaluate HYDRA in an unsupervised setting on patched functions from three diverse real-world software projects: Chrome, Android, and ImageMagick. Our results show HYDRA predicts 13.7%, 20.6%, and 24% of functions from Chrome, Android, and ImageMagick respectively as containing latent risks, including both heuristic matches and cases without heuristic matches ($None$) that may lead to zero-day vulnerabilities. It outperforms baseline models that rely solely on regex-derived features or their combination with embeddings, uncovering truly risky code variants that largely align with known heuristic patterns. These results demonstrate HYDRA's capability to surface hidden, previously undetected risks, advancing software security validation and supporting proactive zero-day vulnerabilities discovery.

</details>


### [7] [Setting $\varepsilon$ is not the Issue in Differential Privacy](https://arxiv.org/abs/2511.06305)
*Edwige Cyffers*

Main category: cs.CR

TL;DR: 本文认为差分隐私的隐私预算不应被视为其相对于其他隐私保护方法的重要限制，指出隐私预算解释困难源于隐私风险评估的内在挑战，而非差分隐私定义本身。


<details>
  <summary>Details</summary>
Motivation: 反驳将隐私预算解释问题作为差分隐私主要限制的观点，防止决策者因此选择不安全的替代方法，澄清隐私风险评估的固有困难。

Method: 通过立场论证，分析隐私预算解释困难的本质根源，指出任何严谨的隐私风险评估方法都会面临类似挑战。

Result: 论证了隐私预算解释困难并非差分隐私特有，而是隐私风险评估的普遍挑战，且任何合理的隐私风险估计方法都应能在差分隐私框架内表达。

Conclusion: 差分隐私框架仍然是隐私保护的严谨方法，隐私预算不应被视为其应用的主要障碍，决策者不应因此选择不安全的替代方案。

Abstract: This position paper argues that setting the privacy budget in differential privacy should not be viewed as an important limitation of differential privacy compared to alternative methods for privacy-preserving machine learning. The so-called problem of interpreting the privacy budget is often presented as a major hindrance to the wider adoption of differential privacy in real-world deployments and is sometimes used to promote alternative mitigation techniques for data protection. We believe this misleads decision-makers into choosing unsafe methods. We argue that the difficulty in interpreting privacy budgets does not stem from the definition of differential privacy itself, but from the intrinsic difficulty of estimating privacy risks in context, a challenge that any rigorous method for privacy risk assessment face. Moreover, we claim that any sound method for estimating privacy risks should, given the current state of research, be expressible within the differential privacy framework or justify why it cannot.

</details>


### [8] [Enhancing Deep Learning-Based Rotational-XOR Attacks on Lightweight Block Ciphers Simon32/64 and Simeck32/64](https://arxiv.org/abs/2511.06336)
*Chengcai Liu,Siwei Chen,Zejun Xiang,Shasha Zhang,Xiangyong Zeng*

Main category: cs.CR

TL;DR: 本文通过神经网络推进旋转异或(RX)攻击，针对轻量级分组密码Simon32/64和Simeck32/64优化区分器并实现密钥恢复攻击。


<details>
  <summary>Details</summary>
Motivation: 自Gohr在CRYPTO 2019开创神经密码分析以来，构建神经区分器已成为实现基于深度学习的密码分析的重要方法。本文旨在通过神经网络推进RX攻击，提高对轻量级分组密码的分析能力。

Method: 1. 专门设计训练RX神经区分器的数据格式；2. 系统识别Hamming权重为1和2的最优RX差异；3. 应用比特敏感度测试压缩数据格式；4. 提出密钥比特敏感度测试和联合错误密钥响应技术。

Result: 获得了Simon32/64的14轮和Simeck32/64的17轮RX神经区分器，分别比之前提高了3轮和2轮。基于改进的16轮RX神经区分器，对Simeck32/64实现了17轮密钥恢复攻击。

Conclusion: 本文成功将神经网络应用于RX密码分析，显著提高了对Simon32/64和Simeck32/64的分析能力，并提出了克服非线性密钥调度挑战的新技术。

Abstract: At CRYPTO 2019, Gohr pioneered neural cryptanalysis by introducing differential-based neural distinguishers to attack Speck32/64, establishing a novel paradigm combining deep learning with differential cryptanalysis.Since then, constructing neural distinguishers has become a significant approach to achieving the deep learning-based cryptanalysis for block ciphers.This paper advances rotational-XOR (RX) attacks through neural networks, focusing on optimizing distinguishers and presenting key-recovery attacks for the lightweight block ciphers Simon32/64 and Simeck32/64.In particular, we first construct the fundamental data formats specially designed for training RX-neural distinguishers by refining the existing data formats for differential-neural distinguishers. Based on these data formats, we systematically identify optimal RX-differences with Hamming weights 1 and 2 that develop high-accuracy RX-neural distinguishers. Then, through innovative application of the bit sensitivity test, we achieve significant compression of data format without sacrificing the distinguisher accuracy. This optimization enables us to add more multi-ciphertext pairs into the data formats, further strengthening the performance of RX-neural distinguishers. As an application, we obtain 14- and 17-round RX-neural distinguishers for Simon32/64 and Simeck32/64, which improves the previous ones by 3 and 2 rounds, respectively.In addition, we propose two novel techniques, key bit sensitivity test and the joint wrong key response, to tackle the challenge of applying Bayesian's key-recovery strategy to the target cipher that adopts nonlinear key schedule in the related-key setting without considering of weak-key space. By this, we can straightforwardly mount a 17-round key-recovery attack on Simeck32/64 based on the improved 16-round RX-nerual distinguisher. To the best of our knowledge, the presented RX-neural......

</details>


### [9] [Ghost in the Transformer: Tracing LLM Lineage with SVD-Fingerprint](https://arxiv.org/abs/2511.06390)
*Suqing Wang,Ziyang Ma,Xinyi Li,Zuchao Li*

Main category: cs.CR

TL;DR: GhostSpec是一种轻量级方法，通过SVD分解内部注意力权重矩阵的不变积来构建模型指纹，无需训练数据即可验证LLM的血统，保护知识产权。


<details>
  <summary>Details</summary>
Motivation: 由于许多开发者基于现有开源模型进行微调但虚假声称原创训练，需要可靠的方法来验证模型来源，保护知识产权。

Method: 应用奇异值分解(SVD)到内部注意力权重矩阵的不变积，构建紧凑且鲁棒的模型指纹，捕捉模型的结构特征。

Result: 实验表明GhostSpec能可靠追踪经过微调、剪枝、块扩展甚至对抗性变换的模型血统，计算开销极小。

Conclusion: GhostSpec为模型验证和重用追踪提供了实用解决方案，有助于保护知识产权并促进透明可信的LLM生态系统。

Abstract: Large Language Models (LLMs) have rapidly advanced and are widely adopted across diverse fields. Due to the substantial computational cost and data requirements of training from scratch, many developers choose to fine-tune or modify existing open-source models. While most adhere to open-source licenses, some falsely claim original training despite clear derivation from public models. This raises pressing concerns about intellectual property protection and highlights the need for reliable methods to verify model provenance. In this paper, we propose GhostSpec, a lightweight yet effective method for verifying LLM lineage without access to training data or modification of model behavior. Our approach constructs compact and robust fingerprints by applying singular value decomposition (SVD) to invariant products of internal attention weight matrices, effectively capturing the structural identity of a model. Unlike watermarking or output-based methods, GhostSpec is fully data-free, non-invasive, and computationally efficient. It demonstrates strong robustness to sequential fine-tuning, pruning, block expansion, and even adversarial transformations. Extensive experiments show that GhostSpec can reliably trace the lineage of transformed models with minimal overhead. By offering a practical solution for model verification and reuse tracking, our method contributes to the protection of intellectual property and fosters a transparent, trustworthy ecosystem for large-scale language models.

</details>


### [10] [Inside LockBit: Technical, Behavioral, and Financial Anatomy of a Ransomware Empire](https://arxiv.org/abs/2511.06429)
*Felipe Castaño,Constantinos Patsakis,Francesco Zola,Fran Casino*

Main category: cs.CR

TL;DR: LockBit从2019年的新兴勒索软件即服务发展为2024年最活跃的勒索软件家族，研究通过分析其管理面板数据、谈判聊天记录和比特币支付链，揭示了其技术演进、社交工程策略和资金洗钱模式。


<details>
  <summary>Details</summary>
Motivation: 研究LockBit如何从新兴勒索软件发展为最活跃的犯罪服务，分析其技术迭代、社交工程策略和资金洗钱机制，以理解其犯罪生态系统的运作模式。

Method: 使用泄露的MySQL管理面板数据进行端到端重建，分析51个谈判聊天记录的自然语言嵌入和聚类，追踪19个比特币地址的支付链模式。

Result: 发现LockBit 3.0相比前代有显著技术强化，谈判过程遵循标准化的修辞阶段，资金洗钱呈现两种不同模式：小部分资金长期留存用于运营，大部分资金最终汇集到两个高交易量地址。

Conclusion: LockBit是一个高度整合的犯罪服务，其韧性依赖于快速代码迭代、脚本化社交工程和工业规模的资金洗钱渠道。

Abstract: LockBit has evolved from an obscure Ransomware-as-a-Service newcomer in 2019 to the most prolific ransomware franchise of 2024. Leveraging a recently leaked MySQL dump of the gang's management panel, this study offers an end-to-end reconstruction of LockBit's technical, behavioral, and financial apparatus. We recall the family's version timeline and map its tactics, techniques, and procedures to MITRE ATT&CK, highlighting the incremental hardening that distinguishes LockBit 3.0 from its predecessors. We then analyze 51 negotiation chat logs using natural-language embeddings and clustering to infer a canonical interaction playbook, revealing recurrent rhetorical stages that underpin the double-extortion strategy. Finally, we trace 19 Bitcoin addresses related to ransom payment chains, revealing two distinct patterns based on different laundering phases. In both cases, a small portion of the ransom is immediately split into long-lived addresses (presumably retained by the group as profit and to finance further operations) while the remainder is ultimately aggregated into two high-volume addresses before likely being sent to the affiliate. These two collector addresses appear to belong to distinct exchanges, each processing over 200k BTC. The combined evidence portrays LockBit as a tightly integrated criminal service whose resilience rests on rapid code iteration, script-driven social engineering, and industrial-scale cash-out pipelines.

</details>


### [11] [EASE: Practical and Efficient Safety Alignment for Small Language Models](https://arxiv.org/abs/2511.06512)
*Haonan Shi,Guoli Wang,Tu Ouyang,An Wang*

Main category: cs.CR

TL;DR: EASE框架为小型语言模型提供高效安全对齐，通过选择性激活安全推理机制，在保持安全性的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 当前浅层对齐方法无法有效防御对抗性越狱攻击，而深思熟虑的安全推理对齐虽然效果更好但计算开销过大，不适合资源受限的边缘设备部署。

Method: 首先识别最优安全推理教师模型，然后训练小型模型选择性激活安全推理机制：仅对危险对抗性查询进行安全推理，对直接恶意查询和一般任务提供直接响应。

Result: 相比浅层对齐方法，EASE将越狱攻击成功率降低高达17%；相比深思熟虑安全推理对齐，推理开销降低高达90%。

Conclusion: EASE框架为小型语言模型提供了实用高效的安全对齐方案，既保持了对复杂攻击的鲁棒安全保证，又满足了边缘部署场景对计算效率的要求。

Abstract: Small language models (SLMs) are increasingly deployed on edge devices, making their safety alignment crucial yet challenging. Current shallow alignment methods that rely on direct refusal of malicious queries fail to provide robust protection, particularly against adversarial jailbreaks. While deliberative safety reasoning alignment offers deeper alignment for defending against sophisticated attacks, effectively implanting such reasoning capability in SLMs with limited capabilities remains an open challenge. Moreover, safety reasoning incurs significant computational overhead as models apply reasoning to nearly all queries, making it impractical for resource-constrained edge deployment scenarios that demand rapid responses. We propose EASE, a novel framework that enables practical and Efficient safety Alignment for Small languagE models. Our approach first identifies the optimal safety reasoning teacher that can effectively distill safety reasoning capabilities to SLMs. We then align models to selectively activate safety reasoning for dangerous adversarial jailbreak queries while providing direct responses to straightforward malicious queries and general helpful tasks. This selective mechanism enables small models to maintain robust safety guarantees against sophisticated attacks while preserving computational efficiency for benign interactions. Experimental results demonstrate that EASE reduces jailbreak attack success rates by up to 17% compared to shallow alignment methods while reducing inference overhead by up to 90% compared to deliberative safety reasoning alignment, making it practical for SLMs real-world edge deployments.

</details>


### [12] [CYPRESS: Transferring Secrets in the Shadow of Visible Packets](https://arxiv.org/abs/2511.06540)
*Sirus Shahini,Robert Ricci*

Main category: cs.CR

TL;DR: CYPRESS是一个隐蔽通信框架，通过在常规网络数据包上搭载秘密网络实体的数据包来创建可靠的隐蔽通信通道，无需为秘密通信生成新的数据包。


<details>
  <summary>Details</summary>
Motivation: 现有的网络隐写和隐蔽通信通道研究实用性有限，仅限于特定用例和网络协议。本文旨在展示网络隐蔽通道在实际秘密通信中具有比以往讨论更大的潜力。

Method: CYPRESS建立了一个统一的去中心化框架，为不同协议定义自定义处理程序的隐蔽通道，并在运行时选择如何以及按什么顺序使用这些通道进行数据分片和隐藏传输。

Result: 在10个用户连接到互联网的网络中，可以达到高达1.6MB/s的秘密带宽。通过各种安全敏感的真实世界实验证明了该方法的鲁棒性和可靠性。

Conclusion: 网络协议为非常规数据存储和隐藏传输提供了显著机会，可以绕过不同类型的安全措施并隐藏各种网络攻击的来源。

Abstract: Network steganography and covert communication channels have been studied extensively in the past. However, prior works offer minimal practical use for their proposed techniques and are limited to specific use cases and network protocols. In this paper, we show that covert channels in networking have a much greater potential for practical secret communication than what has been discussed before. We present a covert channel framework, CYPRESS, that creates a reliable hidden communication channel by mounting packets from secret network entities on regular packets that flow through the network, effectively transmitting a separate network traffic without generating new packets for it. CYPRESS establishes a consolidated decentralized framework in which different covert channels for various protocols are defined with their custom handler code that are plugged into the system and updated on-demand to evade detection. CYPRESS then chooses at run-time how and in what order the covert channels should be used for fragmentation and hidden transmission of data. We can reach up to 1.6MB/s of secret bandwidth in a network of ten users connected to the Internet. We demonstrate the robustness and reliability of our approach in secret communication through various security-sensitive real-world experiments. Our evaluations show that network protocols provide a notable opportunity for unconventional storage and hidden transmission of data to bypass different types of security measures and to hide the source of various cyber attacks.

</details>


### [13] [SteganoSNN: SNN-Based Audio-in-Image Steganography with Encryption](https://arxiv.org/abs/2511.06573)
*Biswajit Kumar Sahoo,Pedro Machado,Isibor Kennedy Ihianle,Andreas Oikonomou,Srinivas Boppu*

Main category: cs.CR

TL;DR: SteganoSNN是一个基于脉冲神经网络的隐写框架，能够在RGBA图像中隐藏音频数据，实现实时、低功耗、高容量的安全通信。


<details>
  <summary>Details</summary>
Motivation: 传统数据隐藏方案面临生成式AI系统带来的安全威胁，需要在计算效率和感知透明度之间找到平衡。

Method: 将数字音频样本通过LIF神经元转换为脉冲序列，使用模映射方案加密，并通过抖动机制嵌入到RGBA图像通道的最低有效位中。

Result: 在DIV2K 2017数据集上，PSNR达到40.4-41.35 dB，SSIM值持续高于0.97，计算效率和鲁棒性优于SteganoGAN。

Conclusion: SteganoSNN为神经形态隐写学奠定了基础，为边缘AI、物联网和生物医学应用提供安全、节能的通信方案。

Abstract: Secure data hiding remains a fundamental challenge in digital communication, requiring a careful balance between computational efficiency and perceptual transparency. The balance between security and performance is increasingly fragile with the emergence of generative AI systems capable of autonomously generating and optimising sophisticated cryptanalysis and steganalysis algorithms, thereby accelerating the exposure of vulnerabilities in conventional data-hiding schemes.
  This work introduces SteganoSNN, a neuromorphic steganographic framework that exploits spiking neural networks (SNNs) to achieve secure, low-power, and high-capacity multimedia data hiding. Digitised audio samples are converted into spike trains using leaky integrate-and-fire (LIF) neurons, encrypted via a modulo-based mapping scheme, and embedded into the least significant bits of RGBA image channels using a dithering mechanism to minimise perceptual distortion. Implemented in Python using NEST and realised on a PYNQ-Z2 FPGA, SteganoSNN attains real-time operation with an embedding capacity of 8 bits per pixel. Experimental evaluations on the DIV2K 2017 dataset demonstrate image fidelity between 40.4 dB and 41.35 dB in PSNR and SSIM values consistently above 0.97, surpassing SteganoGAN in computational efficiency and robustness. SteganoSNN establishes a foundation for neuromorphic steganography, enabling secure, energy-efficient communication for Edge-AI, IoT, and biomedical applications.

</details>


### [14] [Secure Low-altitude Maritime Communications via Intelligent Jamming](https://arxiv.org/abs/2511.06659)
*Jiawei Huang,Aimin Wang,Geng Sun,Jiahui Li,Jiacheng Wang,Weijie Yuan,Dusit Niyato,Xianbin Wang*

Main category: cs.CR

TL;DR: 提出了一种基于SAC-CVAE算法的智能干扰方法，用于保护海上低空无线网络免受动态窃听者攻击，实现安全且节能的通信。


<details>
  <summary>Details</summary>
Motivation: 海上LAWNs中的无人机通信信道开放且清晰，容易受到窃听攻击。现有安全方法假设窃听者遵循预定轨迹，无法捕捉真实海洋环境中窃听者的动态移动模式。

Method: 将问题建模为部分可观测马尔可夫决策过程(POMDP)，提出SAC-CVAE算法（基于条件变分自编码器改进的软演员-评论家算法），利用优势条件潜在表示来解耦和优化策略，并通过降低状态空间维度提高计算效率。

Result: 仿真结果表明，所提出的智能干扰方法能够实现安全且节能的海上通信。

Conclusion: SAC-CVAE算法有效解决了动态窃听者定位不确定的问题，在保密率和无人机能耗之间取得了良好平衡，为海上低空无线网络安全提供了可行解决方案。

Abstract: Low-altitude wireless networks (LAWNs) have emerged as a viable solution for maritime communications. In these maritime LAWNs, unmanned aerial vehicles (UAVs) serve as practical low-altitude platforms for wireless communications due to their flexibility and ease of deployment. However, the open and clear UAV communication channels make maritime LAWNs vulnerable to eavesdropping attacks. Existing security approaches often assume eavesdroppers follow predefined trajectories, which fails to capture the dynamic movement patterns of eavesdroppers in realistic maritime environments. To address this challenge, we consider a low-altitude maritime communication system that employs intelligent jamming to counter dynamic eavesdroppers with uncertain positioning to enhance the physical layer security. Since such a system requires balancing the conflicting performance metrics of the secrecy rate and energy consumption of UAVs, we formulate a secure and energy-efficient maritime communication multi-objective optimization problem (SEMCMOP). To solve this dynamic and long-term optimization problem, we first reformulate it as a partially observable Markov decision process (POMDP). We then propose a novel soft actor-critic with conditional variational autoencoder (SAC-CVAE) algorithm, which is a deep reinforcement learning algorithm improved by generative artificial intelligence. Specifically, the SAC-CVAE algorithm employs advantage-conditioned latent representations to disentangle and optimize policies, while enhancing computational efficiency by reducing the state space dimension. Simulation results demonstrate that our proposed intelligent jamming approach achieves secure and energy-efficient maritime communications.

</details>


### [15] [Adversarial Node Placement in Decentralized Federated Learning: Maximum Spanning-Centrality Strategy and Performance Analysis](https://arxiv.org/abs/2511.06742)
*Adam Piaseczny,Eric Ruzomberka,Rohit Parasnis,Christopher G. Brinton*

Main category: cs.CR

TL;DR: 本文研究了去中心化联邦学习中对抗性节点放置策略的影响，提出了一种混合攻击方法MaxSpAN-FL，并证明了其在各种网络配置下对模型性能的破坏效果最大。


<details>
  <summary>Details</summary>
Motivation: 随着联邦学习的普及，其去中心化变体受到关注，但去中心化FL引入了新的安全漏洞，特别是对抗性节点放置策略的影响尚未被充分探索。

Method: 采用了两种节点放置策略：基于最大跨度的算法和基于网络中心性的方法，并提出了混合攻击策略MaxSpAN-FL，该策略根据网络拓扑特征概率性地调整节点放置。

Result: 数值实验表明，与基线方案相比，MaxSpAN-FL攻击在各种网络配置和对抗者数量下都能对去中心化FL模型造成最大的性能下降。

Conclusion: 研究揭示了去中心化FL系统的脆弱性，为未来开发更安全、更鲁棒的去中心化FL框架奠定了基础。

Abstract: As Federated Learning (FL) becomes more widespread, there is growing interest in its decentralized variants. Decentralized FL leverages the benefits of fast and energy-efficient device-to-device communications to obviate the need for a central server. However, this opens the door to new security vulnerabilities as well. While FL security has been a popular research topic, the role of adversarial node placement in decentralized FL remains largely unexplored. This paper addresses this gap by evaluating the impact of various coordinated adversarial node placement strategies on decentralized FL's model training performance. We adapt two threads of placement strategies to this context: maximum span-based algorithms, and network centrality-based approaches. Building on them, we propose a novel attack strategy, MaxSpAN-FL, which is a hybrid between these paradigms that adjusts node placement probabilistically based on network topology characteristics. Numerical experiments demonstrate that our attack consistently induces the largest degradation in decentralized FL models compared with baseline schemes across various network configurations and numbers of coordinating adversaries. We also provide theoretical support for why eigenvector centrality-based attacks are suboptimal in decentralized FL. Overall, our findings provide valuable insights into the vulnerabilities of decentralized FL systems, setting the stage for future research aimed at developing more secure and robust decentralized FL frameworks.

</details>


### [16] [Differentiated Directional Intervention A Framework for Evading LLM Safety Alignment](https://arxiv.org/abs/2511.06852)
*Peng Zhang,peijie sun*

Main category: cs.CR

TL;DR: 本文提出了一种新的安全对齐解构方法，将拒绝机制分解为伤害检测和拒绝执行两个独立方向，并开发了DBDI框架来精确绕过LLM的安全防护。


<details>
  <summary>Details</summary>
Motivation: 现有研究将LLM的拒绝机制简化为激活空间中的单一线性方向，这种过度简化混淆了伤害检测和拒绝执行两个不同的神经过程。

Method: 提出了区分性双向干预（DBDI）框架，通过自适应投影消除来中和拒绝执行方向，同时通过直接引导抑制伤害检测方向。

Result: 在Llama-2等模型上实现了高达97.88%的攻击成功率，显著优于现有的越狱方法。

Conclusion: 通过提供更细粒度的机制框架，为深入理解LLM安全对齐提供了新方向。

Abstract: Safety alignment instills in Large Language Models (LLMs) a critical capacity to refuse malicious requests. Prior works have modeled this refusal mechanism as a single linear direction in the activation space. We posit that this is an oversimplification that conflates two functionally distinct neural processes: the detection of harm and the execution of a refusal. In this work, we deconstruct this single representation into a Harm Detection Direction and a Refusal Execution Direction. Leveraging this fine-grained model, we introduce Differentiated Bi-Directional Intervention (DBDI), a new white-box framework that precisely neutralizes the safety alignment at critical layer. DBDI applies adaptive projection nullification to the refusal execution direction while suppressing the harm detection direction via direct steering. Extensive experiments demonstrate that DBDI outperforms prominent jailbreaking methods, achieving up to a 97.88\% attack success rate on models such as Llama-2. By providing a more granular and mechanistic framework, our work offers a new direction for the in-depth understanding of LLM safety alignment.

</details>


### [17] [Nearly-Optimal Private Selection via Gaussian Mechanism](https://arxiv.org/abs/2511.06871)
*Ethan Leeman,Pasin Manurangsi*

Main category: cs.CR

TL;DR: 本文解决了Steinke(2025)提出的开放性问题：仅通过（自适应地）调用高斯机制处理低敏感度查询，能否实现近似最优误差的差分隐私选择问题。作者给出了肯定答案，实现了$\tilde{O}(\log |\mathcal{Y}|)$的误差保证，改进了Steinke机制$O(\log^{3/2} |\mathcal{Y}|)$的误差。


<details>
  <summary>Details</summary>
Motivation: 解决Steinke(2025)提出的开放性问题：是否仅通过自适应调用高斯机制处理低敏感度查询就能实现近似最优误差的差分隐私选择问题。

Method: 使用自适应调用高斯机制处理低敏感度查询的方法，针对候选集$\mathcal{Y}$进行差分隐私选择。

Result: 实现了$\tilde{O}(\log |\mathcal{Y}|)$的误差保证，这比指数机制(McSherry和Talwar, 2007)仅多$(\log \log |\mathcal{Y}|)^{O(1)}$因子，显著改进了Steinke机制的$O(\log^{3/2} |\mathcal{Y}|)$误差。

Conclusion: 成功解决了Steinke的开放性问题，证明了仅通过自适应调用高斯机制处理低敏感度查询就能实现近似最优误差的差分隐私选择，为差分隐私算法设计提供了新的思路。

Abstract: Steinke (2025) recently asked the following intriguing open question: Can we solve the differentially private selection problem with nearly-optimal error by only (adaptively) invoking Gaussian mechanism on low-sensitivity queries? We resolve this question positively. In particular, for a candidate set $\mathcal{Y}$, we achieve error guarantee of $\tilde{O}(\log |\mathcal{Y}|)$, which is within a factor of $(\log \log |\mathcal{Y}|)^{O(1)}$ of the exponential mechanism (McSherry and Talwar, 2007). This improves on Steinke's mechanism which achieves an error of $O(\log^{3/2} |\mathcal{Y}|)$.

</details>


### [18] [Uncovering Pretraining Code in LLMs: A Syntax-Aware Attribution Approach](https://arxiv.org/abs/2511.07033)
*Yuanheng Li,Zhuoyang Chen,Xiaoyun Liu,Yuhao Wang,Mingwei Liu,Yang Shi,Kaifeng Huang,Shengjie Zhao*

Main category: cs.CR

TL;DR: SynPrune是一种针对代码的语法剪枝成员推理攻击方法，通过利用编程语言的结构化特性，识别并排除语法必需的后续标记，在计算成员分数时提高检测训练数据中特定代码样本的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力的增强，在训练数据中未经授权使用受版权和许可保护的内容（特别是代码）引发法律和道德担忧。检测特定代码样本是否包含在LLM训练数据中对透明度、问责制和版权合规至关重要。

Method: SynPrune利用编程语言的结构化和规则约束特性，识别并排除语法必需的后续标记（这些标记不反映作者身份），在计算成员分数时进行语法剪枝。

Result: 实验结果表明，SynPrune在性能上持续优于现有最先进方法，并且在不同函数长度和语法类别下都具有鲁棒性。

Conclusion: SynPrune通过利用代码的语法结构特性，为检测训练数据中的代码样本提供了一种有效的成员推理攻击方法，在准确性和鲁棒性方面均表现优异。

Abstract: As large language models (LLMs) become increasingly capable, concerns over the unauthorized use of copyrighted and licensed content in their training data have grown, especially in the context of code. Open-source code, often protected by open source licenses (e.g, GPL), poses legal and ethical challenges when used in pretraining. Detecting whether specific code samples were included in LLM training data is thus critical for transparency, accountability, and copyright compliance. We propose SynPrune, a syntax-pruned membership inference attack method tailored for code. Unlike prior MIA approaches that treat code as plain text, SynPrune leverages the structured and rule-governed nature of programming languages. Specifically, it identifies and excludes consequent tokens that are syntactically required and not reflective of authorship, from attribution when computing membership scores. Experimental results show that SynPrune consistently outperforms the state-of-the-arts. Our method is also robust across varying function lengths and syntax categories.

</details>


### [19] [Harnessing Sparsification in Federated Learning: A Secure, Efficient, and Differentially Private Realization](https://arxiv.org/abs/2511.07123)
*Shuangqing Xu,Yifeng Zheng,Zhongyun Hua*

Main category: cs.CR

TL;DR: Clover是一个联邦学习系统框架，通过top-k梯度稀疏化解决通信瓶颈，使用三服务器分布式信任设置保护隐私，并提供差分隐私保证和恶意服务器安全防护。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临通信瓶颈和隐私泄露风险，需要同时解决高效通信和安全隐私保护的问题。

Method: 采用top-k梯度稀疏化减少通信量，构建三服务器分布式信任机制隐藏稀疏梯度值和索引，集成轻量级分布式噪声生成提供差分隐私，设计完整性检查机制防御恶意服务器。

Result: 相比基于分布式ORAM的基线方法，服务器端通信和运行时性能提升数个数量级，客户端通信成本更小，能够达到与中心差分隐私的普通联邦学习相当的效用。

Conclusion: Clover框架在保持联邦学习效用的同时，有效解决了通信效率、隐私保护和安全性问题，具有实际应用前景。

Abstract: Federated learning (FL) enables multiple clients to jointly train a model by sharing only gradient updates for aggregation instead of raw data. Due to the transmission of very high-dimensional gradient updates from many clients, FL is known to suffer from a communication bottleneck. Meanwhile, the gradients shared by clients as well as the trained model may also be exploited for inferring private local datasets, making privacy still a critical concern in FL. We present Clover, a novel system framework for communication-efficient, secure, and differentially private FL. To tackle the communication bottleneck in FL, Clover follows a standard and commonly used approach-top-k gradient sparsification, where each client sparsifies its gradient update such that only k largest gradients (measured by magnitude) are preserved for aggregation. Clover provides a tailored mechanism built out of a trending distributed trust setting involving three servers, which allows to efficiently aggregate multiple sparse vectors (top-k sparsified gradient updates) into a dense vector while hiding the values and indices of non-zero elements in each sparse vector. This mechanism outperforms a baseline built on the general distributed ORAM technique by several orders of magnitude in server-side communication and runtime, with also smaller client communication cost. We further integrate this mechanism with a lightweight distributed noise generation mechanism to offer differential privacy (DP) guarantees on the trained model. To harden Clover with security against a malicious server, we devise a series of lightweight mechanisms for integrity checks on the server-side computation. Extensive experiments show that Clover can achieve utility comparable to vanilla FL with central DP, with promising performance.

</details>


### [20] [Privacy on the Fly: A Predictive Adversarial Transformation Network for Mobile Sensor Data](https://arxiv.org/abs/2511.07242)
*Tianle Song,Chenhao Lin,Yang Cao,Zhengyu Zhao,Jiahao Sun,Chong Zhang,Le Yang,Chao Shen*

Main category: cs.CR

TL;DR: 提出了PATN框架，通过历史信号生成对抗性扰动来实时保护移动运动传感器数据的隐私，同时保持良性任务的实用性。


<details>
  <summary>Details</summary>
Motivation: 移动运动传感器数据易被第三方应用推断敏感用户特征，现有隐私保护技术存在延迟高、破坏数据实用性的问题。

Method: 使用预测性对抗变换网络，利用历史信号主动生成对抗性扰动，在数据采集时立即应用。

Result: 在两个数据集上，PATN显著降低了隐私推断模型的性能，攻击成功率降至40.11%和44.65%，等错误率从8.30%和7.56%提升至41.65%和46.22%。

Conclusion: PATN框架能够实时保护移动传感器数据隐私，同时保持良性应用的实用性，优于现有基线方法。

Abstract: Mobile motion sensors such as accelerometers and gyroscopes are now ubiquitously accessible by third-party apps via standard APIs. While enabling rich functionalities like activity recognition and step counting, this openness has also enabled unregulated inference of sensitive user traits, such as gender, age, and even identity, without user consent. Existing privacy-preserving techniques, such as GAN-based obfuscation or differential privacy, typically require access to the full input sequence, introducing latency that is incompatible with real-time scenarios. Worse, they tend to distort temporal and semantic patterns, degrading the utility of the data for benign tasks like activity recognition. To address these limitations, we propose the Predictive Adversarial Transformation Network (PATN), a real-time privacy-preserving framework that leverages historical signals to generate adversarial perturbations proactively. The perturbations are applied immediately upon data acquisition, enabling continuous protection without disrupting application functionality. Experiments on two datasets demonstrate that PATN substantially degrades the performance of privacy inference models, achieving Attack Success Rate (ASR) of 40.11% and 44.65% (reducing inference accuracy to near-random) and increasing the Equal Error Rate (EER) from 8.30% and 7.56% to 41.65% and 46.22%. On ASR, PATN outperforms baseline methods by 16.16% and 31.96%, respectively.

</details>


### [21] [JPRO: Automated Multimodal Jailbreaking via Multi-Agent Collaboration Framework](https://arxiv.org/abs/2511.07315)
*Yuxuan Zhou,Yang Bai,Kuofeng Gao,Tao Dai,Shu-Tao Xia*

Main category: cs.CR

TL;DR: JPRO是一个多智能体协作框架，用于自动化攻击视觉语言模型(VLMs)，通过协调四个专业智能体和两个核心模块，在多种先进VLM上实现了超过60%的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有VLM越狱攻击方法存在局限性：要么需要白盒访问(不实用)，要么依赖人工设计模式(样本多样性和可扩展性差)，需要解决这些缺陷。

Method: 提出JPRO多智能体协作框架，包含四个专业智能体和两个核心模块：策略驱动的种子生成和自适应优化循环，生成有效且多样化的攻击样本。

Result: 实验结果显示，JPRO在包括GPT-4o在内的多种先进VLM上实现了超过60%的攻击成功率，显著优于现有方法。

Conclusion: 作为黑盒攻击方法，JPRO不仅揭示了多模态模型的关键安全漏洞，还为评估和增强VLM鲁棒性提供了宝贵见解。

Abstract: The widespread application of large VLMs makes ensuring their secure deployment critical. While recent studies have demonstrated jailbreak attacks on VLMs, existing approaches are limited: they require either white-box access, restricting practicality, or rely on manually crafted patterns, leading to poor sample diversity and scalability. To address these gaps, we propose JPRO, a novel multi-agent collaborative framework designed for automated VLM jailbreaking. It effectively overcomes the shortcomings of prior methods in attack diversity and scalability. Through the coordinated action of four specialized agents and its two core modules: Tactic-Driven Seed Generation and Adaptive Optimization Loop, JPRO generates effective and diverse attack samples. Experimental results show that JPRO achieves over a 60\% attack success rate on multiple advanced VLMs, including GPT-4o, significantly outperforming existing methods. As a black-box attack approach, JPRO not only uncovers critical security vulnerabilities in multimodal models but also offers valuable insights for evaluating and enhancing VLM robustness.

</details>
