<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 15]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [A Research and Development Portfolio of GNN Centric Malware Detection, Explainability, and Dataset Curation](https://arxiv.org/abs/2511.20801)
*Hossein Shokouhinejad,Griffin Higgins,Roozbeh Razavi-Far,Ali A. Ghorbani*

Main category: cs.CR

TL;DR: 该论文组合了六项相关研究，共同解决基于图神经网络的恶意软件检测在可扩展性、可解释性和数据集可靠性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在恶意软件检测中面临可扩展性、可解释性和可靠数据集可用性的重要挑战。

Method: 包括图缩减方法、集成缩减学习方法、基于子图匹配的双重解释技术、注意力引导的堆叠GNN集成模型，并发布了控制流图数据集。

Result: 形成了一套连贯的研究体系，通过提高效率、增加透明度和提供坚实的实验基础来加强基于GNN的恶意软件检测。

Conclusion: 这些贡献共同增强了基于GNN的恶意软件检测能力，在效率、透明度和实验基础方面都有显著提升。

Abstract: Graph Neural Networks (GNNs) have become an effective tool for malware detection by capturing program execution through graph-structured representations. However, important challenges remain regarding scalability, interpretability, and the availability of reliable datasets. This paper brings together six related studies that collectively address these issues. The portfolio begins with a survey of graph-based malware detection and explainability, then advances to new graph reduction methods, integrated reduction-learning approaches, and investigations into the consistency of explanations. It also introduces dual explanation techniques based on subgraph matching and develops ensemble-based models with attention-guided stacked GNNs to improve interpretability. In parallel, curated datasets of control flow graphs are released to support reproducibility and enable future research. Together, these contributions form a coherent line of research that strengthens GNN-based malware detection by enhancing efficiency, increasing transparency, and providing solid experimental foundations.

</details>


### [2] [Private Data Imputation](https://arxiv.org/abs/2511.20832)
*Abdelkarim Kati,Florian Kerschbaum,Marina Blanton*

Main category: cs.CR

TL;DR: 本文提出了首个优化的隐私数据插补协议，针对水平和垂直分割的数据集，通过将大部分计算简化为私有集合交集协议，显著提高了数据插补的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有数据插补方法会泄露隐私，使得下游隐私保护分析失效，因此需要开发隐私保护的数据插补方法。

Method: 使用优化的私有集合交集协议或可编程伪随机函数协议来处理水平和垂直分割的数据集，将大部分计算简化为这些高效协议。

Result: 在垂直分割数据上平均准确率提高20%，水平分割数据提高5%；最差情况下垂直分割数据插补质量提升32.7倍，水平分割提升3.4倍；处理10万条记录仅需2.4-8.4秒。

Conclusion: 隐私数据分析需要隐私数据插补，所提出的协议在保持隐私的同时显著提高了数据插补的准确性和效率。

Abstract: Data imputation is an important data preparation task where the data analyst replaces missing or erroneous values to increase the expected accuracy of downstream analyses. The accuracy improvement of data imputation extends to private data analyses across distributed databases. However, existing data imputation methods violate the privacy of the data rendering the privacy protection in the downstream analyses obsolete. We conclude that private data analysis requires private data imputation.
  In this paper, we present the first optimized protocols for private data imputation. We consider the case of horizontally and vertically split data sets. Our optimization aims to reduce most of the computation to private set intersection (or at least oblivious programmable pseudo-random function) protocols which can be very efficiently computed. We show that private data imputation has -- on average across all evaluated datasets -- an accuracy advantage of 20\% in case of vertically split data and 5\% in case of horizontally split data over imputing data locally. In case of the worst data split we observed that imputing using our method resulted in an increase of up to 32.7 times in the quality of imputation over the vertically split data and 3.4 times in case of horizontally split data. Our protocols are very efficient and run in 2.4 seconds in case of vertically split data and 8.4 seconds in case of horizontally split data for 100,000 records evaluated in the 10 Gbps network setting, performing one data imputation.

</details>


### [3] [Supporting Students in Navigating LLM-Generated Insecure Code](https://arxiv.org/abs/2511.20878)
*Jaehwan Park,Kyungchan Lim,Seonhye Park,Doowon Kim*

Main category: cs.CR

TL;DR: Bifröst是一个教育框架，通过在AI辅助开发中模拟不安全代码生成来培养安全意识和批判性评估能力。


<details>
  <summary>Details</summary>
Motivation: AI辅助开发虽然提高了效率，但LLM可能生成不安全代码，而现有教育方法忽视了这些安全风险，导致学生无法识别和缓解AI辅助工作流中的安全问题。

Method: 开发了Bifröst框架，包含VS Code扩展模拟真实环境、对抗性配置的LLM生成不安全代码、以及突出漏洞的反馈系统，让学生在受污染的LLM环境中完成任务并获得安全分析。

Result: 课堂部署（n=61）显示学生对不安全代码存在漏洞，干预后调查（n=21）表明对LLM输出的怀疑态度增强。

Conclusion: Bifröst通过沉浸式体验有效培养了学生在AI辅助开发中的安全意识和批判性思维。

Abstract: The advent of Artificial Intelligence (AI), particularly large language models (LLMs), has revolutionized software development by enabling developers to specify tasks in natural language and receive corresponding code, boosting productivity. However, this shift also introduces security risks, as LLMs may generate insecure code that can be exploited by adversaries. Current educational approaches emphasize efficiency while overlooking these risks, leaving students underprepared to identify and mitigate security issues in AI-assisted workflows.
  To address this gap, we present Bifröst, an educational framework that cultivates security awareness in AI-augmented development. Bifröst integrates (1) a Visual Studio Code extension simulating realistic environments, (2) adversarially configured LLMs that generate insecure code, and (3) a feedback system highlighting vulnerabilities. By immersing students in tasks with compromised LLMs and providing targeted security analysis, Bifröst cultivates critical evaluation skills; classroom deployments (n=61) show vulnerability to insecure code, while a post-intervention survey (n=21) indicates increased skepticism toward LLM outputs.

</details>


### [4] [A Taxonomy of Pix Fraud in Brazil: Attack Methodologies, AI-Driven Amplification, and Defensive Strategies](https://arxiv.org/abs/2511.20902)
*Glener Lanes Pizzolato,Brenda Medeiros Lopes,Claudio Schepke,Diego Kreutz*

Main category: cs.CR

TL;DR: 对巴西Pix即时支付系统攻击方法的综述研究，分析了针对用户和金融机构的主要欺诈类型及其演变趋势。


<details>
  <summary>Details</summary>
Motivation: 随着Pix系统在巴西的普及，针对该系统的欺诈攻击日益复杂化，需要系统性地识别和分类主要攻击方法，为安全防护提供依据。

Method: 结合结构化文献综述和与银行业专业人士的探索性访谈。

Result: 欺诈方案已从纯粹的社会工程方法演变为结合人为操纵和技术利用的混合策略。

Conclusion: 安全措施必须与攻击方法复杂度的增长同步发展，特别强调适应性防御和持续的用户意识教育。

Abstract: This work presents a review of attack methodologies targeting Pix, the instant payment system launched by the Central Bank of Brazil in 2020. The study aims to identify and classify the main types of fraud affecting users and financial institutions, highlighting the evolution and increasing sophistication of these techniques. The methodology combines a structured literature review with exploratory interviews conducted with professionals from the banking sector. The results show that fraud schemes have evolved from purely social engineering approaches to hybrid strategies that integrate human manipulation with technical exploitation. The study concludes that security measures must advance at the same pace as the growing complexity of attack methodologies, with particular emphasis on adaptive defenses and continuous user awareness.

</details>


### [5] [Securing the Model Context Protocol (MCP): Risks, Controls, and Governance](https://arxiv.org/abs/2511.20920)
*Herman Errico,Jiquan Ngiam,Shanita Sojan*

Main category: cs.CR

TL;DR: MCP协议引入动态用户驱动系统带来新的安全风险，包括内容注入攻击、供应链攻击和代理越权。本文分析了这些威胁并提出实用控制措施，包括认证授权、溯源跟踪、沙箱隔离、策略执行和集中治理。


<details>
  <summary>Details</summary>
Motivation: 随着MCP在社区服务器和主要平台的采用增长，组织面临现有AI治理框架尚未详细覆盖的新安全威胁，需要专门的安全控制措施。

Method: 基于早期事件和概念验证攻击，分析MCP如何通过数据驱动泄露、工具污染和跨系统权限提升扩大攻击面，并提出相应的控制措施。

Result: 识别了三种主要攻击者类型和相应的攻击向量，提出了一套实用的安全控制框架来应对这些威胁。

Conclusion: 需要帮助组织确保未经验证的代码不在沙箱外运行、工具不超出预期范围使用、数据泄露尝试可检测、操作可端到端审计，并提出了开放研究问题。

Abstract: The Model Context Protocol (MCP) replaces static, developer-controlled API integrations with more dynamic, user-driven agent systems, which also introduces new security risks. As MCP adoption grows across community servers and major platforms, organizations encounter threats that existing AI governance frameworks (such as NIST AI RMF and ISO/IEC 42001) do not yet cover in detail. We focus on three types of adversaries that take advantage of MCP s flexibility: content-injection attackers that embed malicious instructions into otherwise legitimate data; supply-chain attackers who distribute compromised servers; and agents who become unintentional adversaries by over-stepping their role. Based on early incidents and proof-of-concept attacks, we describe how MCP can increase the attack surface through data-driven exfiltration, tool poisoning, and cross-system privilege escalation. In response, we propose a set of practical controls, including per-user authentication with scoped authorization, provenance tracking across agent workflows, containerized sandboxing with input/output checks, inline policy enforcement with DLP and anomaly detection, and centralized governance using private registries or gateway layers. The aim is to help organizations ensure that unvetted code does not run outside a sandbox, tools are not used beyond their intended scope, data exfiltration attempts are detectable, and actions can be audited end-to-end. We close by outlining open research questions around verifiable registries, formal methods for these dynamic systems, and privacy-preserving agent operations.

</details>


### [6] [Readout-Side Bypass for Residual Hybrid Quantum-Classical Models](https://arxiv.org/abs/2511.20922)
*Guilin Zhang,Wulan Guo,Ziqi Tan,Hongyang He,Hailong Jiang*

Main category: cs.CR

TL;DR: 提出了一种轻量级残差混合架构，将量子特征与原始输入在分类前连接，绕过测量瓶颈，在保持量子复杂度不变的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习虽然具有紧凑和表达性强的优势，但面临测量瓶颈问题——狭窄的量子-经典读出限制了性能并放大了隐私风险。

Method: 采用残差混合架构，在量子-经典接口处将量子特征与原始输入进行连接，避免增加量子复杂度。

Result: 在集中式和联邦式设置中均优于纯量子模型和先前的混合模型，相比量子基线准确率提升高达+55%，同时保持低通信成本和增强的隐私鲁棒性。

Conclusion: 该方法为在隐私敏感、资源受限的环境（如联邦边缘学习）中集成量子模型提供了一条实用的近期路径。

Abstract: Quantum machine learning (QML) promises compact and expressive representations, but suffers from the measurement bottleneck - a narrow quantum-to-classical readout that limits performance and amplifies privacy risk. We propose a lightweight residual hybrid architecture that concatenates quantum features with raw inputs before classification, bypassing the bottleneck without increasing quantum complexity. Experiments show our model outperforms pure quantum and prior hybrid models in both centralized and federated settings. It achieves up to +55% accuracy improvement over quantum baselines, while retaining low communication cost and enhanced privacy robustness. Ablation studies confirm the effectiveness of the residual connection at the quantum-classical interface. Our method offers a practical, near-term pathway for integrating quantum models into privacy-sensitive, resource-constrained settings like federated edge learning.

</details>


### [7] [Road Network-Aware Personalized Trajectory Protection with Differential Privacy under Spatiotemporal Correlations](https://arxiv.org/abs/2511.21020)
*Minghui Min,Jiahui Liu,Mingge Cao,Shiyin Li,Hongliang Zhang,Miao Pan,Zhu Han*

Main category: cs.CR

TL;DR: 提出个性化轨迹隐私保护机制PTPPM，通过结合地理不可区分性和失真隐私，让用户自定义隐私偏好，在保护位置隐私的同时维持服务质量。


<details>
  <summary>Details</summary>
Motivation: 基于位置的服务虽然便利但存在隐私风险，攻击者可通过时空相关性推断敏感信息。由于用户对位置数据的敏感度因停留时间、访问频率等因素而异，需要个性化隐私保护。

Method: 建模攻击者对轨迹时空相关性的知识，结合地理不可区分性和失真隐私构建保护位置集，提出个性化隐私预算分配算法PPBA和Permute-and-Flip扰动机制。

Result: 仿真结果表明该机制优于现有基准方法，在满足用户服务质量要求的同时提供更好的隐私保护。

Conclusion: PTPPM机制能有效平衡隐私保护和服务质量，通过个性化隐私预算分配和最小化扰动距离实现高效的位置隐私保护。

Abstract: Location-Based Services (LBSs) offer significant convenience to mobile users but pose significant privacy risks, as attackers can infer sensitive personal information through spatiotemporal correlations in user trajectories. Since users' sensitivity to location data varies based on factors such as stay duration, access frequency, and semantic sensitivity, implementing personalized privacy protection is imperative. This paper proposes a Personalized Trajectory Privacy Protection Mechanism (PTPPM) to address these challenges. Our approach begins by modeling an attacker's knowledge of a user's trajectory spatiotemporal correlations, which enables the attacker to identify possible location sets and disregard low-probability location sets. To combat this, we integrate geo-indistinguishability with distortion privacy, allowing users to customize their privacy preferences through a configurable privacy budget and expected inference error bound. This approach provides the theoretical framework for constructing a Protection Location Set (PLS) that obscures users' actual locations. Additionally, we introduce a Personalized Privacy Budget Allocation Algorithm (PPBA), which assesses the sensitivity of locations based on trajectory data and allocates privacy budgets accordingly. This algorithm considers factors such as location semantics and road network constraints. Furthermore, we propose a Permute-and-Flip mechanism that generates perturbed locations while minimizing perturbation distance, thus balancing privacy protection and Quality of Service (QoS). Simulation results demonstrate that our mechanism outperforms existing benchmarks, offering superior privacy protection while maintaining user QoS requirements.

</details>


### [8] [CAHS-Attack: CLIP-Aware Heuristic Search Attack Method for Stable Diffusion](https://arxiv.org/abs/2511.21180)
*Shuhan Xia,Jing Dai,Hui Ouyang,Yadong Shang,Dongxiao Zhao,Peipei Li*

Main category: cs.CR

TL;DR: CAHS-Attack是一种基于CLIP感知的启发式搜索攻击方法，通过蒙特卡洛树搜索和约束遗传算法优化对抗性提示后缀，在无需白盒访问的情况下实现高效的文本到图像模型攻击。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在面对对抗性提示时表现出显著的脆弱性，现有方法需要白盒访问模型梯度或手动提示工程，在现实部署中不可行或效果不佳。

Method: 结合蒙特卡洛树搜索进行细粒度后缀优化，使用约束遗传算法预选高潜力对抗提示作为根节点，并在每次模拟中保留最具语义破坏性的结果进行高效局部搜索。

Result: 在长短不同语义的提示上实现了最先进的攻击性能，发现SD模型的脆弱性源于其基于CLIP的文本编码器的固有漏洞。

Conclusion: 当前文本到图像流水线存在根本性安全风险，CLIP文本编码器的固有漏洞是扩散模型脆弱性的关键原因。

Abstract: Diffusion models exhibit notable fragility when faced with adversarial prompts, and strengthening attack capabilities is crucial for uncovering such vulnerabilities and building more robust generative systems. Existing works often rely on white-box access to model gradients or hand-crafted prompt engineering, which is infeasible in real-world deployments due to restricted access or poor attack effect. In this paper, we propose CAHS-Attack , a CLIP-Aware Heuristic Search attack method. CAHS-Attack integrates Monte Carlo Tree Search (MCTS) to perform fine-grained suffix optimization, leveraging a constrained genetic algorithm to preselect high-potential adversarial prompts as root nodes, and retaining the most semantically disruptive outcome at each simulation rollout for efficient local search. Extensive experiments demonstrate that our method achieves state-of-the-art attack performance across both short and long prompts of varying semantics. Furthermore, we find that the fragility of SD models can be attributed to the inherent vulnerability of their CLIP-based text encoders, suggesting a fundamental security risk in current text-to-image pipelines.

</details>


### [9] [AuthenLoRA: Entangling Stylization with Imperceptible Watermarks for Copyright-Secure LoRA Adapters](https://arxiv.org/abs/2511.21216)
*Fangming Shi,Li Li,Kejiang Chen,Guorui Feng,Xinpeng Zhang*

Main category: cs.CR

TL;DR: AuthenLoRA是一个统一的LoRA水印框架，通过在LoRA训练过程中嵌入不可感知的追踪水印，解决现有方法无法将水印传播到生成图像的问题。


<details>
  <summary>Details</summary>
Motivation: 现有水印技术要么针对基础模型，要么验证LoRA模块本身，但都无法将水印传播到生成的图像中，导致可追溯性存在关键缺口。

Method: 采用双目标优化策略，联合学习目标风格分布和水印引起的分布偏移；设计扩展的LoRA架构以增强多尺度适应；引入零消息正则化机制降低误报率。

Result: 实验表明AuthenLoRA实现了高保真风格化、鲁棒的水印传播，与现有方法相比显著降低了误报率。

Conclusion: AuthenLoRA为LoRA模型提供了一种有效的水印解决方案，能够在保持风格化质量的同时确保生成图像的可追溯性。

Abstract: Low-Rank Adaptation (LoRA) offers an efficient paradigm for customizing diffusion models, but its ease of redistribution raises concerns over unauthorized use and the generation of untraceable content. Existing watermarking techniques either target base models or verify LoRA modules themselves, yet they fail to propagate watermarks to generated images, leaving a critical gap in traceability. Moreover, traceability watermarking designed for base models is not tightly coupled with stylization and often introduces visual degradation or high false-positive detection rates. To address these limitations, we propose AuthenLoRA, a unified watermarking framework that embeds imperceptible, traceable watermarks directly into the LoRA training process while preserving stylization quality. AuthenLoRA employs a dual-objective optimization strategy that jointly learns the target style distribution and the watermark-induced distribution shift, ensuring that any image generated with the watermarked LoRA reliably carries the watermark. We further design an expanded LoRA architecture for enhanced multi-scale adaptation and introduce a zero-message regularization mechanism that substantially reduces false positives during watermark verification. Extensive experiments demonstrate that AuthenLoRA achieves high-fidelity stylization, robust watermark propagation, and significantly lower false-positive rates compared with existing approaches. Open-source implementation is available at: https://github.com/ShiFangming0823/AuthenLoRA

</details>


### [10] [Data Exfiltration by Compression Attack: Definition and Evaluation on Medical Image Data](https://arxiv.org/abs/2511.21227)
*Huiyu Li,Nicholas Ayache,Hervé Delingette*

Main category: cs.CR

TL;DR: 本文提出了一种基于图像压缩技术的数据泄露攻击方法（Data Exfiltration by Compression），能够在仅访问数据湖的情况下，通过导出的深度学习模型窃取医疗图像数据，并研究了差分隐私和模型微调等防御措施。


<details>
  <summary>Details</summary>
Motivation: 随着存储健康数据和AI算法的数据湖快速扩张，从这些数据湖导出机器学习模型的安全性成为重要问题。深度网络模型会编码训练数据信息，可能导致敏感信息泄露。

Method: 提出基于无损或有损图像压缩方法的数据泄露攻击，仅需访问数据湖和导出的网络模型，无需训练过程中的额外信息。探索了限制模型大小和隐藏压缩代码的技术。

Result: 在CT和MR图像公共数据集上验证，该攻击能有效窃取医疗图像并在数据湖外高保真重建，实现压缩与重建质量的最佳平衡。差分隐私措施可防御攻击，但攻击者可通过减少窃取图像数量来增强抗干扰能力。

Conclusion: 数据压缩泄露攻击对医疗图像数据构成严重威胁，需要采取差分隐私或模型微调等预防策略来保护敏感医疗数据的安全。

Abstract: With the rapid expansion of data lakes storing health data and hosting AI algorithms, a prominent concern arises: how safe is it to export machine learning models from these data lakes? In particular, deep network models, widely used for health data processing, encode information from their training dataset, potentially leading to the leakage of sensitive information upon its export. This paper thoroughly examines this issue in the context of medical imaging data and introduces a novel data exfiltration attack based on image compression techniques.
  This attack, termed Data Exfiltration by Compression, requires only access to a data lake and is based on lossless or lossy image compression methods. Unlike previous data exfiltration attacks, it is compatible with any image processing task and depends solely on an exported network model without requiring any additional information to be collected during the training process. We explore various scenarios, and techniques to limit the size of the exported model and conceal the compression codes within the network.
  Using two public datasets of CT and MR images, we demonstrate that this attack can effectively steal medical images and reconstruct them outside the data lake with high fidelity, achieving an optimal balance between compression and reconstruction quality. Additionally, we investigate the impact of basic differential privacy measures, such as adding Gaussian noise to the model parameters, to prevent the Data Exfiltration by Compression Attack. We also show how the attacker can make their attack resilient to differential privacy at the expense of decreasing the number of stolen images. Lastly, we propose an alternative prevention strategy by fine-tuning the model to be exported.

</details>


### [11] [Illuminating the Black Box: Real-Time Monitoring of Backdoor Unlearning in CNNs via Explainable AI](https://arxiv.org/abs/2511.21291)
*Tien Dat Hoang*

Main category: cs.CR

TL;DR: 提出了一种集成Grad-CAM的可解释性后门消除框架，通过TAR指标量化注意力转移，结合梯度上升、EWC和恢复阶段，在CIFAR-10上实现96.51%到5.52%的ASR降低，同时保持99.48%的干净准确率。


<details>
  <summary>Details</summary>
Motivation: 当前后门消除方法缺乏透明度和实时可解释性，需要开发能够实时监控和解释消除过程的框架。

Method: 集成Grad-CAM提供实时监控，提出TAR指标量化注意力转移，采用平衡消除策略：梯度上升消除后门行为、EWC防止灾难性遗忘、恢复阶段恢复干净准确率。

Result: 在CIFAR-10数据集上，攻击成功率从96.51%降至5.52%，ASR降低94.28%，同时保持99.48%的干净准确率（82.06%）。

Conclusion: 该框架实现了透明、可观察和可验证的后门消除，为深度神经网络安全提供了可解释的解决方案。

Abstract: Backdoor attacks pose severe security threats to deep neural networks by embedding malicious triggers that force misclassification. While machine unlearning techniques can remove backdoor behaviors, current methods lack transparency and real-time interpretability. This paper introduces a novel framework that integrates Gradient-weighted Class Activation Mapping (Grad-CAM) into the unlearning process to provide real-time monitoring and explainability. We propose the Trigger Attention Ratio (TAR) metric to quantitatively measure the model's attention shift from trigger patterns to legitimate object features. Our balanced unlearning strategy combines gradient ascent on backdoor samples, Elastic Weight Consolidation (EWC) for catastrophic forgetting prevention, and a recovery phase for clean accuracy restoration. Experiments on CIFAR-10 with BadNets attacks demonstrate that our approach reduces Attack Success Rate (ASR) from 96.51% to 5.52% while retaining 99.48% of clean accuracy (82.06%), achieving a 94.28% ASR reduction. The integration of explainable AI enables transparent, observable, and verifiable backdoor removal.

</details>


### [12] [Empirical Assessment of the Code Comprehension Effort Needed to Attack Programs Protected with Obfuscation](https://arxiv.org/abs/2511.21301)
*Leonardo Regano,Daniele Canavese,Cataldo Basile,Marco Torchiano*

Main category: cs.CR

TL;DR: 该论文通过控制实验评估软件混淆技术的有效性，研究混淆如何延迟代码理解时间，并验证复杂度指标能否预测混淆效果。


<details>
  <summary>Details</summary>
Motivation: 软件混淆技术被广泛采用，但其实际有效性缺乏系统评估。需要研究混淆技术是否能有效延迟攻击者的代码理解，以及客观指标能否准确预测混淆效果。

Method: 使用控制实验方法，让硕士生对经过混淆处理的应用程序执行代码理解任务，评估单层和多层混淆技术的效果，并分析复杂度指标与攻击成功率的关联。

Result: 实验首次评估了多层混淆技术的叠加效果，提供了客观指标与攻击成功率相关性的实验证据，填补了客观和主观评估方法之间的空白。

Conclusion: 混淆技术能有效延迟代码理解，复杂度指标可以预测混淆效果，研究为软件保护效果评估开辟了新途径，指出了需要进一步分析的重要方面。

Abstract: Evaluating the effectiveness of software protection is crucial for selecting the most effective methods to safeguard assets within software applications. Obfuscation involves techniques that deliberately modify software to make it more challenging to understand and reverse-engineer, while maintaining its original functionality. Although obfuscation is widely adopted, its effectiveness remains largely unexplored and unthoroughly evaluated. This paper presents a controlled experiment involving Master's students performing code comprehension tasks on applications hardened with obfuscation. The experiment's goals are to assess the effectiveness of obfuscation in delaying code comprehension by attackers and to determine whether complexity metrics can accurately predict the impact of these protections on success rates and durations of code comprehension tasks. The study is the first to evaluate the effect of layering multiple obfuscation techniques on a single piece of protected code. It also provides experimental evidence of the correlation between objective metrics of the attacked code and the likelihood of a successful attack, bridging the gap between objective and subjective approaches to estimating potency. Finally, the paper highlights significant aspects that warrant additional analysis and opens new avenues for further experiments.

</details>


### [13] [Constructing and Benchmarking: a Labeled Email Dataset for Text-Based Phishing and Spam Detection Framework](https://arxiv.org/abs/2511.21448)
*Rebeka Toth,Tamas Bisztray,Richard Dubniczky*

Main category: cs.CR

TL;DR: 该研究创建了一个包含网络钓鱼、垃圾邮件和合法邮件的综合数据集，明确区分了人类和LLM生成的内容，并标注了情感诉求和动机。通过基准测试评估LLM识别这些线索的能力，并测试了重述邮件对分类性能的影响。


<details>
  <summary>Details</summary>
Motivation: 网络钓鱼和垃圾邮件仍然是主要的网络安全威胁，攻击者越来越多地利用LLM制作高度欺骗性的内容，需要改进AI辅助的邮件安全系统。

Method: 创建综合邮件数据集，标注邮件类别、情感诉求和动机；基准测试多个LLM识别情感和动机线索的能力；使用LLM重述邮件保持意义和意图不变；评估最先进LLM在原始和重述邮件上的分类性能。

Result: LLM在检测网络钓鱼方面表现出强大能力，但在区分垃圾邮件和合法邮件方面仍存在持续挑战。

Conclusion: 该数据集和评估框架有助于改进AI辅助的邮件安全系统，所有代码、模板和资源都已公开以支持开放科学。

Abstract: Phishing and spam emails remain a major cybersecurity threat, with attackers increasingly leveraging Large Language Models (LLMs) to craft highly deceptive content. This study presents a comprehensive email dataset containing phishing, spam, and legitimate messages, explicitly distinguishing between human- and LLM-generated content. Each email is annotated with its category, emotional appeal (e.g., urgency, fear, authority), and underlying motivation (e.g., link-following, credential theft, financial fraud). We benchmark multiple LLMs on their ability to identify these emotional and motivational cues and select the most reliable model to annotate the full dataset. To evaluate classification robustness, emails were also rephrased using several LLMs while preserving meaning and intent. A state-of-the-art LLM was then assessed on its performance across both original and rephrased emails using expert-labeled ground truth. The results highlight strong phishing detection capabilities but reveal persistent challenges in distinguishing spam from legitimate emails. Our dataset and evaluation framework contribute to improving AI-assisted email security systems. To support open science, all code, templates, and resources are available on our project site.

</details>


### [14] [MAD-DAG: Protecting Blockchain Consensus from MEV](https://arxiv.org/abs/2511.21552)
*Roi Bar-Zur,Aviv Tamar,Ittay Eyal*

Main category: cs.CR

TL;DR: MAD-DAG是一种实用的DAG区块链协议，能够抵御自私挖矿攻击，特别是在存在网络传播优势、MEV和贿赂等不利条件下。


<details>
  <summary>Details</summary>
Motivation: 现有最先进的抗自私挖矿协议Colordag无法处理网络传播优势、MEV和贿赂等不利条件，且仅在不可实际的高延迟下被证明安全。

Method: 提出MAD-DAG协议，采用新型账本函数，丢弃竞争最长链的等长链内容。通过马尔可夫决策过程建模理性矿工行为，开发保守奖励规则来获得自私挖矿收益上界。

Result: MAD-DAG在存在贿赂矿工和高MEV水平时，安全阈值范围为11%至31%，而Colordag和比特币的安全阈值均为0%。

Conclusion: MAD-DAG是第一个在不利条件下实用的抗自私挖矿协议，在保持可比安全性的同时，能够抵御Colordag和比特币失败的情况。

Abstract: Blockchain security is threatened by selfish mining, where a miner (operator) deviates from the protocol to increase their revenue. Selfish mining is exacerbated by adverse conditions: rushing (network propagation advantage for the selfish miner), varying block rewards due to block contents, called miner extractable value (MEV), and petty-compliant miners who accept bribes from the selfish miner.
  The state-of-the-art selfish-mining-resistant blockchain protocol, Colordag, does not treat these adverse conditions and was proven secure only when its latency is impractically high.
  We present MAD-DAG, Mutually-Assured-Destruction Directed-Acyclic-Graph, the first practical protocol to counter selfish mining under adverse conditions. MAD-DAG achieves this thanks to its novel ledger function, which discards the contents of equal-length chains competing to be the longest.
  We analyze selfish mining in both Colordag and MAD-DAG by modeling a rational miner using a Markov Decision Process (MDP). We obtain a tractable model for both by developing conservative reward rules that favor the selfish miner to yield an upper bound on selfish mining revenue. To the best of our knowledge, this is the first tractable model of selfish mining in a practical DAG-based blockchain. This enables us to obtain a lower bound on the security threshold, the minimum fraction of computational power a miner needs in order to profit from selfish mining.
  MAD-DAG withstands adverse conditions under which Colordag and Bitcoin fail, while otherwise maintaining comparable security. For example, with petty-compliant miners and high levels of block reward variability, MAD-DAG's security threshold ranges from 11% to 31%, whereas both Colordag and Bitcoin achieve 0% for all levels.

</details>


### [15] [TAB-DRW: A DFT-based Robust Watermark for Generative Tabular Data](https://arxiv.org/abs/2511.21600)
*Yizhou Zhao,Xiang Li,Peter Song,Qi Long,Weijie Su*

Main category: cs.CR

TL;DR: 提出TAB-DRW，一种高效鲁棒的后编辑水印方案，用于生成表格数据，通过在频域嵌入水印信号来确保合成数据的可追溯性。


<details>
  <summary>Details</summary>
Motivation: 生成AI产生的高保真合成表格数据引发了数据来源和滥用的担忧，现有水印方法存在计算成本高、处理混合数据类型困难、鲁棒性不足等问题。

Method: 通过Yeo-Johnson变换和标准化归一化异构特征，应用离散傅里叶变换，根据预计算的伪随机位调整自适应选择条目的虚部，并引入基于排名的伪随机位生成方法。

Result: 在五个基准表格数据集上的实验表明，TAB-DRW实现了强可检测性和对常见后处理攻击的鲁棒性，同时保持高数据保真度并完全支持混合类型特征。

Conclusion: TAB-DRW为生成表格数据提供了一种高效、鲁棒的水印解决方案，解决了现有方法的局限性。

Abstract: The rise of generative AI has enabled the production of high-fidelity synthetic tabular data across fields such as healthcare, finance, and public policy, raising growing concerns about data provenance and misuse. Watermarking offers a promising solution to address these concerns by ensuring the traceability of synthetic data, but existing methods face many limitations: they are computationally expensive due to reliance on large diffusion models, struggle with mixed discrete-continuous data, or lack robustness to post-modifications. To address them, we propose TAB-DRW, an efficient and robust post-editing watermarking scheme for generative tabular data. TAB-DRW embeds watermark signals in the frequency domain: it normalizes heterogeneous features via the Yeo-Johnson transformation and standardization, applies the discrete Fourier transform (DFT), and adjusts the imaginary parts of adaptively selected entries according to precomputed pseudorandom bits. To further enhance robustness and efficiency, we introduce a novel rank-based pseudorandom bit generation method that enables row-wise retrieval without incurring storage overhead. Experiments on five benchmark tabular datasets show that TAB-DRW achieves strong detectability and robustness against common post-processing attacks, while preserving high data fidelity and fully supporting mixed-type features.

</details>
