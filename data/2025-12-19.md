<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 34]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [A Survey on Reconfigurable Intelligent Surfaces in Practical Systems: Security and Privacy Perspectives](https://arxiv.org/abs/2512.15754)
*Ziyu Chen,Yitong Shen,Jingzhe Zhang,Yao Zheng,Yili Ren,Xuyu Wang,Shiwen Mao,Hanqing Guo*

Main category: cs.CR

TL;DR: 这篇综述系统分析了可重构智能表面(RIS)在实际部署中的安全与隐私问题，包括攻击场景、防御策略以及开源工具资源。


<details>
  <summary>Details</summary>
Motivation: 虽然RIS在理论上对通信和感知有显著优势，但在智能家居、车辆、工业等实际环境中的部署仍然有限，特别是在安全与隐私方面的研究不足。需要全面审视RIS在实际系统中的安全威胁和防御策略。

Method: 采用系统性综述方法，分析两种系统场景（有/无合法RIS）和两种攻击者类型（有/无恶意RIS），探讨RIS可能引入的新攻击方式，并回顾相应的防御策略。

Result: 识别了RIS在实际系统中可能引入的窃听、干扰和欺骗等新攻击方式，总结了针对RIS相关攻击的防御策略，包括应用额外安全算法、干扰攻击者以及早期检测未授权RIS等。

Conclusion: RIS技术在实际部署中面临显著的安全与隐私挑战，但同时也提供了新的防御机会。通过系统分析这些挑战和机遇，可以指导研究人员和工程师开发安全、有弹性且保护隐私的RIS使能无线系统。

Abstract: Reconfigurable Intelligent Surfaces (RIS) have emerged as a transformative technology capable of reshaping wireless environments through dynamic manipulation of electromagnetic waves. While extensive research has explored their theoretical benefits for communication and sensing, practical deployments in smart environments such as homes, vehicles, and industrial settings remain limited and under-examined, particularly from security and privacy perspectives. This survey provides a comprehensive examination of RIS applications in real-world systems, with a focus on the security and privacy threats, vulnerabilities, and defensive strategies relevant to practical use. We analyze scenarios with two types of systems (with and without legitimate RIS) and two types of attackers (with and without malicious RIS), and demonstrate how RIS may introduce new attacks to practical systems, including eavesdropping, jamming, and spoofing attacks. In response, we review defenses against RIS-related attacks in these systems, such as applying additional security algorithms, disrupting attackers, and early detection of unauthorized RIS. We also discuss scenarios in which the legitimate user applies an additional RIS to defend against attacks. To support future research, we also provide a collection of open-source tools, datasets, demos, and papers at: https://awesome-ris-security.github.io/. By highlighting RIS's functionality and its security/privacy challenges and opportunities, this survey aims to guide researchers and engineers toward the development of secure, resilient, and privacy-preserving RIS-enabled practical wireless systems and environments.

</details>


### [2] [PHANTOM: Progressive High-fidelity Adversarial Network for Threat Object Modeling](https://arxiv.org/abs/2512.15768)
*Jamal Al-Karaki,Muhammad Al-Zafar Khan,Rand Derar Mohammad Al Athamneh*

Main category: cs.CR

TL;DR: PHANTOM是一个对抗变分框架，用于生成高保真合成攻击数据以解决网络攻击数据稀缺问题，通过渐进训练、双路径VAE-GAN架构和领域特定特征匹配来保持攻击语义。


<details>
  <summary>Details</summary>
Motivation: 网络攻击数据的稀缺性阻碍了鲁棒入侵检测系统的发展。真实攻击数据难以获取，且存在隐私问题，需要能够生成高质量合成攻击数据的方法来训练更好的检测模型。

Method: 提出PHANTOM框架，采用渐进训练策略、双路径VAE-GAN架构（结合变分自编码器和生成对抗网络），以及领域特定的特征匹配机制，确保生成的合成数据保持原始攻击的语义特征。

Result: 在10万个网络流量样本上评估，使用PHANTOM数据训练的模型在真实攻击上达到98%的加权准确率。统计分析证实合成数据保持了真实的分布和多样性。

Conclusion: PHANTOM能够生成高质量的合成攻击数据，有助于训练鲁棒且保护隐私的检测系统。但生成罕见攻击类型存在局限性，突显了严重类别不平衡的挑战。

Abstract: The scarcity of cyberattack data hinders the development of robust intrusion detection systems. This paper introduces PHANTOM, a novel adversarial variational framework for generating high-fidelity synthetic attack data. Its innovations include progressive training, a dual-path VAE-GAN architecture, and domain-specific feature matching to preserve the semantics of attacks. Evaluated on 100,000 network traffic samples, models trained on PHANTOM data achieve 98% weighted accuracy on real attacks. Statistical analyses confirm that the synthetic data preserves authentic distributions and diversity. Limitations in generating rare attack types are noted, highlighting challenges with severe class imbalance. This work advances the generation of synthetic data for training robust, privacy-preserving detection systems.

</details>


### [3] [Data-Chain Backdoor: Do You Trust Diffusion Models as Generative Data Supplier?](https://arxiv.org/abs/2512.15769)
*Junchi Lu,Xinke Li,Yuheng Liu,Qi Alfred Chen*

Main category: cs.CR

TL;DR: 扩散模型等生成模型用于合成数据增强时可能成为后门传播的隐藏载体，导致下游模型继承后门风险，这种威胁在干净标签攻击场景下尤为危险。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型等生成模型被广泛用于合成数据增强以降低数据收集和标注成本，这种新的数据源范式可能引入重要的安全隐患。本研究旨在调查这种新兴生成数据供应链中的后门传播问题。

Method: 研究开源扩散模型如何成为后门的隐藏载体，分析其强大的分布拟合能力如何记忆和重现后门触发器，并研究这些触发器如何被下游模型继承。特别关注干净标签攻击场景，并发现早期阶段触发器显现现象。

Result: 发现扩散模型能够记忆和重现后门触发器，导致下游模型继承后门风险。这种威胁在干净标签攻击场景下特别危险，因为它在几乎不影响合成数据实用性的情况下仍然有效。同时观察到早期阶段触发器显现现象：后门触发模式倾向于在扩散模型反向生成过程的早期高噪声阶段更明显地显现。

Conclusion: 本研究揭示了生成数据管道中一个先前未被充分探索的威胁，为缓解合成数据生成中的后门风险提供了初步见解。扩散模型可能成为后门传播的隐藏载体，对下游感知任务构成严重安全风险。

Abstract: The increasing use of generative models such as diffusion models for synthetic data augmentation has greatly reduced the cost of data collection and labeling in downstream perception tasks. However, this new data source paradigm may introduce important security concerns. This work investigates backdoor propagation in such emerging generative data supply chains, namely Data-Chain Backdoor (DCB). Specifically, we find that open-source diffusion models can become hidden carriers of backdoors. Their strong distribution-fitting ability causes them to memorize and reproduce backdoor triggers during generation, which are subsequently inherited by downstream models, resulting in severe security risks. This threat is particularly concerning under clean-label attack scenarios, as it remains effective while having negligible impact on the utility of the synthetic data. Furthermore, we discover an Early-Stage Trigger Manifestation (ESTM) phenomenon: backdoor trigger patterns tend to surface more explicitly in the early, high-noise stages of the diffusion model's reverse generation process before being subtly integrated into the final samples. Overall, this work reveals a previously underexplored threat in generative data pipelines and provides initial insights toward mitigating backdoor risks in synthetic data generation.

</details>


### [4] [Variable Record Table: A Unified Hardware-Assisted Framework for Runtime Security](https://arxiv.org/abs/2512.15777)
*Suraj Kumar Sah,Love Kumar Sah*

Main category: cs.CR

TL;DR: VRT是一个统一的硬件辅助框架，通过动态构建保护表同时实现空间内存安全、控制流完整性和推测执行攻击检测，零指令开销且硬件开销小。


<details>
  <summary>Details</summary>
Motivation: 现代计算系统面临多种安全威胁（内存损坏攻击、推测执行漏洞、控制流劫持），现有解决方案通常单独处理这些威胁，导致性能开销大且存在安全漏洞，需要统一的综合保护方案。

Method: 提出可变记录表(VRT)硬件框架，通过运行时指令插桩动态构建保护表，提取内存地址、边界元数据和控制流签名，同时强制执行空间内存安全、后向控制流完整性和推测执行攻击检测。

Result: 在MiBench和SPEC基准测试中，VRT成功检测所有测试的攻击变种，零额外指令开销，内存需求低于25KB（512条目），面积/功耗开销分别低于8%和11.65μW。

Conclusion: VRT通过将三种关键安全机制整合到单一硬件结构中，提供了全面的保护同时最小化性能影响，实现了高效统一的安全框架。

Abstract: Modern computing systems face security threats, including memory corruption attacks, speculative execution vul- nerabilities, and control-flow hijacking. Although existing solu- tions address these threats individually, they frequently introduce performance overhead and leave security gaps. This paper presents a Variable Record Table (VRT) with a unified hardware- assisted framework that simultaneously enforces spatial memory safety against buffer overflows, back-edge control-flow integrity (CFI), and speculative execution attack detection. The VRT dynamically constructs a protection table by instrumenting run- time instructions to extract memory addresses, bounds metadata, and control-flow signatures. Our evaluation across MiBench and SPEC benchmarks shows that VRT successfully detects all attack variants tested with zero additional instruction overhead. Fur- thermore, it maintains memory requirements below 25KB (for 512 entries) and maintains area / power overhead under 8% and 11.65 μW, respectively. By consolidating three essential security mechanisms into a single hardware structure, VRT provides comprehensive protection while minimizing performance impact.

</details>


### [5] [RAMBO: Reliability Analysis for Mamba through Bit-flip attack Optimization](https://arxiv.org/abs/2512.15778)
*Sanjay Das,Swastik Bhattacharya,Shamik Kundu,Arnab Raha,Souvik Kundu,Kanad Basu*

Main category: cs.CR

TL;DR: RAMBO框架首次针对Mamba架构进行硬件级比特翻转攻击，仅翻转一个关键比特即可使模型准确率从74.64%降至0%，揭示状态空间模型对对抗性扰动的极端脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着状态空间模型（如Mamba）在实际应用中的部署增加，需要评估其对软硬件威胁的脆弱性，特别是硬件引发的比特翻转攻击，这种攻击通过内存故障破坏模型参数，威胁模型准确性和功能完整性。

Method: 提出RAMBO框架，这是首个专门针对Mamba架构的比特翻转攻击框架。在Mamba-1.4b模型上使用LAMBADA基准测试（填空式单词预测任务）进行实验。

Result: 仅翻转一个关键比特就能灾难性地将准确率从74.64%降至0%，困惑度从18.94增加到3.75×10^6，证明状态空间模型对对抗性扰动具有显著脆弱性。

Conclusion: 状态空间模型对硬件级比特翻转攻击极为脆弱，RAMBO框架揭示了这种严重的安全漏洞，强调了在部署这些模型时需要加强安全防护措施。

Abstract: State-space models (SSMs), exemplified by the Mamba architecture, have recently emerged as state-of-the-art sequence-modeling frameworks, offering linear-time scalability together with strong performance in long-context settings. Owing to their unique combination of efficiency, scalability, and expressive capacity, SSMs have become compelling alternatives to transformer-based models, which suffer from the quadratic computational and memory costs of attention mechanisms. As SSMs are increasingly deployed in real-world applications, it is critical to assess their susceptibility to both software- and hardware-level threats to ensure secure and reliable operation. Among such threats, hardware-induced bit-flip attacks (BFAs) pose a particularly severe risk by corrupting model parameters through memory faults, thereby undermining model accuracy and functional integrity. To investigate this vulnerability, we introduce RAMBO, the first BFA framework specifically designed to target Mamba-based architectures. Through experiments on the Mamba-1.4b model with LAMBADA benchmark, a cloze-style word-prediction task, we demonstrate that flipping merely a single critical bit can catastrophically reduce accuracy from 74.64% to 0% and increase perplexity from 18.94 to 3.75 x 10^6. These results demonstrate the pronounced fragility of SSMs to adversarial perturbations.

</details>


### [6] [Hyperparameter Tuning-Based Optimized Performance Analysis of Machine Learning Algorithms for Network Intrusion Detection](https://arxiv.org/abs/2512.15779)
*Sudhanshu Sekhar Tripathy,Bichitrananda Behera*

Main category: cs.CR

TL;DR: 该研究应用多种机器学习算法优化网络入侵检测系统，通过超参数调优后SVM在KDD CUP数据集上达到99.12%准确率，证明ML方法能有效提升入侵检测性能。


<details>
  <summary>Details</summary>
Motivation: 随着网络威胁日益复杂，传统网络入侵检测系统需要进化以检测新兴威胁和异常行为。本研究旨在探索机器学习方法如何通过分析深度特征网络流量记录来提高NIDS的准确性。

Method: 使用1999年KDD CUP入侵数据集作为基准，评估和优化多种ML算法（SVM、朴素贝叶斯变体、随机森林、k-NN、决策树、AdaBoost、XGBoost等）。采用网格搜索和随机搜索进行超参数优化，使用十倍交叉验证和递归特征消除进行特征选择。

Result: 超参数优化后，SVM分类器达到99.12%准确率和0.0091误报率，优于默认配置（98.08%准确率，0.0123误报率）和其他所有分类器。所有分类器通过特征选择后都表现出良好的性能。

Conclusion: 机器学习分类器具有适应性和可靠性，能够显著提升网络入侵检测系统的准确性。SVM在优化后表现最佳，证明了超参数调优对ML模型性能的重要性。

Abstract: Network Intrusion Detection Systems (NIDS) are essential for securing networks by identifying and mitigating unauthorized activities indicative of cyberattacks. As cyber threats grow increasingly sophisticated, NIDS must evolve to detect both emerging threats and deviations from normal behavior. This study explores the application of machine learning (ML) methods to improve the NIDS accuracy through analyzing intricate structures in deep-featured network traffic records. Leveraging the 1999 KDD CUP intrusion dataset as a benchmark, this research evaluates and optimizes several ML algorithms, including Support Vector Machines (SVM), Naïve Bayes variants (MNB, BNB), Random Forest (RF), k-Nearest Neighbors (k-NN), Decision Trees (DT), AdaBoost, XGBoost, Logistic Regression (LR), Ridge Classifier, Passive-Aggressive (PA) Classifier, Rocchio Classifier, Artificial Neural Networks (ANN), and Perceptron (PPN). Initial evaluations without hyper-parameter optimization demonstrated suboptimal performance, highlighting the importance of tuning to enhance classification accuracy. After hyper-parameter optimization using grid and random search techniques, the SVM classifier achieved 99.12% accuracy with a 0.0091 False Alarm Rate (FAR), outperforming its default configuration (98.08% accuracy, 0.0123 FAR) and all other classifiers. This result confirms that SVM accomplishes the highest accuracy among the evaluated classifiers. We validated the effectiveness of all classifiers using a tenfold cross-validation approach, incorporating Recursive Feature Elimination (RFE) for feature selection to enhance the classifiers accuracy and efficiency. Our outcomes indicate that ML classifiers are both adaptable and reliable, contributing to enhanced accuracy in systems for detecting network intrusions.

</details>


### [7] [Detecting Malicious Entra OAuth Apps with LLM-Based Permission Risk Scoring](https://arxiv.org/abs/2512.15781)
*Ashim Mahara*

Main category: cs.CR

TL;DR: 提出统一的检测框架，构建Microsoft Graph权限完整语料库，生成一致的基于LLM的风险评分，并集成到实时检测引擎中以识别恶意OAuth同意活动


<details>
  <summary>Details</summary>
Motivation: 需要系统性地检测恶意OAuth同意活动，现有方法缺乏统一的权限语料库和一致的风险评估机制

Method: 构建Microsoft Graph权限完整语料库，开发基于LLM的风险评分模型，集成到实时检测引擎中

Result: 建立了统一的检测框架，能够实时识别恶意OAuth同意活动

Conclusion: 该框架为Microsoft Graph权限的恶意OAuth活动检测提供了系统化的解决方案

Abstract: This project presents a unified detection framework that constructs a complete corpus of Microsoft Graph permissions, generates consistent LLM-based risk scores, and integrates them into a real-time detection engine to identify malicious OAuth consent activity.

</details>


### [8] [Auto-Tuning Safety Guardrails for Black-Box Large Language Models](https://arxiv.org/abs/2512.15782)
*Perry Abdulkadir*

Main category: cs.CR

TL;DR: 将安全护栏设计视为超参数优化问题，通过自动化搜索在冻结的基础模型上找到最优的安全配置，相比手动调优更高效可靠。


<details>
  <summary>Details</summary>
Motivation: 当前LLM部署中的安全护栏（系统提示词和内容过滤器）通常是手动调优的，这种方法脆弱、难以复现且不高效。需要一种更系统化的方法来优化安全配置。

Method: 将安全护栏设计视为超参数优化问题，在冻结的Mistral-7B-Instruct模型上，使用模块化的越狱和恶意软件系统提示词，加上基于ModernBERT的有害性分类器。通过网格搜索和Optuna黑盒优化来评估不同配置。

Result: Optuna优化能够可靠地重新发现最佳网格配置，同时需要的评估次数减少一个数量级，墙钟时间减少约8倍。这表明自动化优化是可行的。

Conclusion: 将安全护栏视为可调超参数是在计算和时间约束下强化黑盒LLM部署的可行方法，自动化优化比手动调优更高效可靠。

Abstract: Large language models (LLMs) are increasingly deployed behind safety guardrails such as system prompts and content filters, especially in settings where product teams cannot modify model weights. In practice these guardrails are typically hand-tuned, brittle, and difficult to reproduce. This paper studies a simple but practical alternative: treat safety guardrail design itself as a hyperparameter optimization problem over a frozen base model. Concretely, I wrap Mistral-7B-Instruct with modular jailbreak and malware system prompts plus a ModernBERT-based harmfulness classifier, then evaluate candidate configurations on three public benchmarks covering malware generation, classic jailbreak prompts, and benign user queries. Each configuration is scored using malware and jailbreak attack success rate, benign harmful-response rate, and end-to-end latency. A 48-point grid search over prompt combinations and filter modes establishes a baseline. I then run a black-box Optuna study over the same space and show that it reliably rediscovers the best grid configurations while requiring an order of magnitude fewer evaluations and roughly 8x less wall-clock time. The results suggest that viewing safety guardrails as tunable hyperparameters is a feasible way to harden black-box LLM deployments under compute and time constraints.

</details>


### [9] [Bilevel Optimization for Covert Memory Tampering in Heterogeneous Multi-Agent Architectures (XAMT)](https://arxiv.org/abs/2512.15790)
*Akhil Sharma,Shaikh Yaser Arafat,Jai Kumar Sharma,Ken Huang*

Main category: cs.CR

TL;DR: XAMT提出了一种针对异构多智能体系统的双层优化隐蔽内存篡改攻击框架，针对MARL的共享经验回放缓冲区和RAG智能体的外部知识库，通过最小扰动实现最大系统行为偏离。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体系统在安全关键领域的广泛应用，需要对其对抗鲁棒性进行严格评估。现代异构MAS集成了传统MARL和新兴的LLM-RAG智能体，它们都依赖集中式内存组件（ER缓冲区和知识库），这些组件存在共享的脆弱性。

Method: 提出XAMT框架，将攻击生成形式化为双层优化问题：上层最小化扰动幅度以确保隐蔽性，下层最大化系统行为偏离攻击者定义的目标。为CTDE MARL算法和RAG-based LLM智能体提供了严格的数学实例化。

Result: 在SMAC和SafeRAG基准测试中，以极低的毒化率（MARL≤1%，RAG≤0.1%）实现了显著效果。双层优化能够生成逃避检测启发式方法的隐蔽、最小扰动毒化攻击。

Conclusion: XAMT定义了一类新的训练时威胁类别，对开发本质安全的MAS至关重要，对信任、形式验证以及优先考虑本质安全而非边界检测的防御策略具有重要影响。

Abstract: The increasing operational reliance on complex Multi-Agent Systems (MAS) across safety-critical domains necessitates rigorous adversarial robustness assessment. Modern MAS are inherently heterogeneous, integrating conventional Multi-Agent Reinforcement Learning (MARL) with emerging Large Language Model (LLM) agent architectures utilizing Retrieval-Augmented Generation (RAG). A critical shared vulnerability is reliance on centralized memory components: the shared Experience Replay (ER) buffer in MARL and the external Knowledge Base (K) in RAG agents. This paper proposes XAMT (Bilevel Optimization for Covert Memory Tampering in Heterogeneous Multi-Agent Architectures), a novel framework that formalizes attack generation as a bilevel optimization problem. The Upper Level minimizes perturbation magnitude (delta) to enforce covertness while maximizing system behavior divergence toward an adversary-defined target (Lower Level). We provide rigorous mathematical instantiations for CTDE MARL algorithms and RAG-based LLM agents, demonstrating that bilevel optimization uniquely crafts stealthy, minimal-perturbation poisons evading detection heuristics. Comprehensive experimental protocols utilize SMAC and SafeRAG benchmarks to quantify effectiveness at sub-percent poison rates (less than or equal to 1 percent in MARL, less than or equal to 0.1 percent in RAG). XAMT defines a new unified class of training-time threats essential for developing intrinsically secure MAS, with implications for trust, formal verification, and defensive strategies prioritizing intrinsic safety over perimeter-based detection.

</details>


### [10] [Data Protection and Corporate Reputation Management in the Digital Era](https://arxiv.org/abs/2512.15794)
*Gabriela Wojak,Ernest Górka,Michał Ćwiąkała,Dariusz Baran,Dariusz Reśko,Monika Wyrzykowska-Antkiewicz,Robert Marczuk,Marcin Agaciński,Daniel Zawadzki,Jan Piwnik*

Main category: cs.CR

TL;DR: 该研究分析了数字化转型背景下网络安全治理、数据保护与企业声誉的关系，发现尽管企业普遍建立了正式网络安全框架，但75%仍遭遇安全事件，声誉损害是最常见后果。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索数字化转型背景下，企业如何通过网络安全管理和数据保护来维护利益相关者信任和企业声誉，理解网络安全从合规成本向战略投资的转变。

Method: 采用定量研究方法，通过在线诊断调查收集波兰各行业企业的数据，分析内容包括：正式网络安全策略、技术和程序保障措施、员工意识、事件响应实践以及ISO/IEC 27001和ISO/IEC 27032等国际标准的采用情况。

Result: 研究发现：大多数企业已建立正式网络安全框架、定期审计并投资员工意识培训；但75%的企业在过去12个月内仍遭遇网络安全事件；最常见的后果是声誉损害和客户信任丧失，其次是运营中断和财务/监管影响。

Conclusion: 网络安全正日益被视为支持组织长期稳定的战略投资而非单纯合规成本。研究强调需要将网络安全治理与企业沟通和声誉管理相结合，数据保护是数字信任和组织韧性的关键决定因素。

Abstract: This paper analyzes the relationship between cybersecurity management, data protection, and corporate reputation in the context of digital transformation. The study examines how organizations implement strategies and tools to mitigate cyber risks, comply with regulatory requirements, and maintain stakeholder trust. A quantitative research design was applied using an online diagnostic survey conducted among enterprises from various industries operating in Poland. The analysis covered formal cybersecurity strategies, technical and procedural safeguards, employee awareness, incident response practices, and the adoption of international standards such as ISO/IEC 27001 and ISO/IEC 27032. The findings indicate that most organizations have formalized cybersecurity frameworks, conduct regular audits, and invest in employee awareness programs. Despite this high level of preparedness, 75 percent of surveyed firms experienced cybersecurity incidents within the previous twelve months. The most frequently reported consequences were reputational damage and loss of customer trust, followed by operational disruptions and financial or regulatory impacts. The results show that cybersecurity is increasingly perceived as a strategic investment supporting long-term organizational stability rather than merely a compliance cost. The study highlights the importance of integrating cybersecurity governance with corporate communication and reputation management, emphasizing data protection as a key determinant of digital trust and organizational resilience.

</details>


### [11] [Cybercrime and Computer Forensics in Epoch of Artificial Intelligence in India](https://arxiv.org/abs/2512.15799)
*Sahibpreet Singh,Shikha Dhiman*

Main category: cs.CR

TL;DR: 该研究探讨了生成式AI与印度刑事司法中计算取证的冲突，分析了《数字个人数据保护法》对抗AI威胁的不足，提出了以可解释AI为核心的人本取证模型。


<details>
  <summary>Details</summary>
Motivation: 生成式AI融入数字生态系统需要重新评估印度刑事司法中的计算取证完整性。当前研究存在空白：2023年《数字个人数据保护法》与对抗性AI威胁（特别是反取证和深度伪造）的兼容性问题。AI存在"双重用途"困境——既是网络威胁载体又是取证自动化机制。

Method: 采用法律教义学方法，将《数字个人数据保护法》的法规分析与全球伦理框架（IEEE、欧盟）相结合，评估监管有效性。

Result: 机器学习在模式识别方面准确性高，但引入了数据投毒和算法偏见的漏洞。发现《数字个人数据保护法》的数据最小化原则与取证数据保留要求之间存在关键冲突。现有法律定义未能充分涵盖AI驱动的"工具犯罪"和"目标犯罪"。

Conclusion: 提出以可解释AI为核心的"人本"取证模型以确保证据可采性。建议将印度隐私法规与国际取证标准同步以减轻合成媒体风险，为未来立法修正和技术标准化提供路线图。

Abstract: The integration of generative Artificial Intelligence into the digital ecosystem necessitates a critical re-evaluation of Indian criminal jurisprudence regarding computational forensics integrity. While algorithmic efficiency enhances evidence extraction, a research gap exists regarding the Digital Personal Data Protection Act, 2023's compatibility with adversarial AI threats, specifically anti-forensics and deepfakes. This study scrutinizes the AI "dual-use" dilemma, functioning as both a cyber-threat vector and forensic automation mechanism, to delineate privacy boundaries in high-stakes investigations. Employing a doctrinal legal methodology, the research synthesizes statutory analysis of the DPDP Act with global ethical frameworks (IEEE, EU) to evaluate regulatory efficacy. Preliminary results indicate that while Machine Learning offers high accuracy in pattern recognition, it introduces vulnerabilities regarding data poisoning and algorithmic bias. Findings highlight a critical tension between the Act's data minimization principles and forensic data retention requirements. Furthermore, the paper identifies that existing legal definitions inadequately encompass AI-driven "tool crimes" and "target crimes." Consequently, the research proposes a "human-centric" forensic model prioritizing explainable AI (XAI) to ensure evidence admissibility. These implications suggest that synchronizing Indian privacy statutes with international forensic standards is imperative to mitigate synthetic media risks, establishing a roadmap for future legislative amendments and technical standardization.

</details>


### [12] [An empirical analysis of zero-day vulnerabilities disclosed by the zero day initiative](https://arxiv.org/abs/2512.15803)
*Apurva Shet,Izzat Alsmadi*

Main category: cs.CR

TL;DR: 该研究分析了2024年1-4月ZDI披露的415个零日漏洞，旨在识别趋势、分析严重性分布，并探索使用机器学习和深度学习模型进行严重性预测，以改进补丁优先级策略。


<details>
  <summary>Details</summary>
Motivation: 零日漏洞是网络安全中最关键的威胁之一，它们在厂商开发补丁前就被利用，使系统处于无防御状态。需要更好地理解这些漏洞的特征和严重性，以改进漏洞管理和组织准备。

Method: 分析ZDI披露的415个漏洞数据集，包括漏洞标识符、CVSS v3.0评分、发布日期和文本描述。使用经典机器学习技术和深度学习模型，结合结构化元数据和非结构化文本描述，进行严重性分类预测建模。

Result: 研究识别了零日漏洞披露趋势，分析了不同厂商间的严重性分布，并确定了与高严重性最相关的漏洞特征。比较了不同预测模型在严重性分类上的表现。

Conclusion: 该研究为改进补丁优先级策略、更有效的漏洞管理和增强组织对新兴零日威胁的准备提供了支持，展示了预测建模在漏洞严重性评估中的应用价值。

Abstract: Zero-day vulnerabilities represent some of the most critical threats in cybersecurity, as they correspond to previously unknown flaws in software or hardware that are actively exploited before vendors can develop and deploy patches. During this exposure window, affected systems remain defenseless, making zero-day attacks particularly damaging and difficult to mitigate. This study analyzes the Zero Day Initiative (ZDI) vulnerability disclosures reported between January and April 2024, Cole [2025] comprising a total of 415 vulnerabilities. The dataset includes vulnerability identifiers, Common Vulnerability Scoring System (CVSS) v3.0 scores, publication dates, and short textual descriptions. The primary objectives of this work are to identify trends in zero-day vulnerability disclosures, examine severity distributions across vendors, and investigate which vulnerability characteristics are most indicative of high severity. In addition, this study explores predictive modeling approaches for severity classification, comparing classical machine learning techniques with deep learning models using both structured metadata and unstructured textual descriptions. The findings aim to support improved patch prioritization strategies, more effective vulnerability management, and enhanced organizational preparedness against emerging zero-day threats.

</details>


### [13] [Unveiling the Attribute Misbinding Threat in Identity-Preserving Models](https://arxiv.org/abs/2512.15818)
*Junming Fu,Jishen Zeng,Yi Jiang,Peiyu Zhuang,Baoying Chen,Siyu Lu,Jianquan Yang*

Main category: cs.CR

TL;DR: 提出属性误绑定攻击方法，通过制作看似良性的文本提示绕过文本过滤器，利用身份保持模型的注意力偏差漏洞，将有害描述误绑定到目标身份上生成NSFW内容。


<details>
  <summary>Details</summary>
Motivation: 身份保持模型在生成个性化内容方面取得了显著进展，但同时也加剧了被滥用的风险，例如针对特定个人生成威胁性内容。现有安全措施存在漏洞，需要研究新的攻击方法和评估指标。

Method: 提出属性误绑定攻击方法：1）制作看似良性的文本提示绕过文本过滤器；2）利用模型内部注意力偏差导致的属性绑定缺陷；3）将有害描述误绑定到目标身份。同时提出Misbinding Prompt评估集和属性绑定安全评分（ABSS）指标。

Result: Misbinding Prompt评估集在绕过5个主流文本过滤器（包括GPT-4o）方面比现有评估集成功率高出5.28%，并产生最高比例的NSFW内容。ABSS指标能同时评估内容保真度和安全性。

Conclusion: 身份保持模型存在严重的属性绑定漏洞，可能被用于生成针对特定个人的有害内容。需要开发更强大的安全机制来防御此类攻击，同时ABSS指标为全面评估模型安全性提供了有效工具。

Abstract: Identity-preserving models have led to notable progress in generating personalized content. Unfortunately, such models also exacerbate risks when misused, for instance, by generating threatening content targeting specific individuals. This paper introduces the \textbf{Attribute Misbinding Attack}, a novel method that poses a threat to identity-preserving models by inducing them to produce Not-Safe-For-Work (NSFW) content. The attack's core idea involves crafting benign-looking textual prompts to circumvent text-filter safeguards and leverage a key model vulnerability: flawed attribute binding that stems from its internal attention bias. This results in misattributing harmful descriptions to a target identity and generating NSFW outputs. To facilitate the study of this attack, we present the \textbf{Misbinding Prompt} evaluation set, which examines the content generation risks of current state-of-the-art identity-preserving models across four risk dimensions: pornography, violence, discrimination, and illegality. Additionally, we introduce the \textbf{Attribute Binding Safety Score (ABSS)}, a metric for concurrently assessing both content fidelity and safety compliance. Experimental results show that our Misbinding Prompt evaluation set achieves a \textbf{5.28}\% higher success rate in bypassing five leading text filters (including GPT-4o) compared to existing main-stream evaluation sets, while also demonstrating the highest proportion of NSFW content generation. The proposed ABSS metric enables a more comprehensive evaluation of identity-preserving models by concurrently assessing both content fidelity and safety compliance.

</details>


### [14] [Secure AI-Driven Super-Resolution for Real-Time Mixed Reality Applications](https://arxiv.org/abs/2512.15823)
*Mohammad Waquas Usmani,Sankalpa Timilsina,Michael Zink,Susmit Shannigrahi*

Main category: cs.CR

TL;DR: 提出一个用于360°和6DoF点云视频流媒体的系统，通过服务器端下采样和部分加密减少带宽和延迟，客户端使用ML超分辨率重建原始分辨率


<details>
  <summary>Details</summary>
Motivation: 沉浸式格式如360°和6DoF点云视频需要高带宽和低延迟，对实时AR/VR流媒体构成挑战。需要减少带宽消耗和加密/解密延迟这两个关键因素

Method: 在源服务器对点云内容进行下采样并应用部分加密，在客户端解密并使用基于机器学习的超分辨率模型进行上采样

Result: 评估显示带宽/延迟和加密/解密开销随下采样分辨率降低而几乎线性减少，超分辨率模型能有效重建原始全分辨率点云，误差小且推理时间适中

Conclusion: 该系统能显著降低沉浸式点云视频流媒体的带宽和延迟需求，同时通过ML超分辨率保持高质量重建

Abstract: Immersive formats such as 360° and 6DoF point cloud videos require high bandwidth and low latency, posing challenges for real-time AR/VR streaming. This work focuses on reducing bandwidth consumption and encryption/decryption delay, two key contributors to overall latency. We design a system that downsamples point cloud content at the origin server and applies partial encryption. At the client, the content is decrypted and upscaled using an ML-based super-resolution model. Our evaluation demonstrates a nearly linear reduction in bandwidth/latency, and encryption/decryption overhead with lower downsampling resolutions, while the super-resolution model effectively reconstructs the original full-resolution point clouds with minimal error and modest inference time.

</details>


### [15] [VET Your Agent: Towards Host-Independent Autonomy via Verifiable Execution Traces](https://arxiv.org/abs/2512.15892)
*Artem Grigor,Christian Schroeder de Witt,Simon Birnbach,Ivan Martinovic*

Main category: cs.CR

TL;DR: VET框架通过可验证执行轨迹实现主机无关的智能体输出认证，支持多种证明机制，为完全主机无关的自主性奠定基础。


<details>
  <summary>Details</summary>
Motivation: 当前自主智能体在主机控制的基础设施上运行，主机可能篡改模型、输入或输出，破坏了真正的自主性概念，需要解决这一信任问题。

Method: 提出VET框架，包含Agent Identity Document定义智能体配置和验证系统，支持多种证明机制：可信硬件、简洁密码学证明和公证TLS记录（Web Proofs）。

Result: 实现基于API的LLM智能体VET，评估显示Web Proofs对黑盒API最实用（开销通常低于3倍），TEE Proxy对公开API足够。部署了可验证交易智能体案例。

Conclusion: 实用、主机无关的认证在当前技术下已可实现，为未来实现完全主机无关的自主系统奠定了基础。

Abstract: Recent advances in large language models (LLMs) have enabled a new generation of autonomous agents that operate over sustained periods and manage sensitive resources on behalf of users. Trusted for their ability to act without direct oversight, such agents are increasingly considered in high-stakes domains including financial management, dispute resolution, and governance. Yet in practice, agents execute on infrastructure controlled by a host, who can tamper with models, inputs, or outputs, undermining any meaningful notion of autonomy.
  We address this gap by introducing VET (Verifiable Execution Traces), a formal framework that achieves host-independent authentication of agent outputs and takes a step toward host-independent autonomy. Central to VET is the Agent Identity Document (AID), which specifies an agent's configuration together with the proof systems required for verification. VET is compositional: it supports multiple proof mechanisms, including trusted hardware, succinct cryptographic proofs, and notarized TLS transcripts (Web Proofs).
  We implement VET for an API-based LLM agent and evaluate our instantiation on realistic workloads. We find that for today's black-box, secret-bearing API calls, Web Proofs appear to be the most practical choice, with overhead typically under 3$\times$ compared to direct API calls, while for public API calls, a lower-overhead TEE Proxy is often sufficient. As a case study, we deploy a verifiable trading agent that produces proofs for each decision and composes Web Proofs with a TEE Proxy. Our results demonstrate that practical, host-agnostic authentication is already possible with current technology, laying the foundation for future systems that achieve full host-independent autonomy.

</details>


### [16] [Private Virtual Tree Networks for Secure Multi-Tenant Environments Based on the VIRGO Overlay Network](https://arxiv.org/abs/2512.15915)
*Lican Huang*

Main category: cs.CR

TL;DR: PVTNs扩展VIRGO网络，通过加密和授权证书实现安全的虚拟层次组织，保护隐私且无需全局公钥基础设施


<details>
  <summary>Details</summary>
Motivation: 现实世界中的层次组织结构在分布式系统中缺乏安全性和隐私保护，现有VIRGO网络虽然提供可扩展的覆盖网络，但缺乏内在的安全机制

Method: 基于VIRGO覆盖网络构建私有虚拟树网络，使用经理公钥加密加入请求，通过经理签名的委托证书进行成员授权，公钥仅在直接管理关系中共享

Result: PVTNs实现了可扩展、动态管理的私有非枚举虚拟树，提供强安全保证，无需依赖全局公钥基础设施

Conclusion: PVTNs成功将现实世界的层次组织结构映射到安全的分布式系统中，通过密码学机制实现了隐私保护、安全授权和可扩展性

Abstract: Hierarchical organization is a fundamental structure in real-world society, where authority and responsibility are delegated from managers to subordinates. The VIRGO network (Virtual Hierarchical Overlay Network for scalable grid computing) provides a scalable overlay for organizing distributed systems but lacks intrinsic security and privacy mechanisms. This paper proposes Private Virtual Tree Networks (PVTNs), a cryptographically enforced extension that leverages the VIRGO overlay to mirror real organizational hierarchies. In PVTNs, join requests are encrypted with the manager's public key to ensure confidentiality, while membership authorization is enforced through manager-signed delegation certificates. Public keys are treated as organizational secrets and are disclosed only within direct manager-member relationships, resulting in a private, non-enumerable virtual tree. Our work demonstrates, through the system model, protocols, security analysis, and design rationale, that PVTNs achieve scalability, dynamic management, and strong security guarantees without relying on global public key infrastructures.

</details>


### [17] [Security Aspects of ISO 15118 Plug and Charge Payment](https://arxiv.org/abs/2512.15966)
*Jakob Löw,Vishwa Vasu,Thomas Hutzelmann,Hans-Joachim Hof*

Main category: cs.CR

TL;DR: 论文分析了ISO 15118电动汽车快速充电标准的安全控制缺陷，发现了一个未公开的漏洞，允许攻击车辆充电而让受害车辆付费，并提出了更安全的替代认证方案。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车普及，长途驾驶需要最小化充电时间。虽然存在多种DC快速充电标准，但欧洲主导标准ISO 15118理论上配备了多种安全控制，确保充电通信的真实性、保密性和支付信息安全。然而实践中这些安全控制不足以有效保护充电通信。

Method: 1. 全面审查ISO 15118标准中定义的所有安全控制措施；2. 演示这些安全控制的缺陷；3. 特别展示ISO 15118插电即充功能中一个先前未公开的漏洞；4. 提供该漏洞的概念验证实现；5. 定义替代的插电即充认证方案。

Result: 1. 发现ISO 15118安全控制存在多个缺陷；2. 揭示了一个关键漏洞：允许一辆车充电而让另一辆受害车辆为其付费；3. 成功实现了该漏洞的概念验证；4. 提出了更安全、需要更少证书注册工作、更具弹性和未来性的替代认证方案。

Conclusion: ISO 15118标准的安全控制在实践中不足，特别是发现的插电即充漏洞对快速充电安全至关重要。在实施和推进该标准时应考虑这些发现，修复该漏洞对保障快速充电安全至关重要。提出的替代认证方案提供了更安全的选择。

Abstract: For the rise of electric vehicles, especially for long-distance driving, minimizing charging times is vital. While multiple standards for DC fast charging exist, the leading standard in Europe is ISO 15118. In theory, this standard is accompanied by a variety of security controls, ensuring the authenticity and confidentiality of charging communication, as well as the exchange of payment information. In practice, these security controls are insufficient for effectively securing charging communication. In this paper, we go through all security controls defined in ISO 15118 and demonstrate their shortcomings. Most notably, we present a previously unpublished vulnerability in the plug and charge functionality of ISO 15118. We provide a proof-of-concept implementation of this vulnerability, which, allows a vehicle to be charged while a second, victim vehicle is billed for it. Additionally, we define an alternative plug and charge authentication scheme, which requires fewer efforts towards certificate enrollment and promises to be more resilient and future-proof. Our findings should be considered when implementing and advancing the standard, as the mitigation of the discovered vulnerability is critical for the security of fast charging.

</details>


### [18] [ContextLeak: Auditing Leakage in Private In-Context Learning Methods](https://arxiv.org/abs/2512.16059)
*Jacob Choi,Shuying Cao,Xingjian Dong,Wang Bill Zhu,Robin Jia,Sai Praneeth Karimireddy*

Main category: cs.CR

TL;DR: ContextLeak：首个用于测量ICL中信息泄漏的框架，通过插入可识别标记和针对性查询来检测隐私保护方法的有效性


<details>
  <summary>Details</summary>
Motivation: 虽然已有许多保护上下文隐私的方法，但缺乏对这些方法有效性的审计机制。当示例包含敏感信息时，需要可靠的方法来防止信息通过模型输出泄漏

Method: 提出ContextLeak框架，使用金丝雀插入技术（在示例中嵌入唯一可识别标记），并设计针对性查询来检测这些标记是否出现在模型输出中

Result: ContextLeak与理论隐私预算（ε）紧密相关，能可靠检测信息泄漏。现有方法通常在隐私-效用权衡上表现不佳，要么泄漏敏感信息，要么严重降低性能

Conclusion: ContextLeak是首个实证测量ICL最坏情况信息泄漏的框架，为评估隐私保护方法的有效性提供了重要工具，揭示了当前方法在隐私-效用平衡方面的不足

Abstract: In-Context Learning (ICL) has become a standard technique for adapting Large Language Models (LLMs) to specialized tasks by supplying task-specific exemplars within the prompt. However, when these exemplars contain sensitive information, reliable privacy-preserving mechanisms are essential to prevent unintended leakage through model outputs. Many privacy-preserving methods are proposed to protect the information leakage in the context, but there are less efforts on how to audit those methods. We introduce ContextLeak, the first framework to empirically measure the worst-case information leakage in ICL. ContextLeak uses canary insertion, embedding uniquely identifiable tokens in exemplars and crafting targeted queries to detect their presence. We apply ContextLeak across a range of private ICL techniques, both heuristic such as prompt-based defenses and those with theoretical guarantees such as Embedding Space Aggregation and Report Noisy Max. We find that ContextLeak tightly correlates with the theoretical privacy budget ($ε$) and reliably detects leakage. Our results further reveal that existing methods often strike poor privacy-utility trade-offs, either leaking sensitive information or severely degrading performance.

</details>


### [19] [Design of a Decentralized Fixed-Income Lending Automated Market Maker Protocol Supporting Arbitrary Maturities](https://arxiv.org/abs/2512.16080)
*Tianyi Ma*

Main category: cs.CR

TL;DR: BondMM-A：一种支持任意期限固定收益借贷的改进型AMM协议，通过单一智能合约集成不同期限工具，提升资本效率和操作自由度


<details>
  <summary>Details</summary>
Motivation: 去中心化金融中设计固定收益借贷AMM面临时间复杂性挑战，现有协议仅支持单一期限借贷，需要更灵活的多期限解决方案

Method: 基于BondMM协议，将其数学不变量推广到任意期限，设计BondMM-A协议，在单一智能合约中集成不同期限的固定收益工具

Result: 实验结果显示BondMM-A在利率稳定性和金融稳健性方面表现优异，为用户和流动性提供者提供更大操作自由度和资本效率

Conclusion: BondMM-A成功解决了DeFi固定收益借贷的多期限支持问题，通过优雅的数学推广实现了灵活高效的借贷市场设计

Abstract: In decentralized finance (DeFi), designing fixed-income lending automated market makers (AMMs) is extremely challenging due to time-related complexities. Moreover, existing protocols only support single-maturity lending. Building upon the BondMM protocol, this paper argues that its mathematical invariants are sufficiently elegant to be generalized to arbitrary maturities. This paper thus propose an improved design, BondMM-A, which supports lending activities of any maturity. By integrating fixed-income instruments of varying maturities into a single smart contract, BondMM-A offers users and liquidity providers (LPs) greater operational freedom and capital efficiency. Experimental results show that BondMM-A performs excellently in terms of interest rate stability and financial robustness.

</details>


### [20] [Autoencoder-based Denoising Defense against Adversarial Attacks on Object Detection](https://arxiv.org/abs/2512.16123)
*Min Geun Song,Gang Min Kim,Woonmin Kim,Yongsik Kim,Jeonghyun Sim,Sangbeom Park,Huy Kang Kim*

Main category: cs.CR

TL;DR: 提出基于自编码器的去噪防御方法，对抗性攻击使目标检测性能下降43.3%，自编码器防御能恢复3.7%的性能，无需模型重训练。


<details>
  <summary>Details</summary>
Motivation: 深度学习目标检测模型在自动驾驶、安防监控等关键应用中易受对抗性攻击，需要有效的防御方法。

Method: 使用Perlin噪声对COCO数据集车辆图像进行对抗攻击，采用单层卷积自编码器去除扰动，用YOLOv5评估检测性能。

Result: 对抗攻击使bbox mAP从0.2890降至0.1640（下降43.3%），自编码器防御后bbox mAP恢复至0.1700（恢复3.7%），bbox mAP@50从0.2780提升至0.3080（提升10.8%）。

Conclusion: 基于自编码器的去噪方法能部分防御对抗攻击，无需模型重训练，为对抗防御提供了可行方案。

Abstract: Deep learning-based object detection models play a critical role in real-world applications such as autonomous driving and security surveillance systems, yet they remain vulnerable to adversarial examples. In this work, we propose an autoencoder-based denoising defense to recover object detection performance degraded by adversarial perturbations. We conduct adversarial attacks using Perlin noise on vehicle-related images from the COCO dataset, apply a single-layer convolutional autoencoder to remove the perturbations, and evaluate detection performance using YOLOv5. Our experiments demonstrate that adversarial attacks reduce bbox mAP from 0.2890 to 0.1640, representing a 43.3% performance degradation. After applying the proposed autoencoder defense, bbox mAP improves to 0.1700 (3.7% recovery) and bbox mAP@50 increases from 0.2780 to 0.3080 (10.8% improvement). These results indicate that autoencoder-based denoising can provide partial defense against adversarial attacks without requiring model retraining.

</details>


### [21] [DualGuard: Dual-stream Large Language Model Watermarking Defense against Paraphrase and Spoofing Attack](https://arxiv.org/abs/2512.16182)
*Hao Li,Yubing Ren,Yanan Cao,Yingjie Li,Fang Fang,Shi Wang,Li Guo*

Main category: cs.CR

TL;DR: DualGuard是一种针对大型语言模型的新型水印算法，能够同时防御改写攻击和搭便车欺骗攻击，通过自适应双流水印机制实现检测和溯源功能。


<details>
  <summary>Details</summary>
Motivation: 随着云服务的发展，LLM变得更容易访问，但也增加了模型滥用的风险。现有水印算法主要关注防御改写攻击，忽视了搭便车欺骗攻击，这种攻击会注入有害内容、损害水印可靠性并破坏归因信任。

Method: DualGuard采用自适应双流水印机制，根据语义内容动态注入两个互补的水印信号。这种设计不仅能够检测还能追踪欺骗攻击，确保水印检测的可靠性和可信度。

Result: 在多个数据集和语言模型上的广泛实验表明，DualGuard在可检测性、鲁棒性、可追溯性和文本质量方面表现出色，有效推进了LLM水印在实际应用中的发展。

Conclusion: DualGuard是首个能够同时防御改写和欺骗攻击的水印算法，通过其创新设计确保了水印的可靠性和可信度，为现实世界应用中的LLM水印技术提供了重要进展。

Abstract: With the rapid development of cloud-based services, large language models (LLMs) have become increasingly accessible through various web platforms. However, this accessibility has also led to growing risks of model abuse. LLM watermarking has emerged as an effective approach to mitigate such misuse and protect intellectual property. Existing watermarking algorithms, however, primarily focus on defending against paraphrase attacks while overlooking piggyback spoofing attacks, which can inject harmful content, compromise watermark reliability, and undermine trust in attribution. To address this limitation, we propose DualGuard, the first watermarking algorithm capable of defending against both paraphrase and spoofing attacks. DualGuard employs the adaptive dual-stream watermarking mechanism, in which two complementary watermark signals are dynamically injected based on the semantic content. This design enables DualGuard not only to detect but also to trace spoofing attacks, thereby ensuring reliable and trustworthy watermark detection. Extensive experiments conducted across multiple datasets and language models demonstrate that DualGuard achieves excellent detectability, robustness, traceability, and text quality, effectively advancing the state of LLM watermarking for real-world applications.

</details>


### [22] [Love, Lies, and Language Models: Investigating AI's Role in Romance-Baiting Scams](https://arxiv.org/abs/2512.16280)
*Gilad Gressel,Rahul Pankajakshan,Shir Rozenfeld,Ling Li,Ivan Franceschini,Krishnahsree Achuthan,Yisroel Mirsky*

Main category: cs.CR

TL;DR: LLMs已在浪漫诱饵诈骗中被广泛部署，能比人类操作员获得更高信任和顺从度，而现有安全过滤器完全无法检测此类诈骗对话。


<details>
  <summary>Details</summary>
Motivation: 浪漫诱饵诈骗已成为全球性的财务和情感伤害来源，这些由犯罪集团运营的诈骗活动涉及数千人的强迫劳动。由于诈骗本质上是基于文本的，这引发了关于大型语言模型在当前和未来自动化中作用的紧迫问题。

Method: 1) 采访145名内部人员和5名诈骗受害者；2) 进行盲法长期对话研究，比较LLM诈骗代理与人类操作员的表现；3) 评估商业安全过滤器的效果。

Result: 1) LLMs已在诈骗组织中广泛部署，87%的诈骗劳动由系统化的对话任务组成，易于自动化；2) 在一周的研究中，LLM代理比人类操作员获得更高信任(p=0.007)和顺从度(46% vs 18%)；3) 流行安全过滤器对浪漫诱饵对话的检测率为0.0%。

Conclusion: 浪漫诱饵诈骗可能适合大规模LLM自动化，而现有防御措施不足以防止其扩张，需要更有效的检测和预防机制。

Abstract: Romance-baiting scams have become a major source of financial and emotional harm worldwide. These operations are run by organized crime syndicates that traffic thousands of people into forced labor, requiring them to build emotional intimacy with victims over weeks of text conversations before pressuring them into fraudulent cryptocurrency investments. Because the scams are inherently text-based, they raise urgent questions about the role of Large Language Models (LLMs) in both current and future automation.
  We investigate this intersection by interviewing 145 insiders and 5 scam victims, performing a blinded long-term conversation study comparing LLM scam agents to human operators, and executing an evaluation of commercial safety filters. Our findings show that LLMs are already widely deployed within scam organizations, with 87% of scam labor consisting of systematized conversational tasks readily susceptible to automation. In a week-long study, an LLM agent not only elicited greater trust from study participants (p=0.007) but also achieved higher compliance with requests than human operators (46% vs. 18% for humans). Meanwhile, popular safety filters detected 0.0% of romance baiting dialogues. Together, these results suggest that romance-baiting scams may be amenable to full-scale LLM automation, while existing defenses remain inadequate to prevent their expansion.

</details>


### [23] [Empirical Evaluation of Structured Synthetic Data Privacy Metrics: Novel experimental framework](https://arxiv.org/abs/2512.16284)
*Milton Nicolás Plasencia Palacios,Alexander Boudewijn,Sebastiano Saccani,Andrea Filippo Ferraris,Diana Sofronieva,Giuseppe D'Acquisto,Filiberto Brozzetti,Daniele Panfilo,Luca Bortolussi*

Main category: cs.CR

TL;DR: 提出一个通过受控风险插入来评估表格合成数据隐私量化方法有效性的框架，并在公开数据集上应用该框架测试主流隐私量化方法。


<details>
  <summary>Details</summary>
Motivation: 合成数据作为隐私增强技术越来越受关注，但数据隐私概念难以捉摸，从业者难以评估和基准化合成数据提供的隐私保护程度。

Method: 提出一个通过受控、故意的风险插入来实证评估表格合成数据隐私量化方法有效性的框架，并应用该框架测试无盒威胁模型下的主要隐私量化方法。

Result: 在公开数据集上应用该框架评估了主流隐私量化方法，但摘要中未具体说明评估结果。

Conclusion: 该框架为从业者提供了评估合成数据隐私保护程度的实证方法，有助于更好地理解和基准化合成数据的隐私保护效果。

Abstract: Synthetic data generation is gaining traction as a privacy enhancing technology (PET). When properly generated, synthetic data preserve the analytic utility of real data while avoiding the retention of information that would allow the identification of specific individuals. However, the concept of data privacy remains elusive, making it challenging for practitioners to evaluate and benchmark the degree of privacy protection offered by synthetic data. In this paper, we propose a framework to empirically assess the efficacy of tabular synthetic data privacy quantification methods through controlled, deliberate risk insertion. To demonstrate this framework, we survey existing approaches to synthetic data privacy quantification and the related legal theory. We then apply the framework to the main privacy quantification methods with no-box threat models on publicly available datasets.

</details>


### [24] [In-Context Probing for Membership Inference in Fine-Tuned Language Models](https://arxiv.org/abs/2512.16292)
*Zhexi Lu,Hongliang Chi,Nathalie Baracaldo,Swanand Ravindra Kadhe,Yuseok Jeon,Lei Yu*

Main category: cs.CR

TL;DR: ICP-MIA是一种基于训练动态理论的新型成员推理攻击框架，通过优化间隙作为成员信号，使用上下文探测方法在无需训练的情况下模拟微调行为，显著优于现有黑盒攻击方法。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒成员推理攻击依赖置信度或token似然度，但这些信号与样本内在属性（如内容难度、稀有性）纠缠，导致泛化能力差、信噪比低。需要更有效的隐私风险评估方法。

Method: 提出ICP-MIA框架：1）引入优化间隙作为成员信号 - 收敛时成员样本损失减少潜力最小，非成员仍有显著优化潜力；2）提出上下文探测方法，通过构建策略性输入上下文模拟微调行为，包括基于参考数据和自扰动两种策略。

Result: 在三个任务和多个大语言模型上的实验表明，ICP-MIA显著优于现有黑盒成员推理攻击，特别是在低误报率下表现更优。分析了参考数据对齐、模型类型、PEFT配置和训练计划对攻击效果的影响。

Conclusion: ICP-MIA为部署的大语言模型隐私风险评估提供了一个实用且理论基础的框架，基于训练动态理论，通过优化间隙信号和上下文探测方法有效识别成员样本。

Abstract: Membership inference attacks (MIAs) pose a critical privacy threat to fine-tuned large language models (LLMs), especially when models are adapted to domain-specific tasks using sensitive data. While prior black-box MIA techniques rely on confidence scores or token likelihoods, these signals are often entangled with a sample's intrinsic properties - such as content difficulty or rarity - leading to poor generalization and low signal-to-noise ratios. In this paper, we propose ICP-MIA, a novel MIA framework grounded in the theory of training dynamics, particularly the phenomenon of diminishing returns during optimization. We introduce the Optimization Gap as a fundamental signal of membership: at convergence, member samples exhibit minimal remaining loss-reduction potential, while non-members retain significant potential for further optimization. To estimate this gap in a black-box setting, we propose In-Context Probing (ICP), a training-free method that simulates fine-tuning-like behavior via strategically constructed input contexts. We propose two probing strategies: reference-data-based (using semantically similar public samples) and self-perturbation (via masking or generation). Experiments on three tasks and multiple LLMs show that ICP-MIA significantly outperforms prior black-box MIAs, particularly at low false positive rates. We further analyze how reference data alignment, model type, PEFT configurations, and training schedules affect attack effectiveness. Our findings establish ICP-MIA as a practical and theoretically grounded framework for auditing privacy risks in deployed LLMs.

</details>


### [25] [Beyond the Benchmark: Innovative Defenses Against Prompt Injection Attacks](https://arxiv.org/abs/2512.16307)
*Safwan Shaheer,G. M. Refatul Islam,Mohammad Rafid Hamid,Tahsin Zaman Jilan*

Main category: cs.CR

TL;DR: 论文提出针对LLaMA系列小规模开源LLMs的提示注入攻击防御框架，通过基于思维链的迭代优化方法生成自动防御机制，显著降低目标劫持攻击成功率


<details>
  <summary>Details</summary>
Motivation: 随着小规模开源LLMs在边缘设备上的广泛应用，提示注入攻击带来的安全风险日益突出，需要有效的自动防御机制来保护这些资源受限环境中的模型安全

Method: 提出新防御框架：1)评估现有提示防御对最新攻击的效果；2)使用思维链作为种子防御，迭代优化防御提示；3)系统评估生成的防御机制

Result: 显著降低攻击成功率，减少误检率，有效检测目标劫持攻击能力，为资源受限环境下的小型开源LLMs提供更安全高效的部署方案

Conclusion: 该研究为开源LLMs生态系统安全做出重要贡献，提出的防御框架能有效缓解目标劫持漏洞，推动小型开源LLMs在边缘设备上的安全部署

Abstract: In this fast-evolving area of LLMs, our paper discusses the significant security risk presented by prompt injection attacks. It focuses on small open-sourced models, specifically the LLaMA family of models. We introduce novel defense mechanisms capable of generating automatic defenses and systematically evaluate said generated defenses against a comprehensive set of benchmarked attacks. Thus, we empirically demonstrated the improvement proposed by our approach in mitigating goal-hijacking vulnerabilities in LLMs. Our work recognizes the increasing relevance of small open-sourced LLMs and their potential for broad deployments on edge devices, aligning with future trends in LLM applications. We contribute to the greater ecosystem of open-source LLMs and their security in the following: (1) assessing present prompt-based defenses against the latest attacks, (2) introducing a new framework using a seed defense (Chain Of Thoughts) to refine the defense prompts iteratively, and (3) showing significant improvements in detecting goal hijacking attacks. Out strategies significantly reduce the success rates of the attacks and false detection rates while at the same time effectively detecting goal-hijacking capabilities, paving the way for more secure and efficient deployments of small and open-source LLMs in resource-constrained environments.

</details>


### [26] [Agent Tools Orchestration Leaks More: Dataset, Benchmark, and Mitigation](https://arxiv.org/abs/2512.16310)
*Yuxuan Qiao,Dongqin Liu,Hongchang Yang,Wei Zhou,Songlin Hu*

Main category: cs.CR

TL;DR: 论文发现基于大语言模型的单智能体多工具架构存在严重的工具编排隐私风险（TOP-R），智能体会自主聚合多个工具的信息片段，利用推理能力合成敏感信息。研究建立了评估框架TOP-Bench，提出隐私增强原则（PEP）方法有效降低风险。


<details>
  <summary>Details</summary>
Motivation: 当前流行的单智能体多工具架构虽然简单有效，但引入了新的严重隐私风险。智能体为实现良性用户目标，会自主聚合多个工具的信息片段，利用推理能力合成意外敏感信息，这种工具编排隐私风险（TOP-R）尚未得到系统研究。

Method: 1. 建立形式化框架，将风险根源归因于智能体目标函数错配：过度优化帮助性而忽视隐私意识。2. 构建TOP-Bench评估基准，包含泄漏和良性场景对。3. 引入H-Score作为安全性与鲁棒性权衡的综合性指标。4. 提出隐私增强原则（PEP）方法缓解风险。

Result: 评估显示TOP-R是严重风险：8个代表性模型的平均风险泄漏率（RLR）达90.24%，平均H-Score仅为0.167，没有模型超过0.3。PEP方法有效缓解TOP-R，将风险泄漏率降至46.58%，H-Score显著提升至0.624。

Conclusion: 该研究揭示了当前智能体架构中一类新的风险和固有结构局限性，同时提供了可行的缓解策略。TOP-R风险源于智能体目标函数设计缺陷，需要重新思考智能体架构设计以平衡帮助性和隐私保护。

Abstract: Driven by Large Language Models, the single-agent, multi-tool architecture has become a popular paradigm for autonomous agents due to its simplicity and effectiveness. However, this architecture also introduces a new and severe privacy risk, which we term Tools Orchestration Privacy Risk (TOP-R), where an agent, to achieve a benign user goal, autonomously aggregates information fragments across multiple tools and leverages its reasoning capabilities to synthesize unexpected sensitive information. We provide the first systematic study of this risk. First, we establish a formal framework, attributing the risk's root cause to the agent's misaligned objective function: an overoptimization for helpfulness while neglecting privacy awareness. Second, we construct TOP-Bench, comprising paired leakage and benign scenarios, to comprehensively evaluate this risk. To quantify the trade-off between safety and robustness, we introduce the H-Score as a holistic metric. The evaluation results reveal that TOP-R is a severe risk: the average Risk Leakage Rate (RLR) of eight representative models reaches 90.24%, while the average H-Score is merely 0.167, with no model exceeding 0.3. Finally, we propose the Privacy Enhancement Principle (PEP) method, which effectively mitigates TOP-R, reducing the Risk Leakage Rate to 46.58% and significantly improving the H-Score to 0.624. Our work reveals both a new class of risk and inherent structural limitations in current agent architectures, while also offering feasible mitigation strategies.

</details>


### [27] [A first look at common RPKI publication practices](https://arxiv.org/abs/2512.16369)
*Moritz Müller-Brus,Lisa Bruder,Caspar Schutijser,Ralph Koning*

Main category: cs.CR

TL;DR: 研究首次系统分析了RPKI（资源公钥基础设施）存储库的运营实践，重点关注信息分发情况，发现部署实践存在显著差异，部分做法可能危及RPKI信息的可用性。


<details>
  <summary>Details</summary>
Motivation: RPKI对互联网路由系统安全至关重要，但公共报告显示部分RPKI存储库不可靠。当前互联网草案提出了存储库运营最佳实践，但缺乏对实际运营实践的全面了解，需要建立基线来衡量未来RPKI存储库的成熟度。

Method: 基于互联网草案的启发，首次系统研究RPKI存储库的运营实践，主要关注RPKI信息的分发机制，分析100个分布式存储库的部署差异。

Result: 研究发现RPKI存储库的部署实践存在广泛差异，部分运营做法可能危及RPKI信息的可用性，为未来衡量RPKI存储库成熟度建立了基线。

Conclusion: 该研究首次建立了RPKI存储库运营实践的基线，揭示了当前部署中的风险点，为改进存储库运营质量和提高RPKI系统整体可靠性提供了重要参考。

Abstract: The RPKI is crucial for securing the routing system of the Internet. With the RPKI, owners of Internet resources can make cryptographically backed claims, for example about the legitimate origin of their IP space. Thousands of networks use this information to detect malicious or accidental route hijacks. The RPKI consists out of 100 distributed repositories. However, public reports claim that some of these repositories are unreliable. A current Internet-Draft suggests best practices on how to operate these repositories, with the goal to improve deployment quality.
  Inspired by this draft, we take a first look at the operational practices of repositories of the RPKI. We mainly focus on the distribution of RPKI information. We find that there is a wide variety in deployment practices, of which some might risk the availability of parts of the information in the RPKI. This study creates a baseline for measuring the maturity of RPKI repositories in the future.

</details>


### [28] [SoK: Reviewing Two Decades of Security, Privacy, Accessibility, and Usability Studies on Internet of Things for Older Adults](https://arxiv.org/abs/2512.16394)
*Suleiman Saka,Sanchari Das*

Main category: cs.CR

TL;DR: 本文系统综述了2004-2024年间44项关于老年人IoT的研究，提出了SPAU-IoT框架（包含安全、隐私、可访问性、可用性4个维度27项标准），发现现有研究70%以上关注认证加密，但不到50%涉及可访问性或可用性，并开发了针对老年人特定脆弱性的威胁模型和设计指南。


<details>
  <summary>Details</summary>
Motivation: 物联网(IoT)有潜力提升老年人的独立性和生活质量，但同时使他们面临安全、隐私、可访问性和可用性(SPAU)风险。现有研究缺乏对这些风险的系统性整合分析，特别是针对老年人特殊需求（如认知衰退、感官障碍）的考量。

Method: 采用五阶段筛选流程对2004-2024年间的44项同行评审研究进行系统综述。从每项研究中提取研究设计、IoT类型、SPAU措施等数据，提出包含27项标准的SPAU-IoT框架（安全、隐私、可访问性、可用性四个维度），并开发了针对老年人特定脆弱性的威胁模型。

Result: 研究发现：超过70%的研究实施了认证和加密机制，但不到50%的研究解决了可访问性或可用性问题。威胁模型识别了针对IoT资产、网络和后端服务器的攻击向量（如钓鱼、护理人员利用、弱密码攻击），特别考虑了与年龄相关的脆弱性。现有IoT研究缺乏整合的SPAU方法。

Conclusion: 研究揭示了现有IoT研究中系统性缺乏整合的SPAU方法，并将这些差距转化为针对老年人设计的IoT系统的可操作、符合标准的设计指南，为未来研究和实践提供了重要框架。

Abstract: The Internet of Things (IoT) has the potential to enhance older adults' independence and quality of life, but it also exposes them to security, privacy, accessibility, and usability (SPAU) risks. We conducted a systematic review of 44 peer-reviewed studies published between 2004 and 2024 using a five-phase screening pipeline. From each study, we extracted data on study design, IoT type, SPAU measures, and identified research gaps. We introduce the SPAU-IoT Framework, which comprises 27 criteria across four dimensions: security (e.g., resilience to cyber threats, secure authentication, encrypted communication, secure-by-default settings, and guardianship features), privacy (e.g., data minimization, explicit consent, and privacy-preserving analytics), accessibility (e.g., compliance with ADA/WCAG standards and assistive-technology compatibility), and usability (e.g., guided interaction, integrated assistance, and progressive learning). Applying this framework revealed that more than 70% of studies implemented authentication and encryption mechanisms, whereas fewer than 50% addressed accessibility or usability concerns. We further developed a threat model that maps IoT assets, networks, and backend servers to exploit vectors such as phishing, caregiver exploitation, and weak-password attacks, explicitly accounting for age-related vulnerabilities including cognitive decline and sensory impairment. Our results expose a systemic lack of integrated SPAU approaches in existing IoT research and translate these gaps into actionable, standards-aligned design guidelines for IoT systems designed for older adults.

</details>


### [29] [From Essence to Defense: Adaptive Semantic-aware Watermarking for Embedding-as-a-Service Copyright Protection](https://arxiv.org/abs/2512.16439)
*Hao Li,Yubing Ren,Yanan Cao,Yingjie Li,Fang Fang,Xuebin Wang*

Main category: cs.CR

TL;DR: SemMark：一种基于语义的EaaS水印保护方法，通过局部敏感哈希分割语义空间并注入语义感知水印，在保持嵌入分布的同时实现版权保护。


<details>
  <summary>Details</summary>
Motivation: 现有的EaaS水印方法忽略了嵌入最重要的语义特性，导致无害性和隐蔽性有限，需要一种能保护语义完整性的水印方案。

Method: 使用局部敏感哈希分割语义空间，在特定区域注入语义感知水印；基于局部离群因子设计自适应水印权重机制；提出检测采样和降维攻击来评估方法。

Result: 在四个流行NLP数据集上的实验表明，SemMark在可验证性、多样性、隐蔽性和无害性方面表现优异。

Conclusion: SemMark为EaaS版权保护提供了一种有效的语义感知水印范式，在保护知识产权的同时保持了嵌入的语义特性。

Abstract: Benefiting from the superior capabilities of large language models in natural language understanding and generation, Embeddings-as-a-Service (EaaS) has emerged as a successful commercial paradigm on the web platform. However, prior studies have revealed that EaaS is vulnerable to imitation attacks. Existing methods protect the intellectual property of EaaS through watermarking techniques, but they all ignore the most important properties of embedding: semantics, resulting in limited harmlessness and stealthiness. To this end, we propose SemMark, a novel semantic-based watermarking paradigm for EaaS copyright protection. SemMark employs locality-sensitive hashing to partition the semantic space and inject semantic-aware watermarks into specific regions, ensuring that the watermark signals remain imperceptible and diverse. In addition, we introduce the adaptive watermark weight mechanism based on the local outlier factor to preserve the original embedding distribution. Furthermore, we propose Detect-Sampling and Dimensionality-Reduction attacks and construct four scenarios to evaluate the watermarking method. Extensive experiments are conducted on four popular NLP datasets, and SemMark achieves superior verifiability, diversity, stealthiness, and harmlessness.

</details>


### [30] [A Systematic Study of Code Obfuscation Against LLM-based Vulnerability Detection](https://arxiv.org/abs/2512.16538)
*Xiao Li,Yue Li,Hao Wu,Yue Zhang,Yechao Zhang,Fengyuan Xu,Sheng Zhong*

Main category: cs.CR

TL;DR: 论文系统化研究了代码混淆技术对LLM漏洞检测的影响，评估了19种混淆技术对15个LLM模型的影响，发现混淆既有正面也有负面影响。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地用于代码漏洞检测，其可靠性和鲁棒性成为关注焦点。传统对抗环境中，代码混淆长期被用作绕过审计工具的策略，但现有混淆方法在支持技术、粒度和编程语言方面存在差异，难以系统评估它们对LLM漏洞检测的影响。

Method: 1. 将现有混淆方法系统化为三大类（布局、数据流、控制流），涵盖11个子类和19种具体技术；2. 在四种编程语言（Solidity、C、C++、Python）中统一实现这些技术；3. 使用LLM驱动的方法评估这些技术对15个LLM模型（涵盖DeepSeek、OpenAI、Qwen、LLaMA四个家族）和两个编码代理（GitHub Copilot和Codex）的影响。

Result: 研究发现代码混淆对LLM漏洞检测既有正面也有负面影响，揭示了在哪些条件下混淆会导致性能提升或下降。进一步分析了这些结果与漏洞特征、代码属性和模型属性的关系。

Conclusion: 论文为理解代码混淆对LLM漏洞检测的影响提供了系统化框架，指出了几个开放问题，并提出了增强LLM在实际漏洞检测中鲁棒性的未来方向。

Abstract: As large language models (LLMs) are increasingly adopted for code vulnerability detection, their reliability and robustness across diverse vulnerability types have become a pressing concern. In traditional adversarial settings, code obfuscation has long been used as a general strategy to bypass auditing tools, preserving exploitability without tampering with the tools themselves. Numerous efforts have explored obfuscation methods and tools, yet their capabilities differ in terms of supported techniques, granularity, and programming languages, making it difficult to systematically assess their impact on LLM-based vulnerability detection. To address this gap, we provide a structured systematization of obfuscation techniques and evaluate them under a unified framework. Specifically, we categorize existing obfuscation methods into three major classes (layout, data flow, and control flow) covering 11 subcategories and 19 concrete techniques. We implement these techniques across four programming languages (Solidity, C, C++, and Python) using a consistent LLM-driven approach, and evaluate their effects on 15 LLMs spanning four model families (DeepSeek, OpenAI, Qwen, and LLaMA), as well as on two coding agents (GitHub Copilot and Codex). Our findings reveal both positive and negative impacts of code obfuscation on LLM-based vulnerability detection, highlighting conditions under which obfuscation leads to performance improvements or degradations. We further analyze these outcomes with respect to vulnerability characteristics, code properties, and model attributes. Finally, we outline several open problems and propose future directions to enhance the robustness of LLMs for real-world vulnerability detection.

</details>


### [31] [Protecting Deep Neural Network Intellectual Property with Chaos-Based White-Box Watermarking](https://arxiv.org/abs/2512.16658)
*Sangeeth B,Serena Nicolazzo,Deepa K.,Vinod P*

Main category: cs.CR

TL;DR: 提出一种基于混沌序列的白盒水印框架，将所有权信息嵌入DNN内部参数中，使用遗传算法验证所有权，不影响模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络在各个领域的广泛应用，模型的知识产权保护和防止滥用变得日益重要。训练好的DNN模型是重要的资产，但容易被复制、重新分发或重新利用，因此需要有效的所有权验证机制。

Method: 使用逻辑映射（logistic map）生成混沌序列作为水印，将水印注入到选定的中间层权重中，不改变模型结构。验证过程采用遗传算法，通过优化提取序列与重新生成序列的相似度来恢复原始混沌参数。

Result: 在MNIST和CIFAR-10图像分类任务上的实验表明，嵌入的水印在微调后仍可检测，模型精度损失可忽略。通过权重密度图和基于激活的分类器进行视觉分析，能够区分原始、水印和被篡改的模型。

Conclusion: 该方法为白盒环境下的模型所有权嵌入和验证提供了灵活、可扩展的解决方案，适用于知识产权保护至关重要的实际场景。

Abstract: The rapid proliferation of deep neural networks (DNNs) across several domains has led to increasing concerns regarding intellectual property (IP) protection and model misuse. Trained DNNs represent valuable assets, often developed through significant investments. However, the ease with which models can be copied, redistributed, or repurposed highlights the urgent need for effective mechanisms to assert and verify model ownership. In this work, we propose an efficient and resilient white-box watermarking framework that embeds ownership information into the internal parameters of a DNN using chaotic sequences. The watermark is generated using a logistic map, a well-known chaotic function, producing a sequence that is sensitive to its initialization parameters. This sequence is injected into the weights of a chosen intermediate layer without requiring structural modifications to the model or degradation in predictive performance. To validate ownership, we introduce a verification process based on a genetic algorithm that recovers the original chaotic parameters by optimizing the similarity between the extracted and regenerated sequences. The effectiveness of the proposed approach is demonstrated through extensive experiments on image classification tasks using MNIST and CIFAR-10 datasets. The results show that the embedded watermark remains detectable after fine-tuning, with negligible loss in model accuracy. In addition to numerical recovery of the watermark, we perform visual analyses using weight density plots and construct activation-based classifiers to distinguish between original, watermarked, and tampered models. Overall, the proposed method offers a flexible and scalable solution for embedding and verifying model ownership in white-box settings well-suited for real-world scenarios where IP protection is critical.

</details>


### [32] [Efficient Bitcoin Meta-Protocol Transaction and Data Discovery Through nLockTime Field Repurposing](https://arxiv.org/abs/2512.16683)
*Nikodem Tomczak*

Main category: cs.CR

TL;DR: Lockchain Protocol利用比特币交易中的nLockTime字段作为元数据头，实现零区块空间成本的交易发现和数据验证


<details>
  <summary>Details</summary>
Motivation: 解决比特币交易发现效率低下的问题，特别是在大规模场景下，传统方法需要检查整个交易负载，效率低下

Method: 重新利用比特币交易中强制性的4字节nLockTime字段，将其值约束在未使用的过去Unix时间戳范围（≥500,000,000），编码协议信号、类型、变体和序列标识符

Result: 创建了一个高效的发现层，索引器可以通过检查固定大小的头部字段来筛选候选交易，然后有选择地检查更重的数据（如OP_RETURN输出或见证字段）

Conclusion: 该协议将已建立的协议设计模式应用于未优化的领域（大规模交易发现），不声称新的加密原语或存储方法，但显著提高了交易发现效率

Abstract: We describe the Lockchain Protocol, a lightweight Bitcoin meta-protocol that enables highly efficient transaction discovery at zero marginal block space cost, and data verification without introducing any new on-chain storage mechanism. The protocol repurposes the mandatory 4-byte nLockTime field of every Bitcoin transaction as a compact metadata header. By constraining values to an unused range of past Unix timestamps greater than or equal to 500,000,000, the field can encode a protocol signal, type, variant, and sequence identifier while remaining fully valid under Bitcoin consensus and policy rules. The primary contribution of the protocol is an efficient discovery layer. Indexers can filter candidate transactions by examining a fixed-size header field, independent of transaction payload size, and only then selectively inspect heavier data such as OP RETURN outputs or witness fields. The Lockchain Protocol applies established protocol design patterns to an under-optimised problem domain, namely transaction discovery at scale, and does not claim new cryptographic primitives or storage methods.

</details>


### [33] [PrivateXR: Defending Privacy Attacks in Extended Reality Through Explainable AI-Guided Differential Privacy](https://arxiv.org/abs/2512.16851)
*Ripan Kumar Kundu,Istiak Ahmed,Khaza Anuarul Hoque*

Main category: cs.CR

TL;DR: 提出结合可解释AI和差分隐私的框架，针对AI XR系统选择性保护关键特征，降低隐私攻击成功率同时保持模型性能


<details>
  <summary>Details</summary>
Motivation: AI XR系统使用敏感数据（如眼动追踪），容易遭受成员推理攻击和重新识别攻击。传统差分隐私方法对所有特征均匀加噪会降低模型精度、增加推理时间，影响XR实时部署

Method: 利用后验解释技术识别AI XR模型中最具影响力的特征，在推理过程中仅对这些关键特征选择性应用差分隐私保护

Result: 在三个AI XR模型和三个数据集上评估，成员推理攻击成功率降低43%，重新识别攻击成功率降低39%，模型准确率保持97%，推理时间比传统DP方法提升约2倍

Conclusion: 提出的XAI引导的差分隐私框架有效保护AI XR用户隐私，平衡隐私保护与模型性能，已部署在HTC VIVE Pro头显上，开发了PrivateXR用户界面供用户实时调整隐私级别

Abstract: The convergence of artificial AI and XR technologies (AI XR) promises innovative applications across many domains. However, the sensitive nature of data (e.g., eye-tracking) used in these systems raises significant privacy concerns, as adversaries can exploit these data and models to infer and leak personal information through membership inference attacks (MIA) and re-identification (RDA) with a high success rate. Researchers have proposed various techniques to mitigate such privacy attacks, including differential privacy (DP). However, AI XR datasets often contain numerous features, and applying DP uniformly can introduce unnecessary noise to less relevant features, degrade model accuracy, and increase inference time, limiting real-time XR deployment. Motivated by this, we propose a novel framework combining explainable AI (XAI) and DP-enabled privacy-preserving mechanisms to defend against privacy attacks. Specifically, we leverage post-hoc explanations to identify the most influential features in AI XR models and selectively apply DP to those features during inference. We evaluate our XAI-guided DP approach on three state-of-the-art AI XR models and three datasets: cybersickness, emotion, and activity classification. Our results show that the proposed method reduces MIA and RDA success rates by up to 43% and 39%, respectively, for cybersickness tasks while preserving model utility with up to 97% accuracy using Transformer models. Furthermore, it improves inference time by up to ~2x compared to traditional DP approaches. To demonstrate practicality, we deploy the XAI-guided DP AI XR models on an HTC VIVE Pro headset and develop a user interface (UI), namely PrivateXR, allowing users to adjust privacy levels (e.g., low, medium, high) while receiving real-time task predictions, protecting user privacy during XR gameplay.

</details>


### [34] [How Good is Post-Hoc Watermarking With Language Model Rephrasing?](https://arxiv.org/abs/2512.16904)
*Pierre Fernandez,Tom Sander,Hady Elsahar,Hongyan Chang,Tomáš Souček,Valeriu Lacatusu,Tuan Tran,Sylvestre-Alvise Rebuffi,Alexandre Mourachko*

Main category: cs.CR

TL;DR: 本文探讨后处理水印技术，通过LLM重写现有文本嵌入统计信号，用于版权保护和训练数据检测，研究计算资源分配对质量-可检测性权衡的影响。


<details>
  <summary>Details</summary>
Motivation: 传统生成时水印受限于LLM服务方式，后处理水印为生成和检测提供更多自由度，旨在保护受版权保护的文档，并检测其在训练或RAG中的使用。

Method: 采用后处理水印方法，让LLM重写现有文本并应用生成时水印技术。研究不同计算资源分配策略：使用更大的重写模型、束搜索、多候选生成，以及在检测时进行熵过滤。

Result: 在开放文本（如书籍）上实现了强大的可检测性和语义保真度。Gumbel-max方案在核采样下表现优于近期替代方案，大多数方法受益于束搜索。但在可验证文本（如代码）上，较小模型反而优于较大模型。

Conclusion: 后处理水印具有潜力但也存在局限性，为实际应用和未来研究奠定了基础。在开放文本上表现良好，但在可验证文本上面临挑战，需要进一步研究。

Abstract: Generation-time text watermarking embeds statistical signals into text for traceability of AI-generated content. We explore *post-hoc watermarking* where an LLM rewrites existing text while applying generation-time watermarking, to protect copyrighted documents, or detect their use in training or RAG via watermark radioactivity. Unlike generation-time approaches, which is constrained by how LLMs are served, this setting offers additional degrees of freedom for both generation and detection. We investigate how allocating compute (through larger rephrasing models, beam search, multi-candidate generation, or entropy filtering at detection) affects the quality-detectability trade-off. Our strategies achieve strong detectability and semantic fidelity on open-ended text such as books. Among our findings, the simple Gumbel-max scheme surprisingly outperforms more recent alternatives under nucleus sampling, and most methods benefit significantly from beam search. However, most approaches struggle when watermarking verifiable text such as code, where we counterintuitively find that smaller models outperform larger ones. This study reveals both the potential and limitations of post-hoc watermarking, laying groundwork for practical applications and future research.

</details>
